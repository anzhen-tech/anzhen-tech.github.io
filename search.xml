<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Java并发与多线程</title>
    <url>/2021/11/07/Java%E5%B9%B6%E5%8F%91%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="并发与多线程"><a href="#并发与多线程" class="headerlink" title="并发与多线程"></a>并发与多线程</h1><h2 id="volatile"><a href="#volatile" class="headerlink" title="volatile"></a>volatile</h2><ol>
<li>volatile 有什么用？<ul>
<li>volatile 保证内存可见性和禁止指令重排。volatile 可以提供部分原子性。</li>
<li>volatile 用于多线程环境下的单次操作(单次读或者单次写)。</li>
</ul>
</li>
<li>volatile 变量和 atomic 变量有什么不同？<ul>
<li>volatile 变量，可以确保先行关系，即写操作会发生在后续的读操作之前，但它并不能保证原子性。例如用 volatile 修饰 count 变量，那么 count++ 操作就不是原子性的。</li>
<li>AtomicInteger 类提供的 atomic 方法，可以让这种操作具有原子性。例如 #getAndIncrement() 方法，会原子性的进行增量操作把当前值加一，其它数据类型和引用变量也可以进行相似操作。</li>
</ul>
</li>
<li>Java 中能创建 volatile 数组吗?<ul>
<li>能创建但指向引用,不保证内部元素;</li>
</ul>
</li>
<li>volatile 能使得一个非原子操作变成原子操作吗？<ul>
<li>对任意单个volatile变量的读/写具有原子性，但类似于volatile++这种读写复合操作不具有原子性。</li>
</ul>
</li>
<li>volatile 修饰符的有过什么实践？<ul>
<li>用 volatile 修饰 long 和 double 变量,使其读写支持原子性;</li>
<li>状态标志 Boolean 值;</li>
<li>独立观察,单独监测某个多个线程共享的变量</li>
<li>轻量级锁</li>
</ul>
</li>
<li>volatile 类型变量提供什么保证？<ul>
<li>volatile 变量提供顺序性和可见性保证</li>
<li>避免指令重排</li>
<li>可见性保证</li>
</ul>
</li>
<li>volatile 和 synchronized 的区别？<ul>
<li>volatile 本质是在告诉 JVM 当前变量在寄存器（工作内存）中的值是不确定的，需要从主存中读取。synchronized 则是锁定当前变量，只有当前线程可以访问该变量，其他线程被阻塞住。</li>
<li>volatile 仅能使用在变量级别。synchronized 则可以使用在变量、方法、和类级别的。</li>
<li>volatile 仅能实现变量的修改可见性，不能保证原子性。而synchronized 则可以保证变量的修改可见性和原子性。</li>
<li>volatile 不会造成线程的阻塞。synchronized 可能会造成线程的阻塞。</li>
<li>volatile 标记的变量不会被编译器优化。synchronized标记的变量可以被编译器优化。</li>
</ul>
</li>
<li>什么场景下可以使用 volatile 替换 synchronized ？<ul>
<li>只需要保证共享资源的可见性的时候可以使用 volatile 替代，synchronized 保证可操作的原子性一致性和可见性。</li>
<li>volatile 适用于新值不依赖于旧值的情形。</li>
<li>1 写 N 读。</li>
<li>不与其他变量构成不变性条件时候使用 volatile 。<h2 id="synchronized"><a href="#synchronized" class="headerlink" title="synchronized"></a>synchronized</h2></li>
</ul>
</li>
<li>synchronized 的原理是什么?<ul>
<li>synchronized是 Java 内置的关键字，它提供了一种独占的加锁方式。</li>
<li>synchronized的获取和释放锁由JVM实现，用户不需要显示的释放锁，非常方便。</li>
<li>synchronized 也有一定的局限性。<ul>
<li>当线程尝试获取锁的时候，如果获取不到锁会一直阻塞。</li>
<li>如果获取锁的线程进入休眠或者阻塞，除非当前线程异常，否则其他线程尝试获取锁必须一直等待。</li>
</ul>
</li>
</ul>
</li>
<li>同步方法和同步块，哪个是更好的选择？<ul>
<li>同步块是更好的选择，因为它不会锁住整个对象（当然你也可以让它锁住整个对象）。同步方法会锁住整个对象，哪怕这个类中有多个不相关联的同步块，这通常会导致他们停止执行并需要等待获得这个对象上的锁。</li>
<li>同步块更要符合开放调用的原则，只在需要锁住的代码块锁住相应的对象，这样从侧面来说也可以避免死锁。</li>
</ul>
</li>
<li>当一个线程进入某个对象的一个 synchronized 的实例方法后，其它线程是否可进入此对象的其它方法？<ul>
<li>如果其他方法没有 synchronized 的话，其他线程是可以进入的。</li>
<li>所以要开放一个线程安全的对象时，得保证每个方法都是线程安全的。、</li>
</ul>
</li>
<li>在监视器(Monitor)内部，是如何做线程同步的？<ul>
<li>监视器和锁在 Java 虚拟机中是一块使用的。监视器监视一块同步代码块，确保一次只有一个线程执行同步代码块。每一个监视器都和一个对象引用相关联。线程在获取锁之前不允许执行同步代码。</li>
</ul>
</li>
<li>Java 如何实现“自旋”（spin） <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SpinLock</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> AtomicReference&lt;Thread&gt; sign =<span class="keyword">new</span> AtomicReference&lt;&gt;();</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span> </span>&#123; <span class="comment">// &lt;1&gt;</span></span><br><span class="line">        Thread current = Thread.currentThread();</span><br><span class="line">        <span class="keyword">while</span>(!sign .compareAndSet(<span class="keyword">null</span>, current)) &#123;</span><br><span class="line">            <span class="comment">// &lt;1.1&gt;</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">unlock</span> <span class="params">()</span> </span>&#123; <span class="comment">// &lt;2&gt;</span></span><br><span class="line">        Thread current = Thread.currentThread();</span><br><span class="line">        sign .compareAndSet(current, <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>&lt;1&gt; 处，#lock() 方法，如果获得不到锁，就会“死循环”，直到或得到锁为止。考虑到“死循环”会持续占用 CPU ，可能导致其它线程无法获得到 CPU 执行，可以在 &lt;1.1&gt; 处增加 Thread.yiead() 代码段，出让下 CPU 。</li>
<li>&lt;2&gt; 处，#unlock() 方法，释放锁。</li>
</ul>
</li>
</ol>
<p>#Java Lock 接口<br>    - <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15852735326621.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>什么是 Java Lock 接口？<ul>
<li>java.util.concurrent.locks.Lock 接口，比 synchronized 提供更具拓展性的锁操作。它允许更灵活的结构，可以具有完全不同的性质，并且可以支持多个相关类的条件对象。它的优势有：<ul>
<li>可以使锁更公平。</li>
<li>可以使线程在等待锁的时候响应中断。</li>
<li>可以让线程尝试获取锁，并在无法获取锁的时候立即返回或者等待一段时间。</li>
<li>可以在不同的范围，以不同的顺序获取和释放锁。</li>
</ul>
</li>
</ul>
</li>
<li>什么是可重入锁（ReentrantLock）？<ul>
<li>举例来说明锁的可重入性。代码如下：  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UnReentrant</span></span>&#123;</span><br><span class="line">    Lock lock = <span class="keyword">new</span> Lock();</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">outer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        lock.lock();</span><br><span class="line">        inner();</span><br><span class="line">        lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">inner</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        lock.lock();</span><br><span class="line">        <span class="comment">//do something</span></span><br><span class="line">        lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>outer() 方法中调用了 #inner() 方法，#outer() 方法先锁住了 lock ，这样 #inner() 就不能再获取 lock 。</li>
<li>其实调用 #outer() 方法的线程已经获取了 lock 锁，但是不能在 #inner() 方法中重复利用已经获取的锁资源，这种锁即称之为不可重入。</li>
<li>可重入就意味着：线程可以进入任何一个它已经拥有的锁所同步着的代码块。</li>
<li>synchronized、ReentrantLock 都是可重入的锁，可重入锁相对来说简化了并发编程的开发。</li>
<li>ReenTrantLock 的实现是一种自旋锁，通过循环调用 CAS 操作来实现加锁。它的性能比较好也是因为避免了使线程进入内核态的阻塞状态。想尽办法避免线程进入内核的阻塞状态是我们去分析和理解锁设计的关键钥匙。</li>
</ul>
</li>
<li>synchronized 和 ReentrantLock 异同？<ul>
<li>相同点<ul>
<li>都实现了多线程同步和内存可见性语义。</li>
<li>都是可重入锁。</li>
</ul>
</li>
<li>不同点<ul>
<li>同步实现机制不同<ul>
<li>synchronized 通过 Java 对象头锁标记和 Monitor 对象实现同步。</li>
<li>ReentrantLock 通过CAS、AQS（AbstractQueuedSynchronizer）和 LockSupport（用于阻塞和解除阻塞）实现同步。</li>
</ul>
</li>
<li>可见性实现机制不同<ul>
<li>synchronized 依赖 JVM 内存模型保证包含共享变量的多线程内存可见性。</li>
<li>ReentrantLock 通过 ASQ 的 volatile state 保证包含共享变量的多线程内存可见性。</li>
</ul>
</li>
<li>使用方式不同<ul>
<li>synchronized 可以修饰实例方法（锁住实例对象）、静态方法（锁住类对象）、代码块（显示指定锁对象）。</li>
<li>ReentrantLock 显示调用 tryLock 和 lock 方法，需要在 finally 块中释放锁。</li>
</ul>
</li>
<li>功能丰富程度不同<ul>
<li>synchronized 不可设置等待时间、不可被中断（interrupted）。</li>
<li>ReentrantLock 提供有限时间等候锁（设置过期时间）、可中断锁（lockInterruptibly）、condition（提供 await、condition（提供 await、signal 等方法）等丰富功能</li>
</ul>
</li>
<li>锁类型不同<ul>
<li>synchronized 只支持非公平锁。</li>
<li>ReentrantLock 提供公平锁和非公平锁实现。当然，在大部分情况下，非公平锁是高效的选择。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>ReadWriteLock 是什么？<ul>
<li>ReadWriteLock ，读写锁是，用来提升并发程序性能的锁分离技术的 Lock 实现类。可以用于 “多读少写” 的场景，读写锁支持多个读操作并发执行，写操作只能由一个线程来操作。</li>
<li>ReadWriteLock 对向数据结构相对不频繁地写入，但是有多个任务要经常读取这个数据结构的这类情况进行了优化。ReadWriteLock 使得你可以同时有多个读取者，只要它们都不试图写入即可。如果写锁已经被其他任务持有，那么任何读取者都不能访问，直至这个写锁被释放为止。</li>
<li>ReadWriteLock 对程序性能的提高主要受制于如下几个因素：<ul>
<li>数据被读取的频率与被修改的频率相比较的结果。</li>
<li>读取和写入的时间</li>
<li>有多少线程竞争</li>
<li>是否在多处理机器上运行</li>
</ul>
</li>
</ul>
</li>
<li>Condition 是什么？<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15852741845639.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="-w762"><h2 id="Java-内存模型"><a href="#Java-内存模型" class="headerlink" title="Java 内存模型"></a>Java 内存模型</h2></li>
</ul>
</li>
<li>两个线程之间是如何通信的呢？<ul>
<li>线程之间的通信方式，目前有共享内存和消息传递两种。<ul>
<li>在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。典型的共享内存通信方式，就是通过共享对象进行通信。<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15852745337896.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
<li>在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显式进行通信。在 Java 中典型的消息传递方式，就是 #wait() 和 #notify() ，或者 BlockingQueue 。<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15852745812427.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>为什么代码会重排序？<ul>
<li>在执行程序时，为了提供性能，处理器和编译器常常会对指令进行重排序，但是不能随意重排序，不是你想怎么排序就怎么排序，它需要满足以下两个条件：<ul>
<li>在单线程环境下不能改变程序运行的结果。</li>
<li>存在数据依赖关系的不允许重排序</li>
</ul>
<blockquote>
<p>重排序不会影响单线程环境的执行结果，但是会破坏多线程的执行语义。</p>
</blockquote>
</li>
</ul>
</li>
</ol>
<h2 id="Java-并发容器"><a href="#Java-并发容器" class="headerlink" title="Java 并发容器"></a>Java 并发容器</h2><ol>
<li>Java 中 ConcurrentHashMap 的并发度是什么？<ul>
<li>锁单个Node节点</li>
</ul>
</li>
<li>ConcurrentHashMap 为何读不用加锁？<ul>
<li>Node 的 val 和 next 均为 volatile 型。</li>
<li>tabAt(..,) 和 casTabAt(…) 对应的 Unsafe 操作实现了 volatile 语义。</li>
</ul>
</li>
<li>CopyOnWriteArrayList 可以用于什么应用场景？<ul>
<li>CopyOnWriteArrayList(免锁容器)的好处之一是当多个迭代器同时遍历和修改这个列表时，不会抛出ConcurrentModificationException 异常。在 CopyOnWriteArrayList 中，写入将导致创建整个底层数组的副本，而源数组将保留在原地，使得复制的数组在被修改时，读取操作可以安全地执行。<ul>
<li>由于写操作的时候，需要拷贝数组，会消耗内存，如果原数组的内容比较多的情况下，可能导致 ygc 或者 fgc 。</li>
<li>不能用于实时读的场景，像拷贝数组、新增元素都需要时间，所以调用一个 set 操作后，读取到数据可能还是旧的,虽然 CopyOnWriteArrayList 能做到最终一致性,但是还是没法满足实时性要求。</li>
</ul>
</li>
<li>CopyOnWriteArrayList 透露的思想：</li>
<li>读写分离，读和写分开</li>
<li>最终一致性</li>
<li>使用另外开辟空间的思路，来解决并发冲突<h2 id="Java-阻塞队列"><a href="#Java-阻塞队列" class="headerlink" title="Java 阻塞队列"></a>Java 阻塞队列</h2></li>
</ul>
</li>
<li>什么是阻塞队列？有什么适用场景？<ul>
<li>阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作是：<ul>
<li>在队列为空时，获取元素的线程会等待队列变为非空。</li>
<li>当队列满时，存储元素的线程会等待队列可用。</li>
</ul>
</li>
<li>阻塞队列常用于生产者和消费者的场景：<ul>
<li>生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程</li>
<li>阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。</li>
</ul>
</li>
<li>BlockingQueue 接口，是 Queue 的子接口，它的主要用途并不是作为容器，而是作为线程同步的的工具，因此他具有一个很明显的特性：<ul>
<li>当生产者线程试图向 BlockingQueue 放入元素时，如果队列已满，则线程被阻塞。</li>
<li>当消费者线程试图从中取出一个元素时，如果队列为空，则该线程会被阻塞。</li>
<li>正是因为它所具有这个特性，所以在程序中多个线程交替向BlockingQueue中 放入元素，取出元素，它可以很好的控制线程之间的通信。</li>
<li>阻塞队列使用最经典的场景，就是 Socket 客户端数据的读取和解析：<ul>
<li>读取数据的线程不断将数据放入队列。</li>
<li>然后，解析线程不断从队列取数据解析。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Java 提供了哪些阻塞队列的实现？<ul>
<li>ArrayBlockingQueue ：一个由数组结构组成的有界阻塞队列。</li>
<li>LinkedBlockingQueue ：一个由链表结构组成的有界阻塞队列。</li>
<li>PriorityBlockingQueue ：一个支持优先级排序的无界阻塞队列。</li>
<li>DelayQueue：支持延时获取元素的无界阻塞队列，即可以指定多久才能从队列中获取当前元素。</li>
<li>SynchronousQueue：一个不存储元素的阻塞队列。</li>
</ul>
</li>
<li>简述 ConcurrentLinkedQueue 和 LinkedBlockingQueue 的用处和不同之处？<ul>
<li>阻塞队列，典型例子是 LinkedBlockingQueue 。使用阻塞队列的好处：多线程操作共同的队列时不需要额外的同步，另外就是队列会自动平衡负载，即那边（生产与消费两边）处理快了就会被阻塞掉，从而减少两边的处理速度差距。</li>
<li>非阻塞队列，典型例子是 ConcurrentLinkedQueue 。当许多线程共享访问一个公共集合时，ConcurrentLinkedQueue 是一个恰当的选择。</li>
<li>具体的选择，如下：<ul>
<li>LinkedBlockingQueue 多用于任务队列。<ul>
<li>单生产者，单消费者</li>
<li>多生产者，单消费者</li>
</ul>
</li>
<li>ConcurrentLinkedQueue 多用于消息队列。<ul>
<li>单生产者，多消费者</li>
<li>多生产者，多消费者</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Java-原子操作类"><a href="#Java-原子操作类" class="headerlink" title="Java 原子操作类"></a>Java 原子操作类</h2><ol>
<li>什么是原子操作？<ul>
<li>原子操作（Atomic Operation），意为”不可被中断的一个或一系列操作”。</li>
<li>处理器使用基于对缓存加锁或总线加锁的方式，来实现多处理器之间的原子操作。</li>
<li>在 Java 中，可以通过锁和循环 CAS 的方式来实现原子操作。CAS操作 —— Compare &amp; Set ，或是 Compare &amp; Swap ，现在几乎所有的 CPU 指令都支持 CAS 的原子操作。</li>
</ul>
</li>
<li>CAS 操作有什么缺点？<ul>
<li>ABA 问题<ul>
<li>比如说一个线程 one 从内存位置 V 中取出 A ，这时候另一个线程 two 也从内存中取出 A ，并且 two 进行了一些操作变成了 B ，然后 two 又将 V 位置的数据变成 A ，这时候线程 one 进行 CAS 操作发现内存中仍然是 A ，然后 one 操作成功。尽管线程 one 的 CAS 操作成功，但可能存在潜藏的问题。</li>
</ul>
</li>
<li>循环时间长开销大<ul>
<li>对于资源竞争严重（线程冲突严重）的情况，CAS 自旋的概率会比较大，从而浪费更多的 CPU 资源，效率低于</li>
</ul>
</li>
<li>只能保证一个共享变量的原子操作<ul>
<li>当对一个共享变量执行操作时，我们可以使用循环 CAS 的方式来保证原子操作，但是对多个共享变量操作时，循环 CAS 就无法保证操作的原子性，这个时候就可以用锁。<h2 id="Java-并发工具类"><a href="#Java-并发工具类" class="headerlink" title="Java 并发工具类"></a>Java 并发工具类</h2></li>
</ul>
</li>
</ul>
</li>
<li>Semaphore 是什么？<ul>
<li>Semaphore ，是一种新的同步类，它是一个计数信号。从概念上讲，从概念上讲，信号量维护了一个许可集合。<ul>
<li>如有必要，在许可可用前会阻塞每一个 #acquire() 方法，然后再获取该许可。</li>
<li>每个 #release() 方法，添加一个许可，从而可能释放一个正在阻塞的获取者。</li>
<li>但是，不使用实际的许可对象，Semaphore 只对可用许可的数量进行计数，并采取相应的行动。</li>
</ul>
</li>
</ul>
</li>
<li>说说 CountDownLatch 原理<ul>
<li>CountDownLatch ，字面意思是减小计数（CountDown）的门闩（Latch）。它要做的事情是，等待指定数量的计数被减少，意味着门闩被打开，然后进行执行。</li>
<li>CountDownLatch 默认的构造方法是 CountDownLatch(int count) ，其参数表需要减少的计数，主线程调用 #await() 方法告诉 CountDownLatch 阻塞等待指定数量的计数被减少，然后其它线程调用 CountDownLatch 的 #countDown() 方法，减小计数(不会阻塞)。等待计数被减少到零，主线程结束阻塞等待，继续往下执行。</li>
</ul>
</li>
<li>说说 CyclicBarrier 原理<ul>
<li>CyclicBarrier ，字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。</li>
<li>CyclicBarrier 默认的构造方法是 CyclicBarrier(int parties) ，其参数表示屏障拦截的线程数量，每个线程调用 #await() 方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞，直到 parties 个线程到达，结束阻塞。</li>
</ul>
</li>
<li>CyclicBarrier 和 CountdownLatch 有什么区别？<ul>
<li>CyclicBarrier 可以重复使用，而 CountdownLatch 不能重复使用。</li>
<li>CountDownLatch 其实可以把它看作一个计数器，只不过这个计数器的操作是原子操作。</li>
<li>CyclicBarrier 一个同步辅助类，它允许一组线程互相等待，直到到达某个公共屏障点 (common barrier point)</li>
</ul>
<table>
<thead>
<tr>
<th>CountDownLatch</th>
<th>CyclicBarrier</th>
</tr>
</thead>
<tbody><tr>
<td>减计数方式</td>
<td>加计数方式</td>
</tr>
<tr>
<td>计算为 0 时释放所有等待的线程</td>
<td>计数达到指定值时释放所有等待线程</td>
</tr>
<tr>
<td>计数为 0 时，无法重置</td>
<td>计数达到指定值时，计数置为 0 重新开始</td>
</tr>
<tr>
<td>调用 #countDown() 方法计数减一，调用 #await() 方法只进行阻塞，对计数没任何影响</td>
<td>调用 #await() 方法计数加 1 ，若加 1 后的值不等于构造方法的值，则线程阻塞</td>
</tr>
<tr>
<td>不可重复利用</td>
<td>可重复利用</td>
</tr>
</tbody></table>
</li>
</ol>
<h2 id="Java-线程池"><a href="#Java-线程池" class="headerlink" title="Java 线程池"></a>Java 线程池</h2><ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15852766289037.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
<ol>
<li>什么是 Executor 框架？<ul>
<li>Executor 框架，是一个根据一组执行策略调用，调度，执行和控制的异步任务的框架。</li>
<li>无限制的创建线程，会引起应用程序内存溢出。所以创建一个线程池是个更好的的解决方案，因为可以限制线程的数量并且可以回收再利用这些线程。利用 Executor 框架，可以非常方便的创建一个线程池。</li>
</ul>
</li>
<li>为什么使用 Executor 框架？<ul>
<li>每次执行任务创建线程 new Thread() 比较消耗性能，创建一个线程是比较耗时、耗资源的。</li>
<li>调用 new Thread() 创建的线程缺乏管理，被称为野线程，而且可以无限制的创建，线程之间的相互竞争会导致过多占用系统资源而导致系统瘫痪，还有线程之间的频繁交替也会消耗很多系统资源。</li>
<li>接使用 new Thread() 启动的线程不利于扩展，比如定时执行、定期执行、定时定期执行、线程中断等都不便实现。</li>
</ul>
</li>
<li>在 Java 中 Executor 和 Executors 的区别？<ul>
<li>Executors 是 Executor 的工具类，不同方法按照我们的需求创建了不同的线程池，来满足业务的需求。</li>
<li>Executor 是接口对象，能执行我们的线程任务。<ul>
<li>ExecutorService 接口，继承了 Executor 接口，并进行了扩展，提供了更多的方法我们能获得任务执行的状态并且可以获取任务的返回值。</li>
<li>使用 ThreadPoolExecutor ，可以创建自定义线程池。</li>
<li>Future 表示异步计算的结果，他提供了检查计算是否完成的方法，以等待计算的完成，并可以使用 #get() 方法，获取计算的结果。</li>
</ul>
</li>
</ul>
</li>
<li>创建线程池的几种方式？<ul>
<li>Executors 创建的线程池，分成普通任务线程池，和定时任务线程池。<ul>
<li>普通任务线程池<ul>
<li>1、#newFixedThreadPool(int nThreads) 方法，创建一个固定长度的线程池。每当提交一个任务就创建一个线程，直到达到线程池的最大数量，这时线程规模将不再变化。当线程发生未预期的错误而结束时，线程池会补充一个新的线程。</li>
<li>2、#newCachedThreadPool() 方法，创建一个可缓存的线程池。如果线程池的规模超过了处理需求，将自动回收空闲线程。当需求增加时，则可以自动添加新线程。线程池的规模不存在任何限制。</li>
<li>3、#newSingleThreadExecutor() 方法，创建一个单线程的线程池。它创建单个工作线程来执行任务，如果这个线程异常结束，会创建一个新的来替代它。它的特点是，能确保依照任务在队列中的顺序来串行执行。</li>
</ul>
</li>
<li>定时任务线程池<ul>
<li>4、#newScheduledThreadPool(int corePoolSize) 方法，创建了一个固定长度的线程池，而且以延迟或定时的方式来执行任务，类似 Timer 。</li>
<li>5、#newSingleThreadExecutor() 方法，创建了一个固定长度为 1 的线程池，而且以延迟或定时的方式来执行任务，类似 Timer 。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>如何使用 ThreadPoolExecutor 创建线程池？ <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ThreadPoolExecutor</span><span class="params">(<span class="keyword">int</span> corePoolSize,</span></span></span><br><span class="line"><span class="params"><span class="function">                      <span class="keyword">int</span> maximumPoolSize,</span></span></span><br><span class="line"><span class="params"><span class="function">                      <span class="keyword">long</span> keepAliveTime,</span></span></span><br><span class="line"><span class="params"><span class="function">                      TimeUnit unit,</span></span></span><br><span class="line"><span class="params"><span class="function">                      BlockingQueue&lt;Runnable&gt; workQueue,</span></span></span><br><span class="line"><span class="params"><span class="function">                      ThreadFactory threadFactory,</span></span></span><br><span class="line"><span class="params"><span class="function">                      RejectedExecutionHandler handler)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (corePoolSize &lt; <span class="number">0</span> ||</span><br><span class="line">        maximumPoolSize &lt;= <span class="number">0</span> ||</span><br><span class="line">        maximumPoolSize &lt; corePoolSize ||</span><br><span class="line">        keepAliveTime &lt; <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">    <span class="keyword">if</span> (workQueue == <span class="keyword">null</span> || threadFactory == <span class="keyword">null</span> || handler == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">    <span class="keyword">this</span>.corePoolSize = corePoolSize;</span><br><span class="line">    <span class="keyword">this</span>.maximumPoolSize = maximumPoolSize;</span><br><span class="line">    <span class="keyword">this</span>.workQueue = workQueue;</span><br><span class="line">    <span class="keyword">this</span>.keepAliveTime = unit.toNanos(keepAliveTime);</span><br><span class="line">    <span class="keyword">this</span>.threadFactory = threadFactory;</span><br><span class="line">    <span class="keyword">this</span>.handler = handler;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>corePoolSize 参数，核心线程数大小，当线程数 &lt; corePoolSize ，会创建线程执行任务。</li>
<li>maximumPoolSize 参数，最大线程数， 当线程数 &gt;= corePoolSize 的时候，会把任务放入 workQueue 队列中。</li>
<li>keepAliveTime 参数，保持存活时间，当线程数大于 corePoolSize 的空闲线程能保持的最大时间。</li>
<li>unit 参数，时间单位。</li>
<li>workQueue 参数，保存任务的阻塞队列。</li>
<li>handler 参数，超过阻塞队列的大小时，使用的拒绝策略。</li>
<li>threadFactory 参数，创建线程的工厂。</li>
</ul>
</li>
<li>ThreadPoolExecutor 有哪些拒绝策略？<ul>
<li>ThreadPoolExecutor 默认有四个拒绝策略：<ul>
<li>ThreadPoolExecutor.AbortPolicy() ，直接抛出异常 RejectedExecutionException 。</li>
<li>ThreadPoolExecutor.CallerRunsPolicy() ，直接调用 run 方法并且阻塞执行。</li>
<li>ThreadPoolExecutor.DiscardPolicy() ，直接丢弃后来的任务。</li>
<li>ThreadPoolExecutor.DiscardOldestPolicy() ，丢弃在队列中队首的任务。</li>
<li>如果有需要，可以自己实现 RejectedExecutionHandler 接口，实现自定义的拒绝逻辑</li>
</ul>
</li>
</ul>
</li>
<li>线程池的关闭方式有几种？<ul>
<li>ThreadPoolExecutor 提供了两个方法，用于线程池的关闭，分别是：<ul>
<li>shutdown() 方法，不会立即终止线程池，而是要等所有任务缓存队列中的任务都执行完后才终止，但再也不会接受新的任务。</li>
<li>shutdownNow() 方法，立即终止线程池，并尝试打断正在执行的任务，并且清空任务缓存队列，返回尚未执行的任务。</li>
</ul>
</li>
</ul>
</li>
<li>Java 线程池大小为何会大多被设置成 CPU 核心数 +1 ？<ul>
<li>如果是 CPU 密集型应用，则线程池大小设置为 N+1<ul>
<li>因为 CPU 密集型任务使得 CPU 使用率很高，若开过多的线程数，只能增加上下文切换的次数，因此会带来额外的开销。</li>
</ul>
</li>
<li>如果是 IO 密集型应用，则线程池大小设置为 2N+1<ul>
<li>IO密 集型任务 CPU 使用率并不高，因此可以让 CPU 在等待 IO 的时候去处理别的任务，充分利用 CPU 时间。</li>
</ul>
</li>
<li>如果是混合型应用，那么分别创建线程池<ul>
<li>可以将任务分成 IO 密集型和 CPU 密集型任务，然后分别用不同的线程池去处理。 只要分完之后两个任务的执行时间相差不大，那么就会比串行执行来的高效。</li>
<li>因为如果划分之后两个任务执行时间相差甚远，那么先执行完的任务就要等后执行完的任务，最终的时间仍然取决于后执行完的任务，而且还要加上任务拆分与合并的开销，得不偿失。</li>
</ul>
</li>
</ul>
</li>
<li>线程池容量的动态调整？<ul>
<li>ThreadPoolExecutor 提供了动态调整线程池容量大小的方法：<ul>
<li>setCorePoolSize：设置核心池大小。</li>
<li>setMaximumPoolSize：设置线程池最大能创建的线程数目大小。</li>
</ul>
</li>
<li>当上述参数从小变大时，ThreadPoolExecutor 进行线程赋值，还可能立即创建新的线程来执行任务。</li>
</ul>
</li>
<li>什么是 Callable、Future、FutureTask ？ <ul>
<li>Callable: 可以认为是带有回调的 Runnable 。</li>
<li>Future: 表示异步任务，是还没有完成的任务给出的未来结果。所以说 Callable 用于产生结果，Future 用于获取结果。</li>
<li>FutureTask; 表示一个可以取消的异步运算。<ul>
<li>它有启动和取消运算、查询运算是否完成和取回运算结果等方法。只有当运算完成的时候结果才能取回，如果运算尚未完成 get 方法将会阻塞。</li>
<li>一个 FutureTask 对象，可以对调用了 Callable 和 Runnable 的对象进行包装，由于 FutureTask 也是继承了 Runnable 接口，所以它可以提交给 Executor 来执行。</li>
</ul>
</li>
</ul>
</li>
<li>线程池执行任务的过程？<ul>
<li>刚创建时，里面没有线程调用 execute() 方法，添加任务时：<ul>
<li>如果正在运行的线程数量小于核心参数 corePoolSize ，继续创建线程运行这个任务<ul>
<li>否则，如果正在运行的线程数量大于或等于 corePoolSize ，将任务加入到阻塞队列中。<ul>
<li>否则，如果队列已满，同时正在运行的线程数量小于核心参数 maximumPoolSize ，继续创建线程运行这个任务。<ul>
<li>否则，如果队列已满，同时正在运行的线程数量大于或等于 maximumPoolSize ，根据设置的拒绝策略处理。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>完成一个任务，继续取下一个任务处理。<ul>
<li>没有任务继续处理，线程被中断或者线程池被关闭时，线程退出执行，如果线程池被关闭，线程结束。</li>
<li>否则，判断线程池正在运行的线程数量是否大于核心线程数，如果是，线程结束，否则线程阻塞。因此线程池任务全部执行完成后，继续留存的线程池大小为 corePoolSize 。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>线程池中 submit 和 execute 方法有什么区别？<ul>
<li>两个方法都可以向线程池提交任务。<ul>
<li>execute(…) 方法，返回类型是 void ，它定义在 Executor 接口中。</li>
<li>submit(…) 方法，可以返回持有计算结果的 Future 对象，它定义在 ExecutorService 接口中，它扩展了 Executor 接口，其它线程池类像 ThreadPoolExecutor 和 ScheduledThreadPoolExecutor 都有这些方法。</li>
</ul>
</li>
</ul>
</li>
<li>如果你提交任务时，线程池队列已满，这时会发生什么？<ul>
<li>重点在于线程池的队列是有界还是无界的。</li>
</ul>
</li>
<li>Fork/Join 框架是什么？<ul>
<li>Fork/Join 框架是一个实现了 ExecutorService接口 的多线程处理器。它可以把一个大的任务划分为若干个小的任务并发执行，充分利用可用的资源，进而提高应用的执行效率。</li>
<li>Fork 就是把一个大任务切分为若干子任务并行的执行</li>
<li>Join 就是合并这些子任务的执行结果，最后得到这个大任务的结果。</li>
<li>比如计算 1+2+…＋10000 ，可以分割成 10 个子任务，每个子任务分别对 1000 个数进行求和，最终汇总这 10 个子任务的结果。</li>
</ul>
</li>
<li>如何让一段程序并发的执行，并最终汇总结果？<ul>
<li>1、CountDownLatch：允许一个或者多个线程等待前面的一个或多个线程完成，构造一个 CountDownLatch 时指定需要 countDown 的点的数量，每完成一点就 countDown 一下。当所有点都完成，CountDownLatch 的 #await() 就解除阻塞。</li>
<li>2、CyclicBarrier：可循环使用的 Barrier ，它的作用是让一组线程到达一个 Barrier 后阻塞，直到所有线程都到达 Barrier 后才能继续执行。</li>
<li>3、Fork/Join 框架，fork 把大任务分解成多个小任务，然后汇总多个小任务的结果得到最终结果。使用一个双端队列，当线程空闲时从双端队列的另一端领取任务。</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>RocketMQ</title>
    <url>/2021/11/07/RocketMQ/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="RocketMQ"><a href="#RocketMQ" class="headerlink" title="RocketMQ"></a>RocketMQ</h1><ol>
<li>RocketMQ 由哪些角色组成？<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850381816926.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>生产者（Producer）：负责产生消息，生产者向消息服务器发送由业务应用程序系统生成的消息。</li>
<li>消费者（Consumer）：负责消费消息，消费者从消息服务器拉取信息并将其输入用户应用程序。</li>
<li>消息服务器（Broker）：是消息存储中心，主要作用是接收来自 Producer 的消息并存储， Consumer 从这里取得消息。</li>
<li>名称服务器（NameServer）：用来保存 Broker 相关 Topic 等元信息并给 Producer ，提供 Consumer 查找 Broker 信息。</li>
</ul>
</li>
<li>请描述下 RocketMQ 的整体流程？<pre><code> - ![](https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850382480500.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10)
</code></pre>
<ul>
<li>启动 Namesrv，Namesrv起 来后监听端口，等待 Broker、Producer、Consumer 连上来，相当于一个路由控制中心。</li>
<li>Broker 启动，<strong>跟所有的 Namesrv 保持长连接</strong>，定时发送心跳包。心跳包中，包含当前 Broker 信息(IP+端口等)以及存储所有 Topic 信息。注册成功后，Namesrv 集群中就有 Topic 跟 Broker 的映射关系。</li>
<li>收发消息前，先创建 Topic 。创建 Topic 时，需要指定该 Topic 要存储在 哪些 Broker上。也可以在发送消息时自动创建Topic。</li>
<li>Producer 发送消息。启动时，先跟 Namesrv 集群中的其中一台建立长连接，并从Namesrv 中获取当前发送的 Topic 存在哪些 Broker 上，然后跟对应的 Broker 建立长连接，直接向 Broker 发消息</li>
<li>Consumer 消费消息。Consumer 跟 Producer 类似。跟其中一台 Namesrv 建立长连接，获取当前订阅 Topic 存在哪些 Broker 上，然后直接跟 Broker 建立连接通道，开始消费消息。</li>
</ul>
</li>
<li>请说说你对 Namesrv 的了解？<ul>
<li>Namesrv 用于存储 Topic、Broker 关系信息，功能简单，稳定性高。<ul>
<li>多个 Namesrv 之间相互没有通信，单台 Namesrv 宕机不影响其它 Namesrv 与集群。多个 Namesrv 之间的信息共享，通过 Broker 主动向多个 Namesrv 都发起心跳。正如上文所说，Broker 需要跟所有 Namesrv 连接。</li>
<li>即使整个 Namesrv 集群宕机，已经正常工作的 Producer、Consumer、Broker 仍然能正常工作，但新起的 Producer、Consumer、Broker 就无法工作。(这点和 Dubbo 有些不同，不会缓存 Topic 等元信息到本地文件。)</li>
</ul>
</li>
<li> Namesrv 压力不会太大，平时主要开销是在维持心跳和提供 Topic-Broker 的关系数据。但有一点需要注意，Broker 向 Namesr 发心跳时，会带上当前自己所负责的所有 Topic 信息，如果 Topic 个数太多（万级别），会导致一次心跳中，就 Topic 的数据就几十 M，网络情况差的话，网络传输失败，心跳失败，导致 Namesrv 误认为 Broker 心跳失败。</li>
</ul>
</li>
<li>如何配置 Namesrv 地址到生产者和消费者？<ul>
<li><strong>编程方式</strong>，就像 producer.setNamesrvAddr(“ip:port”) 。</li>
<li>Java 启动参数设置，使用 rocketmq.namesrv.addr 。</li>
<li>环境变量，使用 NAMESRV_ADDR 。</li>
<li>HTTP 端点，例如说：<a href="http://namesrv.rocketmq.xxx.com/">http://namesrv.rocketmq.xxx.com</a> 地址，通过 DNS 解析获得 Namesrv 真正的地址。</li>
</ul>
</li>
<li>请说说你对 Broker 的了解？<ul>
<li>高并发读写服务。Broker的高并发读写主要是依靠以下两点:<ul>
<li>消息顺序写，所有 Topic 数据同时只会写一个文件，一个文件满1G ，再写新文件，真正的顺序写盘，使得发消息 TPS 大幅提高。</li>
<li>消息随机读，RocketMQ 尽可能让读命中系统 Pagecache ，因为操作系统访问 Pagecache 时，即使只访问 1K 的消息，系统也会提前预读出更多的数据，在下次读时就可能命中 Pagecache ，减少 IO 操作。</li>
</ul>
</li>
<li>负载均衡与动态伸缩。<ul>
<li>负载均衡：Broker 上存 Topic 信息，Topic 由多个队列组成，队列会平均分散在多个 Broker 上，而 Producer 的发送机制保证消息尽量平均分布到所有队列中，最终效果就是所有消息都平均落在每个 Broker 上。</li>
<li>动态伸缩能力（非顺序消息）：Broker 的伸缩性体现在两个维度：Topic、Broker。<ul>
<li>Topic 维度：假如一个 Topic 的消息量特别大，但集群水位压力还是很低，就可以扩大该 Topic 的队列数， Topic 的队列数跟发送、消费速度成正比。<ul>
<li>Topic 的队列数一旦扩大，就无法很方便的缩小。因为，生产者和消费者都是基于相同的队列数来处理。如果真的想要缩小，只能新建一个 Topic ，然后使用它。</li>
</ul>
</li>
<li>Broker 维度：如果集群水位很高了，需要扩容，直接加机器部署 Broker 就可以。Broker 启动后向 Namesrv 注册，Producer、Consumer 通过 Namesrv 发现新Broker，立即跟该 Broker 直连，收发消息。<ul>
<li>新增的 Broker 想要下线，想要下线也比较麻烦，暂时没特别好的方案。大体的前提是，消费者消费完该 Broker 的消息，生产者不往这个 Broker 发送消息。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>高可用 &amp; 高可靠。<ul>
<li>高可用：集群部署时一般都为主备，备机实时从主机同步消息，如果其中一个主机宕机，备机提供消费服务，但不提供写服务。</li>
<li>高可靠：所有发往 Broker 的消息，有同步刷盘和异步刷盘机制。<ul>
<li>同步刷盘时，消息写入物理文件才会返回成功。</li>
<li>异步刷盘时，只有机器宕机，才会产生消息丢失，Broker 挂掉可能会发生，但是机器宕机崩溃是很少发生的，除非突然断电。如果 Broker 挂掉，未同步到硬盘的消息，还在 Pagecache 中呆着。</li>
</ul>
</li>
</ul>
</li>
<li>Broker 与 Namesrv 的心跳机制。<ul>
<li>单个 Broker 跟所有 Namesrv 保持心跳请求，心跳间隔为30秒，心跳请求中包括当前 Broker 所有的 Topic 信息。</li>
<li>Namesrv 会反查 Broker 的心跳信息，如果某个 Broker 在 2 分钟之内都没有心跳，则认为该 Broker 下线，调整 Topic 跟 Broker 的对应关系。但此时 Namesrv 不会主动通知Producer、Consumer 有 Broker 宕机。也就说，只能等 Producer、Consumer 下次定时拉取 Topic 信息的时候，才会发现有 Broker 宕机。</li>
</ul>
</li>
</ul>
</li>
<li>Broker 如何实现消息的存储？<ul>
<li>?????????????????????????</li>
</ul>
</li>
<li>请说说你对 Producer 的了解？<ul>
<li>获得 Topic-Broker 的映射关系。<ul>
<li>Producer 启动时，也需要指定 Namesrv 的地址，从 Namesrv 集群中选一台建立长连接。如果该 Namesrv 宕机，会自动连其他 Namesrv ，直到有可用的 Namesrv 为止。</li>
<li>生产者每 30 秒从 Namesrv 获取 Topic 跟 Broker 的映射关系，更新到本地内存中。然后再跟 Topic 涉及的所有 Broker 建立长连接，每隔 30 秒发一次心跳。</li>
<li>在 Broker 端也会每 10 秒扫描一次当前注册的 Producer ，如果发现某个 Producer 超过 2 分钟都没有发心跳，则断开连接。</li>
</ul>
</li>
<li>生产者端的负载均衡。<ul>
<li>生产者发送时，会自动轮询当前所有可发送的broker，一条消息发送成功，下次换另外一个broker发送，以达到消息平均落到所有的broker上。</li>
<li>假如某个 Broker 宕机，意味生产者最长需要 30 秒才能感知到。在这期间会向宕机的 Broker 发送消息。当一条消息发送到某个 Broker 失败后，会自动再重发 2 次，假如还是发送失败，则抛出发送失败异常。客户端里会自动轮询另外一个 Broker 重新发送，这个对于用户是透明的。</li>
</ul>
</li>
</ul>
</li>
<li>Producer 发送消息有几种方式？<ul>
<li>Producer 发送消息，有三种方式：<ul>
<li>同步方式</li>
<li>异步方式</li>
<li>Oneway 方式</li>
</ul>
</li>
</ul>
</li>
<li>请说说你对 Consumer 的了解？<ul>
<li>获得 Topic-Broker 的映射关系。<ul>
<li>Consumer 启动时需要指定 Namesrv 地址，与其中一个 Namesrv 建立长连接。消费者每隔 30 秒从 Namesrv 获取所有Topic 的最新队列情况，这意味着某个 Broker 如果宕机，客户端最多要 30 秒才能感知。连接建立后，从 Namesrv 中获取当前消费 Topic 所涉及的 Broker，直连 Broker 。</li>
<li>Consumer 跟 Broker 是长连接，会每隔 30 秒发心跳信息到Broker 。Broker 端每 10 秒检查一次当前存活的 Consumer ，若发现某个 Consumer 2 分钟内没有心跳，就断开与该 Consumer 的连接，并且向该消费组的其他实例发送通知，触发该消费者集群的负载均衡。</li>
</ul>
</li>
<li>消费者端的负载均衡。根据消费者的消费模式不同，负载均衡方式也不同。<ul>
<li>集群消费：一个 Topic 可以由同一个消费这分组( Consumer Group )下所有消费者分担消费。<ul>
<li>具体例子：假如 TopicA 有 6 个队列，，每个消费者分组起了 2 个消费者实例，那么每个消费者负责消费 3 个队列。如果再增加一个消费者分组相同消费者实例，即当前共有 3 个消费者同时消费 6 个队列，那每个消费者负责 2 个队列的消费。</li>
</ul>
</li>
<li>广播消费：每个消费者消费 Topic 下的所有队列。</li>
</ul>
</li>
</ul>
</li>
<li>消费者消费模式有几种？<ul>
<li>集群消费：一个 Consumer Group 中的各个 Consumer 实例分摊去消费消息，即一条消息只会投递到一个 Consumer Group 下面的一个实例。<ul>
<li>实际上，每个 Consumer 是平均分摊 Message Queue 的去做拉取消费。例如某个 Topic 有 3 个队列，其中一个 Consumer Group 有 3 个实例（可能是 3 个进程，或者 3 台机器），那么每个实例只消费其中的 1 个队列。</li>
<li>而由 Producer 发送消息的时候是轮询所有的队列，所以消息会平均散落在不同的队列上，可以认为队列上的消息是平均的。那么实例也就平均地消费消息了。</li>
<li>这种模式下，消费进度的存储会持久化到 Broker 。</li>
<li>当新建一个 Consumer Group 时，默认情况下，该分组的消费者会从 min offset 开始重新消费消息。</li>
</ul>
</li>
<li>广播消费：消息将对一 个Consumer Group 下的各个 Consumer 实例都投递一遍。即即使这些 Consumer 属于同一个Consumer Group ，消息也会被 Consumer Group 中的每个 Consumer 都消费一次。<ul>
<li>实际上，是一个消费组下的每个消费者实例都获取到了 Topic 下面的每个 Message Queue 去拉取消费。所以消息会投递到每个消费者实例。</li>
<li>这种模式下，消费进度会存储持久化到实例本地。</li>
</ul>
</li>
</ul>
</li>
<li>消费者获取消息有几种模式？<ul>
<li>PushConsumer推送模式（虽然 RocketMQ 使用的是长轮询）的消费者。消息的能及时被消费。使用非常简单，内部已处理如线程池消费、流控、负载均衡、异常处理等等的各种场景。</li>
<li>PullConsumer拉取模式的消费者。应用主动控制拉取的时机，怎么拉取，怎么消费等。主动权更高。但要自己处理各种场景。</li>
<li>决绝绝大多数场景下，我们只会使用 PushConsumer 推送模式。</li>
</ul>
</li>
<li>如何对消息进行重放？<ul>
<li>消费位点就是一个数字，把 Consumer Offset 改一下，就可以达到重放的目的了。</li>
</ul>
</li>
<li>什么是顺序消息？如何实现？<ul>
<li>消费消息的顺序要同发送消息的顺序一致。由于 Consumer 消费消息的时候是针对 Message Queue 顺序拉取并开始消费，且一条 Message Queue 只会给一个消费者（集群模式下），所以能够保证同一个消费者实例对于 Queue 上消息的消费是顺序地开始消费（不一定顺序消费完成，因为消费可能并行）。</li>
<li>RocketMQ 提供了两种顺序级别：<ul>
<li>顺序消息包括两块：Producer 的顺序发送，和 Consumer 的顺序消费。</li>
<li>普通顺序消息 ：Producer 将相关联的消息发送到相同的消息队列。</li>
<li>严格顺序消息 ：在【普通顺序消息】的基础上，Consumer 严格顺序消费。</li>
</ul>
</li>
</ul>
</li>
<li>顺序消息扩容的过程中，如何在不停写的情况下保证消息顺序？<ul>
<li>成倍扩容，实现扩容前后，同样的 key，hash 到原队列，或者 hash 到新扩容的队列。</li>
<li>扩容前，记录旧队列中的最大位点。</li>
<li>对于每个 Consumer Group ，保证旧队列中的数据消费完，再消费新队列，也即：先对新队列进行禁读即可。</li>
</ul>
</li>
<li>什么是定时消息？如何实现？<ul>
<li>定时消息，是指消息发到 Broker 后，不能立刻被 Consumer 消费，要到特定的时间点或者等待特定的时间后才能被消费。</li>
<li>可通过配置文件，自定义每个延迟级别对应的延迟时间。当然，这是全局的。</li>
<li>如果想要实现任一时刻的延迟消息，比较简单的方式是插入延迟消息到数据库中，然后通过定时任务轮询，到达指定时间，发送到 RocketMQ 中。</li>
<li>实现原理：<ul>
<li>定时消息发送到 Broker 后，会被存储 Topic 为 SCHEDULE_TOPIC_XXXX 中，并且所在 Queue 编号为延迟级别 - 1 。（需要 -1 的原因是，延迟级别是从 1 开始的。如果延迟级别为 0 ，意味着无需延迟。）</li>
<li>Broker 针对每个 SCHEDULE_TOPIC_XXXX 的队列，都创建一个定时任务，顺序扫描到达时间的延迟消息，重新存储到延迟消息原始的 Topic 的原始 Queue 中，这样它就可以被 Consumer 消费到。<ul>
<li>为什么是“顺序扫描到达时间的延迟消息”？因为先进 SCHEDULE_TOPIC_XXXX 的延迟消息，在其所在的队列，意味着先到达延迟时间。</li>
<li>会不会存在重复扫描的情况？每个 SCHEDULE_TOPIC_XXXX 的扫描进度，会每 10s 存储到 config/delayOffset.json 文件中，所以正常情况下，不会存在重复扫描。如果异常关闭，则可能导致重复扫描。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>什么是消息重试？如何实现？<ul>
<li>消息重试，Consumer 消费消息失败后，要提供一种重试机制，令消息再消费一次。</li>
<li>Consumer 会将消费失败的消息发回 Broker，进入延迟消息队列。即，消费失败的消息，不会立即消费。也就是说，消息重试是构建在定时消息之上的功能。</li>
<li>消息重试的主要流程：<ul>
<li>Consumer 消费失败，将消息发送回 Broker 。</li>
<li>Broker 收到重试消息之后置换 Topic ，存储消息。</li>
<li>Consumer 会拉取该 Topic 对应的 retryTopic 的消息。</li>
<li>Consumer 拉取到 retryTopic 消息之后，置换到原始的 Topic ，把消息交给 Listener 消费。<ul>
<li>Consumer 消息失败后，会将消息的 Topic 修改为 %RETRY% + Topic 进行，添加 “RETRY_TOPIC” 属性为原始 Topic ，然后再返回给 Broker 中。</li>
<li>Broker 收到重试消息之后，会有两次修改消息的 Topic 。<ul>
<li>首先，会将消息的 Topic 修改为 %RETRY% + ConsumerGroup ，因为这个消息是当前消费这分组消费失败，只能被这个消费组所重新消费。注意，消费者会默认订阅 Topic 为 %RETRY% + ConsumerGroup 的消息。</li>
<li>然后，会将消息的 Topic 修改为 SCHEDULE_TOPIC_XXXX ，添加 “REAL_TOPIC” 属性为 %RETRY% + ConsumerGroup ，因为重试消息需要延迟消费。</li>
<li>Consumer 会拉取该 Topic 对应的 retryTopic 的消息，此处的 retryTopic 为 %RETRY% + ConsumerGroup 。<br>Consumer 拉取到 retryTopic 消息之后，置换到原始的 Topic ，因为有消息的 “RETRY_TOPIC” 属性是原始 Topic ，然后把消息交给 Listener 消费。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>基于RocketMQ的分布式事务解决方案<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850377370094.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>1、在扣款之前，先发送预备消息</li>
<li>2、发送预备消息成功后，执行本地扣款事务</li>
<li>3、扣款成功后，再发送确认消息</li>
<li>4、消息端（加钱业务）可以看到确认消息，消费此消息，进行加钱<blockquote>
<p>注意：上面的确认消息可以为commit消息，可以被订阅者消费；也可以是Rollback消息，即执行本地扣款事务失败后，提交rollback消息，即删除那个预备消息，订阅者无法消费</p>
</blockquote>
</li>
<li>异常1：如果发送预备消息失败，下面的流程不会走下去；这个是正常的</li>
<li>异常2：如果发送预备消息成功，但执行本地事务失败；这个也没有问题，因为此预备消息不会被消费端订阅到，消费端不会执行业务。</li>
<li>异常3：如果发送预备消息成功，执行本地事务成功，但发送确认消息失败；这个就有问题了，因为用户A扣款成功了，但加钱业务没有订阅到确认消息，无法加钱。这里出现了数据不一致。</li>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850378513462.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>RocketMq解决上面的问题，核心思路就是【状态回查】，也就是RocketMq会定时遍历commitlog中的预备消息。<blockquote>
<p>因为预备消息最终肯定会变为commit消息或Rollback消息，所以遍历预备消息去回查本地业务的执行状态，如果发现本地业务没有执行成功就rollBack，如果执行成功就发送commit消息。</p>
</blockquote>
</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>interview</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title>SpringMVC</title>
    <url>/2021/11/07/SpringMVC/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Spring-MVC"><a href="#Spring-MVC" class="headerlink" title="Spring-MVC"></a>Spring-MVC</h1><ol>
<li>Spring MVC 框架有什么用？<ul>
<li>Spring Web MVC 框架提供”模型-视图-控制器”( Model-View-Controller )架构和随时可用的组件，用于开发灵活且松散耦合的 Web 应用程序。</li>
<li>MVC 模式有助于分离应用程序的不同方面，如输入逻辑，业务逻辑和 UI 逻辑，同时在所有这些元素之间提供松散耦合。</li>
</ul>
</li>
<li>介绍下 Spring MVC 的核心组件？<ul>
<li>Spring MVC 一共有九大核心组件，分别是：<ul>
<li>MultipartResolver</li>
<li>LocaleResolver</li>
<li>ThemeResolver</li>
<li>HandlerMapping</li>
<li>HandlerAdapter</li>
<li>HandlerExceptionResolver</li>
<li>RequestToViewNameTranslator</li>
<li>ViewResolver</li>
<li>FlashMapManager</li>
</ul>
</li>
</ul>
</li>
<li>描述一下 DispatcherServlet 的工作流程？<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850164969118.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="-w803"><ul>
<li><ol>
<li>发送请求:用户向服务器发送 HTTP 请求，请求被 Spring MVC 的调度控制器 DispatcherServlet 捕获。</li>
</ol>
</li>
<li><ol start="2">
<li>映射处理器:DispatcherServlet 根据请求 URL ，调用 HandlerMapping 获得该 Handler 配置的所有相关的对象（包括 Handler 对象以及 Handler 对象对应的拦截器），最后以 HandlerExecutionChain 对象的形式返回。</li>
</ol>
</li>
<li><ol start="3">
<li>处理器适配:</li>
<li>DispatcherServlet 根据获得的 Handler，选择一个合适的HandlerAdapter 。（附注：如果成功获得 HandlerAdapter 后，此时将开始执行拦截器的 #preHandler(…) 方法）。</li>
<li>提取请求 Request 中的模型数据，填充 Handler 入参，开始执行Handler（Controller)。 在填充Handler的入参过程中，根据你的配置，Spring 将帮你做一些额外的工作：<ol>
<li>HttpMessageConverter ：会将请求消息（如 JSON、XML 等数据）转换成一个对象。</li>
<li>数据转换：对请求消息进行数据转换。如 String 转换成 Integer、Double 等。</li>
<li>数据格式化：对请求消息进行数据格式化。如将字符串转换成格式化数字或格式化日期等。</li>
<li>数据验证： 验证数据的有效性（长度、格式等），验证结果存储到 BindingResult 或 Error 中。</li>
</ol>
</li>
</ol>
</li>
<li><ol start="4">
<li>Handler(Controller) 执行完成后，向 DispatcherServlet 返回一个 ModelAndView 对象。</li>
</ol>
</li>
<li><ol start="5">
<li>解析视图:根据返回的 ModelAndView ，选择一个适合的 ViewResolver</li>
</ol>
</li>
<li>6 7 渲染视图 + 响应请求</li>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850167871281.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
<li>@Controller 注解有什么用？<ul>
<li>@Controller 注解，它将一个类标记为 Spring Web MVC 控制器 Controller 。</li>
</ul>
</li>
<li>@RestController 和 @Controller 有什么区别？<ul>
<li>@RestController 注解，在 @Controller 基础上，增加了 @ResponseBody 注解，更加适合目前前后端分离的架构下，提供 Restful API ，返回例如 JSON 数据格式。当然，返回什么样的数据格式，根据客户端的 “ACCEPT” 请求头来决定。</li>
</ul>
</li>
<li>@RequestMapping 注解有什么用？<ul>
<li>@RequestMapping 注解，用于将特定 HTTP 请求方法映射到将处理相应请求的控制器中的特定类/方法。此注释可应用于两个级别：<ul>
<li>类级别：映射请求的 URL。</li>
<li>方法级别：映射 URL 以及 HTTP 请求方法。</li>
</ul>
</li>
</ul>
</li>
<li>@RequestMapping 和 @GetMapping 注解的不同之处在哪里？<ul>
<li>@RequestMapping 可注解在类和方法上；@GetMapping 仅可注册在方法上。</li>
<li>@RequestMapping 可进行 GET、POST、PUT、DELETE 等请求方法</li>
<li>@GetMapping 是 @RequestMapping 的 GET 请求方法的特例，目的是为了提高清晰度。</li>
</ul>
</li>
<li>返回 JSON 格式使用什么注解？<ul>
<li>可以使用 @ResponseBody 注解，或者使用包含 @ResponseBody 注解的 @RestController 注解。</li>
<li>当然，还是需要配合相应的支持 JSON 格式化的 HttpMessageConverter 实现类。例如，Spring MVC 默认使用 MappingJackson2HttpMessageConverter 。</li>
</ul>
</li>
<li>介绍一下 WebApplicationContext ？<ul>
<li>WebApplicationContext 是实现ApplicationContext接口的子类，专门为 WEB 应用准备的。</li>
<li>它允许从相对于 Web 根目录的路径中加载配置文件，完成初始化 Spring MVC 组件的工作。</li>
<li>从 WebApplicationContext 中，可以获取 ServletContext 引用，整个 Web 应用上下文对象将作为属性放置在 ServletContext 中，以便 Web 应用环境可以访问 Spring 上下文。</li>
</ul>
</li>
<li>Spring MVC 的异常处理？<ul>
<li>Spring MVC 提供了异常解析器 HandlerExceptionResolver 接口，将处理器( handler )执行时发生的异常，解析( 转换 )成对应的 ModelAndView 结果。代码如下：</li>
<li>一般情况下，我们使用 @ExceptionHandler 注解来实现过异常的处理</li>
</ul>
</li>
<li>Spring MVC 有什么优点？<ul>
<li>使用真的真的真的非常方便，无论是添加 HTTP 请求方法映射的方法，还是不同数据格式的响应。</li>
<li>提供拦截器机制，可以方便的对请求进行拦截处理。</li>
<li>提供异常机制，可以方便的对异常做统一处理。</li>
<li>可以任意使用各种视图技术，而不仅仅局限于 JSP ，例如 Freemarker、Thymeleaf 等等。</li>
<li>不依赖于 Servlet API (目标虽是如此，但是在实现的时候确实是依赖于 Servlet 的，当然仅仅依赖 Servlet ，而不依赖 Filter、Listener )。</li>
</ul>
</li>
<li>Spring MVC 怎样设定重定向和转发 ？<ul>
<li>结果转发：在返回值的前面加 “forward:/“ 。</li>
<li>重定向：在返回值的前面加上 “redirect:/“ 。</li>
</ul>
</li>
<li>Spring MVC 的 Controller 是不是单例？<ul>
<li>绝绝绝大多数情况下，Controller 是单例。</li>
<li>那么，Controller 里一般不建议存在共享的变量。</li>
</ul>
</li>
<li>Spring MVC 和 Struts2 的异同？<ul>
<li>入口不同<ul>
<li>Spring MVC 的入门是一个 Servlet 控制器。</li>
<li>Struts2 入门是一个 Filter 过滤器。</li>
</ul>
</li>
<li>配置映射不同，<ul>
<li>Spring MVC 是基于方法开发，传递参数是通过方法形参，一般设置为单例。</li>
<li>Struts2 是基于类开发，传递参数是通过类的属性，只能设计为多例。</li>
</ul>
</li>
</ul>
</li>
<li>详细介绍下 Spring MVC 拦截器？<ul>
<li><code>org.springframework.web.servlet.HandlerInterceptor</code> ，拦截器接口。代码如下：  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">        <span class="comment">// HandlerInterceptor.java</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 拦截处理器，在 &#123;<span class="doctag">@link</span> HandlerAdapter#handle(HttpServletRequest, HttpServletResponse, Object)&#125; 执行之前</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">default</span> <span class="keyword">boolean</span> <span class="title">preHandle</span><span class="params">(HttpServletRequest request, HttpServletResponse response, Object handler)</span></span></span><br><span class="line"><span class="function">        		<span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        	<span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 拦截处理器，在 &#123;<span class="doctag">@link</span> HandlerAdapter#handle(HttpServletRequest, HttpServletResponse, Object)&#125; 执行成功之后</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">default</span> <span class="keyword">void</span> <span class="title">postHandle</span><span class="params">(HttpServletRequest request, HttpServletResponse response, Object handler,</span></span></span><br><span class="line"><span class="params"><span class="function">        		<span class="meta">@Nullable</span> ModelAndView modelAndView)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 拦截处理器，在 &#123;<span class="doctag">@link</span> HandlerAdapter#handle(HttpServletRequest, HttpServletResponse, Object)&#125; 执行完之后，无论成功还是失败</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * 并且，只有该处理器 &#123;<span class="doctag">@link</span> #preHandle(HttpServletRequest, HttpServletResponse, Object)&#125; 执行成功之后，才会被执行</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">default</span> <span class="keyword">void</span> <span class="title">afterCompletion</span><span class="params">(HttpServletRequest request, HttpServletResponse response, Object handler,</span></span></span><br><span class="line"><span class="params"><span class="function">        		<span class="meta">@Nullable</span> Exception ex)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>preHandle(…) 方法，调用 Controller 方法之前执行。<ul>
<li>preHandle(…) 方法，按拦截器定义顺序调用。若任一拦截器返回 false ，则 Controller 方法不再调用。    </li>
</ul>
</li>
<li>postHandle(…) 方法，调用 Controller 方法之后执行。<ul>
<li>postHandle(…) 和 #afterCompletion(…) 方法，按拦截器定义逆序调用。</li>
<li>postHandler(…) 方法，在调用 Controller 方法之后执行。</li>
</ul>
</li>
<li>afterCompletion(…) 方法，处理完 Controller 方法返回结果之后执行,无论调用 Controller 方法是否成功，都会执行。<ul>
<li>afterCompletion(…) 方法，只有该拦截器在 #preHandle(…) 方法返回 true 时，才能够被调用，且一定会被调用。为什么“且一定会被调用”呢？即使 #afterCompletion(…) 方法，按拦截器定义逆序调用时，前面的拦截器发生异常，后面的拦截器还能够调用，即无视异常。</li>
</ul>
</li>
</ul>
</li>
<li>Spring MVC 的拦截器可以做哪些事情？<ul>
<li>记录访问日志。</li>
<li>记录异常日志。</li>
<li>需要登陆的请求操作，拦截未登陆的用户。</li>
<li>参数替换</li>
</ul>
</li>
<li>Spring MVC 的拦截器和 Filter 过滤器有什么差别？<ul>
<li>功能相同：拦截器和 Filter都能实现相应的功能，谁也不比谁强。</li>
<li>容器不同：拦截器构建在 Spring MVC 体系中；Filter 构建在 Servlet 容器之上。</li>
<li>使用便利性不同：拦截器提供了三个方法，分别在不同的时机执行；过滤器仅提供一个方法，当然也能实现拦截器的执行时机的效果，就是麻烦一些。</li>
</ul>
</li>
</ol>
<h2 id="REST"><a href="#REST" class="headerlink" title="REST"></a>REST</h2><ol>
<li>REST 代表着什么?<ul>
<li>REST 代表着抽象状态转移，它是根据 HTTP 协议从客户端发送数据到服务端，例如：服务端的一本书可以以 XML 或 JSON 格式传递到客户端。</li>
</ul>
</li>
<li>资源是什么?<ul>
<li>资源是指数据在 REST 架构中如何显示的。将实体作为资源公开 ，它允许客户端通过 HTTP 方法如：GET, POST,PUT, DELETE 等读，写，修改和创建资源。</li>
</ul>
</li>
<li>什么是安全的 REST 操作?<ul>
<li>REST 接口是通过 HTTP 方法完成操作。<ul>
<li>一些HTTP操作是安全的，如 GET 和 HEAD ，它不能在服务端修改资源换句话说，PUT,POST 和 DELETE 是不安全的，因为他们能修改服务端的资源。</li>
</ul>
</li>
<li>是否安全的界限，在于是否修改服务端的资源。</li>
</ul>
</li>
<li>什么是幂等操作? 为什么幂等操作如此重要?<ul>
<li>有一些HTTP方法，如：GET，不管你使用多少次它都能产生相同的结果，在没有任何一边影响的情况下，发送多个 GET 请求到相同的URI 将会产生相同的响应结果。因此，这就是所谓幂等操作。</li>
<li>换句话说，POST方法不是幂等操作 ，因为如果发送多个 POST 请求，它将在服务端创建不同的资源。但是，假如你用PUT更新资源，它将是幂等操作。</li>
</ul>
</li>
<li>REST 是可扩展的或说是协同的吗?<ul>
<li>是的，REST 是可扩展的和可协作的。它既不托管一种特定的技术选择，也不定在客户端或者服务端。你可以用 Java, C++, Python, 或 JavaScript 来创建 RESTful Web 服务，也可以在客户端使用它们。</li>
<li>这里的“可拓展”、“协同”对应到我们平时常说的，“跨语言”、“语言无关”。</li>
</ul>
</li>
<li>REST 用哪种 HTTP 方法呢?<ul>
<li>REST 能用任何的 HTTP 方法，但是，最受欢迎的是：<ul>
<li>用 GET 来检索服务端资源</li>
<li>用 POST 来创建服务端资源</li>
<li>用 PUT 来更新服务端资源</li>
<li>用 DELETE 来删除服务端资源。</li>
</ul>
</li>
</ul>
</li>
<li>删除的 HTTP 状态返回码是什么 ?<ul>
<li>在删除成功之后，您的 REST API 应该返回什么状态代码，并没有严格的规则。它可以返回 200 或 204 没有内容。<ul>
<li>一般来说，如果删除操作成功，响应主体为空，返回 204 。</li>
<li>如果删除请求成功且响应体不是空的，则返回 200 。</li>
</ul>
</li>
</ul>
</li>
<li>REST API 是无状态的吗?<ul>
<li>是的，REST API 应该是无状态的，因为它是基于 HTTP 的，它也是无状态的。</li>
<li>REST API 中的请求应该包含处理它所需的所有细节。它不应该依赖于以前或下一个请求或服务器端维护的一些数据，例如会话。</li>
</ul>
</li>
<li>REST安全吗? 你能做什么来保护它?<ul>
<li>REST 通常不是安全的，但是您可以通过使用 Spring Security 来保护它。</li>
<li>至少，你可以通过在 Spring Security 配置文件中使用 HTTP 来启用 HTTP Basic Auth 基本认证。</li>
<li>类似地，如果底层服务器支持 HTTPS ，你可以使用 HTTPS 公开 REST API 。</li>
</ul>
</li>
<li>RestTemplate 的优势是什么?<ul>
<li>在 Spring Framework 中，RestTemplate 类是 模板方法模式 的实现。跟其他主流的模板类相似，如 JdbcTemplate 或 JmsTempalte ，它将在客户端简化跟 RESTful Web 服务的集成。正如在 RestTemplate 例子中显示的一样，你能非常容易地用它来调用 RESTful Web 服务。</li>
<li>实际场景我还是更喜欢使用 OkHttp 作为 HTTP 库，因为更好的性能，使用也便捷，并且无需依赖 Spring 库。</li>
</ul>
</li>
<li>HttpMessageConverter 在 Spring REST 中代表什么?<ul>
<li>HttpMessageConverter 是一种策略接口 ，它指定了一个转换器，它可以转换 HTTP 请求和响应。Spring REST 用这个接口转换 HTTP 响应到多种格式，例如：JSON 或 XML 。</li>
<li>每个 HttpMessageConverter 实现都有一种或几种相关联的MIME协议。Spring 使用 “Accept” 的标头来确定客户端所期待的内容类型。</li>
<li>然后，它将尝试找到一个注册的 HTTPMessageConverter ，它能够处理特定的内容类型，并使用它将响应转换成这种格式，然后再将其发送给客户端。</li>
</ul>
</li>
<li>@PathVariable 注解，在 Spring MVC 做了什么? 为什么 REST 在 Spring 中如此有用<ul>
<li>@PathVariable 注解，是 Spring MVC 中有用的注解之一，它允许您从 URI 读取值，比如查询参数。它在使用 Spring 创建 RESTful Web 服务时特别有用，因为在 REST 中，资源标识符是 URI 的一部分。</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>interview</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title>MyBatis</title>
    <url>/2021/11/07/MyBatis/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="MyBatis"><a href="#MyBatis" class="headerlink" title="MyBatis"></a>MyBatis</h1><ol>
<li><p>MyBatis 编程步骤</p>
<ul>
<li>创建 SqlSessionFactory 对象。</li>
<li>通过 SqlSessionFactory 获取 SqlSession 对象。</li>
<li>通过 SqlSession 获得 Mapper 代理对象。</li>
<li>通过 Mapper 代理对象，执行数据库操作。</li>
<li>执行成功，则使用 SqlSession 提交事务。</li>
<li>执行失败，则使用 SqlSession 回滚事务。</li>
<li>最终，关闭会话。</li>
</ul>
</li>
<li><p>#{} 和 ${} 的区别是什么？</p>
<ul>
<li>${} 是 Properties 文件中的变量占位符，它可以用于 XML 标签属性值和 SQL 内部，属于字符串替换,#{} 是预编译处理，可以有效防止 SQL 注入，提高系统安全性</li>
<li>${} 也可以对传递进来的参数原样拼接在 SQL 中,可能有 SQL 注入的风险。</li>
</ul>
</li>
<li><p>当实体类中的属性名和表中的字段名不一样 ，怎么办？</p>
<ul>
<li>在查询的 SQL 语句中定义字段名的别名</li>
<li>配置自动的下划线转驼峰的功能</li>
<li>通过 <resultMap> 来映射字段名和实体类属性名的一一对应的关系</li>
</ul>
</li>
<li><p>Mybatis 动态 SQL 是做什么的？都有哪些动态 SQL ？能简述一下动态 SQL 的执行原理吗？</p>
<ul>
<li>Mybatis 动态 SQL ，可以让我们在 XML 映射文件内，以 XML 标签的形式编写动态 SQL ，完成逻辑判断和动态拼接 SQL 的功能。</li>
<li>Mybatis 提供了 9 种动态 SQL 标签：<code>&lt;if /&gt;、&lt;choose /&gt;、&lt;when /&gt;、&lt;otherwise /&gt;、&lt;trim /&gt;、&lt;where /&gt;、&lt;set /&gt;、&lt;foreach /&gt;、&lt;bind /&gt; </code>。</li>
<li>其执行原理为，使用 OGNL 的表达式，从 SQL 参数对象中计算表达式的值，根据表达式的值动态拼接 SQL ，以此来完成动态 SQL 的功能。</li>
</ul>
</li>
<li><p>最佳实践中，通常一个 XML 映射文件，都会写一个 Mapper 接口与之对应。请问，这个 Mapper 接口的工作原理是什么？Mapper 接口里的方法，参数不同时，方法能重载吗？</p>
<ul>
<li>接口的全限名，就是映射文件中的 “namespace” 的值。</li>
<li>接口的方法名，就是映射文件中 MappedStatement 的 “id” 值。</li>
<li>接口方法内的参数，就是传递给 SQL 的参数。</li>
<li>另外，Mapper 接口的实现类，通过 MyBatis 使用 JDK Proxy 自动生成其代理对象 Proxy ，而代理对象 Proxy 会拦截接口方法，从而“调用”对应的 MappedStatement 方法，最终执行 SQL ，返回执行结果。整体流程如下图：<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850110187997.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
<li>其中，SqlSession 在调用 Executor 之前，会获得对应的 MappedStatement 方法。例如：DefaultSqlSession#select(String statement, Object parameter, RowBounds rowBounds, ResultHandler handler) 方法，代码如下：  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// DefaultSqlSession.java</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">select</span><span class="params">(String statement, Object parameter, RowBounds rowBounds, ResultHandler handler)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 获得 MappedStatement 对象</span></span><br><span class="line">        MappedStatement ms = configuration.getMappedStatement(statement);</span><br><span class="line">        <span class="comment">// 执行查询</span></span><br><span class="line">        executor.query(ms, wrapCollection(parameter), rowBounds, handler);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> ExceptionFactory.wrapException(<span class="string">&quot;Error querying database.  Cause: &quot;</span> + e, e);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        ErrorContext.instance().reset();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Mapper 接口绑定有几种实现方式,分别是怎么实现的?</p>
<ul>
<li>通过 XML Mapper 里面写 SQL 来绑定。在这种情况下，要指定 XML 映射文件里面的 “namespace” 必须为接口的全路径名。</li>
<li>通过注解绑定，就是在接口的方法上面加上 @Select、@Update、@Insert、@Delete 注解，里面包含 SQL 语句来绑定。</li>
<li>是第二种的特例，也是通过注解绑定，在接口的方法上面加上 @SelectProvider、@UpdateProvider、@InsertProvider、@DeleteProvider 注解，通过 Java 代码，生成对应的动态 SQL </li>
</ul>
</li>
<li><p>Mybatis 的 XML Mapper文件中，不同的 XML 映射文件，id 是否可以重复？</p>
<ul>
<li>不同的 XML Mapper 文件，如果配置了 “namespace” ，那么 id 可以重复；如果没有配置 “namespace” ，那么 id 不能重复。毕竟”namespace” 不是必须的，只是最佳实践而已。</li>
<li>原因就是，namespace + id 是作为 Map&lt;String, MappedStatement&gt; 的 key 使用的。如果没有 “namespace”，就剩下 id ，那么 id 重复会导致数据互相覆盖。如果有了 “namespace”，自然 id 就可以重复，”namespace”不同，namespace + id 自然也就不同。</li>
</ul>
</li>
<li><p>如何获取自动生成的(主)键值?</p>
<ul>
<li>Mysql 自增主键<ul>
<li>使用 useGeneratedKeys + keyProperty 属性  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">insert</span> <span class="attr">id</span>=<span class="string">&quot;insert&quot;</span> <span class="attr">parameterType</span>=<span class="string">&quot;Person&quot;</span> <span class="attr">useGeneratedKeys</span>=<span class="string">&quot;true&quot;</span> <span class="attr">keyProperty</span>=<span class="string">&quot;id&quot;</span>&gt;</span></span><br><span class="line">        INSERT INTO person(name, pswd)</span><br><span class="line">        VALUE (#&#123;name&#125;, #&#123;pswd&#125;)</span><br><span class="line">    <span class="tag">&lt;/<span class="name">insert</span>&gt;</span></span><br><span class="line">    ```    </span><br><span class="line">- 使用 `<span class="tag">&lt;<span class="name">selectKey</span> /&gt;</span>` 标签</span><br><span class="line"></span><br><span class="line">    ```xml</span><br><span class="line">        <span class="tag">&lt;<span class="name">insert</span> <span class="attr">id</span>=<span class="string">&quot;insert&quot;</span> <span class="attr">parameterType</span>=<span class="string">&quot;Person&quot;</span> <span class="attr">useGeneratedKeys</span>=<span class="string">&quot;true&quot;</span> <span class="attr">keyProperty</span>=<span class="string">&quot;id&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">selectKey</span> <span class="attr">keyProperty</span>=<span class="string">&quot;id&quot;</span> <span class="attr">resultType</span>=<span class="string">&quot;long&quot;</span> <span class="attr">order</span>=<span class="string">&quot;AFTER&quot;</span>&gt;</span></span><br><span class="line">            SELECT LAST_INSERT_ID()</span><br><span class="line">        <span class="tag">&lt;/<span class="name">selectKey</span>&gt;</span> </span><br><span class="line">            INSERT INTO person(name, pswd)</span><br><span class="line">            VALUE (#&#123;name&#125;, #&#123;pswd&#125;)</span><br><span class="line">        <span class="tag">&lt;/<span class="name">insert</span>&gt;</span>            </span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>Oracle序列自增  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">insert</span> <span class="attr">id</span>=<span class="string">&quot;add&quot;</span> <span class="attr">parameterType</span>=<span class="string">&quot;Student&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">selectKey</span> <span class="attr">keyProperty</span>=<span class="string">&quot;student_id&quot;</span> <span class="attr">resultType</span>=<span class="string">&quot;int&quot;</span> <span class="attr">order</span>=<span class="string">&quot;BEFORE&quot;</span>&gt;</span></span><br><span class="line">        select student_sequence.nextval FROM dual</span><br><span class="line">    <span class="tag">&lt;/<span class="name">selectKey</span>&gt;</span></span><br><span class="line">    INSERT INTO student(student_id, student_name, student_age)</span><br><span class="line">    VALUES (#&#123;student_id&#125;,#&#123;student_name&#125;,#&#123;student_age&#125;)</span><br><span class="line"><span class="tag">&lt;/<span class="name">insert</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Mybatis 执行批量插入，能返回数据库主键列表吗？</p>
<ul>
<li>能，JDBC 都能做，Mybatis 当然也能做。</li>
</ul>
</li>
<li><p>MyBatis 如何执行批量插入?</p>
<ul>
<li>单条插傻循环</li>
<li>拼接insert into values (),(),();</li>
<li>使用BatchExecutor批处理</li>
</ul>
</li>
<li><p>在 Mapper 中如何传递多个参数?</p>
<ul>
<li>使用 Map 集合，装载多个参数进行传递</li>
<li>保持传递多个参数，使用 @Param 注解</li>
<li>保持传递多个参数，不使用 @Param 注解，#{param2}</li>
<li>使用对象定义VO，装载多个参数进行传递</li>
</ul>
</li>
<li><p>Mybatis 是否可以映射 Enum 枚举类？</p>
<ul>
<li>Mybatis 可以映射枚举类，对应的实现类为 EnumTypeHandler 或 EnumOrdinalTypeHandler 。<ul>
<li>EnumTypeHandler ，基于 Enum.name 属性( String )。默认。</li>
<li>EnumOrdinalTypeHandler ，基于 Enum.ordinal 属性( int )。可通过 <setting name="defaultEnumTypeHandler" value="EnumOrdinalTypeHandler" /> 来设置。</li>
</ul>
</li>
</ul>
</li>
<li><p>Mybatis 都有哪些 Executor 执行器？它们之间的区别是什么？</p>
<ul>
<li>Mybatis 有四种 Executor 执行器，分别是 SimpleExecutor、ReuseExecutor、BatchExecutor、CachingExecutor 。<ul>
<li>SimpleExecutor ：每执行一次 update 或 select 操作，就创建一个 Statement 对象，用完立刻关闭 Statement 对象。</li>
<li>ReuseExecutor ：执行 update 或 select 操作，以 SQL 作为key 查找缓存的 Statement 对象，存在就使用，不存在就创建；用完后，不关闭 Statement 对象，而是放置于缓存 Map&lt;String, Statement&gt; 内，供下一次使用。简言之，就是重复使用 Statement 对象。</li>
<li>BatchExecutor ：执行 update 操作（没有 select 操作，因为 JDBC 批处理不支持 select 操作），将所有 SQL 都添加到批处理中（通过 addBatch 方法），等待统一执行（使用 executeBatch 方法）。它缓存了多个 Statement 对象，每个 Statement 对象都是调用 addBatch 方法完毕后，等待一次执行 executeBatch 批处理。实际上，整个过程与 JDBC 批处理是相同。</li>
<li>CachingExecutor ：在上述的三个执行器之上，增加二级缓存的功能。</li>
</ul>
</li>
</ul>
</li>
<li><p>介绍 MyBatis 的一级缓存和二级缓存的概念和实现原理？</p>
<ul>
<li><p>一级缓存：</p>
<ul>
<li><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850119544106.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>每个SqlSession中持有了Executor，每个Executor中有一个LocalCache。当用户发起查询时，MyBatis根据当前执行的语句生成MappedStatement，在Local Cache进行查询，如果缓存命中的话，直接返回结果给用户，如果缓存没有命中的话，查询数据库，结果写入Local Cache，最后返回结果给用户</p>
</li>
</ul>
</li>
<li><p>二级缓存</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850120537632.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>二级缓存开启后，同一个namespace下的所有操作语句，都影响着同一个Cache，即二级缓存被多个SqlSession共享，是一个全局的变量。</li>
<li>当开启缓存后，数据的查询执行的流程就是 二级缓存 -&gt; 一级缓存 -&gt; 数据库。</li>
</ul>
</li>
</ul>
</li>
<li><p>Mybatis 是否支持延迟加载？如果支持，它的实现原理是什么？</p>
<ul>
<li>Mybatis 仅支持 association 关联对象和 collection 关联集合对象的延迟加载。其中，association 指的就是一对一，collection 指的就是一对多查询。</li>
<li>在 Mybatis 配置文件中，可以配置 <setting name="lazyLoadingEnabled" value="true" /> 来启用延迟加载的功能。默认情况下，延迟加载的功能是关闭的。</li>
<li>它的原理是，使用 CGLIB 或 Javassist( 默认 ) 创建目标对象的代理对象。当调用代理对象的延迟加载属性的 getting 方法时，进入拦截器方法。比如调用 a.getB().getName() 方法，进入拦截器的 invoke(…) 方法，发现 a.getB() 需要延迟加载时，那么就会单独发送事先保存好的查询关联 B 对象的 SQL ，把 B 查询上来，然后调用a.setB(b) 方法，于是 a 对象 b 属性就有值了，接着完成a.getB().getName() 方法的调用。这就是延迟加载的基本原理。</li>
</ul>
</li>
<li><p>简述 Mybatis 的插件运行原理？以及如何编写一个插件？</p>
<ul>
<li>Mybatis 仅可以编写针对 ParameterHandler、ResultSetHandler、StatementHandler、Executor 这 4 种接口的插件。</li>
<li>Mybatis 使用 JDK 的动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这 4 种接口对象的方法时，就会进入拦截方法，具体就是 InvocationHandler 的 #invoke(…)方法。当然，只会拦截那些你指定需要拦截的方法。</li>
</ul>
</li>
<li><p>Mybatis 是如何进行分页的？分页插件的原理是什么？</p>
<ul>
<li>Mybatis 使用 RowBounds 对象进行分页，它是针对 ResultSet 结果集执行的内存分页，而非数据库分页</li>
<li>分页插件的基本原理是使用 Mybatis 提供的插件接口，实现自定义分页插件。在插件的拦截方法内，拦截待执行的 SQL ，然后重写 SQL ，根据dialect 方言，添加对应的物理分页语句和物理分页参数。</li>
<li>目前使用比较广泛的 MyBatis 分页插件有：Mybatis-PageHelper、MyBatis-Plus</li>
</ul>
</li>
<li><p>MyBatis 与 Hibernate 有哪些不同？</p>
<ul>
<li>Mybatis 学习门槛低，简单易学，程序员直接编写原生态 SQL ，可严格控制 SQL 执行性能，灵活度高。但是灵活的前提是 MyBatis 无法做到数据库无关性，如果需要实现支持多种数据库的软件则需要自定义多套 SQL 映射文件，工作量大</li>
<li>Hibernate 对象/关系映射能力强，数据库无关性好。如果用 Hibernate 开发可以节省很多代码，提高效率。但是 Hibernate 的缺点是学习门槛高，要精通门槛更高，而且怎么设计 O/R 映射，在性能和对象模型之间如何权衡，以及怎样用好 Hibernate 需要具有很强的经验和能力才行</li>
</ul>
</li>
<li><p>JDBC 编程有哪些不足之处，MyBatis是如何解决这些问题的？</p>
<ul>
<li>问题一：SQL 语句写在代码中造成代码不易维护，且代码会比较混乱。<ul>
<li>解决方式：将 SQL 语句配置在 Mapper XML 文件中，与 Java 代码分离。</li>
</ul>
</li>
<li>问题二：根据参数不同，拼接不同的 SQL 语句非常麻烦。例如 SQL 语句的 WHERE 条件不一定，可能多也可能少，占位符需要和参数一一对应。<ul>
<li>解决方式：MyBatis 提供 <where />、<if /> 等等动态语句所需要的标签，并支持 OGNL 表达式，简化了动态 SQL 拼接的代码，提升了开发效率。</li>
</ul>
</li>
<li>问题三，对结果集解析麻烦，SQL 变化可能导致解析代码变化，且解析前需要遍历。<ul>
<li>解决方式：Mybatis 自动将 SQL 执行结果映射成 Java 对象。</li>
</ul>
</li>
</ul>
</li>
<li><p>Mybatis 映射文件中，如果 A 标签通过 include 引用了B标签的内容，请问，B 标签能否定义在 A 标签的后面，还是说必须定义在A标签的前面？</p>
<ul>
<li>虽然 Mybatis 解析 XML 映射文件是按照顺序解析的。但是，被引用的 B 标签依然可以定义在任何地方，Mybatis 都可以正确识别。也就是说，无需按照顺序，进行定义。</li>
<li>原理是，Mybatis 解析 A 标签，发现 A 标签引用了 B 标签，但是 B 标签尚未解析到，尚不存在，此时，Mybatis 会将 A 标签标记为未解析状态。然后，继续解析余下的标签，包含 B 标签，待所有标签解析完毕，Mybatis 会重新解析那些被标记为未解析的标签，此时再解析A标签时，B 标签已经存在，A 标签也就可以正常解析完成了。</li>
</ul>
</li>
<li><p>简述 Mybatis 的 XML 映射文件和 Mybatis 内部数据结构之间的映射关系？</p>
<ul>
<li>Mybatis 将所有 XML 配置信息都封装到 All-In-One 重量级对象Configuration内部。</li>
<li>在 XML Mapper 文件中：<ul>
<li><parameterMap> 标签，会被解析为 ParameterMap 对象，其每个子元素会被解析为 ParameterMapping 对象。</li>
<li><resultMap> 标签，会被解析为 ResultMap 对象，其每个子元素会被解析为 ResultMapping 对象。</li>
<li>每一个 <code>&lt;select&gt;、&lt;insert&gt;、&lt;update&gt;、&lt;delete&gt;</code> 标签，均会被解析为一个 MappedStatement 对象，标签内的 SQL 会被解析为一个 BoundSql 对象。</li>
</ul>
</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>interview</tag>
        <tag>ORM</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx</title>
    <url>/2021/11/07/Nginx/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Nginx"><a href="#Nginx" class="headerlink" title="Nginx"></a>Nginx</h1><ol>
<li><p>请解释一下什么是 Nginx ？</p>
<ul>
<li>Nginx ，是一个 Web 服务器和反向代理服务器，用于 HTTP、HTTPS、SMTP、POP3 和 IMAP 协议。</li>
<li>作为 http server</li>
<li>反向代理服务器</li>
<li>正向代理</li>
<li>实现负载均衡</li>
</ul>
</li>
<li><p>Nginx 常用命令？</p>
<ul>
<li>启动 nginx 。</li>
<li>停止 nginx -s stop 或 nginx -s quit 。</li>
<li>重载配置 ./sbin/nginx -s reload(平滑重启) 或 service nginx reload 。</li>
<li>重载指定配置文件 .nginx -c /usr/local/nginx/conf/nginx.conf 。</li>
<li>查看 nginx 版本 nginx -v 。</li>
<li>检查配置文件是否正确 nginx -t 。</li>
<li>显示帮助信息 nginx -h 。</li>
</ul>
</li>
<li><p>Nginx 常用配置？</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">worker_processes  8; # 工作进程个数</span><br><span class="line">worker_connections  65535; # 每个工作进程能并发处理（发起）的最大连接数（包含所有连接数）</span><br><span class="line"># 错误日志打印地址</span><br><span class="line">error_log         /data/logs/nginx/error.log;</span><br><span class="line"># 访问日志打印地址 </span><br><span class="line">access_log      /data/logs/nginx/access.log; </span><br><span class="line"># 日志格式</span><br><span class="line">log_format  main  &#x27;$remote_addr&quot;$request&quot; &#x27;&#x27;$status $upstream_addr &quot;$request_time&quot;&#x27;; </span><br><span class="line">listen       80; # 监听端口</span><br><span class="line"># 允许域名</span><br><span class="line">server_name  rrc.test.jiedaibao.com; </span><br><span class="line"># 项目根目录</span><br><span class="line">root  /data/release/rrc/web; </span><br><span class="line"># 访问根文件</span><br><span class="line">index  index.php index.html index.htm; </span><br></pre></td></tr></table></figure></li>
<li><p>Nginx 日志格式中的<code>$time_local</code>表示的是什么时间？请求开始的时间？请求结束的时间？其次，当我们从前到后观察日志中的 $time_local 时间时，有时候会发现时间顺序前后错乱的现象，请说明原因？</p>
<ul>
<li><code>$time_local</code> ：在服务器里请求开始写入本地的时间。</li>
<li>因为请求发生时间有前有后，所以会时间顺序前后错乱。</li>
</ul>
</li>
<li><p>Nginx 有哪些优点？</p>
<ul>
<li>跨平台、配置简单。</li>
<li>非阻塞、高并发连接</li>
<li>内存消耗小</li>
<li>成本低廉，且开源。</li>
<li>稳定性高，宕机的概率非常小。</li>
</ul>
</li>
<li><p>使用“反向代理服务器”的优点是什么？</p>
<ul>
<li>反向代理服务器可以隐藏源服务器的存在和特征。它充当互联网云和 Web 服务器之间的中间层。这对于安全方面来说是很好的，特别是当我们使用 Web 托管服务时。</li>
</ul>
</li>
<li><p>什么是正向代理？</p>
<ul>
<li>一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。</li>
<li>客户端才能使用正向代理。</li>
<li>正向代理总结就一句话：代理端代理的是客户端</li>
</ul>
</li>
<li><p>什么是反向代理？</p>
<ul>
<li>反向代理（Reverse Proxy）方式，是指以代理服务器来接受 Internet上的连接请求，然后将请求，发给内部网络上的服务器并将从服务器上得到的结果返回给 Internet 上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。</li>
<li>反向代理总结就一句话：代理端代理的是服务端。</li>
</ul>
</li>
<li><p>LVS、Nginx、HAproxy 有什么区别？</p>
<ul>
<li>LVS ：是基于四层的转发。</li>
<li>HAproxy ： 是基于四层和七层的转发，是专业的代理服务器。</li>
<li>Nginx ：是 WEB 服务器，缓存服务器，又是反向代理服务器，可以做七层的转发。 <ul>
<li>Nginx 引入 TCP 插件之后，也可以支持四层的转发。</li>
</ul>
</li>
</ul>
</li>
<li><p>请解释 Nginx 如何处理 HTTP 请求？</p>
<ul>
<li>首先，Nginx 在启动时，会解析配置文件，得到需要监听的端口与 IP 地址，然后在 Nginx 的 Master 进程里面先初始化好这个监控的Socket(创建 S ocket，设置 addr、reuse 等选项，绑定到指定的 ip 地址端口，再 listen 监听)。</li>
<li>然后，再 fork(一个现有进程可以调用 fork 函数创建一个新进程。由 fork 创建的新进程被称为子进程 )出多个子进程出来。</li>
<li>之后，子进程会竞争 accept 新的连接。此时，客户端就可以向 nginx 发起连接了。当客户端与nginx进行三次握手，与 nginx 建立好一个连接后。此时，某一个子进程会 accept 成功，得到这个建立好的连接的 Socket ，然后创建 nginx 对连接的封装，即 ngx_connection_t 结构体。</li>
<li>接着，设置读写事件处理函数，并添加读写事件来与客户端进行数据的交换。</li>
<li>Nginx 或客户端来主动关掉连接，到此，一个连接就寿终正寝了。</li>
</ul>
</li>
<li><p>什么是动态资源、静态资源分离？</p>
<ul>
<li>动态资源、静态资源分离，是让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后我们就可以根据静态资源的特点将其做缓存操作，这就是网站静态化处理的核心思路。</li>
<li>动态资源、静态资源分离简单的概括是：动态文件与静态文件的分离。</li>
</ul>
</li>
<li><p>为什么要做动、静分离？</p>
<ul>
<li>在我们的软件开发中，有些请求是需要后台处理的（如：.jsp,.do 等等），有些请求是不需要经过后台处理的（如：css、html、jpg、js 等等文件），这些不需要经过后台处理的文件称为静态文件，否则动态文件</li>
<li>因此我们后台处理忽略静态文件。这会有人又说那我后台忽略静态文件不就完了吗？当然这是可以的，但是这样后台的请求次数就明显增多了。在我们对资源的响应速度有要求的时候，我们应该使用这种动静分离的策略去解决动、静分离将网站静态资源（HTML，JavaScript，CSS，img等文件）与后台应用分开部署，提高用户访问静态代码的速度，降低对后台应用访问</li>
<li>这里我们将静态资源放到 Nginx 中，动态资源转发到 Tomcat 服务器中去。 </li>
<li>因为现在七牛、阿里云等 CDN 服务已经很成熟，主流的做法，是把静态资源缓存到 CDN 服务中，从而提升访问速度。<ul>
<li>相比本地的 Nginx 来说，CDN 服务器由于在国内有更多的节点，可以实现用户的就近访问。</li>
<li>并且，CDN 服务可以提供更大的带宽，不像我们自己的应用服务，提供的带宽是有限的。</li>
</ul>
</li>
</ul>
</li>
<li><p>什么叫 CDN 服务？</p>
<ul>
<li>CDN ，即内容分发网络。</li>
<li>其目的是，通过在现有的 Internet中 增加一层新的网络架构，将网站的内容发布到最接近用户的网络边缘，使用户可就近取得所需的内容，提高用户访问网站的速度。</li>
<li>一般来说，因为现在 CDN 服务比较大众，所以基本所有公司都会使用 CDN 服务。</li>
</ul>
</li>
<li><p>Nginx 有哪些负载均衡策略？</p>
<ul>
<li><p>轮询（默认）round_robin：每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器 down 掉，能自动剔除。</p>
</li>
<li><p>IP 哈希 ip_hash：每个请求按访问 ip 的 hash 结果分配，这样每个访客固定访问一个后端服务器，可以解决 session 共享的问题。当然，实际场景下，一般不考虑使用 ip_hash 解决 session 共享。</p>
</li>
<li><p>最少连接 least_conn：下一个请求将被分派到活动连接数量最少的服务器</p>
</li>
<li><p>权重</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">weight=1; # (weight 默认为1.weight越大，负载的权重就越大)</span><br><span class="line">down; # (down 表示单前的server暂时不参与负载)</span><br><span class="line">backup; # (其它所有的非backup机器down或者忙的时候，请求backup机器)</span><br><span class="line">max_fails=1; # 允许请求失败的次数默认为 1 。当超过最大次数时，返回</span><br><span class="line">proxy_next_upstream 模块定义的错误</span><br><span class="line">fail_timeout=30; # max_fails 次失败后，暂停的时间</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Nginx 如何实现后端服务的健康检查？</p>
<ul>
<li>利用 nginx 自带模块 ngx_http_proxy_module 和 ngx_http_upstream_module 对后端节点做健康检查。</li>
<li>利用 nginx_upstream_check_module 模块对后端节点做健康检查。</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>Linux</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title>Java基础</title>
    <url>/2021/11/07/Java%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Java核心基础"><a href="#Java核心基础" class="headerlink" title="Java核心基础"></a>Java核心基础</h1><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><ol>
<li><p>JAVA基本数据类型所占长度、</p>
<table>
<thead>
<tr>
<th>基本类型</th>
<th>大小</th>
<th>最小值</th>
<th>最大值</th>
</tr>
</thead>
<tbody><tr>
<td>int</td>
<td>32bit</td>
<td>-2^31</td>
<td>+2^32 -1</td>
</tr>
<tr>
<td>short</td>
<td>16bit</td>
<td>-2^15</td>
<td>+2^15 -1</td>
</tr>
<tr>
<td>byte</td>
<td>8bit</td>
<td>-128</td>
<td>+127</td>
</tr>
<tr>
<td>long</td>
<td>64bit</td>
<td>-2^63</td>
<td>+2^63 -1</td>
</tr>
<tr>
<td>float</td>
<td>32bit</td>
<td>IEEE754</td>
<td>IEEE754</td>
</tr>
<tr>
<td>double</td>
<td>64bit</td>
<td>IEEE754</td>
<td>IEEE754</td>
</tr>
<tr>
<td>char</td>
<td>16bit</td>
<td>Unicode 0</td>
<td>Unicode 2^16 -1</td>
</tr>
<tr>
<td>boolean</td>
<td>1bit</td>
<td>—-</td>
<td>—-</td>
</tr>
</tbody></table>
<ul>
<li>JAVA基本数据类型的长度是平台无关的，32位系统和64位系统一样，因为JAVA是运行在JVM上的。</li>
</ul>
</li>
<li><p>Java String 占用内存大小分析</p>
<ul>
<li>Java 对象在虚拟机的结构如下：<ul>
<li>对象头（object header）：8 个字节（保存对象的 class 信息、ID、在虚拟机中的状态）</li>
<li>Java 原始类型数据：如 int, float, char 等类型的数据</li>
<li>引用（reference）：4 个字节</li>
<li>填充符（padding）</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="线程基础"><a href="#线程基础" class="headerlink" title="线程基础"></a>线程基础</h2><ol>
<li>什么是线程？<ul>
<li>线程是操作系统能够进行运算调度的最小单位，它被包含在进程之中，是进程中的实际运作单位</li>
</ul>
</li>
<li>线程和进程有什么区别？<ul>
<li>线程是进程的子集，一个进程可以有很多线程，每条线程并行执行不同的任务。不同的进程使用不同的内存空间，而所有的线程共享一片相同的内存空间。别把它和栈内存搞混，每个线程都拥有单独的栈内存用来存储本地数据</li>
</ul>
</li>
<li>如何在Java中实现线程？<ul>
<li>继承java.lang.Thread 类或者直接调用Runnable接口来重写run()方法实现线程</li>
</ul>
</li>
<li>用Runnable还是Thread？<ul>
<li>Runnable可以继承其他类</li>
</ul>
</li>
<li>Thread 类中的start() 和 run() 方法有什么区别？<ul>
<li>start()方法被用来启动新创建的线程，而且start()内部调用了run()方法，这和直接调用run()方法的效果不一样。当<strong>调用run()方法的时候，只会是在原来的线程中调用，没有新的线程启动，start()方法才会启动新线程</strong></li>
</ul>
</li>
<li>Java中Runnable和Callable有什么不同？<ul>
<li>Callable的 call() 方法有返回值和抛出异常，而Runnable的run()方法没有这些功能。</li>
<li>Callable可以返回装载有计算结果的Future对象。</li>
</ul>
</li>
<li>什么是线程安全？Vector是一个线程安全类吗？<ul>
<li>多线程操作下数据一致性</li>
</ul>
</li>
</ol>
<h2 id="ThreadLocal-相关"><a href="#ThreadLocal-相关" class="headerlink" title="ThreadLocal 相关"></a>ThreadLocal 相关</h2><ol>
<li>作为线程局部变量使用</li>
<li><code>set(T value)</code>:获取<code>ThreadLocalMap</code>(静态内部类)并保存value,为空则调用<code>createMap(Thread t, T firstValue)</code>;</li>
<li><code>getMap(Thread t)</code>:获取<code>ThreadLocalMap</code>,不为空则获取对应值;为空调用<code>setInitialValue()</code> 初始化value;内部调用<code>initialValue()</code>;</li>
<li>需要初始化值,要重写<code>initialValue</code></li>
<li><a href="http://blog.csdn.net/sonny543/article/details/51336457">threadlocal原理及常用应用场景</a></li>
</ol>
<h2 id="java-集合框架-队列相关"><a href="#java-集合框架-队列相关" class="headerlink" title="java 集合框架 队列相关"></a>java 集合框架 队列相关</h2><ol>
<li>HashMap <ul>
<li>基于Hash表的非同步实现,允许K-V 为null;</li>
<li>底层基于数组实现(HashMap.Entry[]),单项为一个链表</li>
<li>HashMap.Entry 包含K,V,next Entry&lt;K,V&gt;,hash</li>
<li>put(K,V) 通过hash(key.hashCode())计算出hash值决定其在数组中的存储位置，如果此位置上有对象的话，再去使用 equals方法进行比较，如果对此链上的每个对象的 equals 方法比较都为 false，则将该对象放到数组当中，然后将数组中该位置以前存在的那个对象链接到此对象的后面</li>
<li>get(K) 首先计算key的hashCode，找到数组中对应位置的某一元素，然后通过key的equals方法在对应位置的链表中找到需要的元素。</li>
<li>当hash冲突很多时，HashMap退化成链表。</li>
<li>key为null时，都放到table[0]</li>
<li>扩容默认负载因子0.75,重新计算位置单个Entry在新数组中的位置 (resize)</li>
<li>fast-fail volatile modCount</li>
<li>java 8 HashMap 改为 数组+链表/红黑树,同一hash位下链表元素&gt;=8时,链表转换为红黑树</li>
</ul>
</li>
<li>ConcurrentHashMap<ul>
<li>ConcurrentHashMap允许多个修改操作并发进行，其关键在于使用了锁分离技术。</li>
<li><font color="red" >待补充源码及具体实现</font></li>
</ul>
</li>
<li>LinkedHashMap<ul>
<li>LinkedHashMap继承于HashMap，底层使用哈希表和双向链表来保存所有元素，并且它是非同步，允许使用null值和null键</li>
<li>重新定义了数组中保存的元素Entry，来实现自己的链接列表特性。该Entry除了保存当前对象的引用外，还保存了其上一个元素before和下一个元素after的引用，从而构成了双向链接列表</li>
</ul>
</li>
<li>HashSet<ul>
<li>HashSet由哈希表支持,基于HashMap实现，不保证set的迭代顺序，并允许使用null元素。</li>
</ul>
</li>
<li>LinkedHashSet<ul>
<li>对于LinkedHashSet而言，它继承与HashSet、又基于LinkedHashMap来实现的。LinkedHashSet底层使用LinkedHashMap来保存所有元素，它继承与HashSet，其所有的方法操作上又与HashSet相同。</li>
</ul>
</li>
<li>ArrayList<ul>
<li>ArrayList是List接口的可变数组非同步实现，并允许包括null在内的所有元素。底层使用数组实现</li>
<li>该集合是可变长度数组，数组扩容时，会将老数组中的元素重新拷贝一份到新的数组中，每次数组容量增长大约是其容量的<font color="red">1.5倍</font>，这种操作的代价很高。</li>
<li>采用了Fail-Fast机制，面对并发的修改时，迭代器很快就会完全失败，而不是冒着在将来某个不确定时间发生任意不确定行为的风险</li>
</ul>
</li>
<li>LinkedList<ul>
<li>LinkedList是List接口的双向链表非同步实现，并允许包括null在内的所有元素。</li>
<li>底层的数据结构是基于双向链表的，该数据结构我们称为节点(Node)</li>
<li>双向链表节点对应的静态内部类Node<E>的实例，Node中包含成员变量：next，prev，item。</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring</title>
    <url>/2021/11/07/Spring/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Spring"><a href="#Spring" class="headerlink" title="Spring"></a>Spring</h1><h2 id="Spring-整体相关的面试"><a href="#Spring-整体相关的面试" class="headerlink" title="Spring 整体相关的面试"></a>Spring 整体相关的面试</h2><ol>
<li>什么是 Spring Framework？<ul>
<li>Spring 是一个开源应用框架，旨在降低应用程序开发的复杂度。</li>
<li>它是轻量级、松散耦合的。随着 Spring 的体系越来越庞大，大家被 Spring 的配置搞懵逼了，所以后来出了 Spring Boot 。</li>
<li>它具有分层体系结构，允许用户选择组件，同时还为 J2EE 应用程序开发提供了一个有凝聚力的框架。</li>
<li>它可以集成其他框架，如 Spring MVC、Hibernate、MyBatis 等，所以又称为框架的框架( 粘合剂、脚手架 )。</li>
</ul>
</li>
<li>Spring Framework 中有多少个模块，它们分别是什么？<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15849299427756.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>Spring 核心容器:核心容器提供 Spring 框架的基本功能。核心容器的主要组件是 BeanFactory，它是工厂模式的实现。BeanFactory 使用控制反转 （IOC）模式将应用程序的配置和依赖性规范与实际的应用程序代码分开。<ul>
<li>Spring Core</li>
<li>Spring Bean</li>
<li>Spring Context:Spring 上下文是一个配置文件，向 Spring 框架提供上下文信息。Spring 上下文包括企业服务，例如 JNDI、EJB、电子邮件、国际化、事件机制、校验和调度功能。</li>
<li>SpEL (Spring Expression Language):Spring 表达式语言全称为 “Spring Expression Language”，缩写为 “SpEL” ，类似于 Struts2 中使用的 OGNL 表达式语言，能在运行时构建复杂表达式、存取对象图属性、对象方法调用等等，并且能与 Spring 功能完美整合，如能用来配置 Bean 定义。</li>
</ul>
</li>
<li>数据访问:Data Access 。<ul>
<li>JDBC: Spring 对 JDBC 的封装模块，提供了对关系数据库的访问。</li>
<li>ORM: Spring ORM 模块，提供了对 hibernate5 和 JPA 的集成。<ul>
<li>hibernate5 是一个 ORM 框架。</li>
<li>JPA 是一个 Java 持久化 API 。</li>
</ul>
</li>
<li>Transaction:Spring 简单而强大的事务管理功能，包括声明式事务和编程式事务。</li>
</ul>
</li>
<li>Web: 提供了创建 Web 应用程序的支持。它包含以下模块：<ul>
<li>WebMVC 框架是一个全功能的构建 Web 应用程序的 MVC 实现。通过策略接口，MVC 框架变成为高度可配置的，MVC 容纳了大量视图技术，其中包括 JSP、Velocity、Tiles、iText 和 POI。</li>
<li>WebFlux:基于 Reactive 库的响应式的 Web 开发框架</li>
<li>WebSocket:Websocket 提供了一个在 Web 应用中实现高效、双向通讯，需考虑客户端(浏览器)和服务端之间高频和低延时消息交换的机制。</li>
<li>一般的应用场景有：在线交易、网页聊天、游戏、协作、数据可视化等。</li>
</ul>
</li>
<li>AOP:支持面向切面编程。它包含以下模块：<ul>
<li>AOP通过配置管理特性，Spring AOP 模块直接将面向方面的编程功能集成到了 Spring 框架中。所以，可以很容易地使 Spring 框架管理的任何对象支持 AOP。</li>
<li>Spring AOP 模块为基于 Spring 的应用程序中的对象提供了事务管理服务。通过使用 Spring AOP，不用依赖 EJB 组件，就可以将声明性事务管理集成到应用程序中。</li>
</ul>
</li>
<li>JMS</li>
<li>Test</li>
<li>Messaging</li>
</ul>
</li>
<li>使用 Spring 框架能带来哪些好处？<ul>
<li>DI ：依赖注入，使得构造器和 JavaBean、properties 文件中的依赖关系一目了然。</li>
<li>轻量级：与 EJB 容器相比较，IoC 容器更加趋向于轻量级。这样一来 IoC 容器在有限的内存和 CPU 资源的情况下，进行应用程序的开发和发布就变得十分有利。</li>
<li>面向切面编程(AOP)： Spring 支持面向切面编程，同时把应用的业务逻辑与系统的服务分离开来。</li>
<li>集成主流框架：Spring 并没有闭门造车，Spring 集成了已有的技术栈，比如 ORM 框架、Logging 日期框架、J2EE、Quartz 和 JDK Timer ，以及其他视图技术。</li>
<li>模块化：Spring 框架是按照模块的形式来组织的。由包和类的命名，就可以看出其所属的模块，开发者仅仅需要选用他们需要的模块即可。</li>
<li>便捷的测试：要 测试一项用Spring开发的应用程序 十分简单，因为测试相关的环境代码都已经囊括在框架中了。更加简单的是，利用 JavaBean 形式的 POJO 类，可以很方便的利用依赖注入来写入测试数据。</li>
<li>Web 框架：Spring 的 Web 框架亦是一个精心设计的 Web MVC 框架，为开发者们在 Web 框架的选择上提供了一个除了主流框架比如 Struts 、过度设计的、不流行 Web 框架的以外的有力选项。</li>
<li>事务管理：Spring 提供了一个便捷的事务管理接口，适用于小型的本地事物处理（比如在单 DB 的环境下）和复杂的共同事物处理（比如利用 JTA 的复杂 DB 环境）。</li>
</ul>
</li>
<li>Spring 框架中都用到了哪些设计模式？<ul>
<li>代理模式 — 在 AOP 和 remoting 中被用的比较多。</li>
<li>单例模式 — 在 Spring 配置文件中定义的 Bean 默认为单例模式。</li>
<li>模板方法 — 用来解决代码重复的问题。比如 RestTemplate、JmsTemplate、JdbcTemplate 。</li>
<li>前端控制器 — Spring提供了 DispatcherServlet 来对请求进行分发。</li>
<li>依赖注入 — 贯穿于 BeanFactory / ApplicationContext 接口的核心理念。</li>
<li>工厂模式 — BeanFactory 用来创建对象的实例。</li>
</ul>
</li>
</ol>
<h2 id="Spring-IoC-相关的面试题"><a href="#Spring-IoC-相关的面试题" class="headerlink" title="Spring IoC 相关的面试题"></a>Spring IoC 相关的面试题</h2><ol>
<li>什么是 Spring IoC 容器？<pre><code> - ![](https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15849306616719.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10)
</code></pre>
<ul>
<li>Spring 框架的核心是 Spring IoC 容器。容器创建 Bean 对象，将它们装配在一起，配置它们并管理它们的完整生命周期。<ul>
<li>Spring 容器使用依赖注入来管理组成应用程序的 Bean 对象。</li>
<li>容器通过读取提供的配置元数据 Bean Definition 来接收对象进行实例化，配置和组装的指令。</li>
<li>该配置元数据 Bean Definition 可以通过 XML，Java 注解或 Java Config 代码提供。</li>
</ul>
</li>
</ul>
</li>
<li>什么是依赖注入？<ul>
<li>在依赖注入中，你不必主动、手动创建对象，但必须描述如何创建它们。你不是直接在代码中将组件和服务连接在一起，而是描述配置文件中哪些组件需要哪些服务。</li>
<li>然后，再由 IoC 容器将它们装配在一起。</li>
</ul>
</li>
<li>IoC 和 DI 有什么区别？<ul>
<li> IOC就是由 Spring IOC 容器来负责对象的生命周期和对象之间的关系；容器控制应用程序，由容器反向的向应用程序注入应用程序所需要的外部资源。</li>
<li> DI应用程序依赖容器创建并注入它所需要的外部资源；</li>
</ul>
</li>
<li>可以通过多少种方式完成依赖注入？<ul>
<li>可以通过多少种方式完成依赖注入？<ul>
<li>接口注入</li>
<li>构造函数注入</li>
<li>setter 注入</li>
</ul>
</li>
<li>实际场景下，setting 注入使用的更多。</li>
</ul>
</li>
<li>Spring 中有多少种 IoC 容器？<ul>
<li>Spring 提供了两种( 不是“个” ) IoC 容器，分别是 BeanFactory、ApplicationContext <ul>
<li>BeanFactory：spring-beans 项目提供，就像一个包含 Bean 集合的工厂类。它会在客户端要求时实例化 Bean 对象。</li>
<li>ApplicationContext：接口扩展了 BeanFactory 接口，它在 BeanFactory 基础上提供了一些额外的功能。内置如下功能：<ul>
<li>MessageSource ：管理 message ，实现国际化等功能。</li>
<li>ApplicationEventPublisher ：事件发布。</li>
<li>ResourcePatternResolver ：多资源加载。</li>
<li>EnvironmentCapable ：系统 Environment（profile + Properties）相关。</li>
<li>Lifecycle ：管理生命周期。</li>
<li>Closable ：关闭，释放资源</li>
<li>InitializingBean：自定义初始化。</li>
<li>BeanNameAware：设置 beanName 的 Aware 接口。</li>
<li>常用WebApplicationContext ClassPathXmlApplicationContext</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>请介绍下常用的 BeanFactory 容器？<ul>
<li>BeanFactory 最常用的是 XmlBeanFactory 。它可以根据 XML 文件中定义的内容，创建相应的 Bean。</li>
<li>ListableBeanFactory:实现了枚举方法可以列举出当前 BeanFactory 中所有的 bean 对象而不必根据 name 一个一个的获取。</li>
</ul>
</li>
<li>请介绍下常用的 ApplicationContext 容器？<ul>
<li>ClassPathXmlApplicationContext ：从 ClassPath 的 XML 配置文件中读取上下文，并生成上下文定义。应用程序上下文从程序环境变量中取得。示例代码如下：<pre><code>  ```java ApplicationContext context = new ClassPathXmlApplicationContext(“bean.xml”);``` 
</code></pre>
</li>
<li>FileSystemXmlApplicationContext ：由文件系统中的XML配置文件读取上下文。示例代码如下：<pre><code>  ```java ApplicationContext context = new FileSystemXmlApplicationContext(“bean.xml”); ```
</code></pre>
</li>
<li>Spring Boot 使用的是ConfigServletWebServerApplicationContext ApplicationContext 容器，。</li>
</ul>
</li>
<li>列举一些 IoC 的一些好处？<ul>
<li>它将最小化应用程序中的代码量。</li>
<li>它以最小的影响和最少的侵入机制促进松耦合。</li>
<li>它支持即时的实例化和延迟加载 Bean 对象。</li>
<li>它将使应用程序易于测试，因为它不需要单元测试用例中的任何单例或 JNDI 查找机制。</li>
</ul>
</li>
<li>简述 Spring IoC 的实现机制？<ul>
<li>Spring 中的 IoC 的实现原理，就是工厂模式加反射机制</li>
</ul>
</li>
</ol>
<h2 id="Spring-Bean"><a href="#Spring-Bean" class="headerlink" title="Spring Bean"></a>Spring Bean</h2><ol>
<li><p>什么是 Spring Bean ？</p>
<ul>
<li>Bean 由 Spring IoC 容器实例化，配置，装配和管理。</li>
<li>Bean 是基于用户提供给 IoC 容器的配置元数据 Bean Definition 创建。</li>
</ul>
</li>
<li><p>Spring 有哪些配置Bean的方式</p>
<ul>
<li>XML 配置文件。</li>
<li>注解配置。</li>
<li>Java Config 配置，使用 @Bean 和 @Configuration 来实现。</li>
<li>具体举例<ul>
<li>Dubbo 服务的配置，使用 XML 。</li>
<li>Spring MVC 请求的配置，艿艿喜欢使用 @RequestMapping 注解。</li>
<li>Spring MVC 拦截器的配置，艿艿喜欢 Java Config 配置。</li>
<li>Spring Boot 以Java Config 配置为主。</li>
</ul>
</li>
</ul>
</li>
<li><p>Spring 支持几种 Bean Scope ？</p>
<ul>
<li>Singleton - 每个 Spring IoC 容器仅有一个单 Bean 实例。默认</li>
<li>Prototype - 每次请求都会产生一个新的实例。</li>
<li>Request - 每一次 HTTP 请求都会产生一个新的 Bean 实例，并且该 Bean 仅在当前 HTTP 请求内有效。</li>
<li>Session - 每一个的 Session 都会产生一个新的 Bean 实例，同时该 Bean 仅在当前 HTTP Session 内有效。</li>
<li>Application - 每一个 Web Application 都会产生一个新的 Bean ，同时该 Bean 仅在当前 Web Application 内有效。</li>
</ul>
</li>
<li><p>Spring Bean 在容器的生命周期是什么样的？</p>
<ul>
<li>Spring Bean 的初始化流程如下：<ul>
<li>实例化 Bean 对象<ul>
<li>Spring 容器根据配置中的 Bean Definition(定义)中实例化 Bean 对象。(Bean Definition 可以通过 XML，Java 注解或 Java Config 代码提供)。</li>
<li>Spring 使用依赖注入填充所有属性，如 Bean 中所定义的配置。</li>
</ul>
</li>
<li>Aware 相关的属性，注入到 Bean 对象<ul>
<li>如果 Bean 实现 BeanNameAware 接口，则工厂通过传递 Bean 的 beanName 来调用 #setBeanName(String name) 方法。</li>
<li>如果 Bean 实现 BeanFactoryAware 接口，工厂通过传递自身的实例来调用 #setBeanFactory(BeanFactory beanFactory) 方法。</li>
</ul>
</li>
<li>调用相应的方法，进一步初始化 Bean 对象<ul>
<li>如果存在与 Bean 关联的任何 BeanPostProcessor 们，则调用 #preProcessBeforeInitialization(Object bean, String beanName) 方法。</li>
<li>如果 Bean 实现 InitializingBean 接口，则会调用 #afterPropertiesSet() 方法。</li>
<li>如果为 Bean 指定了 init 方法（例如 <bean /> 的 init-method 属性），那么将调用该方法。</li>
<li>如果存在与 Bean 关联的任何 BeanPostProcessor 们，则将调用 #postProcessAfterInitialization(Object bean, String beanName) 方法。</li>
</ul>
</li>
</ul>
</li>
<li>Spring Bean 的销毁流程如下：<ul>
<li>如果 Bean 实现 DisposableBean 接口，当 spring 容器关闭时，会调用 #destroy() 方法。</li>
<li>如果为 bean 指定了 destroy 方法（例如 <bean /> 的 destroy-method 属性），那么将调用该方法。<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15849331702416.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>什么是 Spring 装配？</p>
<ul>
<li>装配，和上文提到的 DI 依赖注入，实际是一个东西。</li>
</ul>
</li>
<li><p>自动装配有哪些方式？</p>
<ul>
<li>Spring 容器能够自动装配 Bean 。也就是说，可以通过检查 BeanFactory 的内容让 Spring 自动解析 Bean 的协作者。</li>
<li>自动装配的不同模式：<ul>
<li>no - 这是默认设置，表示没有自动装配。应使用显式 Bean 引用进行装配。</li>
<li>byName - 它根据 Bean 的名称注入对象依赖项。它匹配并装配其属性与 XML 文件中由相同名称定义的 Bean 。</li>
<li>【最常用】byType - 它根据类型注入对象依赖项。如果属性的类型与 XML 文件中的一个 Bean 类型匹配，则匹配并装配属性。</li>
<li>构造函数 - 它通过调用类的构造函数来注入依赖项。它有大量的参数。</li>
<li>autodetect - 首先容器尝试通过构造函数使用 autowire 装配，如果不能，则尝试通过 byType 自动装配。</li>
</ul>
</li>
</ul>
</li>
<li><p>解释什么叫延迟加载？</p>
<ul>
<li>默认情况下，容器启动之后会将所有作用域为单例的 Bean 都创建好，但是有的业务场景我们并不需要它提前都创建好。此时，我们可以在Bean 中设置 lzay-init = “true” 。</li>
<li>这样，当容器启动之后，作用域为单例的 Bean ，就不在创建。而是在获得该 Bean 时，才真正在创建加载。</li>
</ul>
</li>
<li><p>Spring 框架中的单例 Bean 是线程安全的么？</p>
<ul>
<li>Spring 框架并没有对单例 Bean 进行任何多线程的封装处理。</li>
</ul>
</li>
<li><p>Spring Bean 怎么解决循环依赖的问题？</p>
<ul>
<li><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15849337079754.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>Spring 在创建 bean 的时候并不是等它完全完成，而是在创建过程中将创建中的 bean 的 ObjectFactory 提前曝光（即加入到 singletonFactories 缓存中）。</p>
</li>
<li><p>这样，一旦下一个 bean 创建的时候需要依赖 bean ，则直接使用 ObjectFactory 的 #getObject() 方法来获取了。</p>
</li>
<li><p>实例如 A 依赖 B，B 依赖 C，C 依赖 A：</p>
<ul>
<li>首先 A 完成初始化第一步并将自己提前曝光出来（通过 ObjectFactory 将自己提前曝光），在初始化的时候，发现自己依赖对象 B，此时就会去尝试 get(B)，这个时候发现 B 还没有被创建出来</li>
<li>然后 B 就走创建流程，在 B 初始化的时候，同样发现自己依赖 C，C 也没有被创建出来</li>
<li>这个时候 C 又开始初始化进程，但是在初始化的过程中发现自己依赖 A，于是尝试 get(A)，这个时候由于 A 已经添加至缓存中（一般都是添加至三级缓存 singletonFactories ），通过 ObjectFactory 提前曝光，所以可以通过 ObjectFactory#getObject() 方法来拿到 A 对象，C 拿到 A 对象后顺利完成初始化，然后将自己添加到一级缓存中</li>
<li>回到 B ，B 也可以拿到 C 对象，完成初始化，A 可以顺利拿到 B 完成初始化。到这里整个链路就已经完成了初始化过程了</li>
</ul>
</li>
</ul>
</li>
<li><p>BeanFactory和FactoryBean的区别</p>
<ul>
<li>BeanFactory是个Factory，也就是IOC容器或对象工厂</li>
<li>FactoryBean是个Bean。</li>
<li>Spring中所有的Bean都是由BeanFactory(也就是IOC容器)来进行管理的。但对FactoryBean而言，这个Bean不是简单的Bean，而是一个能生产或者修饰对象生成的工厂Bean,它的实现与设计模式中的工厂模式和修饰器模式类似 <h2 id="Spring-注解"><a href="#Spring-注解" class="headerlink" title="Spring 注解"></a>Spring 注解</h2></li>
</ul>
</li>
<li><p>什么是基于注解的容器配置？</p>
<ul>
<li>不使用 XML 来描述 Bean 装配，开发人员通过在相关的类，方法或字段声明上使用注解将配置移动到组件类本身。它可以作为 XML 设置的替代方案</li>
<li>以Java Config 配置Bean的方式。</li>
</ul>
</li>
<li><p>如何在 Spring 中启动注解装配？</p>
<ul>
<li><code>&lt;context：annotation-config /&gt;</code></li>
<li>Spring Boot默认情况下已经开启。</li>
</ul>
</li>
<li><p>@Component, @Controller, @Repository, @Service 有何区别？</p>
<ul>
<li>@Component ：它将 Java 类标记为 Bean 。它是任何 Spring 管理组件的通用构造型。</li>
<li>@Controller ：它将一个类标记为 Spring Web MVC 控制器。</li>
<li>@Service ：此注解是组件注解的特化。它不会对 @Component 注解提供任何其他行为。您可以在服务层类中使用 @Service 而不是 @Component ，因为它以更好的方式指定了意图。</li>
<li>@Repository ：这个注解是具有类似用途和功能的 @Component 注解的特化。它为 DAO 提供了额外的好处。它将 DAO 导入 IoC 容器，并使未经检查的异常有资格转换为 Spring DataAccessException 。</li>
</ul>
</li>
<li><p>@Required 注解有什么用？</p>
<ul>
<li>此注解仅指示必须在配置时使用 Bean 定义中的显式属性值或使用自动装配填充受影响的 Bean 属性。</li>
<li>如果尚未填充受影响的 Bean 属性，则容器将抛出 BeanInitializationException 异常。</li>
</ul>
</li>
<li><p>@Autowired 注解有什么用？</p>
<ul>
<li>@Autowired 注解，可以更准确地控制应该在何处以及如何进行自动装配。<ul>
<li>此注解用于在 setter 方法，构造函数，具有任意名称或多个参数的属性或方法上自动装配 Bean。</li>
<li>默认情况下，它是类型驱动的注入。</li>
</ul>
</li>
</ul>
</li>
<li><p>@Qualifier 注解有什么用？</p>
<ul>
<li>当你创建多个相同类型的 Bean ，并希望仅使用属性装配其中一个 Bean 时，您可以使用 @Qualifier 注解和 @Autowired 通过指定 ID 应该装配哪个确切的 Bean 来消除歧义。</li>
</ul>
</li>
<li><p>@Autowired注解与@Resource注解的区别</p>
<ul>
<li>@Autowired为Spring提供的注解，需要导入包org.springframework.beans.factory.annotation.Autowired;只按照byType注入。</li>
<li>@Autowired注解是按照类型（byType）装配依赖对象，默认情况下它要求依赖对象必须存在，如果允许null值，可以设置它的required属性为false。如果我们想使用按照名称（byName）来装配，可以结合@Qualifier注解一起使用。(通过类型匹配找到多个candidate,在没有@Qualifier、@Primary注解的情况下，会使用对象名作为最后的fallback匹配)</li>
<li>@Resource默认按照ByName自动注入，由J2EE提供，需要导入包javax.annotation.Resource。@Resource有两个重要的属性：name和type，而Spring将@Resource注解的name属性解析为bean的名字，而type属性则解析为bean的类型。所以，如果使用name属性，则使用byName的自动注入策略，而使用type属性时则使用byType自动注入策略。如果既不制定name也不制定type属性，这时将通过反射机制使用byName自动注入策略。</li>
<li>@Resource装配顺序：<ul>
<li>①如果同时指定了name和type，则从Spring上下文中找到唯一匹配的bean进行装配，找不到则抛出异常。</li>
<li>②如果指定了name，则从上下文中查找名称（id）匹配的bean进行装配，找不到则抛出异常。</li>
<li>③如果指定了type，则从上下文中找到类似匹配的唯一bean进行装配，找不到或是找到多个，都会抛出异常。</li>
<li>④如果既没有指定name，又没有指定type，则自动按照byName方式进行装配；如果没有匹配，则回退为一个原始类型进行匹配，如果匹配则自动装配。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Spring-AOP"><a href="#Spring-AOP" class="headerlink" title="Spring AOP"></a>Spring AOP</h2><ol>
<li>什么是 AOP ？<ul>
<li>AOP(Aspect-Oriented Programming)，即面向切面编程, 它与 OOP( Object-Oriented Programming, 面向对象编程) 相辅相成， 提供了与 OOP 不同的抽象软件结构的视角。</li>
<li>在 OOP 中，以类( Class )作为基本单元</li>
<li>在 AOP 中，以切面( Aspect )作为基本单元。</li>
</ul>
</li>
<li>什么是 Aspect ？<ul>
<li>Aspect 由 PointCut 和 Advice 组成,@Aspect 注解的类就是切面。<ul>
<li>它既包含了横切逻辑的定义，也包括了连接点的定义。</li>
<li>Spring AOP 就是负责实施切面的框架，它将切面所定义的横切逻辑编织到切面所指定的连接点中。</li>
<li>AOP 的工作重心在于如何将增强编织目标对象的连接点上, 这里包含两个工作:<ul>
<li>如何通过 PointCut 和 Advice 定位到特定的 JoinPoint 上。</li>
<li>如何在 Advice 中编写切面代码。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>什么是 JoinPoint ?<ul>
<li>JoinPoint ，切点，程序运行中的一些时间点, 例如：<ul>
<li> 一个方法的执行。</li>
<li> 或者是一个异常的处理。</li>
</ul>
</li>
<li> 在 Spring AOP 中，JoinPoint 总是方法的执行点。</li>
</ul>
</li>
<li>什么是 PointCut ？<ul>
<li>PointCut 是匹配 JoinPoint 的条件。</li>
<li>Advice 是和特定的 PointCut 关联的，并且在 PointCut 相匹配的 JoinPoint 中执行。即 Advice =&gt; PointCut =&gt; JoinPoint 。</li>
<li>在 Spring 中, 所有的方法都可以认为是 JoinPoint ，但是我们并不希望在所有的方法上都添加 Advice 。而 PointCut 的作用，就是提供一组规则(使用 AspectJ PointCut expression language 来描述) 来匹配 JoinPoint ，给满足规则的 JoinPoint 添加 Advice 。</li>
</ul>
</li>
<li>关于 JoinPoint 和 PointCut 的区别<ul>
<li>首先，Advice 通过 PointCut 查询需要被织入的 JoinPoint 。</li>
<li>然后，Advice 在查询到 JoinPoint 上执行逻辑。</li>
</ul>
</li>
<li>什么是 Advice ？<ul>
<li>特定 JoinPoint 处的 Aspect 所采取的动作称为 Advice 。</li>
<li>Spring AOP 使用一个 Advice 作为拦截器，在 JoinPoint “周围”维护一系列的拦截器。</li>
</ul>
</li>
<li>有哪些类型的 Advice？<ul>
<li>1.@Before前置通知在切入点运行前执行，不会影响切入点的逻辑</li>
<li>2.@After后置通知在切入点正常运行结束后执行，如果切入点抛出异常，则在抛出异常前执行</li>
<li>3.@AfterThrowing异常通知:在切入点抛出异常前执行，如果切入点正常运行（未抛出异常），则不执行</li>
<li>4.@AfterReturning返回通知:在切入点正常运行结束后执行，如果切入点抛出异常，则不执行</li>
<li>5.@Around环绕通知是功能最强大的通知，可以在切入点执行前后自定义一些操作。环绕通知需要负责决定是继续处理join point(调用ProceedingJoinPoint的proceed方法)还是中断执行</li>
</ul>
</li>
<li>什么是 Target ？<ul>
<li>Target ，织入 Advice 的目标对象。目标对象也被称为 Advised Object 。<ul>
<li>因为 Spring AOP 使用运行时代理的方式来实现 Aspect ，因此 Advised Object 总是一个代理对象(Proxied Object) 。</li>
<li>注意, Advised Object 指的不是原来的对象，而是织入 Advice 后所产生的代理对象。</li>
<li>Advice + Target Object = Advised Object = Proxy 。</li>
</ul>
</li>
</ul>
</li>
<li>AOP 有哪些实现方式？<ul>
<li>静态代理 - 指使用 AOP 框架提供的命令进行编译，从而在编译阶段就可生成 AOP 代理类，因此也称为编译时增强。<ul>
<li>例如，SkyWalking 基于 Java Agent 机制，配置上 ByteBuddy 库，实现类加载时编织时增强，从而实现链路追踪的透明埋点。</li>
</ul>
</li>
<li>动态代理 - 在运行时在内存中“临时”生成 AOP 动态代理类，因此也被称为运行时增强。目前 Spring 中使用了两种动态代理库：<ul>
<li>JDK 动态代理</li>
<li>CGLIB</li>
</ul>
</li>
</ul>
</li>
<li>Spring 如何使用 AOP 切面？<ul>
<li>基于 XML 方式的切面实现。</li>
<li>基于 注解 方式的切面实现。</li>
</ul>
</li>
</ol>
<h2 id="Spring-Transaction-相关的面试题"><a href="#Spring-Transaction-相关的面试题" class="headerlink" title="Spring Transaction 相关的面试题"></a>Spring Transaction 相关的面试题</h2><ol>
<li>列举 Spring 支持的事务管理类型？<ul>
<li>声明式事务：通过使用注解或基于 XML 的配置事务，从而事务管理与业务代码分离。</li>
<li>编程式事务：通过编码的方式实现事务管理，需要在代码中显式的调用事务的获得、提交、回滚。它为您提供极大的灵活性，但维护起来非常困难。</li>
</ul>
</li>
<li>Spring 事务如何和不同的数据持久层框架做集成？<ul>
<li>Spring 事务的管理，是通过org.springframework.transaction.PlatformTransactionManager 进行管理<ul>
<li>PlatformTransactionManager 是负责事务管理的接口，一共有三个接口方法，分别负责事务的获得、提交、回滚。<ul>
<li><code>getTransaction(TransactionDefinition definition)</code> 方法，根据事务定义 TransactionDefinition ，获得 TransactionStatus 。<ul>
<li>为什么不是创建事务？因为如果当前如果已经有事务，则不会进行创建，一般来说会跟当前线程进行绑定。如果不存在事务，则进行创建。</li>
<li>为什么返回的是 TransactionStatus 对象？在 TransactionStatus 中，不仅仅包含事务属性，还包含事务的其它信息，例如是否只读、是否为新创建的事务等等。</li>
</ul>
</li>
<li><code>commit(TransactionStatus status)</code> 方法，根据 TransactionStatus 情况，提交事务。<ul>
<li>为什么根据 TransactionStatus 情况，进行提交？<ul>
<li>例如，带@Transactional 注解的的 A 方法，会调用 @Transactional 注解的的 B 方法。</li>
<li>在 B 方法结束调用后，会执行 PlatformTransactionManager#commit(TransactionStatus status) 方法，此处事务是不能、也不会提交的。</li>
<li>而是在 A 方法结束调用后，执行 PlatformTransactionManager#commit(TransactionStatus status) 方法，提交事务。</li>
</ul>
</li>
</ul>
</li>
<li><code>rollback(TransactionStatus status)</code> 方法，根据 TransactionStatus 情况，回滚事务。<ul>
<li>为什么根据 TransactionStatus 情况，进行回滚？原因同 #commit(TransactionStatus status) 方法。</li>
</ul>
</li>
</ul>
</li>
<li>PlatformTransactionManager 有抽象子类 <code>org.springframework.transaction.support.AbstractPlatformTransactionManager</code> ，基于 模板方法模式 ，实现事务整体逻辑的骨架，而抽象 <code>doCommit(DefaultTransactionStatus status)</code>、<code>doRollback(DefaultTransactionStatus status)</code> 等等方法，交由子类类来实现</li>
<li>不同的数据持久层框架，会有其对应的 PlatformTransactionManager 实现类<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15849356731423.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>所有的实现类，都基于 AbstractPlatformTransactionManager 这个骨架类。</li>
<li>HibernateTransactionManager ，和 Hibernate5 的事务管理做集成。</li>
<li>DataSourceTransactionManager ，和 JDBC 的事务管理做集成。所以，它也适用于 MyBatis、Spring JDBC 等等。</li>
<li>JpaTransactionManager ，和 JPA 的事务管理做集成。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>为什么在 Spring 事务中不能切换数据源？<ul>
<li>在 Spring 的事务管理中，所使用的数据库连接会和当前线程所绑定，即使我们设置了另外一个数据源，使用的还是当前的数据源连接。</li>
<li>多个数据源且需要事务的场景，本身会带来多事务一致性的问题，暂时没有特别好的解决方案。</li>
<li>所以一般一个应用，推荐除非了读写分离所带来的多数据源，其它情况下，建议只有一个数据源。并且，随着微服务日益身形，一个服务对应一个 DB 是比较常见的架构选择。</li>
</ul>
</li>
<li>@Transactional 注解有哪些属性？如何使用？<ul>
<li>@Transactional 注解的属性如下：<table>
<thead>
<tr>
<th>属性</th>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>value</td>
<td>String</td>
<td>可选的限定描述符，指定使用的事务管理器</td>
</tr>
<tr>
<td>propagation</td>
<td>enum: Propagation</td>
<td>可选的事务传播行为设置</td>
</tr>
<tr>
<td>isolation</td>
<td>enum: Isolation</td>
<td>可选的事务隔离级别设置</td>
</tr>
<tr>
<td>readOnly</td>
<td>boolean</td>
<td>读写或只读事务，默认读写</td>
</tr>
<tr>
<td>timeout</td>
<td>int (in seconds granularity)</td>
<td>事务超时时间设置</td>
</tr>
<tr>
<td>rollbackFor</td>
<td>Class对象数组，必须继承自Throwable</td>
<td>导致事务回滚的异常类数组</td>
</tr>
<tr>
<td>rollbackForClassName</td>
<td>类名数组，必须继承自Throwable</td>
<td>导致事务回滚的异常类名字数组</td>
</tr>
<tr>
<td>noRollbackFor</td>
<td>Class对象数组，必须继承自Throwable</td>
<td>不会导致事务回滚的异常类数组</td>
</tr>
<tr>
<td>noRollbackForClassName</td>
<td>类名数组，必须继承自Throwable</td>
<td>不会导致事务回滚的异常类名字数组</td>
</tr>
</tbody></table>
<ul>
<li>具体用法如下：<ul>
<li>@Transactional 可以作用于接口、接口方法、类以及类方法上。当作用于类上时，该类的所有 public 方法将都具有该类型的事务属性，同时，我们也可以在方法级别使用该标注来覆盖类级别的定义。</li>
<li>虽然 @Transactional 注解可以作用于接口、接口方法、类以及类方法上，但是 Spring 建议不要在接口或者接口方法上使用该注解，因为这只有在使用基于接口的代理时它才会生效。另外， @Transactional 注解应该只被应用到 public 方法上，这是由 Spring AOP 的本质决定的。如果你在 protected、private 或者默认可见性的方法上使用 @Transactional 注解，这将被忽略，也不会抛出任何异常。这一点，非常需要注意。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>什么是事务的传播级别？分成哪些传播级别？<ul>
<li>事务的传播行为，指的是当前带有事务配置的方法，需要怎么处理事务。  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ========== 支持当前事务的情况 ========== </span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果当前存在事务，则使用该事务。</span></span><br><span class="line"><span class="comment"> * 如果当前没有事务，则创建一个新的事务。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> PROPAGATION_REQUIRED = <span class="number">0</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果当前存在事务，则使用该事务。</span></span><br><span class="line"><span class="comment"> * 如果当前没有事务，则以非事务的方式继续运行。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> PROPAGATION_SUPPORTS = <span class="number">1</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果当前存在事务，则使用该事务。</span></span><br><span class="line"><span class="comment"> * 如果当前没有事务，则抛出异常。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> PROPAGATION_MANDATORY = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ========== 不支持当前事务的情况 ========== </span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个新的事务。</span></span><br><span class="line"><span class="comment"> * 如果当前存在事务，则把当前事务挂起。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> PROPAGATION_REQUIRES_NEW = <span class="number">3</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 以非事务方式运行。</span></span><br><span class="line"><span class="comment"> * 如果当前存在事务，则把当前事务挂起。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> PROPAGATION_NOT_SUPPORTED = <span class="number">4</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 以非事务方式运行。</span></span><br><span class="line"><span class="comment"> * 如果当前存在事务，则抛出异常。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> PROPAGATION_NEVER = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ========== 其他情况 ========== </span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行。</span></span><br><span class="line"><span class="comment"> * 如果当前没有事务，则等价于 &#123;<span class="doctag">@link</span> TransactionDefinition#PROPAGATION_REQUIRED&#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> PROPAGATION_NESTED = <span class="number">6</span>;</span><br><span class="line">        </span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>什么是事务的超时属性？<ul>
<li>所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。</li>
<li>在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒。</li>
</ul>
</li>
<li>什么是事务的只读属性？<ul>
<li>事务的只读属性是指，对事务性资源进行只读操作或者是读写操作。</li>
<li>所谓事务性资源就是指那些被事务管理的资源，比如数据源、JMS 资源，以及自定义的事务性资源等等。</li>
<li>如果确定只对事务性资源进行只读操作，那么我们可以将事务标志为只读的，以提高事务处理的性能。</li>
</ul>
</li>
<li>什么是事务的回滚规则？<ul>
<li>回滚规则，定义了哪些异常会导致事务回滚而哪些不会。</li>
<li>默认情况下，事务只有遇到运行期异常时才会回滚，而在遇到检查型异常时不会回滚（这一行为与EJB的回滚行为是一致的）。</li>
<li>但是你可以声明事务在遇到特定的检查型异常时像遇到运行期异常那样回滚。同样，你还可以声明事务遇到特定的异常不回滚，即使这些异常是运行期异常。</li>
<li>注意，事务的回滚规则，并不是数据库事务规范中的名词，而是 Spring 自身所定义的。</li>
</ul>
</li>
<li>简单介绍 TransactionStatus ？ <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// TransactionStatus.java</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">TransactionStatus</span> <span class="keyword">extends</span> <span class="title">SavepointManager</span>, <span class="title">Flushable</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 是否是新创建的事务</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">isNewTransaction</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 是否有 Savepoint</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 在 &#123;<span class="doctag">@link</span> TransactionDefinition#PROPAGATION_NESTED&#125; 传播级别使用。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">hasSavepoint</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 设置为只回滚</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setRollbackOnly</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 是否为只回滚</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">isRollbackOnly</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 执行 flush 操作</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">flush</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 是否事务已经完成</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">isCompleted</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>为什么没有事务对象呢？在 TransactionStatus 的实现类 DefaultTransactionStatus 中，有个 Object transaction 属性，表示事务对象。</li>
<li>isNewTransaction() 方法，表示是否是新创建的事务。有什么用呢？答案结合 「Spring 事务如何和不同的数据持久层框架做集成？」 问题，我们对 commit(TransactionStatus status) 方法的解释。通过该方法，我们可以判断，当前事务是否当前方法所创建的，只有创建事务的方法，才能且应该真正的提交事务。</li>
</ul>
</li>
<li>使用 Spring 事务有什么优点？<ul>
<li>通过 PlatformTransactionManager ，为不同的数据层持久框架提供统一的 API ，无需关心到底是原生 JDBC、Spring JDBC、JPA、Hibernate 还是 MyBatis 。</li>
<li>通过使用声明式事务，使业务代码和事务管理的逻辑分离，更加清晰。</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>interview</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title>JVM</title>
    <url>/2021/11/07/JVM/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="JVM"><a href="#JVM" class="headerlink" title="JVM"></a>JVM</h1><ol>
<li><p>JVM 由哪些部分组成？</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847504847911.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847532504574.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>类加载器：在JVM启动时或者类运行时将需要的class加载到JVM。</li>
<li>运行时数据区：将内存划分为若干个区以模拟实际机器上的存储，记录，调度功能模块。</li>
<li>执行引擎：执行引擎的任务是负责执行 class 文件中包含的字节码指令，相当于实际机器上的 CPU。</li>
<li>本地方法调用：调用 C 或 C++ 实现的本地方法的代码返回结果。</li>
</ul>
</li>
<li><p>JVM 运行内存（运行时数据区）的分类？</p>
<ul>
<li>程序计数器（线程私有）：记录正在执行的Java方法的字节码指令地址，唯一没有OutOfMemoryError情况的区域</li>
<li>栈内存（线程私有）：描述Java方法执行时的内存模型 <ul>
<li>每个方法在执行的时候，都会创建一个栈帧用于存储局部变量，操作数，动态链接，方法出口等信息</li>
<li>每个方法调用都意味着一个栈帧再虚拟机栈中入栈到出栈的过程</li>
<li>局部变量表：基本数据类型（boolean,byte,short,int,long,float,double,char），对象引用（reference类型，不等同与对象，是指针或者资源地址），returnAddress类型（指向一条字节码指令的位置）</li>
<li>线程执行栈深度超出限制，跑出StackOverFlowError</li>
</ul>
</li>
<li>本地方法栈：<ul>
<li>和 Java 虚拟机栈的作用类似，区别是该区域为 JVM 提供使用 Native 方法的服务</li>
</ul>
</li>
<li>堆内存（线程共享）：所有线程共享的一块区域，存放对象实例，垃圾收集器管理的主要区域。<ul>
<li>目前主要的垃圾回收算法都是分代收集算法，所以 Java 堆中还可以细分为：新生代和老年代；再细致一点的有 Eden 空间、From Survivor 空间、To Survivor 空间等，默认情况下新生代按照 8:1:1 的比例来分配。</li>
<li>Java 堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘一样。</li>
<li>可固定，可拓展通过-Xms -Xmx控制</li>
<li>堆内存无法分配内存，且无法拓展，抛出OutOfMemory</li>
</ul>
</li>
<li>方法区（线程共享）：主要用于已被虚拟机加载的类信息、静态变量、常量、JIT编译后的代码<ul>
<li>JDK 1.8 的对 JVM 架构的改造将类元数据放到本地内存中，另外，将常量池和静态变量放到 Java 堆里</li>
<li>JDK 1.7 <code>java.lang.OutOfMemoryError: PermGen space</code> -XX:MaxPermSize -XX:PermSize</li>
<li>JDK 1.8 -XX:MetaspaceSize，初始空间大小;-XX:MaxMetaspaceSize，最大空间，默认是没有限制的</li>
<li>运行时常量池</li>
</ul>
<blockquote>
<p><a href="https://www.cnblogs.com/paddix/p/5309550.html">Java8内存模型—永久代(PermGen)和元空间(Metaspace)</a></p>
</blockquote>
</li>
</ul>
</li>
<li><p>直接内存是不是虚拟机运行时数据区的一部分？</p>
<ul>
<li>直接内存(Direct Memory)不是虚拟机运行时数据区的一部分</li>
<li>使用 native 函数库直接分配堆外内存，使用Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作，避免了在 Java 堆和 Native 堆中来回复制数据显著提高性能</li>
<li>本机直接内存的分配不会受到 Java 堆大小的限制，受到本机总内存大小限制。</li>
<li>配置虚拟机参数时，不要忽略直接内存，防止出现 OutOfMemoryError 异常。</li>
</ul>
</li>
<li><p>Java内存模型?</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15848403457744.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>定义程序中各个变量的访问规则</li>
<li>所有的变量都存储在主内存</li>
<li>线程有自己的工作内存，工作内存保存了该线程使用到的变量的主内存副本拷贝</li>
<li>线程间变量值的传递均需要通过主内存来实现</li>
<li>原子性: synchronized保证了原子性，提供了两个高级的字节码指令monitorenter和monitorexit</li>
<li>可见性：Java内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值的这种依赖主内存作为传递媒介的方式来实现的。可以使用synchronized，volatile， final实现</li>
<li>有序性：在Java中，可以使用synchronized和volatile来保证多线程之间操作的有序性；volatile关键字会禁止指令重排。synchronized关键字保证同一时刻只允许一条线程操作。</li>
</ul>
</li>
<li><p>直接内存（堆外内存）与堆内存比较？</p>
<ul>
<li>直接内存申请空间耗费更高的性能，当频繁申请到一定量时尤为明显。</li>
<li>直接内存 IO 读写的性能要优于普通的堆内存，在多次读写操作的情况下差异明显。</li>
</ul>
</li>
<li><p>为什么要废弃永久代？</p>
<ul>
<li>由于永久代内存经常不够用或发生内存泄露，爆出异常 java.lang.OutOfMemoryError: PermGen</li>
<li>字符串存在永久代中，容易出现性能问题和内存溢出。</li>
<li>类及方法的信息等比较难确定其大小，因此对于永久代的大小指定比较困难，太小容易出现永久代溢出，太大则容易导致老年代溢出。</li>
<li>永久代会为 GC 带来不必要的复杂度，并且回收效率偏低。</li>
</ul>
</li>
<li><p>Java 内存堆和栈区别？</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847579303911.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>栈内存存储基本类型的变量和对象的引用变量；堆内存用来存储Java中的对象，无论是成员变量，局部变量，还是类变量，它们指向的对象都存储在堆内存中。</li>
<li>栈内存归属于单个线程，每个线程都会有一个栈内存，其存储的变量只能在其所属线程中可见，即栈内存可以理解成线程的私有内存；堆内存中的对象对所有线程可见。堆内存中的对象可以被所有线程访问。</li>
<li>栈溢出 java.lang.StackOverFlowError；堆溢出java.lang.OutOfMemoryError</li>
<li>栈的内存要远远小于堆内存，如果你使用递归的话，那么你的栈很快就会充满。-Xss 选项设置栈内存的大小，-Xms 选项可以设置堆的开始时的大小。</li>
</ul>
<blockquote>
<p>JVM 中堆和栈属于不同的内存区域，使用目的也不同。栈常用于保存方法帧和局部变量，而对象总是在堆上分配。栈通常都比堆小，也不会在多个线程之间共享，而堆被整个 JVM 的所有线程共享。</p>
</blockquote>
</li>
<li><p>JAVA 对象创建的过程？</p>
<pre><code> - ![](https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847585045516.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10)                                                    
 - ![](https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847593821321.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10)        
</code></pre>
<ol>
<li>检测类是否被加载:当虚拟机遇到 new 指令时，首先先去检查这个指令的参数是否能在常量池(方法区-运行时常量池)中定位到一<strong>个类的符号引用</strong>，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，就执行类加载过程</li>
<li>为对象分配内存：<ul>
<li>内存空间绝对规整：虚拟机只需要在被占用的内存和可用空间之间移动指针即可，这种方式被称为“<strong>指针碰撞</strong>”。</li>
<li>内存不规整：虚拟机需要维护一个列表，来记录哪些内存是可用的。分配内存的时候需要找到一个可用的内存空间，然后在列表上记录下已被分配，这种方式成为“<strong>空闲列表</strong>”。</li>
</ul>
</li>
<li>为分配的内存空间初始化零值：对象的内存分配完成后，还需要将对象的内存空间都初始化为零值，这样能保证对象的实例字段即使没有赋初值，也可以直接使用。</li>
<li>对对象进行其他设置：在对象头中设置对象所属的类，类的元数据信息，对象的 hashcode ，GC 分代年龄等信息。</li>
<li>执行 init 方法：Java 在编译之后会在字节码文件中生成 init 方法，称之为实例构造器，该实例构造器会将语句块，变量初始化，调用父类的构造器等操作收敛到 init 方法中，收敛顺序为：<ol>
<li>父类变量初始化</li>
<li>父类语句块</li>
<li>父类构造函数</li>
<li>子类变量初始化</li>
<li>子类语句块</li>
<li>子类构造函数</li>
</ol>
</li>
</ol>
</li>
<li><p>A a = new A() 经历过什么过程?</p>
<blockquote>
<p>同上</p>
</blockquote>
</li>
<li><p>对象的内存布局是怎样的？JAVA对象模型？</p>
<ul>
<li>对象头：对象头包括两部分信息。<ul>
<li>第一部分，是存储对象自身的运行时数据，如哈希码，GC 分代年龄，锁状态标志，线程持有的锁等等。</li>
<li>第二部分，是类型指针，即对象指向类元数据的指针。</li>
</ul>
</li>
<li>实例数据：对象真正存储的有效信息</li>
<li>对齐填充：不是必然的存在，就是为了对齐。</li>
</ul>
</li>
<li><p>对象是如何定位访问的？</p>
<ul>
<li>对象的访问定位有两种：<ol>
<li>句柄定位：Java 堆会画出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息。<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847606523598.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
<li>直接指针访问：Java 堆对象的不居中就必须考虑如何放置访问类型数据的相关信息，而 reference 中存储的直接就是对象地址。<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847606809648.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
</ol>
</li>
<li>对比两种方式？<ul>
<li>使用句柄来访问的最大好处，就是 reference 中存储的是稳定的句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。</li>
<li>使用直接指针访问方式的最大好处就是速度更快，它节省了一次指针定位的时间开销。</li>
</ul>
</li>
</ul>
</li>
<li><p>有哪些 OutOfMemoryError 异常？</p>
<ul>
<li>除了程序计数器外，虚拟机内存的其它几个运行时区域都有发生的 OutOfMemoryError(简称为“OOM”) 异常的可能。</li>
<li>Java 堆溢出:不停new对象，保证GCRoot可达性</li>
<li>虚拟机栈和本地方法栈溢出：栈容量由 -Xss参数设定 递归</li>
<li>方法区和运行时常量池溢出 大量类<blockquote>
<p>从 JDK8 开始，就变成元数据区的内存溢出。</p>
</blockquote>
</li>
<li>元数据区的内存溢出:-XX:MaxMetaspaceSize=10m</li>
<li>本机直接内存溢出 申请对外内存</li>
</ul>
</li>
<li><p>当出现了内存溢出，怎么排错？</p>
<ol>
<li>首先，控制台查看错误日志。</li>
<li>使用 JDK 自带的 jvisualvm 工具查看系统的堆栈日志。<code>jstat</code> <code>jmap</code></li>
<li>定位出内存溢出的空间：堆，栈还是永久代（JDK8 以后不会出现永久代的内存溢出）。<ol>
<li>如果是堆内存溢出，看是否创建了超大的对象。</li>
<li>如果是栈内存溢出，看是否创建了超大的对象，或者产生了死循环。</li>
</ol>
</li>
</ol>
<blockquote>
<p><a href="https://www.jianshu.com/p/2fdee831ed03">Java内存溢出(OOM)异常完全指南</a></p>
</blockquote>
</li>
<li><p>Java 中会存在内存泄漏吗？</p>
<ul>
<li>Hibernate 的 Session（一级缓存）中的对象属于持久态，垃圾回收器是不会回收这些对象的，然而这些对象中可能存在无用的垃圾对象。</li>
<li>使用 Netty 的堆外的 ByteBuf 对象，在使用完后，并未归还，导致使用的一点一点在泄露。</li>
</ul>
</li>
</ol>
<h2 id="垃圾收集器与内存分配策略"><a href="#垃圾收集器与内存分配策略" class="headerlink" title="垃圾收集器与内存分配策略"></a>垃圾收集器与内存分配策略</h2><ol>
<li><p>什么是垃圾回收机制？</p>
<ul>
<li>Java 中对象是采用 new 或者反射的方法创建的，这些对象的创建都是在堆(Heap)中分配的，所有对象的回收都是由 Java 虚拟机通过垃圾回收机制完成的。GC 为了能够正确释放对象，会监控每个对象的运行状况，对他们的申请、引用、被引用、赋值等状况进行监控。</li>
<li>Java 程序员不用担心内存管理，因为垃圾收集器会自动进行管理。</li>
<li>可以调用下面的方法之一：<code>System.gc()</code> 或 <code>Runtime.getRuntime().gc()</code> ，但 JVM 也可以屏蔽掉显示的垃圾回收调用。</li>
</ul>
</li>
<li><p>为什么不建议在程序中显式的声明 System.gc() ？</p>
<ul>
<li>因为显式声明是做堆内存全扫描，也就是 Full GC ，是需要停止所有的活动的(Stop The World Collection)，对应用很大可能存在影响。</li>
<li>调用 System.gc() 方法后，不会立即执行 Full GC ，而是虚拟机自己决定的。</li>
</ul>
</li>
<li><p>如果一个对象的引用被设置为 null , GC 会立即释放该对象的内存么?</p>
<ul>
<li>不会, 这个对象将会在下一次 GC 循环中被回收。</li>
</ul>
</li>
<li><p><code>finalize()</code> 方法什么时候被调用？它的目的是什么？</p>
<ul>
<li><code>finallize()</code>方法，是在释放该对象内存前由 GC (垃圾回收器)调用。</li>
<li><del>通常建议在这个方法中释放该对象持有的资源，例如持有的堆外内存、和远程服务的长连接。</del></li>
<li>对于一个对象，该方法有且仅会被调用一次。</li>
</ul>
</li>
<li><p>如何判断一个对象是否已经死去？</p>
<ul>
<li>引用计数<ul>
<li>每个对象有一个引用计数属性，新增一个引用时计数加 1 ，引用释放时计数减 1 ，计数为 0 时可以回收。此方法简单，无法解决对象相互循环引用的问题。目前在用的有 Python、ActionScript3 等语言。</li>
</ul>
</li>
<li>可达性分析<ul>
<li>从 GC Roots 开始向下搜索，搜索所走过的路径称为引用链。当一个对象到 GC Roots 没有任何引用链相连时，则证明此对象是不可用的。不可达对象。目前在用的有 Java、C# 等语言。</li>
</ul>
</li>
</ul>
</li>
<li><p>如果 A 和 B 对象循环引用，是否可以被 GC？</p>
<ul>
<li>可以，因为 Java 采用可达性分析的判断方式。</li>
</ul>
</li>
<li><p>在 Java 语言里，可作为 GC Roots 的对象包括以下几种？</p>
<ul>
<li>虚拟机栈（栈帧中的本地变量表）中引用的对象。(参数)</li>
<li>方法区中的类静态属性引用的对象。</li>
<li>方法区中常量引用的对象。</li>
<li>本地方法栈中 JNI(即一般说的 Native 方法)中引用的对象。</li>
</ul>
</li>
<li><p>方法区是否能被回收？</p>
<ul>
<li>方法区可以被回收，但是价值很低，主要回收废弃的常量和无用的类。</li>
<li>如何判断无用的类，需要完全满足如下三个条件：<ul>
<li>该类所有实例都被回收（Java 堆中没有该类的对象）。</li>
<li>加载该类的 ClassLoader 已经被回收。</li>
<li>该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方利用反射访问该类。</li>
</ul>
</li>
</ul>
</li>
<li><p>Java 对象有哪些引用类型?</p>
<ul>
<li>强引用<ul>
<li>以前我们使用的大部分引用实际上都是强引用，这是使用最普遍的引用。如果一个对象具有强引用，那就类似于必不可少的生活用品，垃圾回收器绝不会回收它。当内存空间不足，Java 虚拟机宁愿抛出 OutOfMemoryError 错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。</li>
</ul>
</li>
<li>软引用（SoftReference）<ul>
<li>如果一个对象只具有软引用，那就类似于可有可物的生活用品。如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。</li>
<li>软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，JAVA 虚拟机就会把这个软引用加入到与之关联的引用队列中。</li>
<li><code>Mybatis SoftCache</code></li>
</ul>
</li>
<li>弱引用（WeakReference）<ul>
<li>如果一个对象只具有弱引用，那就类似于可有可物的生活用品。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它 所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。</li>
<li>弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。</li>
<li><code>Mybatis WeakCache</code></li>
</ul>
</li>
<li>虚引用（PhantomReference）<ul>
<li>“虚引用”顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。</li>
<li>虚引用主要用来跟踪对象被垃圾回收的活动。虚引用与软引用和弱引用的一个区别在于：虚引用必须和引用队列（ReferenceQueue）联合使用。当垃 圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。</li>
</ul>
</li>
</ul>
</li>
<li><p>WeakReference 与 SoftReference的区别？</p>
<ul>
<li>虽然 WeakReference 与 SoftReference 都有利于提高 GC 和 内存的效率。但是 WeakReference 一旦失去最后一个强引用，就会被 GC 回收而 SoftReference 虽然不能阻止被回收，但是可以延迟到 JVM 内存不足的时候。</li>
</ul>
</li>
<li><p>为什么要有不同的引用类型？</p>
<ul>
<li>不像 C 语言，我们可以控制内存的申请和释放，在 Java 中有时候我们需要适当的控制对象被回收的时机，因此就诞生了不同的引用类型，可以说不同的引用类型实则是对 GC 回收时机不可控的妥协。有以下几个使用场景可以充分的说明：</li>
<li>利用软引用和弱引用解决 OOM 问题。用一个 HashMap 来保存图片的路径和相应图片对象关联的软引用之间的映射关系，在内存不足时，JVM 会自动回收这些缓存图片对象所占用的空间，从而有效地避免了 OOM 的问题.</li>
<li>通过软引用实现 Java 对象的高速缓存。比如我们创建了一 Person 的类，如果每次需要查询一个人的信息，哪怕是几秒中之前刚刚查询过的，都要重新构建一个实例，这将引起大量 Person 对象的消耗，并且由于这些对象的生命周期相对较短，会引起多次 GC 影响性能。此时，通过软引用和 HashMap 的结合可以构建高速缓存，提供性能。</li>
</ul>
</li>
<li><p>JVM 垃圾回收算法？</p>
<ul>
<li><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847757529717.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>标记-清除算法</p>
<ul>
<li>标记-清除算法将垃圾回收分为两个阶段：标记阶段和清除阶段。</li>
<li>一种可行的实现是，在标记阶段，首先通过根节点，标记所有从根节点开始的可达对象。因此，未被标记的对象就是未被引用的垃圾对象（好多资料说标记出要回收的对象，其实明白大概意思就可以了）。然后，在清除阶段，清除所有未被标记的对象。</li>
<li>缺点：<ol>
<li>效率问题，标记和清除两个过程的效率都不高。</li>
<li>空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行过程中需要分配较大的对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。</li>
</ol>
</li>
</ul>
</li>
<li><p>标记-整理算法</p>
<ul>
<li><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847758515877.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>标记整理算法，类似与标记清除算法，不过它标记完对象后，不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉边界以外的内存。</p>
</li>
<li><p>优点：</p>
<ol>
<li>相对标记清除算法，解决了内存碎片问题。</li>
<li>没有内存碎片后，对象创建内存分配也更快速了（可以使用TLAB进行分配）。</li>
</ol>
</li>
<li><p>缺点：</p>
<ol start="3">
<li>效率问题，（同标记清除算法）标记和整理两个过程的效率都不高。</li>
</ol>
</li>
</ul>
</li>
<li><p>复制算法</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847759518607.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>复制算法，可以解决效率问题，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块，当这一块内存用完了，就将还存活着的对象复制到另一块上面，然后再把已经使用过的内存空间一次清理掉，这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可（还可使用TLAB进行高效分配内存）。</li>
<li>图的上半部分是未回收前的内存区域，图的下半部分是回收后的内存区域。通过图，可以发现不管回收前还是回收后都有一半的空间未被利用。</li>
<li>优点：<ol start="4">
<li>效率高，没有内存碎片。</li>
</ol>
</li>
<li>缺点：<ol start="5">
<li>浪费一半的内存空间。</li>
<li>复制收集算法在对象存活率较高时就要进行较多的复制操作，效率将会变低。</li>
</ol>
</li>
</ul>
</li>
<li><p>分代收集算法</p>
<ul>
<li><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847762126339.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>当前商业虚拟机都是采用分代收集算法，它根据对象存活周期的不同将内存划分为几块，一般是把 Java 堆分为新生代和老年代，然后根据各个年代的特点采用最适当的收集算法。</p>
</li>
<li><p>在新生代中，每次垃圾收集都发现有大批对象死去，只有少量存活，就选用复制算法。</p>
</li>
<li><p>而老年代中，因为对象存活率高，没有额外空间对它进行分配担保，就必须使用“标记清理”或者“标记整理”算法来进行回收。</p>
</li>
<li><p>对象分配策略：</p>
<ul>
<li>对象优先在 Eden 区域分配，如果对象过大直接分配到 Old 区域。</li>
<li>长时间存活的对象进入到 Old 区域。</li>
</ul>
</li>
<li><p>改进自复制算法</p>
<ul>
<li>现在的商业虚拟机都采用这种收集算法来回收新生代，IBM 公司的专门研究表明，新生代中的对象 98% 是“朝生夕死”的，所以并不需要按照 1:1 的比例来划分内存空间，而是将内存分为一块较大的 Eden 空间和两块较小的 Survivor 空间，每次使用 Eden 和其中一块 Survivor 。当回收时，将 Eden 和 Survivor 中还存活着的对象一次性地复制到另外一块 Survivor 空间上，最后清理掉 Eden 和刚才用过的 Survivor 空间。</li>
<li>HotSpot 虚拟机默认 Eden 和 2 块 Survivor 的大小比例是 8:1:1，也就是每次新生代中可用内存空间为整个新生代容量的 90%（80%+10%），只有 10% 的内存会被“浪费”。当然，98% 的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于 10% 的对象存活，当 Survivor 空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>什么是安全点？</p>
<ul>
<li>SafePoint 安全点，顾名思义是指一些特定的位置，当线程运行到这些位置时，线程的一些状态可以被确定(the thread’s representation of it’s Java machine state is well described)，比如记录OopMap 的状态，从而确定 GC Root 的信息，使 JVM 可以安全的进行一些操作，比如开始 GC 。</li>
</ul>
</li>
<li><p>那些位置可以作为安全点</p>
<ul>
<li>循环的末尾 (防止大循环的时候一直不进入 Safepoint ，而其他线程在等待它进入 Safepoint )。</li>
<li>方法返回前。</li>
<li>调用方法的 Call 之后。</li>
<li>抛出异常的位置。</li>
</ul>
</li>
<li><p>GC发生时停止到安全点的方式</p>
<ul>
<li>主动式<ul>
<li> JVM 设置一个全局变量，线程去按照某种策略检查这个变量一旦发现是 SafePoint 就主动挂起。轮询点==安全点</li>
<li> HostSpot 虚拟机采用的是主动式使线程中断。</li>
</ul>
</li>
<li>抢先式<ul>
<li>JVM发出信号，所有线程全部停止，检查非安全点的线程，让其恢复跑到安全点</li>
</ul>
</li>
<li>安全区域<ul>
<li>如果程序长时间不执行，比如线程调用的 sleep 方法，这时候程序无法响应 JVM 中断请求这时候线程无法到达安全点，显然 JVM 也不可能等待程序唤醒，这时候就需要安全区域了。</li>
<li>安全区域是指一段代码片中，引用关系不会发生变化，在这个区域任何地方 GC 都是安全的，安全区域可以看做是安全点的一个扩展。</li>
<li>线程执行到安全区域的代码时，首先标识自己进入了安全区域，这样 GC 时就不用管进入安全区域的线程了.</li>
<li>线程要离开安全区域时就检查 JVM 是否完成了 GC Roots 枚举（或者整个 GC 过程），如果完成就继续执行，如果没有完成就等待直到收到可以安全离开的信号。</li>
</ul>
</li>
</ul>
</li>
<li><p>JVM 垃圾收集器有哪些？</p>
<ul>
<li>新生代收集器<ul>
<li>Serial 收集器</li>
<li>ParNew 收集器<blockquote>
<p>ParNew 收集器，是 Serial 收集器的多线程版。</p>
</blockquote>
</li>
<li>Parallel Scavenge 收集器</li>
</ul>
</li>
<li>老年代收集器<ul>
<li>Serial Old 收集器<blockquote>
<p>Serial Old 收集器，是 Serial 收集器的老年代版本。</p>
</blockquote>
</li>
<li>Parallel Old 收集器<blockquote>
<p>Parallel Old 收集器，是 Parallel Scavenge 收集器的老年代版本。</p>
</blockquote>
</li>
<li>CMS 收集器</li>
</ul>
</li>
<li>新生代 + 老年代收集器<ul>
<li>G1 收集器</li>
<li>ZGC 收集器</li>
</ul>
</li>
<li>对比<table>
<thead>
<tr>
<th>收集器</th>
<th>串行/并行/并发</th>
<th>新生代/老年代</th>
<th>算法</th>
<th>目标</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>Serial</td>
<td>串行</td>
<td>新生代</td>
<td>复制算法</td>
<td>响应速度</td>
<td>单CPU环境下的Client模式</td>
</tr>
<tr>
<td>Serial Old</td>
<td>串行</td>
<td>老年代</td>
<td>标记-整理</td>
<td>响应速度</td>
<td>单CPU环境下的Client模式、CMS的后备预案</td>
</tr>
<tr>
<td>ParNew</td>
<td>并行</td>
<td>新生代</td>
<td>复制算法</td>
<td>响应速度</td>
<td>多CPU环境时在Server模式下与CMS配合</td>
</tr>
<tr>
<td>Parallel Scavenge</td>
<td>并行</td>
<td>新生代</td>
<td>复制算法</td>
<td>吞吐量</td>
<td>在后台运算而不需要太多交互的任务</td>
</tr>
<tr>
<td>Parallel Old</td>
<td>并行</td>
<td>老年代</td>
<td>标记-整理</td>
<td>吞吐量</td>
<td>在后台运算而不需要太多交互的任务</td>
</tr>
<tr>
<td>CMS</td>
<td>并发</td>
<td>老年代</td>
<td>标记-清除</td>
<td>响应速度</td>
<td>集中在互联网站或B/S系统服务端上的Java应用</td>
</tr>
<tr>
<td>G1</td>
<td>并发</td>
<td>both</td>
<td>标记-整理+复制算法</td>
<td>响应速度</td>
<td>面向服务端应用，将来替换CMS</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>JDK默认的垃圾回收器?</p>
<ul>
<li>JDK1.7: Parallel Scavenge（新生代）+Parallel Old（老年代）</li>
<li>JDK1.8: Parallel Scavenge（新生代）+Parallel Old（老年代）</li>
<li>JDK1.9: G1</li>
<li>-XX:+PrintCommandLineFlagsjvm 看默认设置收集器类型</li>
<li>-XX:+PrintGCDetails 通过打印的GC日志的新生代、老年代名称判断</li>
</ul>
</li>
<li><p>G1 和 CMS 的区别？</p>
<ul>
<li>CMS ：并发标记清除。他的主要步骤有：初始收集，并发标记，重新标记，并发清除（删除）、重置。</li>
<li>G1：主要步骤：初始标记，并发标记，重新标记，复制清除（整理）</li>
<li>CMS 的缺点是对 CPU 的要求比较高。G1是将内存化成了多块，所有对内存的大小有很大的要求。</li>
<li>CMS是清除，所以会存在很多的内存碎片。G1是整理，所以碎片空间较小。</li>
<li>G1 和 CMS 都是响应优先，他们的目的都是尽量控制 STW 时间。</li>
<li>G1 和 CMS 的 Full GC 都是单线程 mark sweep compact 算法，直到 JDK10 才优化为并行的。</li>
</ul>
</li>
<li><p>CMS 算法回收过程中 JVM 是否需要暂停？</p>
<ul>
<li>会有短暂的停顿</li>
</ul>
</li>
<li><p>如何使用指定的垃圾收集器</p>
<table>
<thead>
<tr>
<th>配置</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>-XX:+UserSerialGC</td>
<td>串行垃圾收集器</td>
</tr>
<tr>
<td>-XX:+UserParrallelGC</td>
<td>并行垃圾收集器</td>
</tr>
<tr>
<td>-XX:+UseConcMarkSweepGC</td>
<td>并发标记扫描垃圾回收器</td>
</tr>
<tr>
<td>-XX:ParallelCMSThreads</td>
<td>并发标记扫描垃圾回收器 =为使用的线程数量</td>
</tr>
<tr>
<td>-XX:+UseG1GC</td>
<td>G1垃圾回收器</td>
</tr>
</tbody></table>
</li>
<li><p>对象分配规则是什么？</p>
<ul>
<li><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847831931490.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="-w655"></p>
</li>
<li><p>对象优先分配在 Eden 区。</p>
<ul>
<li>如果 Eden 区无法分配，那么尝试把活着的对象放到 Survivor0 中去（Minor GC）<ul>
<li>如果 Survivor0 可以放入，那么放入之后清除 Eden 区。</li>
<li>如果 Survivor0 不可以放入，那么尝试把 Eden 和 Survivor0 的存活对象放到 Survivor1 中。<ul>
<li>如果 Survivor1 可以放入，那么放入 Survivor1 之后清除 Eden 和 Survivor0 ，之后再把 Survivor1 中的对象复制到 Survivor0 中，保持 Survivor1 一直为空。</li>
<li>如果 Survivor1 不可以放入，那么直接把它们放入到老年代中，并清除 Eden 和 Survivor0 ，这个过程也称为分配担保。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>大对象直接进入老年代（大对象是指需要大量连续内存空间的对象）。</p>
<ul>
<li>这样做的目的是，避免在 Eden 区和两个 Survivor 区之间发生大量的内存拷贝（新生代采用复制算法收集内存）。</li>
</ul>
</li>
<li><p>长期存活的对象进入老年代。</p>
<ul>
<li>虚拟机为每个对象定义了一个年龄计数器，如果对象经过了 1 次 Minor GC 那么对象会进入 Survivor 区，之后每经过一次 Minor GC 那么对象的年龄加 1 ，知道达到阀值对象进入老年区。</li>
</ul>
</li>
<li><p>动态判断对象的年龄。</p>
<ul>
<li>为了更好的适用不同程序的内存情况，虚拟机并不是永远要求对象的年龄必须达到 MaxTenuringThreshold 才能晋升老年代。</li>
<li>如果 Survivor 区中相同年龄的所有对象大小的总和大于 Survivor 空间的一半，年龄大于或等于该年龄的对象可以直接进入老年代。</li>
</ul>
</li>
<li><p>空间分配担保。</p>
<ul>
<li>每次进行 Minor GC 时，JVM 会计算 Survivor 区移至老年区的对象的平均大小，如果这个值大于老年区的剩余值大小则进行一次 Full GC ，如果小于检查 HandlePromotionFailure 设置，如果 true 则只进行  or GC ，如果 false 则进行 Full GC 。</li>
</ul>
</li>
</ul>
</li>
<li><p>为什么新生代内存需要有两个 Survivor 区？</p>
<ul>
<li>解决了碎片化</li>
<li>减少被送到老年代的对象，进而减少Full GC的发生，Survivor的预筛选保证，只有经历16次Minor GC还能在新生代中存活的对象，才会被送到老年代。</li>
</ul>
</li>
<li><p>什么是新生代 GC 和老年代 GC？</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847835594680.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="-w435"></li>
<li>默认新生代(Young)与老年代(Old)的比例的值为 1:2 (该值可以通过参数 –XX:NewRatio 来指定)。</li>
<li>默认的 Eden:from:to=8:1:1 (可以通过参数 –XX:SurvivorRatio 来设定)。</li>
<li>新生代GC（MinorGC/YoungGC）：指发生在新生代的垃圾收集动作，因为 Java 对象大多都具备朝生夕灭的特性，所以 MinorGC 非常频繁，一般回收速度也比较快。</li>
<li>老年代GC（MajorGC/FullGC）：指发生在老年代的 GC，出现了 MajorGC，经常会伴随至少一次的 MinorGC（但非绝对的，在 Parallel Scavenge 收集器的收集策略里就有直接进行 MajorGC 的策略选择过程）。MajorGC 的速度一般会比 MinorGC 慢 10 倍以上。</li>
</ul>
</li>
<li><p>什么情况下会出现 Young GC？</p>
<ul>
<li>对象优先在新生代 Eden 区中分配，如果 Eden 区没有足够的空间时，就会触发一次 Young GC 。</li>
</ul>
</li>
<li><p>什么情况下回出现 Full GC？</p>
<ul>
<li>Full GC 的触发条件有多个，FULL GC 的时候会 STOP THE WORD 。<ul>
<li>在执行 Young GC 之前，JVM 会进行空间分配担保——如果老年代的连续空间小于新生代对象的总大小（或历次晋升的平均大小），则触发一次 Full GC 。</li>
<li>大对象直接进入老年代，从年轻代晋升上来的老对象，尝试在老年代分配内存时，但是老年代内存空间不够。</li>
<li>显式调用 <code>System.gc()</code> 方法时。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="虚拟机性能监控与故障处理工具"><a href="#虚拟机性能监控与故障处理工具" class="headerlink" title="虚拟机性能监控与故障处理工具"></a>虚拟机性能监控与故障处理工具</h2><ol>
<li><p>JDK 的命令行工具有哪些可以监控虚拟机？</p>
<ul>
<li>jps ：虚拟机进程状况工具<ul>
<li>JVM Process Status Tool ，显示系统内所有的HotSpot虚拟机进程。</li>
</ul>
</li>
<li>jstat ：虚拟机统计信息监控工具<ul>
<li>JVM statistics Monitoring ，是用于监视虚拟机运行时状态信息的命令，它可以显示出虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据。</li>
<li><code>jstat -gccause PID 1000</code></li>
</ul>
</li>
<li>jinfo ：Java 配置信息工具<ul>
<li>JVM Configuration info ，这个命令作用是实时查看和调整虚拟机运行参数。</li>
</ul>
</li>
</ul>
</li>
<li><p>JDK 的可视化工具有哪些可以监控虚拟机？</p>
<ul>
<li>JConsole对 JVM 中内存，线程和类等的监控。</li>
<li>VisualVM 可以分析内存快照、线程快照、监控内存变化、GC变化等。</li>
<li>JProfile</li>
<li>GC日志分析工具</li>
</ul>
</li>
<li><p>怎么获取 Java 程序使用的内存？</p>
<ul>
<li>可以通过 java.lang.Runtime 类中与内存相关方法来获取剩余的内存，总内存及最大堆内存。通过这些方法你也可以获取到堆使用的百分比及堆内存的剩余空间。<ul>
<li>Runtime.freeMemory() 方法，返回剩余空间的字节数。</li>
<li>Runtime.totalMemory() 方法，总内存的字节数。</li>
<li>Runtime.maxMemory() 方法，返回最大内存的字节数。</li>
</ul>
</li>
</ul>
</li>
<li><p>常见 GC 的优化配置？</p>
<table>
<thead>
<tr>
<th>配置</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>-Xms</td>
<td>初始化堆内存大小</td>
</tr>
<tr>
<td>-Xmx</td>
<td>堆内存最大值</td>
</tr>
<tr>
<td>-Xmn</td>
<td>新生代大小</td>
</tr>
<tr>
<td>-XX:PermSize</td>
<td>初始化永久代大小</td>
</tr>
<tr>
<td>-XX:MaxPermSize</td>
<td>永久代最大容量</td>
</tr>
<tr>
<td>-XX:SurvivorRatio</td>
<td>设置年轻代中 Eden 区与 Survivor 区的比值</td>
</tr>
<tr>
<td>-XX:Xmn</td>
<td>设置年轻代大小</td>
</tr>
</tbody></table>
</li>
<li><p>如何排查线程 Full GC 频繁的问题</p>
<ul>
<li>System.gc()方法的调用</li>
<li>老年代代空间不足</li>
<li>永生区空间不足</li>
<li>统计得到的Minor GC晋升到旧生代的平均大小大于老年代的剩余空间</li>
<li>堆中分配很大的对象</li>
</ul>
</li>
<li><p>类加载器是有了解吗？</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847900544255.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
<li><p>什么是双亲委派模型（Parent Delegation Model）？</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847901946443.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>类加载器 ClassLoader 是具有层次结构的，也就是父子关系</li>
<li>Bootstrap ClassLoader ：根类加载器，负责加载 Java 的核心类，它不是 java.lang.ClassLoader 的子类，而是由 JVM 自身实现。</li>
<li>Extension ClassLoader ：扩展类加载器</li>
<li>Application ClassLoader ：系统(应用)类加载器</li>
<li>该模型要求除了顶层的 Bootstrap 启动类加载器外，其余的类加载器都应当有自己的父类加载器。子类加载器和父类加载器不是以继承（Inheritance）的关系来实现，而是通过组合（Composition）关系来复用父加载器的代码</li>
</ul>
</li>
<li><p>Java 虚拟机是如何判定两个 Java 类是相同的？</p>
<ul>
<li>Java 虚拟机不仅要看类的全名是否相同，还要看加载此类的类加载器是否一样。只有两者都相同的情况，才认为两个类是相同的。即便是同样的字节代码，被不同的类加载器加载之后所得到的类，也是不同的。</li>
</ul>
</li>
<li><p>双亲委派模型的工作过程？</p>
<ul>
<li>当前 ClassLoader 首先从自己已经加载的类中，查询是否此类已经加载，如果已经加载则直接返回原来已经加载的类。</li>
<li>当前 ClassLoader 的缓存中没有找到被加载的类的时候<ul>
<li>委托父类加载器去加载，父类加载器采用同样的策略，首先查看自己的缓存，然后委托父类的父类去加载，一直到 bootstrap ClassLoader。</li>
<li>当所有的父类加载器都没有加载的时候，再由当前的类加载器加载，并将其放入它自己的缓存中，以便下次有加载请求的时候直接返回。</li>
</ul>
</li>
</ul>
</li>
<li><p>为什么优先使用父 ClassLoader 加载类？</p>
<ul>
<li>共享功能：可以避免重复加载，当父亲已经加载了该类的时候，子类不需要再次加载，一些 Framework 层级的类一旦被顶层的 ClassLoader 加载过就缓存在内存里面，以后任何地方用到都不需要重新加载。</li>
<li>隔离功能：主要是为了安全性，避免用户自己编写的类动态替换 Java 的一些核心类，比如 String ，同时也避免了重复加载，因为 JVM 中区分不同类，不仅仅是根据类名，相同的 class 文件被不同的 ClassLoader 加载就是不同的两个类，如果相互转型的话会抛 java.lang.ClassCaseException 。  </li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>设计模式</title>
    <url>/2021/11/07/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="设计模式-TODO"><a href="#设计模式-TODO" class="headerlink" title="设计模式(TODO)"></a>设计模式(TODO)</h1><h3 id="单例模式"><a href="#单例模式" class="headerlink" title="单例模式"></a>单例模式</h3><ol>
<li>饿汉 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton_Hunger</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Singleton_Hunger SINGLETON_HUNGER = <span class="keyword">new</span> Singleton_Hunger();</span><br><span class="line">    <span class="comment">//限制产生多个对象</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Singleton_Hunger</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//通过该方法获得实例对象</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Singleton_Hunger <span class="title">getSingletonHunger</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> SINGLETON_HUNGER;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//类中其他方法，尽量是static</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">doSomething</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>懒汉 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton_Lazy</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 静态实例变量加上volatile</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">volatile</span> Singleton_Lazy instance;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 私有化构造函数</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Singleton_Lazy</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 双重检查锁</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Singleton_Lazy <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (instance == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span>(Singleton_Lazy.class)&#123;</span><br><span class="line">                <span class="keyword">if</span>(instance == <span class="keyword">null</span>)&#123;</span><br><span class="line">                    instance = <span class="keyword">new</span> Singleton_Lazy();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> instance;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>静态内部类(懒汉) <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 利用静态内部类特性实现外部类的单例</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SingleTon</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 利用静态内部类特性实现外部类的单例</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SingleTonBuilder</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> SingleTon singleTon = <span class="keyword">new</span> SingleTon();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 私有化构造函数</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">SingleTon</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> SingleTon <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> SingleTonBuilder.singleTon;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SingleTon instance = getInstance();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>枚举实现 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 利用静态内部类特性实现外部类的单例</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">enum</span> <span class="title">SingleTon</span> </span>&#123;</span><br><span class="line">    ONE , TWO</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="工厂模式"><a href="#工厂模式" class="headerlink" title="工厂模式"></a>工厂模式</h2><ul>
<li><p>定义一个创建对象的接口，让其子类自己决定实例化哪一个工厂类，工厂模式使其创建过程延迟到子类进行。</p>
</li>
<li><p>主要解决：主要解决接口选择的问题。</p>
</li>
<li><p>实现</p>
  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 定义接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 实现接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Rectangle</span> <span class="keyword">implements</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      System.out.println(<span class="string">&quot;Inside Rectangle::draw() method.&quot;</span>);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Square</span> <span class="keyword">implements</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      System.out.println(<span class="string">&quot;Inside Square::draw() method.&quot;</span>);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Circle</span> <span class="keyword">implements</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      System.out.println(<span class="string">&quot;Inside Circle::draw() method.&quot;</span>);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 定义工厂</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ShapeFactory</span> </span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> Shape <span class="title">getShape</span><span class="params">(String shapeType)</span></span>&#123;</span><br><span class="line">      <span class="keyword">if</span>(shapeType == <span class="keyword">null</span>)&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">      &#125;        </span><br><span class="line">      <span class="keyword">if</span>(shapeType.equalsIgnoreCase(<span class="string">&quot;CIRCLE&quot;</span>))&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Circle();</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span>(shapeType.equalsIgnoreCase(<span class="string">&quot;RECTANGLE&quot;</span>))&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Rectangle();</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span>(shapeType.equalsIgnoreCase(<span class="string">&quot;SQUARE&quot;</span>))&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Square();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="抽象工厂模式"><a href="#抽象工厂模式" class="headerlink" title="抽象工厂模式"></a>抽象工厂模式</h2><h2 id="建造者模式"><a href="#建造者模式" class="headerlink" title="建造者模式"></a>建造者模式</h2><h2 id="原型模式"><a href="#原型模式" class="headerlink" title="原型模式"></a>原型模式</h2><h2 id="适配器模式"><a href="#适配器模式" class="headerlink" title="适配器模式"></a>适配器模式</h2><h2 id="桥接模式"><a href="#桥接模式" class="headerlink" title="桥接模式"></a>桥接模式</h2><h2 id="过滤器模式"><a href="#过滤器模式" class="headerlink" title="过滤器模式"></a>过滤器模式</h2><h2 id="组合模式"><a href="#组合模式" class="headerlink" title="组合模式"></a>组合模式</h2><h2 id="装饰器模式"><a href="#装饰器模式" class="headerlink" title="装饰器模式"></a>装饰器模式</h2><h2 id="外观模式"><a href="#外观模式" class="headerlink" title="外观模式"></a>外观模式</h2><h2 id="享元模式"><a href="#享元模式" class="headerlink" title="享元模式"></a>享元模式</h2><h2 id="代理模式"><a href="#代理模式" class="headerlink" title="代理模式"></a>代理模式</h2><h2 id="责任链模式"><a href="#责任链模式" class="headerlink" title="责任链模式"></a>责任链模式</h2><h2 id="命令模式"><a href="#命令模式" class="headerlink" title="命令模式"></a>命令模式</h2><h2 id="解释器模式"><a href="#解释器模式" class="headerlink" title="解释器模式"></a>解释器模式</h2><h2 id="迭代器模式"><a href="#迭代器模式" class="headerlink" title="迭代器模式"></a>迭代器模式</h2><h2 id="中介者模式"><a href="#中介者模式" class="headerlink" title="中介者模式"></a>中介者模式</h2><h2 id="备忘录模式"><a href="#备忘录模式" class="headerlink" title="备忘录模式"></a>备忘录模式</h2><h2 id="观察者模式"><a href="#观察者模式" class="headerlink" title="观察者模式"></a>观察者模式</h2><h2 id="状态模式"><a href="#状态模式" class="headerlink" title="状态模式"></a>状态模式</h2><h2 id="空对象模式"><a href="#空对象模式" class="headerlink" title="空对象模式"></a>空对象模式</h2><h2 id="策略模式"><a href="#策略模式" class="headerlink" title="策略模式"></a>策略模式</h2><h2 id="模板模式"><a href="#模板模式" class="headerlink" title="模板模式"></a>模板模式</h2><h2 id="访问者模式"><a href="#访问者模式" class="headerlink" title="访问者模式"></a>访问者模式</h2><h2 id="MVC-模式"><a href="#MVC-模式" class="headerlink" title="MVC 模式"></a>MVC 模式</h2><h2 id="业务代表模式"><a href="#业务代表模式" class="headerlink" title="业务代表模式"></a>业务代表模式</h2><h2 id="组合实体模式"><a href="#组合实体模式" class="headerlink" title="组合实体模式"></a>组合实体模式</h2><h2 id="数据访问对象模式"><a href="#数据访问对象模式" class="headerlink" title="数据访问对象模式"></a>数据访问对象模式</h2><h2 id="前端控制器模式"><a href="#前端控制器模式" class="headerlink" title="前端控制器模式"></a>前端控制器模式</h2><h2 id="拦截过滤器模式"><a href="#拦截过滤器模式" class="headerlink" title="拦截过滤器模式"></a>拦截过滤器模式</h2><h2 id="服务定位器模式"><a href="#服务定位器模式" class="headerlink" title="服务定位器模式"></a>服务定位器模式</h2><h2 id="传输对象模式"><a href="#传输对象模式" class="headerlink" title="传输对象模式"></a>传输对象模式</h2>]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis面试题</title>
    <url>/2021/11/07/Redis%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h1><ol>
<li>什么是 Redis ？<ul>
<li>Redis是一个基于内存的高性能 Key-Value 数据库</li>
</ul>
</li>
<li>Redis 有什么优点？<ul>
<li>速度快:数据存在内存中，类似于 HashMap ，HashMap 的优势就是查找和操作的时间复杂度都是O (1) 每秒可以处理超过 10 万次读写操作，是已知性能最快的 Key-Value 数据库。</li>
<li>支持丰富数据类型:String ，List，Set，Sorted Set，Hash 五种基础的数据结构。</li>
<li>丰富的特性: 订阅发布 Pub / Sub 功能,Key 过期策略,事务,支持多个 DB,计数</li>
<li>持久化存储: Redis 提供 RDB 和 AOF 两种数据的持久化存储方案，解决内存数据库最担心的万一 Redis 挂掉，数据会消失掉。</li>
<li>高可用:内置 Redis Sentinel ，提供高可用方案，实现主从故障自动转移;内置 Redis Cluster ，提供集群方案，实现基于槽的分片方案，从而支持更大的 Redis 规模。</li>
</ul>
</li>
<li>Redis 有什么缺点？<ul>
<li>由于 Redis 是内存数据库，所以，单台机器，存储的数据量，跟机器本身的内存大小。虽然 Redis 本身有 Key 过期策略，但是还是需要提前预估和节约内存。如果内存增长过快，需要定期删除数据。(可使用 Redis Cluster、Codis 等方案，对 Redis 进行分区，从单机 Redis 变成集群 Redis 。)</li>
<li>如果进行完整重同步，由于需要生成 RDB 文件，并进行传输，会占用主机的 CPU ，并会消耗现网的带宽。不过 Redis2.8 版本，已经有部分重同步的功能，但是还是有可能有完整重同步的。比如，新上线的备机。</li>
<li>修改配置文件，进行重启，将硬盘中的数据加载进内存，时间比较久。在这个过程中，Redis 不能提供服务。</li>
</ul>
</li>
<li>请说说 Redis 的线程模型？<ul>
<li>Redis 是非阻塞 IO ，多路复用。</li>
</ul>
</li>
<li>为什么 Redis 单线程模型也能效率这么高？<ul>
<li> C 语言实现。</li>
<li> 纯内存操作。</li>
<li> 基于非阻塞的 IO 多路复用机制。</li>
<li> 单线程，避免了多线程的频繁上下文切换问题。</li>
</ul>
</li>
<li>Redis 是单线程的，如何提高多核 CPU 的利用率？<ul>
<li>可以在同一个服务器部署多个 Redis 的实例，并把他们当作不同的服务器来使用，在某些时候，无论如何一个服务器是不够的， 所以，如果你想使用多个 CPU ，你可以考虑一下分区。</li>
</ul>
</li>
<li>Redis 有几种持久化方式？<ul>
<li>【全量】RDB 持久化<ul>
<li>在指定的时间间隔内将内存中的数据集快照写入磁盘。实际操作过程是，fork 一个子进程，先将数据集写入临时文件，写入成功后，再替换之前的文件，用二进制压缩存储</li>
<li>RDB 优点：<ul>
<li>灵活设置备份频率和周期。你可能打算每个小时归档一次最近 24 小时的数据，同时还要每天归档一次最近 30 天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复</li>
<li>非常适合冷备份，对于灾难恢复而言，RDB 是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。推荐，可以将这种完整的数据文件发送到一些远程的安全存储上去，</li>
<li>性能最大化。对于 Redis 的服务进程而言，在开始持久化时，它唯一需要做的只是 fork 出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行 IO 操作了。也就是说，RDB 对 Redis 对外提供的读写服务，影响非常小，可以让 Redis 保持高性能。</li>
<li>恢复更快。相比于 AOF 机制，RDB 的恢复速度更更快，更适合恢复数据，特别是在数据集非常大的情况</li>
</ul>
</li>
<li>RDB 缺点：<ul>
<li>如果想保证数据的高可用性，即最大限度的避免数据丢失，那么 RDB 将不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。</li>
<li>由于 RDB 是通过 fork 子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是 1 秒钟。</li>
</ul>
</li>
</ul>
</li>
<li>【增量】AOF持久化<ul>
<li>以日志的形式记录服务器所处理的每一个写、删除操作，查询操作不会记录，以文本的方式记录，可以打开文件看到详细的操作记录。</li>
<li>AOF 优点<ul>
<li>该机制可以带来更高的数据安全性，即数据持久性。Redis 中提供了 3 种同步策略，即每秒同步、每修改(执行一个命令)同步和不同步。</li>
<li>由于该机制对日志文件的写入操作采用的是 append 模式，因此在写入过程中即使出现宕机现象，也不会破坏日志文件中已经存在的内容。（redis-check-aof）</li>
<li>如果 AOF 日志过大，Redis 可以自动启用 rewrite 机制。即使出现后台重写操作，也不会影响客户端的读写。因为在 rewrite log 的时候，会对其中的指令进行压缩，创建出一份需要恢复数据的最小日志出来。再创建新日志文件的时候，老的日志文件还是照常写入。当新的 merge 后的日志文件 ready 的时候，再交换新老日志文件即可。</li>
</ul>
</li>
<li>AOF 缺点<ul>
<li>对于相同数量的数据集而言，AOF 文件通常要大于 RDB 文件。RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。</li>
<li>根据同步策略的不同，AOF 在运行效率上往往会慢于 RDB 。总之，每秒同步策略的效率是比较高的，同步禁用策略的效率和 RDB 一样高效。</li>
<li>以前 AOF 发生过 bug ，就是通过 AOF 记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似 AOF 这种较为复杂的基于命令日志/merge/回放的方式，比基于 RDB 每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有 bug 。不过 AOF 就是为了避免 rewrite 过程导致的 bug ，因此每次 rewrite 并不是基于旧的指令日志进行 merge 的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。</li>
</ul>
</li>
</ul>
</li>
<li>如何选择：<ul>
<li>不要仅仅使用 RDB，因为那样会导致你丢失很多数据。</li>
<li>也不要仅仅使用 AOF，因为那样有两个问题，第一，你通过 AOF 做冷备，没有 RDB 做冷备，来的恢复速度更快; 第二，RDB 每次简单粗暴生成数据快照，更加健壮，可以避免 AOF 这种复杂的备份和恢复机制的 bug 。</li>
<li>Redis 支持同时开启开启两种持久化方式，我们可以综合使用 AOF 和 RDB 两种持久化机制，用 AOF 来保证数据不丢失，作为数据恢复的第一选择; 用 RDB 来做不同程度的冷备，在 AOF 文件都丢失或损坏不可用的时候，还可以使用 RDB 来进行快速的数据恢复。</li>
</ul>
</li>
<li>AOF rewrite 机制，和 RDB 一样，也需要 fork 出一次子进程，如果 Redis 内存比较大，可能会因为 fork 阻塞下主进程。</li>
</ul>
</li>
<li>Redis 有哪几种数据“淘汰”策略？<ul>
<li>Redis 内存数据集大小上升到一定大小的时候，就会进行数据淘汰策略。Redis 提供了 6 种数据淘汰策略：<ul>
<li>volatile-lru</li>
<li>volatile-ttl</li>
<li>volatile-random</li>
<li>allkeys-lru</li>
<li>allkeys-random</li>
<li>【默认策略】no-enviction</li>
</ul>
</li>
</ul>
</li>
<li>Redis LRU 算法<ul>
<li> Redis 的 LRU 算法，并不是一个严格的 LRU 实现。这意味着 Redis 不能选择最佳候选键来回收，也就是最久未被访问的那些键。相反，Redis 会尝试执行一个近似的 LRU 算法，通过采样一小部分键，然后在采样键中回收最适合(拥有最久未被访问时间)的那个。</li>
<li> Redis 没有使用真正实现严格的 LRU 算是的原因是，因为消耗更多的内存。然而对于使用 Redis 的应用来说，使用近似的 LRU 算法，事实上是等价的。</li>
</ul>
</li>
<li>MySQL 里有 2000w 数据，Redis 中只存 20w 的数据，如何保证 Redis 中的数据都是热点数据？<ul>
<li>选择 volatile-lru 或 allkeys-lru 这两个基于 LRU 算法的淘汰策略。</li>
</ul>
</li>
<li>Redis 回收进程如何工作的？<ul>
<li>一个客户端运行了新的写命令，添加了新的数据。</li>
<li>Redis 检查内存使用情况，如果大于 maxmemory 的限制, 则根据设定好的策略进行回收。</li>
<li>Redis 执行新命令。</li>
<li>不断地穿越内存限制的边界，通过不断达到边界然后不断地回收回到边界以下（跌宕起伏）。</li>
</ul>
</li>
<li>如果有大量的 key 需要设置同一时间过期，一般需要注意什么？<ul>
<li>如果大量的 key 过期时间设置的过于集中，到过期的那个时间点，Redis可能会出现短暂的卡顿现象。</li>
<li>调大 hz 参数，每次过期的 key 更多，从而最终达到避免一次过期过多。<ul>
<li> hz 参数代表了一秒钟内，后台任务期望被调用的次数,hz 调大将会提高 Redis 主动淘汰的频率</li>
</ul>
</li>
<li>一般需要在时间上加一个随机值，使得过期时间分散一些。</li>
</ul>
</li>
<li>Redis 有哪些数据结构？<ul>
<li>初级<ul>
<li>字符串 String</li>
<li>字典Hash</li>
<li>列表List</li>
<li>集合Set</li>
<li>有序集合 SortedSet</li>
</ul>
</li>
<li>中级<ul>
<li>HyperLogLog</li>
<li>Geo</li>
<li>Bitmap</li>
</ul>
</li>
<li>高级<ul>
<li>BloomFilter</li>
<li>RedisSearch</li>
<li>Redis-ML</li>
<li>JSON</li>
</ul>
</li>
</ul>
</li>
<li>聊聊 Redis 使用场景/为什么使用redis<ul>
<li>数据缓存</li>
<li>会话缓存</li>
<li>时效性数据</li>
<li>访问频率</li>
<li>计数器</li>
<li>社交列表</li>
<li>记录用户判定信息</li>
<li>交集、并集和差集</li>
<li>热门列表与排行榜</li>
<li>最新动态</li>
<li>消息队列</li>
<li>分布式锁</li>
</ul>
</li>
<li>Redis 支持的 Java 客户端都有哪些？<ol>
<li>Redisson：封装好</li>
<li>Jedis：命令全</li>
<li>Lettuce：是一个可伸缩线程安全的 Redis 客户端。多个线程可以共享同一个 RedisConnection 。它利用优秀 Netty NIO 框架来高效地管理多个连接。</li>
</ol>
</li>
<li>如何使用 Redis 实现分布式锁？<ul>
<li>正确的获得锁：set 指令附带 nx 参数，保证有且只有一个进程获得到。</li>
<li>正确的释放锁：使用 Lua 脚本，比对锁持有的是不是自己。如果是，则进行删除来释放。</li>
<li>超时的自动释放锁：set 指令附带 expire 参数，通过过期机制来实现超时释放。</li>
<li>未获得到锁的等待机制：sleep 或者基于 Redis 的订阅 Pub/Sub 机制。一些业务场景，可能需要支持获得不到锁，直接返回 false ，不等待。</li>
<li>锁超时的处理：告警 + 后台线程自动续锁的超时时间。通过这样的机制，保证有且仅有一个线程，正在持有锁。</li>
<li>set 指令：<code>SET key value [EX seconds] [PX milliseconds] [NX|XX]</code></li>
<li>Redlock: Redisson 实现，所有master超过半数</li>
</ul>
</li>
<li>Redis 分布式锁 对比 Zookeeper 分布式锁<ul>
<li>从可靠性上来说，Zookeeper 分布式锁好于 Redis 分布式锁。</li>
<li>从性能上来说，Redis 分布式锁好于 Zookeeper 分布式锁。</li>
</ul>
</li>
<li>如何使用 Redis 实现消息队列？<ul>
<li>使用 list 结构作为队列，rpush 生产消息，lpop 消费消息。当 lpop 没有消息的时候，要适当 sleep 一会再重试。</li>
<li>如果对方追问可不可以不用 sleep 呢？list 还有个指令叫 blpop ，在没有消息的时候，它会阻塞住直到消息到来。</li>
<li>如果对方追问能不能生产一次消费多次呢？使用 pub / sub 主题订阅者模式，可以实现 1:N 的消息队列。</li>
<li>如果对方追问 pub / sub 有什么缺点？在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如 rabbitmq 等。</li>
<li>果对方追问 redis 如何实现延时队列？使用 sortedset ，拿时间戳作为 score ，消息内容作为 key 调用 zadd 来生产消息，消费者用 zrangebyscore 指令获取 N 秒之前的数据轮询进行处理。</li>
</ul>
</li>
<li>Redis 如何做大量数据插入？<ul>
<li>Redis-cli 支持一种新的被称之为 pipe mode 的新模式用于执行大量数据插入工作。</li>
</ul>
</li>
<li>什么是 Redis 事务？<ul>
<li>MULTI / EXEC / DISCARD / WATCH 这四个命令是我们实现事务的基石<ul>
<li>在事务中的所有命令都将会被串行化的顺序执行，事务执行期间，Redis 不会再为其它客户端的请求提供任何服务，从而保证了事物中的所有命令被原子的执行。</li>
<li>和关系型数据库中的事务相比，在 Redis 事务中如果有某一条命令执行失败，其后的命令仍然会被继续执行。</li>
<li>我们可以通过 MULTI 命令开启一个事务，有关系型数据库开发经验的人可以将其理解为 “BEGIN TRANSACTION” 语句。在该语句之后执行的命令，都将被视为事务之内的操作，最后我们可以通过执行 EXEC / DISCARD 命令来提交 / 回滚该事务内的所有操作。这两个 Redis 命令，可被视为等同于关系型数据库中的 COMMIT / ROLLBACK 语句。(开启事务后，所有语句，发送给 Redis Server ，都会暂存在 Server 中。)</li>
<li>在事务开启之前，如果客户端与服务器之间出现通讯故障并导致网络断开，其后所有待执行的语句都将不会被服务器执行。然而如果网络中断事件是发生在客户端执行 EXEC 命令之后，那么该事务中的所有命令都会被服务器执行。</li>
</ul>
</li>
</ul>
</li>
<li>如何实现 Redis CAS 操作？<ul>
<li>在 Redis 的事务中，WATCH 命令可用于提供 CAS(check-and-set) 功能。</li>
<li>假设我们通过 WATCH 命令在事务执行之前监控了多个 keys ，倘若在 WATCH 之后有任何 Key 的值发生了变化，EXEC 命令执行的事务都将被放弃，同时返回 nil 应答以通知调用者事务执行失败</li>
</ul>
</li>
<li>Redis 集群都有哪些方案？<ul>
<li>Redis Sentinel<ul>
<li>体量较小时，选择 Redis Sentinel ，单主 Redis 足以支撑业务。</li>
</ul>
</li>
<li>Redis Cluster<ul>
<li>体量较大时，选择 Redis Cluster ，通过分片，使用更多内存。</li>
</ul>
</li>
<li>多大体量需要使用 Redis Cluster 呢<ol>
<li>一次 RDB 时间随着内存越大，会变大越来越久。同时，一次 fork 的时间也会变久。还有，重启通过 RDB 文件，或者 AOF 日志，恢复时间都会变长。</li>
<li>体量大之后，读写的 QPS 势必比体量小的时候打的多，那么使用 Redis Cluster 相比 Redis Sentinel ，可以分散读写压力到不同的集群中。</li>
</ol>
</li>
</ul>
</li>
<li>什么是 Redis 主从同步？<ul>
<li>Redis 的主从同步(replication)机制，允许 Slave 从 Master 那里，通过网络传输拷贝到完整的数据备份，从而达到主从机制。</li>
<li>主数据库可以进行读写操作，当发生写操作的时候自动将数据同步到从数据库，而从数据库一般是只读的，并接收主数据库同步过来的数据。</li>
<li>一个主数据库可以有多个从数据库，而一个从数据库只能有一个主数据库。</li>
<li>第一次同步时，主节点做一次 bgsave 操作，并同时将后续修改操作记录到内存 buffer ，待完成后将 RDB 文件全量同步到复制节点，复制节点接受完成后将 RDB 镜像加载到内存。加载完成后，再通知主节点将期间修改的操作记录同步到复制节点进行重放就完成了同步过程。</li>
</ul>
</li>
<li>Redis Cluster 的主从复制模型是怎样的？<ul>
<li>为了使在部分节点失败或者大部分节点无法通信的情况下集群仍然可用，所以集群使用了主从复制模型，每个节点都会有 N-1 个复制节点。</li>
<li>所以，Redis Cluster 可以说是 Redis Sentinel 带分片的加强版。也可以说：<ul>
<li>Redis Sentinel 着眼于高可用，在 master 宕机时会自动将 slave 提升为 master ，继续提供服务。</li>
<li>Redis Cluster 着眼于扩展性，在单个 Redis 内存不足时，使用 Cluster 进行分片存储。</li>
</ul>
</li>
</ul>
</li>
<li>Redis Cluster 方案什么情况下会导致整个集群不可用？<ul>
<li>有 A，B，C 三个节点的集群，在没有复制模型的情况下，如果节点 B 宕机了，那么整个集群就会以为缺少 5501-11000 这个范围的槽而不可用。当然，这种情况也可以配置 cluster-require-full-coverage=no ，整个集群无需所有槽位覆盖。</li>
</ul>
</li>
<li>Redis Cluster 会有写操作丢失吗？为什么？<ul>
<li>Redis 并不能保证数据的强一致性，而是【异步复制】，这意味这在实际中集群在特定的条件下可能会丢失写操作。</li>
<li>无论对于 Redis Sentinel 还是 Redis Cluster 方案，都是通过主从复制，所以在数据的复制方面，都存在相同的情况。</li>
</ul>
</li>
<li>Redis 集群如何选择数据库？<ul>
<li>Redis 集群目前无法做数据库选择，默认在 0 数据库。</li>
</ul>
</li>
<li>请说说生产环境中的 Redis 是怎么部署的？<ul>
<li>Redis Cluster ，10 台机器，5 台机器部署了 Redis 主实例，另外 5 台机器部署了 Redis 的从实例，每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰 qps 可能可以达到每秒 5 万，5 台机器最多是 25 万读写请求每秒。</li>
<li>机器是什么配置？32G 内存 + 8 核 CPU + 1T 磁盘，但是分配给 Redis 进程的是 10G 内存，一般线上生产环境，Redis 的内存尽量不要超过 10G，超过 10G 可能会有问题。那么，5 台机器对外提供读写，一共有 50G 内存。</li>
<li>因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，Redis 从实例会自动变成主实例继续提供读写服务。</li>
<li>你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是 10kb 。100 条数据是 1mb ，10 万条数据是 1G 。常驻内存的是 200 万条商品数据，占用内存是 20G ，仅仅不到总内存的 50% 。目前高峰期每秒就是 3500 左右的请求量。</li>
<li>公司体量大了之后，建议是一个业务线独占一个或多个 Redis Cluster 集群，实现好业务线与业务线之间的隔离。</li>
</ul>
</li>
<li>什么是 Redis 分区？<ul>
<li>分区是分割数据到多个Redis实例的处理过程，因此每个实例只保存key的一个子集。</li>
<li>分区的优势:通过利用多台计算机内存的和值，允许我们构造更大的数据库。通过多核和多台计算机，允许我们扩展计算能力；通过多台计算机和网络适配器，允许我们扩展网络带宽。</li>
<li>分区的不足:不支持涉及多个key的操作通</li>
<li>分区类型:范围分区 &amp;&amp; 哈希分区</li>
</ul>
</li>
<li>Redis 有哪些重要的健康指标？<ul>
<li>存活情况</li>
<li>连接数</li>
<li>阻塞客户端数量</li>
<li>使用内存峰值</li>
<li>内存碎片率</li>
<li>缓存命中率</li>
<li>OPS</li>
<li>持久化</li>
<li>失效KEY</li>
<li>慢日志</li>
</ul>
</li>
<li>一个 Redis 实例最多能存放多少的 keys？List、Set、Sorted Set 他们最多能存放多少元素？<ul>
<li>Redis 可以处理多达 2^32 的 keys ，并且在实际中进行了测试，每个实例至少存放了 2 亿 5 千万的 keys。</li>
<li>任何 list、set、和 sorted set 都可以放 2^32 </li>
</ul>
</li>
<li>假如 Redis 里面有 1 亿个 key，其中有 10w 个 key 是以某个固定的已知的前缀开头的，如果将它们全部找出来？<ul>
<li>keys 指令可以扫出指定模式的 key 列表。</li>
<li>对方接着追问：如果这个 Redis 正在给线上的业务提供服务，那使用 keys 指令会有什么问题？</li>
<li>这个时候你要回答 Redis 关键的一个特性：Redis 的单线程的。keys 指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用 scan 指令，scan 指令可以无阻塞的提取出指定模式的 key 列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用 keys 指令长。</li>
</ul>
</li>
<li>Redis 常见的性能问题都有哪些？如何解决？<ul>
<li>Master 最好不要做任何持久化工作，如 RDB 内存快照和 AOF 日志文件。</li>
<li></li>
</ul>
</li>
<li>Redis雪崩<ul>
<li><ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15849210200503.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
<li>同一时间大量缓存失效，请求直接落到数据库，造成数据库崩溃</li>
<li>解决方式：<ul>
<li>失效期设置永不过期</li>
<li>失效期添加随机数</li>
<li>设置多层缓存，redis失效的情况下，使用内部缓存</li>
</ul>
</li>
</ul>
</li>
</ol>
<ul>
<li>缓存穿透，缓存击穿,雪崩的区别<ul>
<li>缓存穿透是指缓存和数据库中都没有的数据，比如id=-1导致数据库压力过大<ul>
<li>参数校验保证数据合法性</li>
<li>缓存假结果，添加失效期</li>
<li><ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15849213541298.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
</ul>
</li>
<li>缓存击穿是指一个Key非常热点，在不停的扛着大并发，大并发集中对这一个点进行访问，当这个Key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库<ul>
<li>设置热点数据永远不过期</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Dubbo面试题</title>
    <url>/2021/11/07/Dubbo%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Dubbo"><a href="#Dubbo" class="headerlink" title="Dubbo"></a>Dubbo</h1><ol>
<li><p>Dubbo 有几种配置方式？</p>
<ul>
<li>XML 配置</li>
<li>注解配置</li>
<li>Java API 配置</li>
<li>属性配置</li>
</ul>
</li>
<li><p>Dubbo 如何和 Spring Boot 进行集成？</p>
<ul>
<li>官方提供提供了集成库 dubbo-spring-boot</li>
</ul>
</li>
<li><p>Dubbo 框架的分层设计</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850280657994.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>总体分成 Business、RPC、Remoting 三大层<ul>
<li>Service 业务层：业务代码的接口与实现。我们实际使用 Dubbo 的业务层级。接口层，给服务提供者和消费者来实现的。</li>
<li>RPC层：<ul>
<li>config 配置层：主要是对 Dubbo 进行各种配置的。</li>
<li>proxy 服务代理层：服务代理层，无论是 consumer 还是 provider，Dubbo 都会给你生成代理，代理之间进行网络通信。（ 对比Spring Cloud 体系，可以类比成 Feign 对于 consumer ，Spring MVC 对于 provider 。）</li>
<li>registry 注册中心层：服务注册层，负责服务的注册与发现。（对比 Spring Cloud 体系，可以类比成 Eureka Client ）</li>
<li>cluster 路由层：封装多个服务提供者的路由以及负载均衡，将多个实例组合成一个服务。（对比Spring Cloud 体系，可以类比成 Ribbon ）</li>
<li>monitor 监控层：对 rpc 接口的调用次数和调用时间进行监控。</li>
</ul>
</li>
<li>Remoting：<ul>
<li>protocol 远程调用层：远程调用层，封装 rpc 调用。</li>
<li>exchange 信息交换层：信息交换层，封装请求响应模式，同步转异步。</li>
<li>transport 网络传输层：抽象 mina 和 netty 为统一接口。</li>
<li>serialize 数据序列化层：可复用的一些工具，扩展接口为 Serialization, ObjectInput, ObjectOutput</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 调用流程</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850284072280.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850285070885.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>Provider<ul>
<li>第 0 步，start 启动服务。</li>
<li>第 1 步，register 注册服务到注册中心。</li>
</ul>
</li>
<li>Consumer<ul>
<li>第 2 步，subscribe 向注册中心订阅服务。<ul>
<li>注意，只订阅使用到的服务。</li>
<li>再注意，首次会拉取订阅的服务列表，缓存在本地。</li>
<li>【异步】第 3 步，notify 当服务发生变化时，获取最新的服务列表，更新本地缓存。</li>
</ul>
</li>
</ul>
</li>
<li>invoke 调用<ul>
<li>Consumer 直接发起对 Provider 的调用，无需经过注册中心。而对多个 Provider 的负载均衡，Consumer 通过 cluster 组件实现。</li>
</ul>
</li>
<li>count 监控<ul>
<li>【异步】Consumer 和 Provider 都异步通知监控中心。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 调用是同步的吗？</p>
<ul>
<li>默认情况下，调用是同步的方式。</li>
</ul>
</li>
<li><p>谈谈对 Dubbo 的异常处理机制？</p>
<ul>
<li>dubbo的异常处理类是com.alibaba.dubbo.rpc.filter.ExceptionFilter 类,源码这里就不贴了.归纳下对异常的处理分为下面几类:<ul>
<li>1)如果provider实现了GenericService接口,直接抛出</li>
<li>2)如果是checked异常，直接抛出</li>
<li>3)在方法签名上有声明，直接抛出</li>
<li>4)异常类和接口类在同一jar包里，直接抛出</li>
<li>5)是JDK自带的异常，直接抛出</li>
<li>6)是Dubbo本身的异常，直接抛出</li>
<li>7)否则，包装成RuntimeException抛给客户端（防止客户端反序列化失败.前面几种情况都能保证反序列化正常.）</li>
</ul>
</li>
<li><a href="https://blog.csdn.net/qq315737546/article/details/53915067">dubbo异常处理</a></li>
</ul>
</li>
<li><p>Dubbo 如何做参数校验？</p>
<ul>
<li>参数校验功能，通过参数校验过滤器 ValidationFilter 来实现。</li>
<li>ValidationFilter 在 Dubbo Provider 和 Consumer 都可生效。<ul>
<li>如果我们将校验注解写在 Service 接口的方法上，那么 Consumer 在本地就会校验。如果校验不通过，直接抛出校验失败的异常，不会发起 Dubbo 调用。</li>
<li>如果我们将校验注解写在 Service 实现的方法上，那么 Consumer 在本地不会校验，而是由 Provider 校验。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 可以对调用结果进行缓存吗?</p>
<ul>
<li>Dubbo 通过 CacheFilter 过滤器，提供结果缓存的功能，且既可以适用于 Consumer 也可以适用于 Provider 。</li>
<li>通过结果缓存，用于加速热门数据的访问速度，Dubbo 提供声明式缓存，以减少用户加缓存的工作量。</li>
<li>Dubbo 目前提供三种实现：<ul>
<li>lru ：基于最近最少使用原则删除多余缓存，保持最热的数据被缓存。</li>
<li>threadlocal ：当前线程缓存，比如一个页面渲染，用到很多 portal，每个 portal 都要去查用户信息，通过线程缓存，可以减少这种多余访问。</li>
<li>jcache ：与 JSR107 集成，可以桥接各种缓存实现。</li>
</ul>
</li>
</ul>
</li>
<li><p>注册中心挂了还可以通信吗？</p>
<ul>
<li>可以。对于正在运行的 Consumer 调用 Provider 是不需要经过注册中心，所以不受影响。并且，Consumer 进程中，内存已经缓存了 Provider 列表。</li>
<li>此时 Provider 如果下线呢？<ul>
<li>如果 Provider 是正常关闭，它会主动且直接对和其处于连接中的 Consumer 们，发送一条“我要关闭”了的消息。那么，Consumer 们就不会调用该 Provider ，而调用其它的 Provider 。</li>
<li>因为 Consumer 也会持久化 Provider 列表到本地文件。所以，此处如果 Consumer 重启，依然能够通过本地缓存的文件，获得到 Provider 列表。</li>
<li>一般情况下，注册中心是一个集群，如果一个节点挂了，Dubbo Consumer 和 Provider 将自动切换到集群的另外一个节点上。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 在 Zookeeper 存储了哪些信息？</p>
<pre><code> - ![](https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850292909765.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10)
</code></pre>
<ul>
<li>服务提供者启动时: 向 /dubbo/com.foo.BarService/providers 目录下写入自己的 URL 地址</li>
<li>服务消费者启动时: 订阅 /dubbo/com.foo.BarService/providers 目录下的提供者 URL 地址。并向 /dubbo/com.foo.BarService/consumers 目录下写入自己的 URL 地址（服务消费者启动后，不仅仅订阅了 “providers” 分类，也订阅了 “routes” “configurations” 分类。）</li>
<li>监控中心启动时: 订阅 /dubbo/com.foo.BarService 目录下的所有提供者和消费者 URL 地址。</li>
<li>Zookeeper 的节点层级，自上而下是：<ul>
<li> Root 层：根目录，可通过 &lt;dubbo:registry group=”dubbo” /&gt; 的 “group” 设置 Zookeeper 的根节点，缺省使用 “dubbo” 。</li>
<li> Service 层：服务接口全名。</li>
<li> Type 层：分类。目前除了我们在图中看到的 “providers”( 服务提供者列表 ) “consumers”( 服务消费者列表 ) 外，还有 “routes”( 路由规则列表 ) 和 “configurations”( 配置规则列表 )。</li>
<li> URL 层：URL ，根据不同 Type 目录，下面可以是服务提供者 URL 、服务消费者 URL 、路由规则 URL 、配置规则 URL 。</li>
<li> 实际上 URL 上带有 “category” 参数，已经能判断每个 URL 的分类，但是 Zookeeper 是基于节点目录订阅的，所以增加了 Type 层。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo Provider 如何实现优雅停机？</p>
<ul>
<li>Dubbo 是通过 JDK 的 ShutdownHook 来完成优雅停机的，所以如果用户使用 kill -9 PID 等强制关闭指令，是不会执行优雅停机的，只有通过 kill PID 时，才会执行。</li>
<li>因为大多数情况下，Dubbo 的声明周期是交给 Spring 进行管理，所以在最新的 Dubbo 版本中，增加了对 Spring 关闭事件的监听，从而关闭 Dubbo 服务</li>
<li>服务提供方的优雅停机过程<ul>
<li>首先，从注册中心中取消注册自己，从而使消费者不要再拉取到它。</li>
<li>然后，sleep 10 秒( 可配 )，等到服务消费，接收到注册中心通知到该服务提供者已经下线，加大了在不重试情况下优雅停机的成功率。</li>
<li>之后，广播 READONLY 事件给所有 Consumer 们，告诉它们不要在调用我了！！！如果此处注册中心挂掉的情况，依然能达到告诉 Consumer ，我要下线了的功能。</li>
<li>再之后，sleep 10 毫秒，保证 Consumer 们，尽可能接收到该消息。</li>
<li>再再之后，先标记为不接收新请求，新请求过来时直接报错，让客户端重试其它机器。</li>
<li>再再再之后，关闭心跳线程。</li>
<li>最后，检测线程池中的线程是否正在运行，如果有，等待所有线程执行完成，除非超时，则强制关闭。</li>
<li>最最后，关闭服务器。</li>
</ul>
</li>
<li>服务消费方的优雅停机过程<ul>
<li>停止时，不再发起新的调用请求，所有新的调用在客户端即报错。</li>
<li>然后，检测有没有请求的响应还没有返回，等待响应返回，除非超时，则强制关闭。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo Provider 异步关闭时，如何从注册中心下线？</p>
<ul>
<li>服务提供者，注册到 Zookeeper 上时，创建的是 EPHEMERAL 临时节点。所以在服务提供者异常关闭时，等待 Zookeeper 会话超时，那么该临时节点就会自动删除。</li>
</ul>
</li>
<li><p>Dubbo Consumer 只能调用从注册中心获取的 Provider 么？</p>
<ul>
<li>不是，Consumer 可以强制直连 Provider 。</li>
<li>在开发及测试环境下，经常需要绕过注册中心，只测试指定服务提供者，这时候可能需要点对点直连，点对点直连方式，将以服务接口为单位，忽略注册中心的提供者列表，A 接口配置点对点，不影响 B 接口从注册中心获取列表。</li>
<li>另外，直连 Dubbo Provider 时，如果要 Debug 调试 Dubbo Provider ，可以通过配置，禁用该 Provider 注册到注册中心。否则，会被其它 Consumer 调用到</li>
</ul>
</li>
<li><p>Dubbo 支持哪些通信协议？对应【protocol 远程调用层】。</p>
<ul>
<li>dubbo://</li>
<li>rest://</li>
<li>rmi://</li>
<li>webservice://</li>
<li>hessian://</li>
<li>thrift://</li>
<li>memcached://</li>
<li>redis://</li>
<li>http://</li>
</ul>
</li>
<li><p>什么是本地暴露和远程暴露，他们的区别？</p>
<ul>
<li>远程暴露:每次 Consumer 调用 Provider 都是跨进程，需要进行网络通信。</li>
<li>本地暴露:使用了 injvm:// 协议，是一个伪协议，它不开启端口，不发起远程调用，只在 JVM 内直接关联，但执行 Dubbo 的 Filter 链。</li>
</ul>
</li>
<li><p>Dubbo 使用什么通信框架？对应【transport 网络传输层】。</p>
<ul>
<li>Netty3</li>
<li>Netty4</li>
<li>Mina</li>
<li>Grizzly</li>
<li>在 Dubbo 的最新版本，默认使用 Netty4 的版本</li>
</ul>
</li>
<li><p>Dubbo 支持哪些序列化方式？对应【serialize 数据序列化层】。</p>
<ul>
<li>Dubbo 目前支付如下 7 种序列化方式：<ul>
<li>【重要】Hessian2 ：基于 Hessian 实现的序列化拓展。dubbo:// 协议的默认序列化方案。<ul>
<li>Hessian 除了是 Web 服务，也提供了其序列化实现，因此 Dubbo 基于它实现了序列化拓展。</li>
<li>另外，Dubbo 维护了自己的 hessian-lite ，对 Hessian 2 的 序列化 部分的精简、改进、BugFix 。</li>
</ul>
</li>
<li>Dubbo ：Dubbo 自己实现的序列化拓展。</li>
<li>Kryo ：基于 Kryo 实现的序列化拓展。</li>
<li>FST ：基于 FST 实现的序列化拓展。</li>
<li>JSON ：基于 Fastjson 实现的序列化拓展。</li>
<li>NativeJava ：基于 Java 原生的序列化拓展。</li>
<li>CompactedJava ：在 NativeJava 的基础上，实现了对 ClassDescriptor 的处理。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 有哪些负载均衡策略？对应【cluster 路由层】的 LoadBalance 组件。</p>
<ul>
<li>Random LoadBalance <ul>
<li>随机，按权重设置随机概率。</li>
<li>在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。</li>
</ul>
</li>
<li>RoundRobin LoadBalance<ul>
<li>轮询，按公约后的权重设置轮询比率。</li>
<li>存在慢的提供者累积请求的问题，比如</li>
</ul>
</li>
<li>LeastActive LoadBalance<ul>
<li>最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。</li>
<li>使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。</li>
</ul>
</li>
<li>ConsistentHash LoadBalance<ul>
<li>一致性 Hash，相同参数的请求总是发到同一提供者。</li>
<li>当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 有哪些集群容错策略？对应【cluster 路由层】的 Cluster 组件。</p>
<ul>
<li><p>Consumer 仅仅引用服务 ***-api.jar 包，那么可以获得到需要服务的 XXXService 接口。那么，通过动态创建对应调用 Dubbo 服务的实现类。简化代码如下：</p>
  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ProxyFactory.java</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * create proxy.</span></span><br><span class="line"><span class="comment"> * 创建 Proxy ，在引用服务调用。</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> invoker Invoker 对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> proxy</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Adaptive(&#123;Constants.PROXY_KEY&#125;)</span></span><br><span class="line">&lt;T&gt; <span class="function">T <span class="title">getProxy</span><span class="params">(Invoker&lt;T&gt; invoker)</span> <span class="keyword">throws</span> RpcException</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>方法参数 invoker ，实现了调用 Dubbo 服务的逻辑。</li>
<li>返回的 <T> 结果，就是 XXXService 的实现类，而这个实现类，就是通过动态代理的工具类进行生成。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo SPI 的设计思想是什么？</p>
<ul>
<li>？？？？？？？？？？？？？？？？？</li>
</ul>
</li>
<li><p>Dubbo 服务如何监控和管理？</p>
<ul>
<li>Dubbo 管理平台 + 监控平台<ul>
<li>dubbo-monitor 监控平台，基于 Dubbo 的【monitor 监控层】，实现相应的监控数据的收集到监控平台。</li>
<li>dubbo-admin 管理平台，基于注册中心，可以获取到服务相关的信息。</li>
</ul>
</li>
<li>链路追踪<ul>
<li>目前能够实现链路追踪的组件还是比较多的，如下：<ul>
<li>Apache SkyWalking 【推荐】</li>
<li>Zipkin</li>
<li>Cat</li>
<li>PinPoint</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 服务如何做降级？比如说服务 A 调用服务 B，结果服务 B 挂掉了。服务 A 再重试几次调用服务 B，还是不行，那么直接降级，走一个备用的逻辑，给用户返回响应。</p>
<ul>
<li>Dubbo 原生自带的服务降级功能：不能实现现代微服务的熔断器的功能</li>
<li>引入支持服务降级的组件<ul>
<li>目前开源社区常用的有两种组件支持服务降级的功能，分别是：<ul>
<li>Alibaba Sentinel</li>
<li>Netflix Hystrix</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 如何做限流？</p>
<ul>
<li>Dubbo 原生自带的限流功能：通过 TpsLimitFilter 实现，仅适用于服务提供者</li>
<li>引入支持限流的组件：推荐集成 Sentinel 组件。</li>
</ul>
</li>
<li><p>Dubbo 的失败重试是什么？</p>
<ul>
<li>所谓失败重试，就是 consumer 调用 provider 要是失败了，比如抛异常了，此时应该是可以重试的，或者调用超时了也可以重试。</li>
<li>实际场景下，我们一般会禁用掉重试。因为，因为超时后重试会有问题，超时你不知道是成功还是失败。例如，可能会导致两次扣款的问题。</li>
<li>所以，我们一般使用 failfast 集群容错策略，而不是 failover 策略。配置如下：  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dubbo:service</span> <span class="attr">cluster</span>=<span class="string">&quot;failfast&quot;</span> <span class="attr">timeout</span>=<span class="string">&quot;2000&quot;</span> /&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>一定一定一定要配置适合自己业务的超时时间。</li>
<li>当然，可以将操作分成读和写两种，前者支持重试，后者不支持重试。因为，读操作天然具有幂等性。</li>
</ul>
</li>
<li><p>Dubbo 支持哪些注册中心？</p>
<ul>
<li>【默认】Zookeeper </li>
<li>Redis </li>
<li>Multicast</li>
<li>Simple 注册中心</li>
<li>Nacos </li>
</ul>
</li>
<li><p>Dubbo 如何升级接口？</p>
<ul>
<li>当一个接口实现，出现不兼容升级时，可以用版本号过渡，版本号不同的服务相互间不引用。</li>
<li>可以按照以下的步骤进行版本迁移：<ul>
<li>在低压力时间段，先升级一半提供者为新版本。</li>
<li>再将所有消费者升级为新版本。</li>
<li>然后将剩下的一半提供者升级为新版本。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 在安全机制方面是如何解决的？</p>
<pre><code> - ![](https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850320360538.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10)
</code></pre>
<ul>
<li>通过令牌验证在注册中心控制权限，以决定要不要下发令牌给消费者，可以防止消费者绕过注册中心访问提供者。</li>
<li>另外通过注册中心可灵活改变授权方式，而不需修改或升级提供者。</li>
</ul>
</li>
<li><p>Dubbo 需要 Web 容器吗？Dubbo 服务启动是否需要启动类似 Tomcat、Jetty 等服务器。</p>
<ul>
<li>这个答案可以是，也可以是不是。为什么呢？根据协议的不同，Provider 会启动不同的服务器。<ul>
<li>在使用 dubbo:// 协议时，答案是否，因为 Provider 启动 Netty、Mina 等 NIO Server 。</li>
<li>在使用 rest:// 协议时，答案是是，Provider 启动 Tomcat、Jetty 等 HTTP 服务器，或者也可以使用 Netty 封装的 HTTP 服务器。</li>
<li>在使用 hessian:// 协议时，答案是是，Provider 启动 Jetty、Tomcat 等 HTTP 服务器。</li>
</ul>
</li>
</ul>
</li>
<li><p>为什么要将系统进行拆分？SOA?微服务?</p>
<ul>
<li>维护成本</li>
<li>分布式挑战</li>
</ul>
</li>
<li><p>Dubbo 如何集成配置中心？</p>
<ul>
<li>对于使用了 Dubbo 的系统，配置分成两类：<ul>
<li>① Dubbo 自身配置。如：Dubbo 请求超时，Dubbo 重试次数等等。</li>
<li>② 非 Dubbo 自身配置<ul>
<li>基建配置，例如：数据库、Redis 等配置。</li>
<li>业务配置，例如：订单超时时间，下单频率等等配置。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 如何实现分布式事务？</p>
<ul>
<li>？？？？？？？？？？？？？？？？？？</li>
</ul>
</li>
<li><p>Spring Cloud 与 Dubbo 怎么选择？</p>
<ul>
<li>？？？？？？？？？？？？</li>
</ul>
</li>
<li><p>如何自己设计一个类似 Dubbo 的 RPC 框架？</p>
<ul>
<li>服务提供者在注册中心服务发布</li>
<li>消费者去注册中心拿对应的服务信息</li>
<li>基于动态代理发起请求</li>
<li>负载均衡算法</li>
<li>通信方式，序列化方式</li>
<li>服务提供者生成一个动态代理，监听某个网络端口，然后代理本地的服务代码。接收到请求的时候，就调用对应的服务代码</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Dubbo</category>
      </categories>
      <tags>
        <tag>Dubbo</tag>
        <tag>RPC</tag>
        <tag>面试宝典</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive进阶</title>
    <url>/2021/11/07/Hive%E8%BF%9B%E9%98%B6/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Hive进阶"><a href="#Hive进阶" class="headerlink" title="Hive进阶"></a>Hive进阶</h1><h1 id="第1章-Explain-查看执行计划"><a href="#第1章-Explain-查看执行计划" class="headerlink" title="第1章 Explain 查看执行计划"></a>第1章 Explain 查看执行计划</h1><h2 id="1-1-创建测试用表"><a href="#1-1-创建测试用表" class="headerlink" title="1.1 创建测试用表"></a>1.1 创建测试用表</h2><ol>
<li><p>建大表、小表和 JOIN 后表的语句</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建大表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable (</span><br><span class="line">    id        <span class="type">bigint</span>,</span><br><span class="line">    t         <span class="type">bigint</span>,</span><br><span class="line">    uid       string,</span><br><span class="line">    keyword   string,</span><br><span class="line">    url_rank  <span class="type">int</span>,</span><br><span class="line">    click_num <span class="type">int</span>,</span><br><span class="line">    click_url string</span><br><span class="line">) <span class="type">row</span> format delimited</span><br><span class="line">    fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建小表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> smalltable</span><br><span class="line">(</span><br><span class="line">    id        <span class="type">bigint</span>,</span><br><span class="line">    t         <span class="type">bigint</span>,</span><br><span class="line">    uid       string,</span><br><span class="line">    keyword   string,</span><br><span class="line">    url_rank  <span class="type">int</span>,</span><br><span class="line">    click_num <span class="type">int</span>,</span><br><span class="line">    click_url string</span><br><span class="line">) <span class="type">row</span> format delimited</span><br><span class="line">    fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建 JOIN 后表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> jointable</span><br><span class="line">(</span><br><span class="line">    id        <span class="type">bigint</span>,</span><br><span class="line">    t         <span class="type">bigint</span>,</span><br><span class="line">    uid       string,</span><br><span class="line">    keyword   string,</span><br><span class="line">    url_rank  <span class="type">int</span>,</span><br><span class="line">    click_num <span class="type">int</span>,</span><br><span class="line">    click_url string</span><br><span class="line">) <span class="type">row</span> format delimited</span><br><span class="line">    fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>分别向大表和小表中导入数据</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/data/bigtable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> bigtable;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/data/smalltable&#x27;</span> <span class="keyword">into</span>  <span class="keyword">table</span> smalltable;</span><br></pre></td></tr></table></figure>
<h2 id="1-2-基本语法"><a href="#1-2-基本语法" class="headerlink" title="1.2 基本语法"></a>1.2 基本语法</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">EXPLAIN [EXTENDED <span class="operator">|</span> DEPENDENCY <span class="operator">|</span> <span class="keyword">AUTHORIZATION</span>] query<span class="operator">-</span><span class="keyword">sql</span></span><br></pre></td></tr></table></figure>
<h2 id="1-3-案例实操"><a href="#1-3-案例实操" class="headerlink" title="1.3 案例实操"></a>1.3 案例实操</h2></li>
<li><p>查看下面这条语句的执行计划</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> explain <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> bigtable;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                      Explain                       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> STAGE DEPENDENCIES:                                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage<span class="number">-0</span> <span class="keyword">is</span> a root stage                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> STAGE PLANS:                                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage: Stage<span class="number">-0</span>                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>     <span class="keyword">Fetch</span> Operator                                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       limit: <span class="number">-1</span>                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Processor Tree:                              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>         TableScan                                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           alias: bigtable                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           <span class="keyword">Select</span> Operator                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             expressions: id (type: <span class="type">bigint</span>), t (type: <span class="type">bigint</span>), uid (type: string), keyword (type: string), url_rank (type: <span class="type">int</span>), click_num (type: <span class="type">int</span>), click_url (type: string) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             ListSink                               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">17</span> <span class="keyword">rows</span> selected (<span class="number">0.112</span> seconds)</span><br><span class="line"></span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span>  explain <span class="keyword">select</span> click_url, <span class="built_in">count</span>(<span class="operator">*</span>) ct <span class="keyword">from</span> bigtable <span class="keyword">group</span> <span class="keyword">by</span> click_url;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                      Explain                       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> STAGE DEPENDENCIES:                                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage<span class="number">-1</span> <span class="keyword">is</span> a root stage                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage<span class="number">-0</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-1</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> STAGE PLANS:                                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage: Stage<span class="number">-1</span>                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>     Map Reduce                                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Map Operator Tree:                           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           TableScan                                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             alias: bigtable                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             <span class="keyword">Select</span> Operator                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               expressions: click_url (type: string) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               outputColumnNames: click_url         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               <span class="keyword">Group</span> <span class="keyword">By</span> Operator                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 aggregations: <span class="built_in">count</span>()              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 keys: click_url (type: string)     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 mode: hash                         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 outputColumnNames: _col0, _col1    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 Reduce Output Operator             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   key expressions: _col0 (type: string) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   sort <span class="keyword">order</span>: <span class="operator">+</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   Map<span class="operator">-</span>reduce <span class="keyword">partition</span> columns: _col0 (type: string) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   <span class="keyword">value</span> expressions: _col1 (type: <span class="type">bigint</span>) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Execution mode: vectorized                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Reduce Operator Tree:                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>         <span class="keyword">Group</span> <span class="keyword">By</span> Operator                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           aggregations: <span class="built_in">count</span>(VALUE._col0)         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           keys: KEY._col0 (type: string)           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           mode: mergepartial                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           outputColumnNames: _col0, _col1          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           File Output Operator                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             compressed: <span class="literal">false</span>                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             <span class="keyword">table</span>:                                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 input format: org.apache.hadoop.mapred.SequenceFileInputFormat <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage: Stage<span class="number">-0</span>                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>     <span class="keyword">Fetch</span> Operator                                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       limit: <span class="number">-1</span>                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Processor Tree:                              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>         ListSink                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">49</span> <span class="keyword">rows</span> selected (<span class="number">0.072</span> seconds)</span><br></pre></td></tr></table></figure>
<ul>
<li>STAGE DEPENDENCIES: 各个stage之间的依赖性</li>
<li>STAGE PLANS: 各个stage的执行计划</li>
<li>Map Operator Tree: MAP端的执行计划树</li>
<li>Reduce Operator Tree: Reduce端的执行计划树</li>
<li>TableScan: 表扫描操作，常见的属性：<ul>
<li>alias： 表名称</li>
<li>Statistics： 表统计信息，包含表中数据条数，数据大小等</li>
<li>Select Operator： 选取操作，常见的属性 ：</li>
<li>expressions：需要的字段名称及字段类型</li>
<li>outputColumnNames：输出的列名称</li>
</ul>
</li>
<li>Group By Operator：分组聚合操作，常见的属性：<ul>
<li>aggregations：显示聚合函数信息</li>
<li>mode：聚合模式，值有 hash：随机聚合，就是hash partition；partial：局部聚合；final：最终聚合</li>
<li>keys：分组的字段，如果没有分组，则没有此字段</li>
<li>outputColumnNames：聚合之后输出列名</li>
<li>Statistics： 表统计信息，包含分组聚合之后的数据条数，数据大小等</li>
</ul>
</li>
<li>Reduce Output Operator：输出到reduce操作，常见属性：<ul>
<li>sort order：值为空 不排序；值为 + 正序排序，值为 - 倒序排序；值为 +- 排序的列为两列，第一列为正序，第二列为倒序</li>
</ul>
</li>
<li>Filter Operator：过滤操作，常见的属性：<ul>
<li>predicate：过滤条件，如sql语句中的where id&gt;=1，则此处显示(id &gt;= 1)</li>
</ul>
</li>
<li>Map Join Operator：join 操作，常见的属性：<ul>
<li>condition map：join方式 ，如Inner Join 0 to 1 Left Outer Join0 to 2</li>
<li>keys: join 的条件字段</li>
<li>outputColumnNames： join 完成之后输出的字段</li>
<li>Statistics： join 完成之后生成的数据条数，大小等</li>
</ul>
</li>
<li>File Output Operator：文件输出操作，常见的属性<ul>
<li>compressed：是否压缩</li>
<li>table：表的信息，包含输入输出文件格式化方式，序列化方式等</li>
</ul>
</li>
<li>Fetch Operator 客户端获取数据操作，常见的属性：<ul>
<li>limit，值为 -1 表示不限制条数，其他值为限制的条数</li>
</ul>
</li>
</ul>
</li>
<li><p>查看详细执行计划</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain extended <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> bigtable;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain extended <span class="keyword">select</span> click_url, <span class="built_in">count</span>(<span class="operator">*</span>) ct <span class="keyword">from</span> bigtable <span class="keyword">group</span> <span class="keyword">by</span> click_url;</span><br></pre></td></tr></table></figure>
<h1 id="第2章-Hive-建表优化"><a href="#第2章-Hive-建表优化" class="headerlink" title="第2章 Hive 建表优化"></a>第2章 Hive 建表优化</h1><h2 id="2-1-分区表"><a href="#2-1-分区表" class="headerlink" title="2.1 分区表"></a>2.1 分区表</h2><h2 id="2-2-分桶表"><a href="#2-2-分桶表" class="headerlink" title="2.2 分桶表"></a>2.2 分桶表</h2><h2 id="2-3-合适的文件格式"><a href="#2-3-合适的文件格式" class="headerlink" title="2.3 合适的文件格式"></a>2.3 合适的文件格式</h2><h2 id="2-4-合适的压缩格式"><a href="#2-4-合适的压缩格式" class="headerlink" title="2.4 合适的压缩格式"></a>2.4 合适的压缩格式</h2></li>
</ol>
<h1 id="第3章-HQL-语法优化"><a href="#第3章-HQL-语法优化" class="headerlink" title="第3章 HQL 语法优化"></a>第3章 HQL 语法优化</h1><h2 id="3-1-列裁剪与分区裁剪"><a href="#3-1-列裁剪与分区裁剪" class="headerlink" title="3.1 列裁剪与分区裁剪"></a>3.1 列裁剪与分区裁剪</h2><ul>
<li>列裁剪就是在查询时只读取需要的列，分区裁剪就是只读取需要的分区。当列很多或者<br>数据量很大时，如果 select * 或者不指定分区，全列扫描和全表扫描效率都很低。</li>
<li>Hive 在读数据的时候，可以只读取查询中所需要用到的列，而忽略其他的列。这样做<br>可以节省读取开销：中间表存储开销和数据整合开销。</li>
</ul>
<h2 id="3-2-Group-By"><a href="#3-2-Group-By" class="headerlink" title="3.2 Group By"></a>3.2 Group By</h2><ol>
<li>开启 Map 端聚合参数设置<ol>
<li>是否在 Map 端进行聚合，默认为 True<br> <code>set hive.map.aggr = true; </code></li>
<li>在 Map 端进行聚合操作的条目数目<br> <code>set hive.groupby.mapaggr.checkinterval = 100000;</code></li>
<li>有数据倾斜的时候进行负载均衡（默认是 false）<br> <code>set hive.groupby.skewindata = true;</code><br> 当选项设定为 true，生成的查询计划会有两个 MR Job。<ul>
<li>第一个 MR Job 中，Map 的输出结果会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce中，从而达到负载均衡的目的；</li>
<li>第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作（虽然能解决数据倾斜，但是不能让运行速度的更快）。<h2 id="3-3-Vectorization"><a href="#3-3-Vectorization" class="headerlink" title="3.3 Vectorization"></a>3.3 Vectorization</h2>vectorization : 矢量计算的技术，在计算类似scan, filter, aggregation的时候，vectorization技术以设置批处理的增量大小为 1024 行单次来达到比单条记录单次获得更高的效率。<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.vectorized.execution.enabled <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.vectorized.execution.reduce.enabled <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
</li>
</ol>
<h2 id="3-4-多重模式"><a href="#3-4-多重模式" class="headerlink" title="3.4 多重模式"></a>3.4 多重模式</h2><ul>
<li>如果碰到一堆 SQL，并且这一堆 SQL 的模式还一样。都是从同一个表进行扫描，做不<br>同的逻辑。</li>
<li>可优化的地方：如果有 n 条 SQL，每个 SQL 执行都会扫描一次这张表。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> .... <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> age <span class="operator">&gt;</span> <span class="number">17</span>;</span><br><span class="line"><span class="keyword">insert</span> .... <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> age <span class="operator">&gt;</span> <span class="number">18</span>;</span><br><span class="line"><span class="keyword">insert</span> .... <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> age <span class="operator">&gt;</span> <span class="number">19</span>;</span><br></pre></td></tr></table></figure></li>
<li>隐藏了一个问题：这种类型的 SQL 有多少个，那么最终。这张表就被全表扫描了多少次  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>A). <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> city<span class="operator">=</span> A;</span><br><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>B). <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> city<span class="operator">=</span> B;</span><br><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>c). <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> city<span class="operator">=</span> c;</span><br><span class="line"><span class="comment">-- 修改为：</span></span><br><span class="line"><span class="keyword">from</span> student</span><br><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>A) <span class="keyword">select</span> id,name,sex, age <span class="keyword">where</span> city<span class="operator">=</span> A</span><br><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>B) <span class="keyword">select</span> id,name,sex, age <span class="keyword">where</span> city<span class="operator">=</span> B</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>如果一个 HQL 底层要执行 10 个 Job，那么能优化成 8 个一般来说，肯定能有所提高，多重插入就是一个非常实用的技能。一次读取，多次插入，有些场景是从一张表读取数据后，要多次利用。</li>
</ul>
</li>
</ul>
<h2 id="3-5-in-exists-语句"><a href="#3-5-in-exists-语句" class="headerlink" title="3.5 in/exists 语句"></a>3.5 in/exists 语句</h2><ul>
<li>使用 Hive 的一个高效替代方案：left semi join</li>
<li>比如：– in / exists 实现  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">where</span> a.id <span class="keyword">in</span> (<span class="keyword">select</span> b.id <span class="keyword">from</span> b);</span><br><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">where</span> <span class="keyword">exists</span> (<span class="keyword">select</span> id <span class="keyword">from</span> b <span class="keyword">where</span> a.id <span class="operator">=</span> b.id);</span><br></pre></td></tr></table></figure></li>
<li>可以使用 join 来改写：  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">join</span> b <span class="keyword">on</span> a.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure></li>
<li>应该转换成left semi join 实现  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">left</span> semi <span class="keyword">join</span> b <span class="keyword">on</span> a.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="3-6-CBO-优化"><a href="#3-6-CBO-优化" class="headerlink" title="3.6 CBO 优化"></a>3.6 CBO 优化</h2><ul>
<li>CBO 优化可以自动优化 HQL 中多个 Join 的顺序，并选择合适的 Join 算法。代价最小的执行计划就是最好的执行计划</li>
<li>要使用基于成本的优化（也称为 CBO），请在查询开始设置以下参数：<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.cbo.enable<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.compute.query.using.stats<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.stats.fetch.column.stats<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.stats.fetch.partition.stats<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="3-7-谓词下推"><a href="#3-7-谓词下推" class="headerlink" title="3.7 谓词下推"></a>3.7 谓词下推</h2><ul>
<li>将 SQL 语句中的 where 谓词逻辑都尽可能提前执行，减少下游处理的数据量。对应逻辑优化器是 PredicatePushDown，配置项为 hive.optimize.ppd，默认为 true。</li>
<li>案例实操：</li>
</ul>
<ol>
<li><p>打开谓词下推优化属性</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 谓词下推，默认是 true</span></span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">set</span> hive.optimize.ppd;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"><span class="operator">|</span>           <span class="keyword">set</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"><span class="operator">|</span> hive.optimize.ppd<span class="operator">=</span><span class="literal">true</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.01</span> seconds) </span><br></pre></td></tr></table></figure></li>
<li><p>查看先关联两张表，再用 where 条件过滤的执行计划</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain <span class="keyword">select</span> o.id <span class="keyword">from</span> bigtable b <span class="keyword">join</span> bigtable o <span class="keyword">on</span> o.id <span class="operator">=</span> b.id <span class="keyword">where</span> o.id <span class="operator">&lt;=</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure></li>
<li><p>查看子查询后，再关联表的执行计划</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain <span class="keyword">select</span> b.id <span class="keyword">from</span> bigtable b <span class="keyword">join</span> (<span class="keyword">select</span> id <span class="keyword">from</span> bigtable <span class="keyword">where</span> id <span class="operator">&lt;=</span> <span class="number">10</span>) o <span class="keyword">on</span> b.id <span class="operator">=</span> o.id;</span><br></pre></td></tr></table></figure>
<h2 id="3-8-MapJoin"><a href="#3-8-MapJoin" class="headerlink" title="3.8 MapJoin"></a>3.8 MapJoin</h2><p>MapJoin 是将 Join 双方比较小的表直接分发到各个 Map 进程的内存中，在 Map 进程中进行 Join 操 作，这样就不用进行 Reduce 步骤，从而提高了速度。如果不指定 MapJoin或者不符合 MapJoin 的条件，那么 Hive 解析器会将 Join 操作转换成 Common Join，即：在Reduce 阶段完成 Join。容易发生数据倾斜。可以用 MapJoin 把小表全部加载到内存在 Map端进行 Join，避免 Reducer 处理。</p>
</li>
<li><p>开启 MapJoin 参数设置</p>
<ol>
<li>设置自动选择 MapJoin <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 默认为 true</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join<span class="operator">=</span><span class="literal">true</span>; </span><br></pre></td></tr></table></figure></li>
<li>大表小表的阈值设置（默认 25M 以下认为是小表）： <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize<span class="operator">=</span><span class="number">25000000</span>;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>MapJoin 工作机制</p>
<ul>
<li>MapJoin 是将 Join 双方比较小的表直接分发到各个 Map 进程的内存中，在 Map 进程中进行 Join 操作，这样就不用进行 Reduce 步骤，从而提高了速度。</li>
</ul>
</li>
<li><p>案例实操：</p>
<ol>
<li>开启 MapJoin 功能 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 默认为 true</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join <span class="operator">=</span> <span class="literal">true</span>; </span><br></pre></td></tr></table></figure></li>
<li>执行小表 JOIN 大表语句<ul>
<li>注意：此时小表(左连接)作为主表，所有数据都要写出去，因此此时会走 reduce，mapjoin失效<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">Explain <span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> smalltable s</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> bigtable b</span><br><span class="line"><span class="keyword">on</span> s.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>执行大表 JOIN 小表语句 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">Explain <span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> bigtable b</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> smalltable s</span><br><span class="line"><span class="keyword">on</span> s.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure>
<h2 id="3-9-大表、大表-SMB-Join"><a href="#3-9-大表、大表-SMB-Join" class="headerlink" title="3.9 大表、大表 SMB Join"></a>3.9 大表、大表 SMB Join</h2></li>
</ol>
</li>
</ol>
<ul>
<li>SMB Join ：Sort Merge Bucket Join</li>
</ul>
<h2 id="3-10-笛卡尔积"><a href="#3-10-笛卡尔积" class="headerlink" title="3.10 笛卡尔积"></a>3.10 笛卡尔积</h2><p>Join 的时候不加 on 条件，或者无效的 on 条件，因为找不到 Join key，Hive 只能使用1 个 Reducer 来完成笛卡尔积。当 Hive 设定为严格模式（hive.mapred.mode=strictnonstrict） 时，不允许在 HQL 语句中出现笛卡尔积。</p>
<h1 id="第4章-数据倾斜"><a href="#第4章-数据倾斜" class="headerlink" title="第4章 数据倾斜"></a>第4章 数据倾斜</h1><h2 id="4-1-单表数据倾斜优化"><a href="#4-1-单表数据倾斜优化" class="headerlink" title="4.1 单表数据倾斜优化"></a>4.1 单表数据倾斜优化</h2><h3 id="4-1-1-使用参数"><a href="#4-1-1-使用参数" class="headerlink" title="4.1.1 使用参数"></a>4.1.1 使用参数</h3><p>当任务中存在 GroupBy 操作同时聚合函数为 count 或者 sum 可以设置参数来处理数据<br>倾斜问题。</p>
<ul>
<li>是否在 Map 端进行聚合，默认为 True  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>在 Map 端进行聚合操作的条目数目  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval <span class="operator">=</span> <span class="number">100000</span>;</span><br></pre></td></tr></table></figure></li>
<li>有数据倾斜的时候进行负载均衡（默认是 false）  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>当选项设定为 true，生成的查询计划会有两个 MR Job。</li>
</ul>
</li>
</ul>
<h3 id="4-1-2-增加-Reduce-数量"><a href="#4-1-2-增加-Reduce-数量" class="headerlink" title="4.1.2 增加 Reduce 数量"></a>4.1.2 增加 Reduce 数量</h3><ol>
<li>调整 reduce 个数方法一<ol>
<li>每个 Reduce 处理的数据量默认是 256MB <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer <span class="operator">=</span> <span class="number">256000000</span></span><br></pre></td></tr></table></figure></li>
<li>每个任务最大的 reduce 数，默认为 1009 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.max <span class="operator">=</span> <span class="number">1009</span></span><br></pre></td></tr></table></figure></li>
<li>计算 reducer 数的公式 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">N<span class="operator">=</span><span class="built_in">min</span>(参数 <span class="number">2</span>，总输入数据量<span class="operator">/</span>参数 <span class="number">1</span>)(参数 <span class="number">2</span> 指的是上面的 <span class="number">1009</span>，参数 <span class="number">1</span> 值得是 <span class="number">256</span>M)</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>调整 reduce 个数方法二<ol>
<li>在 hadoop 的 mapred-default.xml 文件中修改设置每个 job 的 Reduce 个数 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces <span class="operator">=</span> <span class="number">15</span>;</span><br></pre></td></tr></table></figure>
<h2 id="4-2-Join-数据倾斜优化"><a href="#4-2-Join-数据倾斜优化" class="headerlink" title="4.2 Join 数据倾斜优化"></a>4.2 Join 数据倾斜优化</h2><h3 id="4-2-1-使用参数"><a href="#4-2-1-使用参数" class="headerlink" title="4.2.1 使用参数"></a>4.2.1 使用参数</h3></li>
</ol>
</li>
</ol>
<ul>
<li>在编写 Join 查询语句时，如果确定是由于 join 出现的数据倾斜，那么请做如下设置：  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># <span class="keyword">join</span> 的键对应的记录条数超过这个值则会进行分拆，值根据具体数据量设置</span><br><span class="line"><span class="keyword">set</span> hive.skewjoin.key<span class="operator">=</span><span class="number">100000</span>;</span><br><span class="line"># 如果是 <span class="keyword">join</span> 过程出现倾斜应该设置为 <span class="literal">true</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.skewjoin<span class="operator">=</span><span class="literal">false</span>;</span><br></pre></td></tr></table></figure></li>
<li>如果开启了，在 Join 过程中 Hive 会将计数超过阈值 hive.skewjoin.key（默认 100000）的倾斜 key 对应的行临时写进文件中，然后再启动另一个 job 做 map join 生成结果。通过hive.skewjoin.mapjoin.map.tasks 参数还可以控制第二个 job 的 mapper 数量，默认 10000。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.skewjoin.mapjoin.map.tasks<span class="operator">=</span><span class="number">10000</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="4-2-2-MapJoin"><a href="#4-2-2-MapJoin" class="headerlink" title="4.2.2 MapJoin"></a>4.2.2 MapJoin</h2><h1 id="第5章-Hive-Job-优化"><a href="#第5章-Hive-Job-优化" class="headerlink" title="第5章 Hive Job 优化"></a>第5章 Hive Job 优化</h1><h2 id="5-1-Hive-Map-优化"><a href="#5-1-Hive-Map-优化" class="headerlink" title="5.1 Hive Map 优化"></a>5.1 Hive Map 优化</h2><h3 id="5-1-1-复杂文件增加-Map-数"><a href="#5-1-1-复杂文件增加-Map-数" class="headerlink" title="5.1.1 复杂文件增加 Map 数"></a>5.1.1 复杂文件增加 Map 数</h3><ul>
<li><p>当 input 的文件都很大，任务逻辑复杂，map 执行非常慢的时候，可以考虑增加 Map 数，来使得每个 map 处理的数据量减少，从而提高任务的执行效率。增加 map 的方法为：根据<code>computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M </code>公式，调整 maxSize 最大值。让 maxSize 最大值低于 blocksize 就可以增加 map 的个数。</p>
</li>
<li><p>案例实操：</p>
<ol>
<li>执行查询 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> emp;</span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage<span class="number">-1</span>: number <span class="keyword">of</span> mappers: <span class="number">1</span>; number <span class="keyword">of</span> reducers: <span class="number">1</span></span><br></pre></td></tr></table></figure></li>
<li>设置最大切片值为 100 个字节 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.input.fileinputformat.split.maxsize<span class="operator">=</span><span class="number">100</span>;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> emp;</span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage<span class="number">-1</span>: number <span class="keyword">of</span> mappers: <span class="number">6</span>; number <span class="keyword">of</span> reducers: <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="5-1-2-小文件进行合并"><a href="#5-1-2-小文件进行合并" class="headerlink" title="5.1.2 小文件进行合并"></a>5.1.2 小文件进行合并</h2></li>
</ol>
</li>
</ul>
<ol>
<li>在 map 执行前合并小文件，减少 map 数：CombineHiveInputFormat 具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat 没有对小文件合并功能。 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure></li>
<li>在 Map-Reduce 的任务结束时合并小文件的设置： <ul>
<li>在 map-only 任务结束时合并小文件，默认 true  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.merge.mapfiles <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>在 map-reduce 任务结束时合并小文件，默认 false  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.merge.mapredfiles <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>合并文件的大小，默认 256M  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.merge.size.per.task <span class="operator">=</span> <span class="number">268435456</span>;</span><br></pre></td></tr></table></figure></li>
<li>当输出文件的平均大小小于该值时，启动一个独立的 map-reduce 任务进行文件 merge  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.merge.smallfiles.avgsize <span class="operator">=</span> <span class="number">16777216</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="5-1-3-Map-端聚合"><a href="#5-1-3-Map-端聚合" class="headerlink" title="5.1.3 Map 端聚合"></a>5.1.3 Map 端聚合</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 相当于 map 端执行 combiner</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<h3 id="5-1-4-推测执行"><a href="#5-1-4-推测执行" class="headerlink" title="5.1.4 推测执行"></a>5.1.4 推测执行</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">#默认是 <span class="literal">true</span></span><br><span class="line"><span class="keyword">set</span> mapred.map.tasks.speculative.execution <span class="operator">=</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h2 id="5-2-Hive-Reduce-优化"><a href="#5-2-Hive-Reduce-优化" class="headerlink" title="5.2 Hive Reduce 优化"></a>5.2 Hive Reduce 优化</h2><h3 id="5-2-1-合理设置-Reduce-数"><a href="#5-2-1-合理设置-Reduce-数" class="headerlink" title="5.2.1 合理设置 Reduce 数"></a>5.2.1 合理设置 Reduce 数</h3><ol>
<li>调整 reduce 个数方法一<ol>
<li>每个 Reduce 处理的数据量默认是 256MB <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer <span class="operator">=</span> <span class="number">256000000</span></span><br></pre></td></tr></table></figure></li>
<li>每个任务最大的 reduce 数，默认为 1009 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.max <span class="operator">=</span> <span class="number">1009</span></span><br></pre></td></tr></table></figure></li>
<li>计算 reducer 数的公式 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">N=min(参数 2，总输入数据量/参数 1)(参数 2 指的是上面的 1009，参数 1 值得是 256M)</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>调整 reduce 个数方法二<ul>
<li>在 hadoop 的 mapred-default.xml 文件中修改,设置每个 job 的 Reduce 个数  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces <span class="operator">=</span> <span class="number">15</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>reduce 个数并不是越多越好<ol>
<li>过多的启动和初始化 reduce 也会消耗时间和资源；</li>
<li>另外，有多少个 reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</li>
<li>在设置 reduce 个数的时候也需要考虑这两个原则：<ol>
<li>处理大数据量利用合适的 reduce 数；</li>
<li>使单个 reduce 任务处理数据量大小要合适；</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="5-3-2-推测执行"><a href="#5-3-2-推测执行" class="headerlink" title="5.3.2 推测执行"></a>5.3.2 推测执行</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># hadoop 里面的</span><br><span class="line">mapred.reduce.tasks.speculative.execution</span><br><span class="line"># hive 里面相同的参数，效果和hadoop 里面的一样两个随便哪个都行</span><br><span class="line">hive.mapred.reduce.tasks.speculative.execution</span><br></pre></td></tr></table></figure>

<h2 id="5-3-Hive-任务整体优化"><a href="#5-3-Hive-任务整体优化" class="headerlink" title="5.3 Hive 任务整体优化"></a>5.3 Hive 任务整体优化</h2><h3 id="5-3-1-Fetch-抓取"><a href="#5-3-1-Fetch-抓取" class="headerlink" title="5.3.1 Fetch 抓取"></a>5.3.1 Fetch 抓取</h3><ul>
<li>Fetch 抓取是指，Hive 中对某些情况的查询可以不必使用 MapReduce 计算。例如：<code>SELECT * FROM emp</code></li>
<li>在这种情况下，Hive 可以简单地读取 emp 对应的存储目录下的文件，然后输出<br>查询结果到控制台。</li>
<li>在 hive-default.xml.template 文件中 hive.fetch.task.conversion 默认是 more，老版本 hive默认是 minimal，该属性修改为 more 以后，在全局查找、字段查找、limit 查找等都不走mapreduce。  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.fetch.task.conversion<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>more<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">        Expects one of [none, minimal, more].</span><br><span class="line">        Some select queries can be converted to single FETCH task minimizing latency.</span><br><span class="line">        Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incurs RS), lateral views and joins.</span><br><span class="line">        0. none : disable hive.fetch.task.conversion</span><br><span class="line">        1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only</span><br><span class="line">        2. more : SELECT, FILTER, LIMIT only (support TABLESAMPLE and </span><br><span class="line">        virtual columns)</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop压缩、优化、高可用</title>
    <url>/2021/11/07/Hadoop%E5%8E%8B%E7%BC%A9%E3%80%81%E4%BC%98%E5%8C%96%E3%80%81%E9%AB%98%E5%8F%AF%E7%94%A8/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Hadoop压缩、优化、高可用"><a href="#Hadoop压缩、优化、高可用" class="headerlink" title="Hadoop压缩、优化、高可用"></a>Hadoop压缩、优化、高可用</h1><h1 id="一、Hadoop数据压缩"><a href="#一、Hadoop数据压缩" class="headerlink" title="一、Hadoop数据压缩"></a>一、Hadoop数据压缩</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><h3 id="1-1-1-压缩概述"><a href="#1-1-1-压缩概述" class="headerlink" title="1.1.1 压缩概述"></a>1.1.1 压缩概述</h3><ul>
<li>压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在运行MR程序时，I/O操作、网络数据传输、 Shuffle和Merge要花大量的时间，尤其是数据规模很大和工作负载密集的情况下，因此，使用数据压缩显得非常重要。</li>
<li>鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。可以在任意MapReduce阶段启用压缩。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价。<h2 id="1-1-2-压缩策略和原则"><a href="#1-1-2-压缩策略和原则" class="headerlink" title="1.1.2 压缩策略和原则"></a>1.1.2 压缩策略和原则</h2></li>
<li>压缩是提高Hadoop运行效率的一种优化策略</li>
<li>通过对Mapper、Reducer运行过程的数据进行压缩，以减少磁盘IO，提高MR程序运行速度</li>
<li>注意：采用压缩技术减少了磁盘IO，但同时增加了CPU运算负担。所以，压缩特性运用得当能提高性能，但运用不当也可能降低性能</li>
<li>压缩基本原则<ul>
<li>运算密集型的job，少用压缩</li>
<li>IO密集型的job，多用压缩</li>
</ul>
</li>
</ul>
<hr>
<h2 id="1-2-MR支持的压缩编码"><a href="#1-2-MR支持的压缩编码" class="headerlink" title="1.2 MR支持的压缩编码"></a>1.2 MR支持的压缩编码</h2><ul>
<li>支持的压缩编码<table>
<thead>
<tr>
<th>压缩格式</th>
<th>hadoop自带？</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
<th>换成压缩格式后，原来的程序是否需要修改</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>Gzip</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>bzip2</td>
<td>是，直接使用</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>LZO</td>
<td>否，需要安装</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
<td>需要建索引，还需要指定输入格式</td>
</tr>
<tr>
<td>Snappy</td>
<td>是，直接使用</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
</tbody></table>
</li>
<li>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示。<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码/解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
</li>
<li>压缩性能的比较<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB/s</td>
<td>74.6MB/s</td>
</tr>
</tbody></table>
</li>
</ul>
<hr>
<h2 id="1-3-压缩方式选择"><a href="#1-3-压缩方式选择" class="headerlink" title="1.3 压缩方式选择"></a>1.3 压缩方式选择</h2><h3 id="1-3-1-Gzip"><a href="#1-3-1-Gzip" class="headerlink" title="1.3.1 Gzip"></a>1.3.1 Gzip</h3><ul>
<li>优点：压缩率比较高，而且压缩/解压速度也比较快；Hadoop本身支持，在应用中处理Gzip格式的文件就和直接处理文本一样；大部分Linux系统都自带Gzip命令，使用方便。</li>
<li>缺点：不支持Split。</li>
<li>应用场景：当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用Gzip压缩格式。例如说一天或者一个小时的日志压缩成一个Gzip文件。</li>
</ul>
<h3 id="1-3-2-Bzip2"><a href="#1-3-2-Bzip2" class="headerlink" title="1.3.2 Bzip2"></a>1.3.2 Bzip2</h3><ul>
<li>优点：支持Split；具有很高的压缩率，比Gzip压缩率都高；Hadoop本身自带，使用方便</li>
<li>缺点：压缩/解压速度慢</li>
<li>应用场景：适合对速度要求不高，但需要较高的压缩率的时候；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持Split，而且兼容之前的应用程序的情况</li>
</ul>
<h3 id="1-3-3-Lzo压缩"><a href="#1-3-3-Lzo压缩" class="headerlink" title="1.3.3 Lzo压缩"></a>1.3.3 Lzo压缩</h3><ul>
<li>优点：压缩/解压速度也比较快，合理的压缩率；支持Split，是Hadoop中最流行的压缩格式；可以在Linux系统下安装lzop命令，使用方便</li>
<li>缺点：压缩率比Gzip要低一些；Hadoop本身不支持，需要安装；在应用中对Lzo格式的文件需要做一些特殊处理（为了支持Split需要建索引，还需要指定InputFormat为Lzo格式）</li>
<li>应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，Lzo优点越越明显</li>
</ul>
<h3 id="1-3-4-Snappy压缩"><a href="#1-3-4-Snappy压缩" class="headerlink" title="1.3.4 Snappy压缩"></a>1.3.4 Snappy压缩</h3><ul>
<li>优点：高速压缩速度和合理的压缩率</li>
<li>缺点：不支持Split；压缩率比Gzip要低；Hadoop本身不支持，需要安装</li>
<li>应用场景：当MapReduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个MapReduce作业的输出和另外一个MapReduce作业的输入</li>
</ul>
<hr>
<h2 id="1-4-压缩位置选择"><a href="#1-4-压缩位置选择" class="headerlink" title="1.4 压缩位置选择"></a>1.4 压缩位置选择</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345428872480.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<hr>
<h2 id="1-5-压缩参数配置"><a href="#1-5-压缩参数配置" class="headerlink" title="1.5 压缩参数配置"></a>1.5 压缩参数配置</h2><p>要在Hadoop中启用压缩，可以配置如下参数：<br>|参数    |默认值    |阶段    |建议|<br>|——|——|——|—-|<br>|io.compression.codecs（在core-site.xml中配置）    |无，这个需要在命令行输入hadoop checknative查看    |输入压缩    |Hadoop使用文件扩展名判断是否支持某种编解码器|<br>|mapreduce.map.output.compress（在mapred-site.xml中配置）    |false    |mapper输出    |这个参数设为true启用压缩|<br>|mapreduce.map.output.compress.codec（在mapred-site.xml中配置）    |org.apache.hadoop.io.compress.DefaultCodec    |mapper输出    |企业多使用LZO或Snappy编解码器在此阶段压缩数据|<br>|mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置）    |false    |reducer输出    |这个参数设为true启用压缩|<br>|mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置）    |org.apache.hadoop.io.compress.DefaultCodec    |reducer输出    |使用标准工具或者编解码器，如gzip和bzip2|<br>|mapreduce.output.fileoutputformat.compress.type（在mapred-site.xml中配置）    |RECORD    |reducer输出    |SequenceFile输出使用的压缩类型：NONE和BLOCK|</p>
<hr>
<h2 id="1-6-压缩实操案例"><a href="#1-6-压缩实操案例" class="headerlink" title="1.6 压缩实操案例"></a>1.6 压缩实操案例</h2><h3 id="1-6-1-数据流的压缩和解压缩"><a href="#1-6-1-数据流的压缩和解压缩" class="headerlink" title="1.6.1 数据流的压缩和解压缩"></a>1.6.1 数据流的压缩和解压缩</h3><ul>
<li>使用createOutputStream(OutputStreamout)方法创建一个CompressionOutputStream，将其以压缩格式写入底层的流</li>
<li>调用createInputStream(InputStreamin)函数，获得一个CompressionInputStream，从底层的流读取未压缩的数据</li>
</ul>
<h3 id="1-6-2-Map输出端压缩"><a href="#1-6-2-Map输出端压缩" class="headerlink" title="1.6.2 Map输出端压缩"></a>1.6.2 Map输出端压缩</h3><ul>
<li>即使MapReduce的输入输出文件都是未压缩的文件，仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可。  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 开启map端输出压缩</span></span><br><span class="line">conf.setBoolean(<span class="string">&quot;mapreduce.map.output.compress&quot;</span>, <span class="keyword">true</span>);</span><br><span class="line"><span class="comment">// 设置map端输出压缩方式</span></span><br><span class="line">conf.setClass(<span class="string">&quot;mapreduce.map.output.compress.codec&quot;</span>, BZip2Codec.class,CompressionCodec.class);</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="1-6-3-Reduce输出端压缩"><a href="#1-6-3-Reduce输出端压缩" class="headerlink" title="1.6.3 Reduce输出端压缩"></a>1.6.3 Reduce输出端压缩</h3><pre><code><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 设置reduce端输出压缩开启</span></span><br><span class="line">FileOutputFormat.setCompressOutput(job, <span class="keyword">true</span>);</span><br><span class="line"><span class="comment">// 设置压缩的方式</span></span><br><span class="line">FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class); </span><br></pre></td></tr></table></figure>
</code></pre>
<h1 id="二、Hadoop性能优化"><a href="#二、Hadoop性能优化" class="headerlink" title="二、Hadoop性能优化"></a>二、Hadoop性能优化</h1><h2 id="2-1-MapReduce跑得慢的原因"><a href="#2-1-MapReduce跑得慢的原因" class="headerlink" title="2.1 MapReduce跑得慢的原因"></a>2.1 MapReduce跑得慢的原因</h2><ol>
<li>计算机性能<ul>
<li>CPU，内存，磁盘，网络···</li>
</ul>
</li>
<li>I/O操作优化<ol>
<li>数据倾斜</li>
<li>Map和Reduce数设置不合理</li>
<li>Map运行时间太长，导致Reduce等待过久</li>
<li>小文件过多</li>
<li>大量不可切片的超大压缩文件</li>
<li>Spill次数过多</li>
<li>Merge次数过多</li>
</ol>
</li>
</ol>
<h2 id="2-2-MapReduce优化"><a href="#2-2-MapReduce优化" class="headerlink" title="2.2 MapReduce优化"></a>2.2 MapReduce优化</h2><ul>
<li>MapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题、常用的调优参数</li>
</ul>
<h3 id="2-2-1-数据输入"><a href="#2-2-1-数据输入" class="headerlink" title="2.2.1 数据输入"></a>2.2.1 数据输入</h3><ol>
<li>合并小文件：执行MR任务前将小文件合并，大量小文件会产生大量Map任务，增大Map任务装载次数，而任务的装载比较耗时，从而导致MR运行较慢</li>
<li>采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景</li>
</ol>
<h3 id="2-2-2-Map阶段"><a href="#2-2-2-Map阶段" class="headerlink" title="2.2.2 Map阶段"></a>2.2.2 Map阶段</h3><ol>
<li>减少溢写（Spill）次数：通过调整mapreduce.task.io.sort.mb及mapreduce.map.sort.spill.percent参数值，增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO</li>
<li>减少合并（Merge）次数：通过调整mapreduce.task.io.sort.factor参数，增大Merge的文件数目，减少Merge的次数，从而缩短MR处理时间</li>
<li>在Map之后，不影响业务逻辑前提下，先进行Combine处理，减少 I/O</li>
</ol>
<h3 id="2-2-3-Reduce阶段"><a href="#2-2-3-Reduce阶段" class="headerlink" title="2.2.3 Reduce阶段"></a>2.2.3 Reduce阶段</h3><ol>
<li>合理设置Map和Reduce数：两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致Map、Reduce任务间竞争资源，造成处理超时等错误</li>
<li>设置Map、Reduce共存：调整mapreduce.job.reduce.slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间</li>
<li>规避使用Reduce：因为Reduce在用于连接数据集的时候将会产生大量的网络消耗</li>
<li>合理设置Reduce端的Buffer：默认情况下，数据达到一个阈值的时候，Buffer中的数据就会写入磁盘，然后Reduce会从磁盘中获得所有的数据。也就是说，Buffer和Reduce是没有直接关联的，中间多次写磁盘-&gt;读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得Buffer中的一部分数据可以直接输送到Reduce，从而减少IO开销：<font color ='red' >mapreduce.reduce.input.buffer.percent，默认为0.0。当值大于0的时候，会保留指定比例的内存读Buffer中的数据直接拿给Reduce使用。这样一来，设置Buffer需要内存，读取数据需要内存，Reduce计算也要内存，所以要根据作业的运行情况进行调整</font></li>
</ol>
<h3 id="2-2-4-I-O传输"><a href="#2-2-4-I-O传输" class="headerlink" title="2.2.4 I/O传输"></a>2.2.4 I/O传输</h3><ol>
<li>采用数据压缩的方式，减少网络I/O传输的数据量，从而减少I/O传输时间，安装Snappy和LZO压缩编码器。</li>
<li>使用SequenceFile二进制文件</li>
</ol>
<h3 id="2-2-5-数据倾斜问题"><a href="#2-2-5-数据倾斜问题" class="headerlink" title="2.2.5 数据倾斜问题"></a>2.2.5 数据倾斜问题</h3><ol>
<li>数据倾斜现象<ul>
<li>数据频率倾斜——某一个区域的数据量要远远大于其他区域</li>
<li>数据大小倾斜——部分记录的大小远远大于平均值</li>
</ul>
</li>
<li>减少数据倾斜的方法<ul>
<li>方法1：抽样和范围分区<ol>
<li>可以通过对原始数据进行抽样得到的结果集来预设分区边界值</li>
</ol>
</li>
<li>方法2：自定义分区<ol>
<li>基于输出键的背景知识进行自定义分区。例如，如果Map输出键的单词来源于一本书。且其中某几个专业词汇较多。那么就可以自定义分区将这这些专业词汇发送给固定的一部分Reduce实例。而将其他的都发送给剩余的Reduce实例</li>
</ol>
</li>
<li>方法3：Combiner<ol>
<li>使用Combiner可以大量地减小数据倾斜。在可能的情况下，Combine的目的就是聚合并精简数据</li>
</ol>
</li>
<li>方法4：采用Map Join，尽量避免Reduce Join</li>
</ul>
</li>
</ol>
<h2 id="2-3常用的调优参数"><a href="#2-3常用的调优参数" class="headerlink" title="2.3常用的调优参数"></a>2.3常用的调优参数</h2><ol>
<li><p>资源相关参数：<br> 在MR应用程序中配置就可以生效（mapred-default.xml）</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.map.memory.mb</td>
<td>一个MapTask可使用的资源上限（单位:MB），默认为1024。如果MapTask实际使用的资源量超过该值，则会被强制杀死。</td>
</tr>
<tr>
<td>mapreduce.reduce.memory.mb</td>
<td>一个ReduceTask可使用的资源上限（单位:MB），默认为1024。如果ReduceTask实际使用的资源量超过该值，则会被强制杀死。</td>
</tr>
<tr>
<td>mapreduce.map.cpu.vcores</td>
<td>每个MapTask可使用的最多cpu core数目，默认值: 1</td>
</tr>
<tr>
<td>mapreduce.reduce.cpu.vcores</td>
<td>每个ReduceTask可使用的最多cpu core数目，默认值: 1</td>
</tr>
<tr>
<td>mapreduce.reduce.shuffle.parallelcopies</td>
<td>每个Reduce去Map中取数据的并行数。默认值是5</td>
</tr>
<tr>
<td>mapreduce.reduce.shuffle.merge.percent</td>
<td>Buffer中的数据达到多少比例开始写入磁盘。默认值0.66</td>
</tr>
<tr>
<td>mapreduce.reduce.shuffle.input.buffer.percent</td>
<td>Buffer大小占Reduce可用内存的比例。默认值0.7</td>
</tr>
<tr>
<td>mapreduce.reduce.input.buffer.percent</td>
<td>指定多少比例的内存用来存放Buffer中的数据，默认值是0.0</td>
</tr>
</tbody></table>
</li>
<li><p>在YARN启动之前就配置在服务器的配置文件中才能生效（yarn-default.xml）</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>yarn.scheduler.minimum-allocation-mb</td>
<td>给应用程序Container分配的最小内存，默认值：1024</td>
</tr>
<tr>
<td>yarn.scheduler.maximum-allocation-mb</td>
<td>给应用程序Container分配的最大内存，默认值：8192</td>
</tr>
<tr>
<td>yarn.scheduler.minimum-allocation-vcores</td>
<td>每个Container申请的最小CPU核数，默认值：1</td>
</tr>
<tr>
<td>yarn.scheduler.maximum-allocation-vcores</td>
<td>每个Container申请的最大CPU核数，默认值：32</td>
</tr>
<tr>
<td>yarn.nodemanager.resource.memory-mb</td>
<td>给Containers分配的最大物理内存，默认值：8192</td>
</tr>
</tbody></table>
</li>
<li><p>Shuffle性能优化的关键参数，应在YARN启动之前就配置好（mapred-default.xml）</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.task.io.sort.mb</td>
<td>Shuffle的环形缓冲区大小，默认100m</td>
</tr>
<tr>
<td>mapreduce.map.sort.spill.percent</td>
<td>环形缓冲区溢出的阈值，默认80%</td>
</tr>
</tbody></table>
</li>
<li><p>容错相关参数（MapReduce性能优化）</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.map.maxattempts</td>
<td>每个Map Task最大重试次数，一旦重试次数超过该值，则认为Map Task运行失败，默认值：4。</td>
</tr>
<tr>
<td>mapreduce.reduce.maxattempts</td>
<td>每个Reduce Task最大重试次数，一旦重试次数超过该值，则认为Map Task运行失败，默认值：4。</td>
</tr>
<tr>
<td>mapreduce.task.timeout</td>
<td>Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是600000（10分钟）。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是：“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”。</td>
</tr>
</tbody></table>
</li>
</ol>
<h2 id="2-4-Hadoop小文件优化方法"><a href="#2-4-Hadoop小文件优化方法" class="headerlink" title="2.4 Hadoop小文件优化方法"></a>2.4 Hadoop小文件优化方法</h2><h3 id="2-4-1-Hadoop小文件弊端"><a href="#2-4-1-Hadoop小文件弊端" class="headerlink" title="2.4.1 Hadoop小文件弊端"></a>2.4.1 Hadoop小文件弊端</h3><ul>
<li>HDFS上每个文件都要在NameNode上创建对应的元数据，这个元数据的大小约为150byte，这样当小文件比较多的时候，就会产生很多的元数据文件，一方面会大量占用NameNode的内存空间，另一方面就是元数据文件过多，使得寻址索引速度变慢。</li>
<li>小文件过多，在进行MR计算时，会生成过多切片，需要启动过多的MapTask。每个MapTask处理的数据量小，导致MapTask的处理时间比启动时间还小，白白消耗资源。<h3 id="2-4-2-Hadoop小文件解决方案"><a href="#2-4-2-Hadoop小文件解决方案" class="headerlink" title="2.4.2 Hadoop小文件解决方案"></a>2.4.2 Hadoop小文件解决方案</h3></li>
</ul>
<ol>
<li>小文件优化的方向：<ul>
<li>（1）在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS。</li>
<li>（2）在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并。</li>
<li>（3）在MapReduce处理时，可采用CombineTextInputFormat提高效率。</li>
<li>（4）开启uber模式，实现jvm重用</li>
</ul>
</li>
<li>Hadoop Archive<ul>
<li>是一个高效的将小文件放入HDFS块中的文件存档工具，能够将多个小文件打包成一个HAR文件，从而达到减少NameNode的内存使用</li>
</ul>
</li>
<li>SequenceFile<ul>
<li>SequenceFile是由一系列的二进制k/v组成，如果为key为文件名，value为文件内容，可将大批小文件合并成一个大文件</li>
</ul>
</li>
<li>CombineTextInputFormat<ul>
<li>CombineTextInputFormat用于将多个小文件在切片过程中生成一个单独的切片或者少量的切片。 </li>
</ul>
</li>
<li>开启uber模式，实现jvm重用。默认情况下，每个Task任务都需要启动一个jvm来运行，如果Task任务计算的数据量很小，我们可以让同一个Job的多个Task运行在一个Jvm中，不必为每个Task都开启一个Jvm. <ul>
<li>开启uber模式，在mapred-site.xml中添加如下配置  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--  开启uber模式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- uber模式中最大的mapTask数量，可向下修改  --&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.maxmaps<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>9<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- uber模式中最大的reduce数量，可向下修改 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.maxreduces<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- uber模式中最大的输入数据量，默认使用dfs.blocksize 的值，可向下修改 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.maxbytes<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h1 id="三、Hadoop新特性"><a href="#三、Hadoop新特性" class="headerlink" title="三、Hadoop新特性"></a>三、Hadoop新特性</h1><h2 id="3-1-Hadoop2-x新特性"><a href="#3-1-Hadoop2-x新特性" class="headerlink" title="3.1 Hadoop2.x新特性"></a>3.1 Hadoop2.x新特性</h2><h3 id="3-1-1-集群间数据拷贝"><a href="#3-1-1-集群间数据拷贝" class="headerlink" title="3.1.1 集群间数据拷贝"></a>3.1.1 集群间数据拷贝</h3><ul>
<li>采用distcp命令实现两个Hadoop集群之间的递归数据复制<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ hadoop distcp hdfs://hadoop002:9820/WeCom_3.1.18.90318.dmg hdfs://hadoop002:9820/testDistct</span><br><span class="line">···</span><br><span class="line">···</span><br><span class="line">2021-10-19 17:56:03,705 INFO mapreduce.Job: Job job_1634633871057_0003 completed successfully</span><br><span class="line">2021-10-19 17:56:03,754 INFO mapreduce.Job: Counters: 36</span><br><span class="line">	File System Counters</span><br><span class="line">		FILE: Number of bytes <span class="built_in">read</span>=0</span><br><span class="line">		FILE: Number of bytes written=227128</span><br><span class="line">		FILE: Number of <span class="built_in">read</span> operations=0</span><br><span class="line">		FILE: Number of large <span class="built_in">read</span> operations=0</span><br><span class="line">		FILE: Number of write operations=0</span><br><span class="line">		HDFS: Number of bytes <span class="built_in">read</span>=325449295</span><br><span class="line">		HDFS: Number of bytes written=325448910</span><br><span class="line">		HDFS: Number of <span class="built_in">read</span> operations=19</span><br><span class="line">		HDFS: Number of large <span class="built_in">read</span> operations=0</span><br><span class="line">		HDFS: Number of write operations=5</span><br><span class="line">	Job Counters</span><br><span class="line">		Launched map tasks=1</span><br><span class="line">		Other <span class="built_in">local</span> map tasks=1</span><br><span class="line">		Total time spent by all maps <span class="keyword">in</span> occupied slots (ms)=5517</span><br><span class="line">		Total time spent by all reduces <span class="keyword">in</span> occupied slots (ms)=0</span><br><span class="line">		Total time spent by all map tasks (ms)=5517</span><br><span class="line">		Total vcore-milliseconds taken by all map tasks=5517</span><br><span class="line">		Total megabyte-milliseconds taken by all map tasks=5649408</span><br><span class="line">	Map-Reduce Framework</span><br><span class="line">		Map input records=1</span><br><span class="line">		Map output records=0</span><br><span class="line">		Input split bytes=136</span><br><span class="line">		Spilled Records=0</span><br><span class="line">		Failed Shuffles=0</span><br><span class="line">		Merged Map outputs=0</span><br><span class="line">		GC time elapsed (ms)=52</span><br><span class="line">		CPU time spent (ms)=1730</span><br><span class="line">		Physical memory (bytes) snapshot=270876672</span><br><span class="line">		Virtual memory (bytes) snapshot=2578894848</span><br><span class="line">		Total committed heap usage (bytes)=217055232</span><br><span class="line">		Peak Map Physical memory (bytes)=270876672</span><br><span class="line">		Peak Map Virtual memory (bytes)=2578894848</span><br><span class="line">	File Input Format Counters</span><br><span class="line">		Bytes Read=249</span><br><span class="line">	File Output Format Counters</span><br><span class="line">		Bytes Written=0</span><br><span class="line">	DistCp Counters</span><br><span class="line">		Bandwidth <span class="keyword">in</span> Btyes=81362227</span><br><span class="line">		Bytes Copied=325448910</span><br><span class="line">		Bytes Expected=325448910</span><br><span class="line">		Files Copied=1</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="3-1-2-小文件存档"><a href="#3-1-2-小文件存档" class="headerlink" title="3.1.2 小文件存档"></a>3.1.2 小文件存档</h3><ol>
<li>HDFS存储小文件弊端<ul>
<li>每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关。例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB</li>
</ul>
</li>
<li>解决存储小文件办法之一<ul>
<li>HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，HDFS存档文件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346376074095.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
<li>案例实操<ul>
<li>归档文件:把/user/atguigu/input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/user/atguigu/output路径下。  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop archive -archiveName input.har -p  /user/atguigu/input   /user/atguigu/output</span><br></pre></td></tr></table></figure></li>
<li>查看归档  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -ls /user/atguigu/output/input.har</span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -ls har:///user/atguigu/output/input.har</span><br></pre></td></tr></table></figure></li>
<li>解归档文件  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -cp har:/// user/atguigu/output/input.har/*    /user/atguigu</span><br></pre></td></tr></table></figure>
<h3 id="3-1-2-回收站"><a href="#3-1-2-回收站" class="headerlink" title="3.1.2 回收站"></a>3.1.2 回收站</h3></li>
</ul>
</li>
</ol>
<ul>
<li>开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用</li>
<li>开启回收站功能参数说明<ol>
<li>默认值fs.trash.interval=0，0表示禁用回收站;其他值表示设置文件的存活时间</li>
<li>默认值fs.trash.checkpoint.interval=0，检查回收站的间隔时间。如果该值为0，则该值设置和fs.trash.interval的参数值相等</li>
<li>要求fs.trash.checkpoint.interval&lt;=fs.trash.interval</li>
</ol>
</li>
<li>回收站工作机制<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346388522940.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>回收站使用<ol>
<li>启用回收站：修改core-site.xml，配置垃圾回收时间为1分钟。 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.checkpoint.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>查看回收站 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#回收站目录在hdfs集群中的路径：</span></span><br><span class="line">/user/atguigu/.Trash/</span><br></pre></td></tr></table></figure></li>
<li>通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Trash trash = <span class="function">New <span class="title">Trash</span><span class="params">(conf)</span></span>;</span><br><span class="line">trash.moveToTrash(path);</span><br></pre></td></tr></table></figure></li>
<li>通过网页上直接删除的文件也不会走回收站。</li>
<li>只有在命令行利用hadoop fs -rm命令删除的文件才会走回收站。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -rm -r /user/atguigu/input</span><br><span class="line">2020-07-14 16:13:42,643 INFO fs.TrashPolicyDefault: Moved: <span class="string">&#x27;hdfs://hadoop102:9820/user/atguigu/input&#x27;</span> to trash at: hdfs://hadoop102:9820/user/atguigu/.Trash/Current/user/atguigu/input</span><br></pre></td></tr></table></figure></li>
<li>恢复回收站数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -mv /user/atguigu/.Trash/Current/user/atguigu/input    /user/atguigu/input</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<h2 id="3-2-Hadoop3-x新特性"><a href="#3-2-Hadoop3-x新特性" class="headerlink" title="3.2 Hadoop3.x新特性"></a>3.2 Hadoop3.x新特性</h2><h3 id="3-2-1-多NN的HA架构"><a href="#3-2-1-多NN的HA架构" class="headerlink" title="3.2.1 多NN的HA架构"></a>3.2.1 多NN的HA架构</h3><ul>
<li>HDFS NameNode高可用性的初始实现为单个活动NameNode和单个备用NameNode，将edits复制到三个JournalNode。该体系结构能够容忍系统中一个NN或一个JN的故障。但是，某些部署需要更高程度的容错能力。</li>
<li>Hadoop3.x允许用户运行多个备用NameNode。例如，通过配置三个NameNode和五个JournalNode，群集能够容忍两个节点而不是一个节点的故障。<h3 id="3-2-2-纠删码"><a href="#3-2-2-纠删码" class="headerlink" title="3.2.2 纠删码"></a>3.2.2 纠删码</h3></li>
<li>HDFS中的默认3副本方案在存储空间和其他资源（例如，网络带宽）中具有200％的开销。但是，对于I / O活动相对较低暖和冷数据集，在正常操作期间很少访问其他块副本，但仍会消耗与第一个副本相同的资源量。</li>
<li>纠删码（Erasure Coding）能够在不到50% 的数据冗余情况下提供和3副本相同的容错能力，因此，使用纠删码作为副本机制的改进是自然而然的。</li>
<li>查看集群支持的纠删码策略：hdfs ec -listPolicies</li>
</ul>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346151537039.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h1 id="四、HadoopHA高可用"><a href="#四、HadoopHA高可用" class="headerlink" title="四、HadoopHA高可用"></a>四、HadoopHA高可用</h1><h2 id="4-1-现有集群存在哪些问题？"><a href="#4-1-现有集群存在哪些问题？" class="headerlink" title="4.1 现有集群存在哪些问题？"></a>4.1 现有集群存在哪些问题？</h2><ol>
<li>HDFS集群 单个NN场景下NN如果故障了，整个HDFS集群就不可用（中心化集群） <ul>
<li>解决方案：配置多个NN !!!</li>
</ul>
</li>
<li>多个NN的场景下由哪一台对外进行服务？<ul>
<li>当HDFS实现多NN的高可用后，但是只有一台 NN 对外提供服务(Active)，其他的NN都是替补（Standby），当正在提供服务的NN宕机故障，其他的NN自动切换成Active状态</li>
</ul>
</li>
<li>HA实现后，元数据的管理策略是否发生改变？<ul>
<li>不改变，但是在HA的HDFS中合并 fsimage和edits编辑日志的合并工作交给Standby状态的NN去完成！</li>
</ul>
</li>
<li>2NN 在高可用的集群中还要不要？<ul>
<li>不要了！2NN的工作有Standby状态的NN完成！</li>
</ul>
</li>
<li>为了保证整个集群中的所有NN 能够共享元数据信息，会新增一个 JournalNode 服务，在集群中我们会启动多个JournalNode服务 形成一个集群，每个JournalNode服务对应一个NN。进行数共享！</li>
<li>JournalNode如何实现元数据的共享？<ul>
<li>在集群状态下，当一个请求对元数据进行更改的时候，此时Active状态的NN会处理请求，会往磁盘上的编辑日志edits文件追加记录，并且会通过当前机器的JournalNode服务同步edits日志文件。接下来请求也会被转发到Standby状态的NN上，Standby状态的NN接收到请求后，只去读取自己的JournalNode服务中保存的最新的编辑日志信息，加载内存中形成最新的元数据映像，保证一旦Active状态的NN宕机，Standby自己马上顶上后能够展示最新的元数据。  </li>
<li>到达checkPoint之后，Standby都会尝试进行合并Edit和Fsimage，以接收到的第一个为准</li>
</ul>
</li>
<li>当一台NN故障后，其他NN如何争抢上位？<ul>
<li>采用高可用集群中的自动故障转移机制来完成切换。</li>
</ul>
</li>
<li>自动故障转移的机制如何实现？</li>
</ol>
<h2 id="4-2-HDFS-HA工作机制"><a href="#4-2-HDFS-HA工作机制" class="headerlink" title="4.2 HDFS-HA工作机制"></a>4.2 HDFS-HA工作机制</h2><ul>
<li>通过多个NameNode消除单点故障<h3 id="4-2-1-HDFS-HA工作要点"><a href="#4-2-1-HDFS-HA工作要点" class="headerlink" title="4.2.1 HDFS-HA工作要点"></a>4.2.1 HDFS-HA工作要点</h3></li>
</ul>
<ol>
<li>元数据管理方式需要改变<ul>
<li>内存中各自保存一份元数据；</li>
<li>Edits日志只有Active状态的NameNode节点可以做写操作；</li>
<li>所有的NameNode都可以读取Edits；</li>
<li>共享的Edits放在一个共享存储中管理（qjournal和NFS两个主流实现）；</li>
</ul>
</li>
<li>需要一个状态管理功能模块<ul>
<li>实现了一个zkfailover，常驻在每一个namenode所在的节点，每一个zkfailover负责监控自己所在NameNode节点，利用zk进行状态标识，当需要进行状态切换时，由zkfailover来负责切换，切换时需要防止brain split现象的发生。</li>
</ul>
</li>
<li>必须保证两个NameNode之间能够ssh无密码登录</li>
<li>隔离（Fence），即同一时刻仅仅有一个NameNode对外提供服务</li>
</ol>
<h3 id="4-2-2-HDFS-HA自动故障转移工作机制"><a href="#4-2-2-HDFS-HA自动故障转移工作机制" class="headerlink" title="4.2.2 HDFS-HA自动故障转移工作机制"></a>4.2.2 HDFS-HA自动故障转移工作机制</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346403266012.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>自动故障转移为HDFS部署增加了两个新组件：ZooKeeper和ZKFailoverController（ZKFC）进程。</li>
<li>ZooKeeper是维护少量协调数据，通知客户端这些数据的改变和监视客户端故障的高可用服务。</li>
<li>HA的自动故障转移依赖于ZooKeeper的以下功能：<ol>
<li>故障检测<ul>
<li>集群中的每个NameNode在ZooKeeper中维护了一个会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode需要触发故障转移。</li>
</ul>
</li>
<li>现役NameNode选择<ul>
<li>ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode。</li>
</ul>
</li>
</ol>
</li>
<li>ZKFC是自动故障转移中的另一个新组件，是ZooKeeper的客户端，也监视和管理NameNode的状态。每个运行NameNode的主机也运行了一个ZKFC进程，ZKFC负责：<ol>
<li>健康监测<ul>
<li>ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。</li>
</ul>
</li>
<li>ZooKeeper会话管理<ul>
<li>当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。</li>
</ul>
</li>
<li>基于ZooKeeper的选择<ul>
<li>如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为Active。</li>
</ul>
</li>
</ol>
</li>
</ul>
<h2 id="4-3-HDFS-HA集群配置"><a href="#4-3-HDFS-HA集群配置" class="headerlink" title="4.3 HDFS-HA集群配置"></a>4.3 HDFS-HA集群配置</h2><h3 id="4-3-1-环境准备"><a href="#4-3-1-环境准备" class="headerlink" title="4.3.1 环境准备"></a>4.3.1 环境准备</h3><ul>
<li>准备三台服务器</li>
<li>JDK,Hadoop安装包</li>
<li>干净的集群</li>
<li>环境变量<h3 id="4-3-2-规划集群"><a href="#4-3-2-规划集群" class="headerlink" title="4.3.2 规划集群"></a>4.3.2 规划集群</h3><table>
<thead>
<tr>
<th>hadoop102</th>
<th>hadoop103</th>
<th>hadoop104</th>
</tr>
</thead>
<tbody><tr>
<td>NameNode</td>
<td>NameNode</td>
<td>NameNode</td>
</tr>
<tr>
<td>ZKFC</td>
<td>ZKFC</td>
<td>ZKFC</td>
</tr>
<tr>
<td>JournalNode</td>
<td>JournalNode</td>
<td>JournalNode</td>
</tr>
<tr>
<td>DataNode</td>
<td>DataNode</td>
<td>DataNode</td>
</tr>
<tr>
<td>ZK</td>
<td>ZK</td>
<td>ZK</td>
</tr>
<tr>
<td>-</td>
<td>ResourceManager</td>
<td>-</td>
</tr>
<tr>
<td>NodeManager</td>
<td>NodeManager</td>
<td>NodeManager</td>
</tr>
</tbody></table>
</li>
</ul>
<h3 id="4-3-3-配置HA集群"><a href="#4-3-3-配置HA集群" class="headerlink" title="4.3.3 配置HA集群"></a>4.3.3 配置HA集群</h3><ol>
<li>修改配置文件 core-site.xml  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 把多个NameNode的地址组装成一个集群mycluster --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- 指定hadoop数据的存储目录 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/ha/hadoop-3.1.3/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- 配置HDFS网页登录使用的静态用户为atguigu --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- 配置该atguigu(superUser)允许通过代理访问的主机节点 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 配置该atguigu(superUser)允许通过代理用户所属组 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 配置该atguigu(superUser)允许通过代理的用户--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.users<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>修改配置文件 hdfs-site.xml    <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- NameNode数据存储目录 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- DataNode数据存储目录 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- JournalNode数据存储目录 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;hadoop.tmp.dir&#125;/jn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 完全分布式HDFS集群名称 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 集群中NameNode节点都有哪些 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2,nn3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- NameNode的RPC通信地址 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:9820<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:9820<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:9820<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- NameNode的Web通信地址 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 指定NameNode元数据在JournalNode上的存放位置 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 访问代理类：client用于确定哪个NameNode为Active --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 使用隔离机制时需要ssh秘钥登录--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/atguigu/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	</span><br><span class="line">  <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>修改每一台机器的HADOOP_HOME 的环境变量打开 /etc/profile.d/set_evn.sh 修改如下： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">HADOOP_HOME=/opt/module/ha/hadoop-3.1.3</span><br></pre></td></tr></table></figure></li>
<li>将/opt/ha/hadoop-3.1.3 分发到103 和 104 并且修改103和104的 HADOOP_HOME=/opt/module/ha/hadoop-3.1.3<ul>
<li>注意：修改完环境变量后一定要重新加载 profile 文件</li>
</ul>
</li>
<li>在102、103、104 各个JournalNode节点上，输入以下命令启动journalnode服务 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs --daemon start journalnode</span><br></pre></td></tr></table></figure></li>
<li>在 hadoop102的 nn1 上，对其进行格式化，并启动 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs namenode -format</span><br><span class="line">hdfs --daemon start namenode</span><br></pre></td></tr></table></figure></li>
<li>分别在 hadoop103的nn2 和 hadoop104的nn3上，同步nn1的元数据信息 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure></li>
<li>分别在 hadoop103上启动nn2 和 hadoop104上启动nn3 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs --daemon start namenode</span><br></pre></td></tr></table></figure></li>
<li>通过web地址访问nn1 nn2 nn3    <ul>
<li>nn1:<a href="http://hadoop102:9870/">http://hadoop102:9870</a></li>
<li>nn2:<a href="http://hadoop103:9870/">http://hadoop103:9870</a></li>
<li>nn3:<a href="http://hadoop104:9870/">http://hadoop104:9870</a></li>
</ul>
</li>
<li>在每台机器上启动DN <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">    hdfs --daemon start datanode</span><br><span class="line">    ```   </span><br><span class="line">9. 将其中的一个nn切换成Active状态</span><br><span class="line">    ```bash</span><br><span class="line">    hdfs haadmin -transitionToActive nn1</span><br></pre></td></tr></table></figure></li>
<li>查看是否Active<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs haadmin -getServiceState nn1</span><br></pre></td></tr></table></figure>
<h3 id="4-3-4-实现HA的故障自动转移"><a href="#4-3-4-实现HA的故障自动转移" class="headerlink" title="4.3.4 实现HA的故障自动转移"></a>4.3.4 实现HA的故障自动转移</h3></li>
<li>在core-site.xml文件中增加 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定zkfc要连接的zkServer地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:2181,hadoop103:2181,hadoop104:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>在hdfs-site.xml中增加 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 启用nn故障自动转移 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>修改后分发配置文件<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">   xsync /opt/module/ha/hadoop-3.1.3/etc/hadoop </span><br><span class="line">   ```   </span><br><span class="line">4. 关闭HDFS集群</span><br><span class="line">   ```bash</span><br><span class="line">   stop-dfs.sh</span><br></pre></td></tr></table></figure></li>
<li>启动Zookeeper集群<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zk.sh start</span><br></pre></td></tr></table></figure></li>
<li>初始化HA在Zookeeper中状态<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs zkfc -formatZK</span><br></pre></td></tr></table></figure></li>
<li>启动HDFS服务<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure></li>
<li>可以去zkCli.sh客户端查看Namenode选举锁节点内容 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">get /hadoop-ha/mycluster/ActiveStandbyElectorLock</span><br></pre></td></tr></table></figure></li>
<li>测试故障自动转移<ul>
<li>将当前状态为Active的namenode 杀死</li>
<li>刷新另外两台namenode的web端，关注状态</li>
<li>最后可以到zk中验证锁内容的名称</li>
</ul>
</li>
</ol>
<h2 id="4-4-YARN-HA配置"><a href="#4-4-YARN-HA配置" class="headerlink" title="4.4 YARN-HA配置"></a>4.4 YARN-HA配置</h2><p>YARN HA 集群搭建步骤</p>
<ol>
<li>修改yarn-site.xml <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 启用resourcemanager ha --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 声明三台resourcemanager的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster-yarn1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定resourcemanager的逻辑列表--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2,rm3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- ========== rm1的配置 ========== --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定rm1的主机名 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定rm1的web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定rm1的 RPC 通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定AM向rm1申请资源的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定供NM连接的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- ========== rm2的配置 ========== --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定rm2的主机名 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- ========== rm3的配置 ========== --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定rm3的主机名 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定zookeeper集群的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:2181,hadoop103:2181,hadoop104:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 启用自动恢复,启用自动故障转移 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定resourcemanager的状态信息存储在zookeeper集群 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>将yarn-site.xml文件进行分发<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xsync /opt/module/ha/hadoop-3.1.3/etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure></li>
<li>在任意的机器上启动yarn <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure></li>
<li>通过访问web地址验证<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346426629749.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"> </li>
<li>测试Yarn故障自动转移<ul>
<li>kill 当前active节点，会有另一个standby节点自动升级成active</li>
</ul>
</li>
</ol>
<h2 id="4-5-HDFS-Federation架构设计"><a href="#4-5-HDFS-Federation架构设计" class="headerlink" title="4.5 HDFS Federation架构设计"></a>4.5 HDFS Federation架构设计</h2><h3 id="4-5-1-NameNode架构的局限性"><a href="#4-5-1-NameNode架构的局限性" class="headerlink" title="4.5.1 NameNode架构的局限性"></a>4.5.1 NameNode架构的局限性</h3><ol>
<li>Namespace（命名空间）的限制<ul>
<li>由于NameNode在内存中存储所有的元数据（metadata），因此单个NameNode所能存储的对象（文件+块）数目受到NameNode所在JVM的heap size的限制。50G的heap能够存储20亿（200million）个对象，这20亿个对象支持4000个DataNode，12PB的存储（假设文件平均大小为40MB）。随着数据的飞速增长，存储的需求也随之增长。单个DataNode从4T增长到36T，集群的尺寸增长到8000个DataNode。存储的需求从12PB增长到大于100PB。</li>
</ul>
</li>
<li>隔离问题<ul>
<li>由于HDFS仅有一个NameNode，无法隔离各个程序，因此HDFS上的一个实验程序就很有可能影响整个HDFS上运行的程序。</li>
</ul>
</li>
<li>性能的瓶颈<ul>
<li>由于是单个NameNode的HDFS架构，因此整个HDFS文件系统的吞吐量受限于单个NameNode的吞吐量。</li>
</ul>
</li>
</ol>
<h3 id="4-5-2-HDFS-Federation架构设计"><a href="#4-5-2-HDFS-Federation架构设计" class="headerlink" title="4.5.2 HDFS Federation架构设计"></a>4.5.2 HDFS Federation架构设计</h3><p>多个NameNode集群管理不同业务线的元数据<br>| NameNode | NameNode | NameNode          |<br>| ——– | ——– | —————– |<br>| 元数据   | 元数据   | 元数据            |<br>| Log      | machine  | 电商数据/话单数据 |</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346432680052.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="4-5-3-HDFS-Federation应用思考"><a href="#4-5-3-HDFS-Federation应用思考" class="headerlink" title="4.5.3 HDFS Federation应用思考"></a>4.5.3 HDFS Federation应用思考</h3><p>不同应用可以使用不同NameNode进行数据管理图片业务、爬虫业务、日志审计业务。Hadoop生态系统中，不同的框架使用不同的NameNode进行管理NameSpace。（隔离性）<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346433814694.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="4-5-4-联邦机制原理："><a href="#4-5-4-联邦机制原理：" class="headerlink" title="4.5.4 联邦机制原理："></a>4.5.4 联邦机制原理：</h3><ol>
<li>将NameNode划分成不同的命名空间并进行编号。不同的命名空间之间相互隔离互不干扰。</li>
<li>在DataNode中创建目录，此目录对应命名空间的编号。</li>
<li>由此，编号相同的数据由对应的命名空间进行管理</li>
<li>适用场景分析 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">128G(内存空间大小) * 1024(M) * 1024(KB) * 1024(bety) / 150 = xxx（元数据的数量）</span><br><span class="line">xxx * 256M（每一个文件大小） = yyy</span><br><span class="line">yyy / 1024(G) / 1024(TB) / 1024(PB) = 200 左右PB的数据</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="问答"><a href="#问答" class="headerlink" title="问答"></a>问答</h1><h3 id="一、Hadoop中的压缩作为一种常用的优化手段，经常被用在什么场景下？"><a href="#一、Hadoop中的压缩作为一种常用的优化手段，经常被用在什么场景下？" class="headerlink" title="一、Hadoop中的压缩作为一种常用的优化手段，经常被用在什么场景下？"></a>一、Hadoop中的压缩作为一种常用的优化手段，经常被用在什么场景下？</h3><ol>
<li>降低磁盘占用</li>
<li>减少网络IO</li>
</ol>
<h3 id="二、如果想要使用压缩，Hadoop如何对某一种压缩编码格式进行取舍？"><a href="#二、如果想要使用压缩，Hadoop如何对某一种压缩编码格式进行取舍？" class="headerlink" title="二、如果想要使用压缩，Hadoop如何对某一种压缩编码格式进行取舍？"></a>二、如果想要使用压缩，Hadoop如何对某一种压缩编码格式进行取舍？</h3><ol>
<li>压缩/解压的速度，资源占用</li>
<li>压缩率</li>
<li>压缩后是否支持切片</li>
</ol>
<h3 id="三、你们公司常用的压缩方式有哪些？"><a href="#三、你们公司常用的压缩方式有哪些？" class="headerlink" title="三、你们公司常用的压缩方式有哪些？"></a>三、你们公司常用的压缩方式有哪些？</h3><ol>
<li>单文件压缩后再130M以内使用gzip，如每天的日志文件，可以支持并行处理</li>
<li>单文件压缩后大于400M考虑支持切片的lzo 或者bzip</li>
</ol>
<h3 id="四、从哪些方面定位MR执行的效率（如何分析MR执行慢的原因）"><a href="#四、从哪些方面定位MR执行的效率（如何分析MR执行慢的原因）" class="headerlink" title="四、从哪些方面定位MR执行的效率（如何分析MR执行慢的原因）"></a>四、从哪些方面定位MR执行的效率（如何分析MR执行慢的原因）</h3><ol>
<li>计算机性能<ul>
<li>cpu，内存，磁盘，网络</li>
</ul>
</li>
<li>I/O操作优化<ol>
<li>数据倾斜</li>
<li>Map和Reduce数量设置不合理</li>
<li>Map运行时间过长，导致Reduce等待过久</li>
<li>小文件过多</li>
<li>大量不可切片的超大压缩文件</li>
<li>Spill次数过多</li>
<li>Merge次数过多</li>
</ol>
</li>
</ol>
<h3 id="五、如果想对MR程序进行优化，应该从哪些方面入手以及可能用到的优化手段？"><a href="#五、如果想对MR程序进行优化，应该从哪些方面入手以及可能用到的优化手段？" class="headerlink" title="五、如果想对MR程序进行优化，应该从哪些方面入手以及可能用到的优化手段？"></a>五、如果想对MR程序进行优化，应该从哪些方面入手以及可能用到的优化手段？</h3><ol>
<li>数据输入<ol>
<li>合并小文件</li>
<li>使用CombineTextInputFormat作为输入解决小文件场景</li>
</ol>
</li>
<li>Map阶段<ol>
<li>减少Spill溢写次数，调整环形缓冲区大小和触发溢写的内存上限，减少磁盘IO</li>
<li>减少Merge合并次数：调整一次merge合并的Spill溢写文件数量，减少Merge次数</li>
<li>Map之后合理使用Combine，减少网络IO</li>
</ol>
</li>
<li>Reduce阶段<ol>
<li>合理设置Map和Reduce数量</li>
<li>设置Map和Reduce并行处理，减少Reduce等待时间</li>
<li>规避使用Reduce，减少连接数据集产生的网络IO</li>
<li>合理设置Reduce端的Buffer</li>
</ol>
</li>
<li>IO传输<ol>
<li>选择合适的压缩算法，减少网络I\O传输的数据量</li>
<li>使用sequenceFile二进制文件</li>
</ol>
</li>
<li>数据倾斜<ol>
<li>对原始数据进行抽样得到结果集来预设分区边界值</li>
<li>根据业务分析自定义分区</li>
<li>使用combiner减少数据倾斜</li>
<li>尽量使用MapJoin，避免ReduceJoin</li>
</ol>
</li>
<li>参数调优<ol>
<li>MR源相关配置：MapTask和ReduceTask使用的内存，cpu</li>
<li>Yarn资源配置：Container使用的内存，cpu</li>
<li>Shuffer配置：环形缓冲区的大小，触发溢写的内存比例</li>
<li>容错配置：任务重试次数，超时时间</li>
</ol>
</li>
</ol>
<h3 id="六、在Hadoop针对小文件的处理方案有哪些？"><a href="#六、在Hadoop针对小文件的处理方案有哪些？" class="headerlink" title="六、在Hadoop针对小文件的处理方案有哪些？"></a>六、在Hadoop针对小文件的处理方案有哪些？</h3><ol>
<li>数据采集的时候，将小文件或小批数据合并成大文件在上传HDFS，从源头上避免小文件产生</li>
<li>业务处理之前，使用MapReduce程序对HDFS上的小文件进行合并</li>
<li>使用CombineTextInputFormat处理小文件的输入</li>
<li>开启uber模式，实现jvm重用</li>
</ol>
<h3 id="七、如何解决MR中Reduce的数据倾斜问题？"><a href="#七、如何解决MR中Reduce的数据倾斜问题？" class="headerlink" title="七、如何解决MR中Reduce的数据倾斜问题？"></a>七、如何解决MR中Reduce的数据倾斜问题？</h3><ol>
<li>对原始数据进行抽样得到结果集来预设分区边界值</li>
<li>根据业务分析自定义分区</li>
<li>使用combiner减少数据倾斜</li>
<li>尽量使用MapJoin，避免ReduceJoin</li>
</ol>
<h3 id="八、大概简述一下-Hadoop每一代版本的新特性？"><a href="#八、大概简述一下-Hadoop每一代版本的新特性？" class="headerlink" title="八、大概简述一下 Hadoop每一代版本的新特性？"></a>八、大概简述一下 Hadoop每一代版本的新特性？</h3><ol>
<li>Hadoop 2.x<ul>
<li>distcp命令实现两个Hadoop集群之间的递归数据复制</li>
<li>小文件存档</li>
<li>回收站</li>
</ul>
</li>
<li>Hadoop 3.x<ul>
<li>多NN的HA架构：提高集群的可用性</li>
<li>纠删码：降低磁盘占用</li>
</ul>
</li>
</ol>
<h3 id="九、什么是Hadoop的HA"><a href="#九、什么是Hadoop的HA" class="headerlink" title="九、什么是Hadoop的HA?"></a>九、什么是Hadoop的HA?</h3><ol>
<li>集群可实现7*24小时不中断服务</li>
<li>不存在单点故障</li>
<li>可以实现故障自动转移</li>
</ol>
<h3 id="十、描述一下HDFS-HA的工作机制？"><a href="#十、描述一下HDFS-HA的工作机制？" class="headerlink" title="十、描述一下HDFS-HA的工作机制？"></a>十、描述一下HDFS-HA的工作机制？</h3><ol>
<li>多NN消除单点故障</li>
<li>由Active状态的NN负责写操作，JournalNode负责同步Edits，Standby状态的NN读取自己的Edit，加载到内存形成完整元数据</li>
<li>Standby状态的NN负责合并Edit和FsImage</li>
<li>依赖Zookeeper实现故障自动转移</li>
</ol>
<h3 id="十一、如何实现HA的集群搭建-用话术描述即可！！！"><a href="#十一、如何实现HA的集群搭建-用话术描述即可！！！" class="headerlink" title="十一、如何实现HA的集群搭建?(用话术描述即可！！！)"></a>十一、如何实现HA的集群搭建?(用话术描述即可！！！)</h3><ol>
<li>配置集群名称</li>
<li>配置集群节点</li>
<li>配置JournalNode</li>
<li>配置zookeeper连接地址</li>
</ol>
<h3 id="十二、HDFS如何实现自动故障转移？"><a href="#十二、HDFS如何实现自动故障转移？" class="headerlink" title="十二、HDFS如何实现自动故障转移？"></a>十二、HDFS如何实现自动故障转移？</h3><ol>
<li>HDFS故障自动转移依赖zookeeper和zkfc进程</li>
<li>zookeeper实现功能<ol>
<li>故障检测：集群中的每个NameNode在ZooKeeper中维护了一个会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode触发故障转移</li>
<li>现役NameNode选择：ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode</li>
</ol>
</li>
<li>zkfc进程是ZooKeeper的客户端，也监视和管理NameNode的状态。每个运行NameNode的主机也运行了一个ZKFC进程，ZKFC负责：<ol>
<li>健康监测<ul>
<li>ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。</li>
</ul>
</li>
<li>ZooKeeper会话管理<ul>
<li>当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。</li>
</ul>
</li>
<li>基于ZooKeeper的选择<ul>
<li>如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为Active。</li>
</ul>
</li>
</ol>
</li>
</ol>
<h3 id="十三、什么是脑裂问题？HDFS-HA中如何解决的脑裂问题？"><a href="#十三、什么是脑裂问题？HDFS-HA中如何解决的脑裂问题？" class="headerlink" title="十三、什么是脑裂问题？HDFS-HA中如何解决的脑裂问题？"></a>十三、什么是脑裂问题？HDFS-HA中如何解决的脑裂问题？</h3><ol>
<li>active节点的zkfc进程检测到namenode异常，会通知另一台NameNode的zkfc</li>
<li>接收通知的zkf会通过ssh在异常namenode上执行kill命令，确保异常NameNode死透</li>
<li>避免了出现两个active节点，解决了脑裂问题</li>
</ol>
<h3 id="十四、YARN-HA-实现高可用的思路"><a href="#十四、YARN-HA-实现高可用的思路" class="headerlink" title="十四、YARN-HA 实现高可用的思路"></a>十四、YARN-HA 实现高可用的思路</h3><ol>
<li>ResourceManager启动时候会向ZK的/rmstore目录写lock文件，写成功就为active，否则standby.</li>
<li>ResourceManager节点zkfc会一直监控这个lock文件是否存在，假如不存在，就为active，否则为standby.</li>
<li>zookeeper存储RMStateStore。选举active RM。</li>
<li>RMStateStore: 存储在zk的/rmstore目录下。</li>
<li>activeRM会向这个目录写APP信息</li>
<li>当activeRM挂了，另外一个standby RM通过ZKFC选举成功为active，会从/rmstore读取相应的作业信息。重新构建作业的内存信息，启动内部的服务，开始接收NM的心跳，构建集群的资源信息，并且接收客户端的作业提交请求。</li>
</ol>
<h3 id="十五、简单说一下-联邦架构-HDFS-Federation-架构设计思想。-了解"><a href="#十五、简单说一下-联邦架构-HDFS-Federation-架构设计思想。-了解" class="headerlink" title="十五、简单说一下 联邦架构(HDFS Federation) 架构设计思想。(了解)"></a>十五、简单说一下 联邦架构(HDFS Federation) 架构设计思想。(了解)</h3><ol>
<li>解决Namespace（命名空间）的限制</li>
<li>解决隔离问题</li>
<li>解决性能的瓶颈</li>
<li>将NameNode划分成不同的命名空间并进行编号。不同的命名空间之间相互隔离互不干扰。</li>
<li>在DataNode中创建目录，此目录对应命名空间的编号。</li>
<li>由此，编号相同的数据由对应的命名空间进行管理</li>
</ol>
<p>空间进行管理</p>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper</title>
    <url>/2021/11/07/Zookeeper/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h1><h1 id="一、Zookeeper入门"><a href="#一、Zookeeper入门" class="headerlink" title="一、Zookeeper入门"></a>一、Zookeeper入门</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><ul>
<li>Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。</li>
<li>Zookeeper从设计模式角度来理解，是一个基于<font color ='red' >观察者模式</font>设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生了变化，Zookeeper就负责通知已经在Zookeeper上注册的那些观察者做出相应的反应.</li>
<li>Zookeeper = 文件系统 + 通知机制</li>
</ul>
<hr>
<h2 id="1-2-特点"><a href="#1-2-特点" class="headerlink" title="1.2 特点"></a>1.2 特点</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345175804501.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>Zookeeper：一个领导者（Leader），多个跟随者（Follower）组成的集群。</li>
<li>集群中只要有半数以上节点存活，Zookeeper集群就能正常服务</li>
<li>全局数据一致：每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的</li>
<li>更新请求顺序进行，来自同一个Client的更新请求按其发送顺序依次执行</li>
<li>数据更新原子性，一次数据更新要么全部成功（超集群半数节点），要么所有节点全部失败</li>
<li>实时性，在一定时间范围内，Client能读到最新数据</li>
</ul>
<hr>
<h2 id="1-3-数据结构"><a href="#1-3-数据结构" class="headerlink" title="1.3 数据结构"></a>1.3 数据结构</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345177645379.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。每一个ZNode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识。</li>
</ul>
<hr>
<h2 id="1-4-应用场景"><a href="#1-4-应用场景" class="headerlink" title="1.4 应用场景"></a>1.4 应用场景</h2><ul>
<li><p>提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等。</p>
</li>
<li><p>统一命名服务：在分布式环境下，经常需要对应用/服务进行统一命名，便于识别。<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345532077391.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>统一配置管理<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345533954441.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>分布式环境下，配置文件同步非常常见，配置管理可交由ZooKeeper实现</li>
<li>一般要求一个集群中，所有节点的配置信息是一致的，比如 Kafka 集群</li>
<li>对配置文件修改后，希望能够快速同步到各个节点上</li>
<li>将配置信息写入ZooKeeper上的一个Znode</li>
<li>各个客户端服务器监听这个Znode</li>
<li>Znode中的数据被修改，ZooKeeper将通知各个客户端服务器</li>
</ol>
</li>
<li><p>统一集群管理<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345535437380.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>分布式环境中，实时掌握每个节点的状态是必要的。 <ol>
<li>可根据节点实时状态做出一些调整。 </li>
</ol>
</li>
<li>ZooKeeper可以实现实时监控节点状态变化<ol>
<li>可将节点信息写入ZooKeeper上的一个ZNode</li>
<li>监听这个ZNode可获取它的实时状态变化。</li>
</ol>
</li>
</ol>
</li>
<li><p>服务器动态上下线<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345535923783.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>软负载均衡：在Zookeeper中记录每台服务器的访问数，让访问数最少的服务器去处理最新的客户端请求<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345536248922.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
</ul>
<hr>
<h1 id="二、Zookeeper-安装"><a href="#二、Zookeeper-安装" class="headerlink" title="二、Zookeeper 安装"></a>二、Zookeeper 安装</h1><h2 id="2-1-安装ZK"><a href="#2-1-安装ZK" class="headerlink" title="2.1 安装ZK:"></a>2.1 安装ZK:</h2><ol>
<li>把软件包上传的Linux的 /opt/software 下</li>
<li>加压ZK到 /opt/module 下</li>
<li>将加压后的目录名称修改一下（选做）</li>
<li>将zk的安装目录下 conf/zoo_sample.cfg 文件改名为 zoo.cfg</li>
<li>在ZK的安装目录下创建一个新的目录，作为zk的数据持久化目录</li>
<li>修改zoo.cfg配置文件<code>dataDir=/opt/module/zookeeper-3.5.7/zkData</code></li>
<li>配置ZK的环境变量 （选做）</li>
</ol>
<h2 id="2-2-单点模式的简单操作"><a href="#2-2-单点模式的简单操作" class="headerlink" title="2.2 单点模式的简单操作"></a>2.2 单点模式的简单操作</h2><ol>
<li>启停zk服务端 和 zk客户端<ul>
<li>启停 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">    [atguigu@hadoop001 zookeeper-3.5.7]$ bin/zkServer.sh start</span><br><span class="line">    ZooKeeper JMX enabled by default</span><br><span class="line">    Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">    Starting zookeeper ... STARTED</span><br><span class="line">    [atguigu@hadoop001 zookeeper-3.5.7]$ jps</span><br><span class="line">    1456 NameNode</span><br><span class="line">    8225 QuorumPeerMain</span><br><span class="line">    1619 DataNode</span><br><span class="line">    2076 JobHistoryServer</span><br><span class="line">    1901 NodeManager</span><br><span class="line">    8269 Jps</span><br><span class="line">    [atguigu@hadoop001 zookeeper-3.5.7]$ bin/zkServer.sh stop</span><br><span class="line">    ZooKeeper JMX enabled by default</span><br><span class="line">    Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">    Stopping zookeeper ... STOPPED</span><br><span class="line">    [atguigu@hadoop001 zookeeper-3.5.7]$ jps</span><br><span class="line">    1456 NameNode</span><br><span class="line">    1619 DataNode</span><br><span class="line">    2076 JobHistoryServer</span><br><span class="line">    1901 NodeManager</span><br><span class="line">    8302 Jps</span><br><span class="line">    [atguigu@hadoop001 zookeeper-3.5.7]$</span><br><span class="line">    ```   </span><br><span class="line">- 客户端连接</span><br><span class="line">    ```bash</span><br><span class="line">    [atguigu@hadoop001 zookeeper-3.5.7]$ bin/zkCli.sh -server hadoop001:2181</span><br><span class="line">    Connecting to hadoop001:2181</span><br><span class="line">    ········</span><br><span class="line"></span><br><span class="line">    2021-10-18 16:36:10,536 [myid:hadoop001:2181] - INFO  [main-SendThread(hadoop001:2181):ClientCnxn<span class="variable">$SendThread</span>@1394] - Session establishment complete on server hadoop001/192.168.2.6:2181, sessionid = 0x10000a6d44b0000, negotiated timeout = 30000</span><br><span class="line">    </span><br><span class="line">    WATCHER::</span><br><span class="line">    </span><br><span class="line">    WatchedEvent state:SyncConnected <span class="built_in">type</span>:None path:null</span><br><span class="line">    [zk: hadoop001:2181(CONNECTED) 0]</span><br></pre></td></tr></table></figure>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ jps</span><br><span class="line">1456 NameNode</span><br><span class="line">1619 DataNode</span><br><span class="line">8483 ZooKeeperMain</span><br><span class="line">8532 Jps</span><br><span class="line">8378 QuorumPeerMain</span><br><span class="line">2076 JobHistoryServer</span><br><span class="line">1901 NodeManager</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>查看一下zk的服务端和客户端对应的进程 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ jps</span><br><span class="line">1456 NameNode</span><br><span class="line">1619 DataNode</span><br><span class="line">8483 ZooKeeperMain      <span class="comment">#客户端</span></span><br><span class="line">8532 Jps</span><br><span class="line">8378 QuorumPeerMain     <span class="comment">#服务端</span></span><br><span class="line">2076 JobHistoryServer</span><br><span class="line">1901 NodeManager</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
<li>退出客户端 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 0] quit</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:Closed <span class="built_in">type</span>:None path:null</span><br><span class="line">2021-10-18 16:41:05,769 [myid:] - INFO  [main:ZooKeeper@1422] - Session: 0x10000a6d44b0001 closed</span><br><span class="line">2021-10-18 16:41:05,769 [myid:] - INFO  [main-EventThread:ClientCnxn<span class="variable">$EventThread</span>@524] - EventThread shut down <span class="keyword">for</span> session: 0x10000a6d44b0001</span><br><span class="line">[atguigu@hadoop001 zookeeper-3.5.7]$ jps</span><br><span class="line">1456 NameNode</span><br><span class="line">1619 DataNode</span><br><span class="line">8552 Jps</span><br><span class="line">8378 QuorumPeerMain</span><br><span class="line">2076 JobHistoryServer</span><br><span class="line">1901 NodeManager</span><br><span class="line">[atguigu@hadoop001 zookeeper-3.5.7]$</span><br></pre></td></tr></table></figure></li>
<li>当客户端启动连接后不能单独关闭，当服务端关闭后，客户端也就消失了。</li>
</ol>
<h2 id="2-2-配置参数解读"><a href="#2-2-配置参数解读" class="headerlink" title="2.2 配置参数解读"></a>2.2 配置参数解读</h2><p>Zookeeper中的配置文件zoo.cfg中参数含义解读如下：</p>
<ol>
<li>tickTime =2000：通信心跳数，Zookeeper服务器与客户端心跳时间，单位毫秒<ul>
<li>Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。</li>
<li>它用于心跳检测机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime)</li>
</ul>
</li>
<li>initLimit =10：LF初始通信时限<ul>
<li>集群中的Follower跟随者服务器与Leader领导者服务器之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。</li>
</ul>
</li>
<li>syncLimit =5：LF同步通信时限<ul>
<li>集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。</li>
</ul>
</li>
<li>dataDir：数据文件目录+数据持久化路径<ul>
<li>用于保存Zookeeper中的数据。</li>
</ul>
</li>
<li>clientPort=2181：客户端连接端口<ul>
<li>监听客户端连接的端口。</li>
</ul>
</li>
</ol>
<h1 id="三、Zookeeper-实战"><a href="#三、Zookeeper-实战" class="headerlink" title="三、Zookeeper 实战"></a>三、Zookeeper 实战</h1><h2 id="3-1-搭建ZK的集群"><a href="#3-1-搭建ZK的集群" class="headerlink" title="3.1 搭建ZK的集群"></a>3.1 搭建ZK的集群</h2><ol>
<li>集群规划：在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper </li>
<li>每个节点执行单节点部署的步骤</li>
<li>修改zoo.cfg 配置文件 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 数据存储路径</span><br><span class="line">dataDir=/opt/module/zookeeper-3.5.7/zkData</span><br><span class="line"></span><br><span class="line">#集群节点配置</span><br><span class="line">server.2=hadoop102:2888:3888</span><br><span class="line">server.3=hadoop103:2888:3888</span><br><span class="line">server.4=hadoop104:2888:3888</span><br></pre></td></tr></table></figure></li>
<li>zkData目录下创建myid的文件对应上述配置编号</li>
<li>配置参数解读<ul>
<li>server.A=B:C:D</li>
<li>A是一个数字，表示这个是第几号服务器，对应zkData/myid中的编号；Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server</li>
<li>B是Zookeeper节点的地址（域名/IP）</li>
<li>C是这个服务器Follower与集群中的Leader服务器交换信息的端口</li>
<li>D是集群中的Leader服务器挂掉之后，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口</li>
</ul>
</li>
<li>编写启动/停止Zookeeper集群脚本 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#检验参数</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$#</span> -lt 1 ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&#x27;参数不能为空！！！&#x27;</span></span><br><span class="line">    <span class="built_in">exit</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#循环遍历每一台机器，分别启动或者停止ZK服务</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> host <span class="keyword">in</span> hadoop001 hadoop002 hadoop003 hadoop004 hadoop005; <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line">    <span class="string">&quot;start&quot;</span>)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;*****************<span class="variable">$1</span> <span class="variable">$host</span> zookeeper****************&quot;</span></span><br><span class="line">        ssh <span class="variable">$host</span> /opt/module/zookeeper-3.5.7/bin/zkServer.sh <span class="variable">$1</span></span><br><span class="line">        ;;</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;stop&quot;</span>)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;*****************<span class="variable">$1</span> <span class="variable">$host</span> zookeeper****************&quot;</span></span><br><span class="line">        ssh <span class="variable">$host</span> /opt/module/zookeeper-3.5.7/bin/zkServer.sh <span class="variable">$1</span></span><br><span class="line">        ;;</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;status&quot;</span>)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;*****************<span class="variable">$1</span> <span class="variable">$host</span> zookeeper****************&quot;</span></span><br><span class="line">        ssh <span class="variable">$host</span> /opt/module/zookeeper-3.5.7/bin/zkServer.sh <span class="variable">$1</span></span><br><span class="line">        ;;</span><br><span class="line"></span><br><span class="line">    *)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&#x27;参数有误！！！&#x27;</span></span><br><span class="line">        <span class="built_in">exit</span></span><br><span class="line">        ;;</span><br><span class="line">    <span class="keyword">esac</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></li>
<li>执行脚本 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 bin]$ zookeeper_cluster.sh start</span><br><span class="line">*****************start hadoop001 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">*****************start hadoop002 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">*****************start hadoop003 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">[atguigu@hadoop001 bin]$ zookeeper_cluster.sh status</span><br><span class="line">*****************status hadoop001 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: follower</span><br><span class="line">*****************status hadoop002 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: follower</span><br><span class="line">*****************status hadoop003 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: leader</span><br><span class="line">[atguigu@hadoop001 bin]$ zookeeper_cluster.sh stop</span><br><span class="line">*****************stop hadoop001 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Stopping zookeeper ... STOPPED</span><br><span class="line">*****************stop hadoop002 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Stopping zookeeper ... STOPPED</span><br><span class="line">*****************stop hadoop003 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Stopping zookeeper ... STOPPED</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="3-2-客户端命令行操作"><a href="#3-2-客户端命令行操作" class="headerlink" title="3.2 客户端命令行操作"></a>3.2 客户端命令行操作</h2><h2 id="3-2-1-基本命令"><a href="#3-2-1-基本命令" class="headerlink" title="3.2.1 基本命令"></a>3.2.1 基本命令</h2><table>
<thead>
<tr>
<th>命令基本语法</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td>help</td>
<td>显示所有操作命令</td>
</tr>
<tr>
<td>ls path</td>
<td>使用 ls 命令来查看当前znode的子节点 <br> -w  监听子节点变化 <br>-s   附加次级信息</td>
</tr>
<tr>
<td>create</td>
<td>普通创建 <br>-s  含有序列 <br>-e  临时（重启或者超时消失）</td>
</tr>
<tr>
<td>get path</td>
<td>获得节点的值<br>-w  监听节点内容变化<br>-s   附加次级信息</td>
</tr>
<tr>
<td>set</td>
<td>设置节点的具体值</td>
</tr>
<tr>
<td>stat</td>
<td>查看节点状态</td>
</tr>
<tr>
<td>delete</td>
<td>删除节点</td>
</tr>
<tr>
<td>deleteall</td>
<td>递归删除节点</td>
</tr>
</tbody></table>
<h2 id="3-2-2-具体操作"><a href="#3-2-2-具体操作" class="headerlink" title="3.2.2 具体操作"></a>3.2.2 具体操作</h2><ol>
<li><p>启动客户端</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 zookeeper-3.5.7]$ bin/zkCli.sh -server hadoop001:2181</span><br><span class="line">Connecting to hadoop001:2181</span><br><span class="line">·······</span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:None path:null</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 0]</span><br></pre></td></tr></table></figure></li>
<li><p>显示所有操作命令</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 0] <span class="built_in">help</span></span><br><span class="line">ZooKeeper -server host:port cmd args</span><br><span class="line">	addauth scheme auth</span><br><span class="line">	close</span><br><span class="line">	config [-c] [-w] [-s]</span><br><span class="line">	connect host:port</span><br><span class="line">	create [-s] [-e] [-c] [-t ttl] path [data] [acl]</span><br><span class="line">	delete [-v version] path</span><br><span class="line">	deleteall path</span><br><span class="line">	delquota [-n|-b] path</span><br><span class="line">	get [-s] [-w] path</span><br><span class="line">	getAcl [-s] path</span><br><span class="line">	<span class="built_in">history</span></span><br><span class="line">	listquota path</span><br><span class="line">	ls [-s] [-w] [-R] path</span><br><span class="line">	ls2 path [watch]</span><br><span class="line">	printwatches on|off</span><br><span class="line">	quit</span><br><span class="line">	reconfig [-s] [-v version] [[-file path] | [-members serverID=host:port1:port2;port3[,...]*]] | [-add serverId=host:port1:port2;port3[,...]]* [-remove serverId[,...]*]</span><br><span class="line">	redo cmdno</span><br><span class="line">	removewatches path [-c|-d|-a] [-l]</span><br><span class="line">	rmr path</span><br><span class="line">	<span class="built_in">set</span> [-s] [-v version] path data</span><br><span class="line">	setAcl [-s] [-v version] [-R] path acl</span><br><span class="line">	setquota -n|-b val path</span><br><span class="line">	<span class="built_in">stat</span> [-w] path</span><br><span class="line">	sync path</span><br></pre></td></tr></table></figure></li>
<li><p>查看当前znode中所包含的内容</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 2] ls /</span><br><span class="line">[zookeeper]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 3]</span><br></pre></td></tr></table></figure></li>
<li><p>查看当前节点详细数据</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 3] ls -s /</span><br><span class="line">[zookeeper]cZxid = 0x0</span><br><span class="line">ctime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">mZxid = 0x0</span><br><span class="line">mtime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">pZxid = 0x100000011</span><br><span class="line">cversion = 1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 0</span><br><span class="line">numChildren = 1</span><br><span class="line"></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 4]</span><br></pre></td></tr></table></figure></li>
<li><p>分别创建2个普通节点</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 4] create /sanguo <span class="string">&quot;diaochan&quot;</span></span><br><span class="line">Created /sanguo</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 6] create /sanguo/shuguo <span class="string">&quot;liubei&quot;</span></span><br><span class="line">Created /sanguo/shuguo</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 8] ls  /</span><br><span class="line">[sanguo, zookeeper]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 9] ls  /sanguo</span><br><span class="line">[shuguo]</span><br></pre></td></tr></table></figure></li>
<li><p>获得节点的值</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 10] get /sanguo</span><br><span class="line">diaochan</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 11] get -s /sanguo</span><br><span class="line">diaochan</span><br><span class="line">cZxid = 0x300000004</span><br><span class="line">ctime = Mon Oct 18 17:31:28 CST 2021</span><br><span class="line">mZxid = 0x300000004</span><br><span class="line">mtime = Mon Oct 18 17:31:28 CST 2021</span><br><span class="line">pZxid = 0x300000006</span><br><span class="line">cversion = 1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 8</span><br><span class="line">numChildren = 1</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 12] get -s /sanguo/shuguo</span><br><span class="line">liubei</span><br><span class="line">cZxid = 0x300000006</span><br><span class="line">ctime = Mon Oct 18 17:32:12 CST 2021</span><br><span class="line">mZxid = 0x300000006</span><br><span class="line">mtime = Mon Oct 18 17:32:12 CST 2021</span><br><span class="line">pZxid = 0x300000006</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 6</span><br><span class="line">numChildren = 0</span><br></pre></td></tr></table></figure></li>
<li><p>创建临时节点</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建临时节点</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 13] create -e /sanguo/wuguo <span class="string">&quot;zhouyu&quot;</span></span><br><span class="line">Created /sanguo/wuguo</span><br><span class="line"><span class="comment"># 查看节点存在</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 15] ls /sanguo</span><br><span class="line">[shuguo, wuguo]</span><br><span class="line"><span class="comment"># 退出客户端</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 16] quit</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:Closed <span class="built_in">type</span>:None path:null</span><br><span class="line">2021-10-18 17:35:37,751 [myid:] - INFO  [main:ZooKeeper@1422] - Session: 0x10000c30dbf0001 closed</span><br><span class="line">2021-10-18 17:35:37,752 [myid:] - INFO  [main-EventThread:ClientCnxn<span class="variable">$EventThread</span>@524] - EventThread shut down <span class="keyword">for</span> session: 0x10000c30dbf0001</span><br><span class="line"><span class="comment"># 重新连接客户端</span></span><br><span class="line">[atguigu@hadoop001 zookeeper-3.5.7]$ bin/zkCli.sh -server hadoop001:2181</span><br><span class="line">Connecting to hadoop001:2181</span><br><span class="line">···</span><br><span class="line">···</span><br><span class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:None path:null</span><br><span class="line"><span class="comment"># 再次查看节点 发现临时节点被删除</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 0] ls /sanguo</span><br><span class="line">[shuguo]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 1]</span><br></pre></td></tr></table></figure></li>
<li><p>创建带序号的节点</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 6] ls /sanguo/weiguo</span><br><span class="line">[]</span><br><span class="line"><span class="comment"># 创建普通节点xiaobing</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 7] create /sanguo/weiguo/xiaobing</span><br><span class="line">Created /sanguo/weiguo/xiaobing</span><br><span class="line"><span class="comment"># 再次创建普通节点xiaobing--Node already exists</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 8] create /sanguo/weiguo/xiaobing</span><br><span class="line">Node already exists: /sanguo/weiguo/xiaobing</span><br><span class="line"><span class="comment"># 创建普通节点xiaobing1</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 9] create /sanguo/weiguo/xiaobing1</span><br><span class="line">Created /sanguo/weiguo/xiaobing1</span><br><span class="line"><span class="comment"># 创建带序号的节点xiaobing</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 10] create -s /sanguo/weiguo/xiaobing</span><br><span class="line">Created /sanguo/weiguo/xiaobing0000000002</span><br><span class="line"><span class="comment"># 创建带序号的节点xiaobing</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 11] create -s /sanguo/weiguo/xiaobing</span><br><span class="line">Created /sanguo/weiguo/xiaobing0000000003</span><br><span class="line"><span class="comment"># 创建带序号的节点xiaobing</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 12] create -s /sanguo/weiguo/xiaobing</span><br><span class="line">Created /sanguo/weiguo/xiaobing0000000004</span><br><span class="line"><span class="comment"># 查看节点，如果节点下原来没有子节点，序号从0开始依次递增。如果原节点下已有2个节点，则再排序时从2开始，以此类推</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 13] ls /sanguo/weiguo</span><br><span class="line">[xiaobing, xiaobing0000000002, xiaobing0000000003, xiaobing0000000004, xiaobing1]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 14]</span><br></pre></td></tr></table></figure></li>
<li><p>修改节点数据值</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 0] get /sanguo/weiguo</span><br><span class="line">caocao</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 1] <span class="built_in">set</span> /sanguo/weiguo <span class="string">&#x27;caopi&#x27;</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 2] get /sanguo/weiguo</span><br><span class="line">caopi</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 3]</span><br></pre></td></tr></table></figure></li>
<li><p>节点的值变化监听</p>
<ol>
<li>在hadoop002主机上注册监听/sanguo节点数据变化 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop002:2181(CONNECTED) 0] get -w /sanguo</span><br><span class="line">null</span><br><span class="line">[zk: hadoop002:2181(CONNECTED) 1]</span><br></pre></td></tr></table></figure></li>
<li>在hadoop001主机上修改/sanguo节点的数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 5] <span class="built_in">set</span> /sanguo <span class="string">&quot;xishi&quot;</span></span><br></pre></td></tr></table></figure></li>
<li>观察hadoop002主机收到数据变化的监听 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:NodeDataChanged path:/sanguo</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>节点的子节点变化监听路径变化.</p>
<ol>
<li>在hadoop002主机上注册监听/sanguo节点的子节点变化 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop002:2181(CONNECTED) 0] ls -w /sanguo</span><br><span class="line">[shuguo, weiguo, wuguo]</span><br></pre></td></tr></table></figure></li>
<li>在hadoop001主机/sanguo节点上创建子节点 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 4] create /sanguo/jin</span><br><span class="line">Created /sanguo/jin</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 5]</span><br></pre></td></tr></table></figure></li>
<li>观察hadoop002主机收到子节点变化的监听 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop002:2181(CONNECTED) 1]</span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:NodeChildrenChanged path:/sanguo</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>删除节点</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 7] ls /sanguo</span><br><span class="line">[jin, shuguo, weiguo, wuguo]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 8] delete /sanguo/jin</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 9] ls /sanguo</span><br><span class="line">[shuguo, weiguo, wuguo]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 10]</span><br></pre></td></tr></table></figure></li>
<li><p>递归删除节点</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 12] ls /sanguo/weiguo</span><br><span class="line">[xiaobing, xiaobing0000000002, xiaobing0000000003, xiaobing0000000004, xiaobing1]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 13] deleteall /sanguo/weiguo</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 14] ls /sanguo/weiguo</span><br><span class="line">Node does not exist: /sanguo/weiguo</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 15]</span><br></pre></td></tr></table></figure></li>
<li><p>查看节点状态</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 15] <span class="built_in">stat</span> /sanguo</span><br><span class="line">cZxid = 0x300000016</span><br><span class="line">ctime = Mon Oct 18 17:41:54 CST 2021</span><br><span class="line">mZxid = 0x300000028</span><br><span class="line">mtime = Mon Oct 18 17:58:53 CST 2021</span><br><span class="line">pZxid = 0x300000030</span><br><span class="line">cversion = 6</span><br><span class="line">dataVersion = 1</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 5</span><br><span class="line">numChildren = 2</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 16]</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="四、Zookeeper-原理"><a href="#四、Zookeeper-原理" class="headerlink" title="四、Zookeeper 原理"></a>四、Zookeeper 原理</h1><h2 id="4-1-节点类型"><a href="#4-1-节点类型" class="headerlink" title="4.1 节点类型"></a>4.1 节点类型</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345542078737.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>持久（Persistent）：客户端和服务器端断开连接后，创建的节点不删除</li>
<li>短暂（Ephemeral）：客户端和服务器端断开连接后，创建的节点自己删除</li>
<li>在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序</li>
<li>创建znode时设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护</li>
</ul>
<h2 id="4-2-Stat结构体"><a href="#4-2-Stat结构体" class="headerlink" title="4.2 Stat结构体"></a>4.2 Stat结构体</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 15] <span class="built_in">stat</span> /sanguo</span><br><span class="line"><span class="comment"># czxid-创建节点的事务zxid</span></span><br><span class="line"><span class="comment">#   每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。</span></span><br><span class="line"><span class="comment">#   事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。</span></span><br><span class="line">cZxid = 0x300000016</span><br><span class="line"><span class="comment"># ctime - znode被创建的毫秒数(从1970年开始)</span></span><br><span class="line">ctime = Mon Oct 18 17:41:54 CST 2021</span><br><span class="line"><span class="comment"># mzxid - znode最后更新的事务zxid</span></span><br><span class="line">mZxid = 0x300000028</span><br><span class="line"><span class="comment"># mtime - znode最后修改的毫秒数(从1970年开始)</span></span><br><span class="line">mtime = Mon Oct 18 17:58:53 CST 2021</span><br><span class="line"><span class="comment"># pZxid - znode最后更新的子节点zxid</span></span><br><span class="line">pZxid = 0x300000030</span><br><span class="line"><span class="comment"># cversion - znode子节点变化号，znode子节点修改次数</span></span><br><span class="line">cversion = 6</span><br><span class="line"><span class="comment"># dataversion - znode数据变化号</span></span><br><span class="line">dataVersion = 1</span><br><span class="line"><span class="comment"># aclVersion - znode访问控制列表的变化号</span></span><br><span class="line">aclVersion = 0</span><br><span class="line"><span class="comment"># ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。</span></span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line"><span class="comment"># dataLength - znode的数据长度</span></span><br><span class="line">dataLength = 5</span><br><span class="line"><span class="comment"># numChildren - znode子节点数量</span></span><br><span class="line">numChildren = 2</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 16]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="4-3-监听器原理"><a href="#4-3-监听器原理" class="headerlink" title="4.3 监听器原理"></a>4.3 监听器原理</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345549940089.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="4-3-1-监听原理详解"><a href="#4-3-1-监听原理详解" class="headerlink" title="4.3.1 监听原理详解"></a>4.3.1 监听原理详解</h3><ol>
<li>首先要有一个main()线程</li>
<li>在main线程中创建Zookeeper客户端，这时就会创建两个线程，一个负责网络连接通信（connet），一个负责监听（listener）。 </li>
<li>通过connect线程将注册的监听事件发送给Zookeeper。 </li>
<li>在Zookeeper的注册监听器列表中将注册的监听事件添加到列表中。 </li>
<li>Zookeeper监听到有数据或路径变化，就会将这个消息发送给listener线程。 </li>
<li>listener线程内部调用了process()方法。 <h3 id="4-3-2-常见的监听"><a href="#4-3-2-常见的监听" class="headerlink" title="4.3.2 常见的监听"></a>4.3.2 常见的监听</h3></li>
<li>监听节点数据的变化<ul>
<li><code>get path [watch]</code></li>
</ul>
</li>
<li>监听子节点增减的变化<ul>
<li><code>ls path [watch]</code></li>
</ul>
</li>
</ol>
<h2 id="4-4-选举机制"><a href="#4-4-选举机制" class="headerlink" title="4.4 选举机制"></a>4.4 选举机制</h2><ol>
<li>半数机制：集群中半数以上机器存活，集群可用。所以<font color ='red' >Zookeeper适合安装奇数台服务器</font>。</li>
<li>Zookeeper虽然在配置文件中并没有指定Master和Slave。但是，Zookeeper工作时，是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的。</li>
<li>以一个简单的例子来说明整个选举的过程。<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345388234410.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><ul>
<li>假设有五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么。</li>
<li>Zookeeper的选举机制<ol>
<li>服务器1启动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上（3票），选举无法完成，服务器1状态保持为LOCKING；</li>
<li>服务器2启动，再发起一次选举。服务器1和2分别投自己一票并交换选票信息：此时服务器1发现服务器2的ID比自己目前投票推举的（服务器1）大，更改选票为推举服务器2。此时服务器1票数0票，服务器2票数2票，没有半数以上结果，选举无法完成，服务器1，2状态保持LOCKING</li>
<li>服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING；</li>
<li>服务器4启动，发起一次选举。此时服务器1，2，3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为FOLLOWING；</li>
<li>服务器5启动，同4一样当小弟。</li>
</ol>
</li>
<li>举例1<ul>
<li>场景：以5台机器为例，集群的机器顺时启动，当前集群中没有任何数据。<ol>
<li>server1 启动，首先server1给自己投一票，然后看当前票数是否超过半数，结果没有超过，这时候leader就没选出来，当前选举状态是Locking状态。</li>
<li>server2 启动，首先server2先给自己投一票，因为当前集群已经有两台机器已启动，所以server1</li>
<li>server2会交换选票，交换后发现各自有一票，接下来比较 myid 发现server2的myid值 &gt; server1的myid值此时server2胜出，最后server2有两票。最后再看当前票数是否半，发现未过半，集群的选举状态集训保持locking状态。</li>
<li>server3启动， 首先自己投自己一票，server1和server2也会投自己一票，然后交换选票发现都一样，接着比较myid 最后server3胜出，此时server3就有3票，同时server3的票数超过半数。所以server3成为leader。</li>
<li>server4启动，发现当前集群已经有leader 它自己自动成为follower</li>
<li>server5启动，发现当前集群已经有leader 它自己自动成为follower</li>
</ol>
</li>
</ul>
</li>
<li>举例说明2<ul>
<li>场景：以5台机器为例，当前集群正在使用（有数据/没数据），leader突然宕机的情况。</li>
<li>当集群中的leader挂掉，集群会重新选出一个leader，此时首先会比较每一台机器的mzxid,mzxid最大的被选为leader。极端情况，mzxid都相等的情况，那么就会直接比较myid。</li>
</ul>
</li>
<li>举例说明3<ul>
<li>场景：集群中有数据，重启的时候Leader该如何选？</li>
<li>和场景一选举机制是一样！</li>
</ul>
</li>
</ul>
</li>
<li>一般情况下ZK集群更推荐使用奇数台机器原因？<ul>
<li>在ZK集群中 奇数台 和 偶数台（接近的台数） 机器的容错能力是一样的，所以在考虑资源节省的情况我们推荐使用奇数台方案</li>
</ul>
</li>
</ol>
<h2 id="4-5-写数据流程"><a href="#4-5-写数据流程" class="headerlink" title="4.5 写数据流程"></a>4.5 写数据流程</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345398429599.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>客户端会向ZK集群中的一台机器server1发送写数据的请求。</li>
<li>server1接收到请求后，马上会通知leader 有写数据的请求来了</li>
<li>leader拿到请求后，进行广播，让集群每一台机器都准备要写数据</li>
<li>集群中的所有机机器接收到leader广播后都回应一下leader</li>
<li>leader接收机器数过半的机器回应后，再次进行广播 开始写数据，<br>其他机器接收到广播后也开始写数据</li>
<li>数据成功写入后，回应leader，最后由leader来做整个事务提交</li>
<li>当数据成功写入后，由最初和客户端发生连接的 server1 回应客户端数据写入成功。</li>
</ol>
<h1 id="五、Zookeeper-面试真题"><a href="#五、Zookeeper-面试真题" class="headerlink" title="五、Zookeeper 面试真题"></a>五、Zookeeper 面试真题</h1><h2 id="5-1-请简述ZooKeeper的选举机制"><a href="#5-1-请简述ZooKeeper的选举机制" class="headerlink" title="5.1 请简述ZooKeeper的选举机制"></a>5.1 请简述ZooKeeper的选举机制</h2><h2 id="5-2-ZooKeeper的监听原理是什么？"><a href="#5-2-ZooKeeper的监听原理是什么？" class="headerlink" title="5.2 ZooKeeper的监听原理是什么？"></a>5.2 ZooKeeper的监听原理是什么？</h2><h2 id="5-3-ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？"><a href="#5-3-ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？" class="headerlink" title="5.3 ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？"></a>5.3 ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？</h2><ol>
<li>部署方式单机模式、集群模式</li>
<li>角色：Leader和Follower</li>
<li>集群最少需要机器数：3</li>
</ol>
<h2 id="5-4-ZooKeeper的常用命令Keeper的常用命令"><a href="#5-4-ZooKeeper的常用命令Keeper的常用命令" class="headerlink" title="5.4 ZooKeeper的常用命令Keeper的常用命令"></a>5.4 ZooKeeper的常用命令Keeper的常用命令</h2>]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Yarn</title>
    <url>/2021/11/07/Yarn/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h1><h1 id="一、Yarn资源调度器"><a href="#一、Yarn资源调度器" class="headerlink" title="一、Yarn资源调度器"></a>一、Yarn资源调度器</h1><ul>
<li>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个<font color ='red' >分布式操作系统平台</font>，而MapReduce等运算程序则相当于<font color ='red' >运行于操作系统之上的应用程序</font>。<h2 id="1-1-Yarn基本架构"><a href="#1-1-Yarn基本架构" class="headerlink" title="1.1 Yarn基本架构"></a>1.1 Yarn基本架构</h2></li>
<li>YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16344718758198.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
<h2 id="1-2-Yarn工作机制"><a href="#1-2-Yarn工作机制" class="headerlink" title="1.2 Yarn工作机制"></a>1.2 Yarn工作机制</h2><ol>
<li>MR程序在客户端通过job.submit()方法提交任务到本地或者远程hadoop集群，创建YranRunner</li>
<li>YarnRunner向ResourceManager申请一个Application</li>
<li>ResourceManager将程序运行的资源路径(资源提交路径及application_id)返回给Yarnrunner</li>
<li>程序将运行所需资源提交到hdfs上（jar包，配置文件，split信息）</li>
<li>程序资源提交之后申请运行MRAppMaster</li>
<li>ResourceManager将用户请求初始化为一个Task，该task会被放到任务队列中，等待调度器分配资源</li>
<li>NodeManager领取task任务</li>
<li>该NodeManager创建container，并启动MRAppMaster</li>
<li>container从hdfs拷贝资源到本地</li>
<li>MRAppMaster向RM申请运行MapTask资源</li>
<li>ResourceManager将MapTask任务分配给NodeManager，领取到任务的NodeManager创建容器</li>
<li>MRAppMaster向接收到任务的NodeManager发送启动程序脚本，NodeManeger启动MapTask,执行map逻辑，输出分区排序后的结果</li>
<li>MRAppMaster等待所有MapTak运行结束，或者指定数量的MapTask运行结束后，向ResourceManager申请容器，运行ReduceTask.</li>
<li>ReduceTask从MapTask输出中取对应分区的数据，执行reduce逻辑</li>
<li>程序运行结束，MRAppMaster会向ResourceManager申请注销自己<h2 id="1-3-作业提交过程"><a href="#1-3-作业提交过程" class="headerlink" title="1.3 作业提交过程"></a>1.3 作业提交过程</h2><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16344721580647.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16344723700779.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>作业提交阶段<ol>
<li>client调用job.waitForCompletion()方法，提交任务到集群</li>
<li>client想ResourceManager申请一个作业id</li>
<li>ResourceManager返回改job资源的提交路径和作业id</li>
<li>client提交jar，切片信息，配置文件到指定的资源提交路径</li>
<li>client提交资源后，向ResourceManager申请运行MRAppMaster</li>
</ol>
</li>
<li>作业初始化<ol>
<li>ResourceManager收到请求后，将job添加到任务队列</li>
<li>某一个空闲的NodeManager领取到任务后，创建Container，运行MRAppMaster</li>
<li>下载client提交的资源文件到本地</li>
</ol>
</li>
<li>任务分配<ol>
<li>MrAppMaster向ResourceManager申请MapTask运行资源</li>
<li>ResourceManager将MapTask分配给其他NodeManager</li>
<li>领取到任务的NodeManager创建MapTask容器</li>
</ol>
</li>
<li>任务运行<ol>
<li>MRAppMaster向领取到任务的NodeManager发送启动程序脚本</li>
<li>NodeManeger启动MapTask,执行map逻辑，输出分区排序后的结果</li>
<li>MRAppMaster等待所有MapTask运行结束，或者指定数量的MapTask运行结束，向ResourceManager申请容器，运行ReduceTask</li>
<li>ReduceTask从MapTask的输出文件中获取响应分区数据，执行Reduce逻辑</li>
<li>程序运行结束，MRAppMaster会向ResourceManager申请注销自己</li>
</ol>
</li>
<li>更新运行状态和进度<ol>
<li>Yarn中的任务进度和状态返回给应用管理器，客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求更新进度，展示给用户</li>
</ol>
</li>
<li>作业完成<ol>
<li>除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。</li>
</ol>
</li>
</ol>
<h2 id="1-4-资源调度器"><a href="#1-4-资源调度器" class="headerlink" title="1.4 资源调度器"></a>1.4 资源调度器</h2><ul>
<li><p>目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。Hadoop3.1.3默认的资源调度器是Capacity Scheduler。</p>
</li>
<li><p>具体设置详见：yarn-default.xml文件</p>
  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The class to use as the resource scheduler.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="1-4-1-FIFO先进先出调度器"><a href="#1-4-1-FIFO先进先出调度器" class="headerlink" title="1.4.1 FIFO先进先出调度器"></a>1.4.1 FIFO先进先出调度器</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16362766014573.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>单队列，根据提交作业的先后顺序，先来先服务。</p>
</li>
<li><p>优点：简单易懂；</p>
</li>
<li><p>缺点：不支持多队列，生产环境很少使用；</p>
</li>
</ul>
<h3 id="1-4-2-容量调度器（Capacity-Scheduler）"><a href="#1-4-2-容量调度器（Capacity-Scheduler）" class="headerlink" title="1.4.2 容量调度器（Capacity Scheduler）"></a>1.4.2 容量调度器（Capacity Scheduler）</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16362766636662.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>Capacity Scheduler Capacity Scheduler 是Yahoo开发的多用户调度器，它以队列为单位划分资源，每个队列可设定一定比例的资源最低保证和使用上限，同时，每个用户也可设定一定的资源使用上限以防止资源滥用。而当一个队列的资源有剩余时，可暂时将剩余资源共享给其他队列。</li>
<li>总之，Capacity Scheduler 主要有以下几个特点：<ol>
<li>多队列：每个队列可配置一定的资源量，每个队列采用FIFO调度策略</li>
<li>容量保证：管理员可为每个队列设置资源最低保证和资源使用上限，所有提交到该队列的应用程序共享这些资源。</li>
<li>灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。这种资源灵活分配的方式可明显提高资源利用率。</li>
<li>多重租赁：支持多用户共享集群和多应用程序同时运行。为防止单个应用程序、用户或者队列独占集群中的资源，管理员可为之增加多重约束（比如单个应用程序同时运行的任务数等）。</li>
<li>安全保证：每个队列有严格的ACL列表规定它的访问用户，每个用户可指定哪些用户允许查看自己应用程序的运行状态或者控制应用程序（比如杀死应用程序）。此外，管理员可指定队列管理员和集群系统管理员。</li>
<li>动态更新配置文件：管理员可根据需要动态修改各种配置参数，以实现在线集群管理。</li>
</ol>
</li>
</ul>
<h3 id="1-4-3-公平调度器（Fair-Scheduler）"><a href="#1-4-3-公平调度器（Fair-Scheduler）" class="headerlink" title="1.4.3 公平调度器（Fair Scheduler）"></a>1.4.3 公平调度器（Fair Scheduler）</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16362766917702.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>Fair Scheduler Fair Schedulere是Facebook开发的多用户调度器。</li>
<li>公平调度器的目的是让所有的作业随着时间的推移，都能平均地获取等同的共享资源。当有作业提交上来，系统会将空闲的资源分配给新的作业，每个任务大致上会获取平等数量的资源。和传统的调度策略不同的是它会让小的任务在合理的时间完成，同时不会让需要长时间运行的耗费大量资源的任务挨饿！同Capacity Scheduler类似，它以队列为单位划分资源，每个队列可设定一定比例的资源最低保证和使用上限，同时，每个用户也可设定一定的资源使用上限以防止资源滥用；当一个队列的资源有剩余时，可暂时将剩余资源共享给其他队列。当然，Fair Scheduler也存在很多与Capacity Scheduler不同之处，这主要体现在以下几个方面：<ol>
<li>资源公平共享。在每个队列中，Fair Scheduler 可选择按照FIFO、Fair或DRF策略为应用程序分配资源。其中， </li>
<li>FIFO策略: 公平调度器每个队列资源分配策略如果选择FIFO的话，就是禁用掉每个队列中的Task共享队列资源，此时公平调度器相当于上面讲过的容量调度器。</li>
<li>Fair策略: 基于最大最小公平算法实现的资源多路复用方式，默认情况下，每个队列内部采用该方式分配资源。这意味着，如果一个队列中有两个应用程序同时运行，则每个应用程序可得到1/2的资源；如果三个应用程序同时运行，则每个应用程序可得到1/3的资源。<ul>
<li>扩展：最大最小公平算法举例：<ol>
<li>不加权(关注点是job的个数)： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> 有一条队列总资源12个, 有4个job，对资源的需求分别是: </span><br><span class="line">job1-&gt;1,  job2-&gt;2 , job3-&gt;6,  job4-&gt;5</span><br><span class="line">     第一次算:  12 / 4 = 3 </span><br><span class="line">		job1: 分3 --&gt; 多2个 </span><br><span class="line">		job2: 分3 --&gt; 多1个</span><br><span class="line">		job3: 分3 --&gt; 差3个</span><br><span class="line">		job4: 分3 --&gt; 差2个</span><br><span class="line">	第二次算: 3 / 2  = 1.5 </span><br><span class="line">         job1: 分1</span><br><span class="line">		job2: 分2</span><br><span class="line">		job3: 分3 --&gt; 差3个 --&gt; 分1.5 --&gt; 最终: 4.5 </span><br><span class="line">		job4: 分3 --&gt; 差2个 --&gt; 分1.5 --&gt; 最终: 4.5 </span><br><span class="line">	第n次算: 一直算到没有空闲资源</span><br></pre></td></tr></table></figure></li>
<li>加权(关注点是job的权重)： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">有一条队列总资源16，有4个job </span><br><span class="line">对资源的需求分别是: job1-&gt;4   job2-&gt;2  job3-&gt;10  job4-&gt;4 </span><br><span class="line">每个job的权重为:   job1-&gt;5   job2-&gt;8  job3-&gt;1   job4-&gt;2	</span><br><span class="line">	第一次算: 16 / (5+8+1+2) =  1</span><br><span class="line">	    job1:  分5 --&gt; 多1</span><br><span class="line">	    job2:  分8 --&gt; 多6</span><br><span class="line">	    job3:  分1 --&gt; 少9</span><br><span class="line">	    job4:  分2 --&gt; 少2            </span><br><span class="line">	第二次算: 7 / (1+2) = 7/3</span><br><span class="line">	    job1: 分4</span><br><span class="line">	    job2: 分2</span><br><span class="line">	    job3: 分1 --&gt; 分7/3（2.33） --&gt; 少 6.67</span><br><span class="line">	    job4: 分2 --&gt; 分14/3(4.66) --&gt;多2.66</span><br><span class="line">    第三次算: </span><br><span class="line">	    job1: 分4</span><br><span class="line">	    job2: 分2</span><br><span class="line">	    job3: 分1 --&gt; 分7/3 --&gt; 分2.66</span><br><span class="line">	    job4: 分4</span><br><span class="line">    第n次算: 一直算到没有空闲资源</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<ol start="4">
<li>DRF策略: DRF(Dominant Resource Fairness)，我们之前说的资源，都是单一标准，例如只考虑内存(也是yarn默认的情况)。但是很多时候我们资源有很多种，例如内存，CPU，网络带宽等，这样我们很难衡量两个应用应该分配的资源比例。那么在YARN中，我们用DRF来决定如何调度：假设集群一共有100 CPU和10T 内存，而应用A需要(2 CPU, 300GB)，应用B需要(6 CPU, 100GB)。则两个应用分别需要A(2%CPU, 3%内存)和B(6%CPU, 1%内存)的资源，这就意味着A是内存主导的, B是CPU主导的，针对这种情况，我们可以选择DRF策略对不同应用进行不同资源（CPU和内存）的一个不同比例的限制。 <ul>
<li>支持资源抢占。当某个队列中有剩余资源时，调度器会将这些资源共享给其他队列，而当该队列中有新的应用程序提交时，调度器要为它回收资源。为了尽可能降低不必要的计算浪费，调度器采用了先等待再强制回收的策略，即如果等待一段时间后尚有未归还的资源，则会进行资源抢占：从那些超额使用资源的队列中杀死一部分任务，进而释放资源。yarn.scheduler.fair.preemption=true 通过该配置开启资源抢占。</li>
<li>提高小应用程序响应时间。由于采用了最大最小公平算法，小作业可以快速获取资源并运行完成<h2 id="1-5-Yarn常用命令"><a href="#1-5-Yarn常用命令" class="headerlink" title="1.5 Yarn常用命令"></a>1.5 Yarn常用命令</h2><h3 id="1-5-1-yarn-application查看任务"><a href="#1-5-1-yarn-application查看任务" class="headerlink" title="1.5.1 yarn application查看任务"></a>1.5.1 yarn application查看任务</h3></li>
</ul>
</li>
</ol>
</li>
</ol>
</li>
</ul>
<ol>
<li>yarn application -list 查看所有任务 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn application -list</span><br><span class="line">Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):1</span><br><span class="line">                Application-Id	    Application-Name	    Application-Type	      User	     Queue	             State	       Final-State	       Progress	                       Tracking-URL</span><br><span class="line">application_1635042010561_0004	sgg-hadoop-mapreduce-0.0.1-SNAPSHOT.jar	           MAPREDUCE	   atguigu	      hive	           RUNNING	         UNDEFINED	             5%	             http://hadoop002:38122</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
<li>根据Application状态过滤：yarn application -list -appStates<ul>
<li>所有状态：ALL、NEW、NEW_SAVING、SUBMITTED、ACCEPTED、RUNNING、FINISHED、FAILED、KILLED<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn application -list -appStates FINISHED</span><br><span class="line">Total number of applications (application-types: [], states: [FINISHED] and tags: []):15</span><br><span class="line">                Application-Id	    Application-Name	    Application-Type	      User	     Queue	             State	       Final-State	       Progress	                       Tracking-URL</span><br><span class="line">application_1635042010561_0004	sgg-hadoop-mapreduce-0.0.1-SNAPSHOT.jar	           MAPREDUCE	   atguigu	      hive	          FINISHED	         SUCCEEDED	           100%	http://hadoop002:19888/jobhistory/job/job_1635042010561_0004</span><br><span class="line">application_1634633871057_0005	hadoop-archives-3.1.3.jar	           MAPREDUCE	   atguigu	   default	          FINISHED	         SUCCEEDED	           100%	http://hadoop001:19888/jobhistory/job/job_1634633871057_0005</span><br><span class="line">application_1634633871057_0004	hadoop-archives-3.1.3.jar	           MAPREDUCE	   atguigu	   default	          FINISHED	         SUCCEEDED	           100%	http://hadoop001:19888/jobhistory/job/job_1634633871057_0004</span><br><span class="line">application_1634633871057_0003	              distcp	           MAPREDUCE	   atguigu	   default	          FINISHED	         SUCCEEDED	           100%	http://hadoop001:19888/jobhistory/job/job_1634633871057_0003</span><br><span class="line">application_1634633871057_0002	sgg-hadoop-mapreduce-0.0.1-SNAPSHOT.jar	           MAPREDUCE	   atguigu	   default	          FINISHED	         SUCCEEDED	           100%	http://hadoop001:19888/jobhistory/job/job_1634633871057_0002</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>Kill掉Application：yarn application -kill Application-Id <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn application -<span class="built_in">kill</span> application_1635042010561_0006</span><br><span class="line">Killing application application_1635042010561_0006</span><br><span class="line">2021-10-24 13:25:54,458 INFO impl.YarnClientImpl: Killed application application_1635042010561_0006</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
 客户端 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[INFO] [2021-10-24 13:25:55][org.apache.hadoop.mapreduce.Job]Job job_1635042010561_0006 failed with state KILLED due to: Application application_1635042010561_0006 was killed by user atguigu at 192.168.2.17</span><br><span class="line">[INFO] [2021-10-24 13:25:55][org.apache.hadoop.mapreduce.Job]Counters: 0</span><br></pre></td></tr></table></figure>
<h3 id="1-5-2-yarn-logs查看日志"><a href="#1-5-2-yarn-logs查看日志" class="headerlink" title="1.5.2 yarn logs查看日志"></a>1.5.2 yarn logs查看日志</h3></li>
<li>查询Application日志：yarn logs -applicationId <ApplicationId> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn logs -applicationId application_1635042010561_0007 &gt; application_1635042010561_0007.log</span><br><span class="line">2021-10-24 13:31:26,239 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">2021-10-24 13:31:26,308 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">[atguigu@hadoop001 ~]$ head application_1635042010561_0007.log</span><br><span class="line">Container: container_e06_1635042010561_0007_01_000003 on hadoop001_34161</span><br><span class="line">LogAggregationType: AGGREGATED</span><br><span class="line">========================================================================</span><br><span class="line">LogType:directory.info</span><br><span class="line">LogLastModifiedTime:星期日 十月 24 13:30:33 +0800 2021</span><br><span class="line">LogLength:2011</span><br><span class="line">LogContents:</span><br><span class="line">ls -l:</span><br><span class="line">total 20</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu  129 Oct 24 13:30 container_tokens</span><br><span class="line">[atguigu@hadoop001 ~]$ tail application_1635042010561_0007.log</span><br><span class="line">2021-10-24 13:30:31,976 INFO [Thread-75] org.apache.hadoop.ipc.Server: Stopping server on 33721</span><br><span class="line">2021-10-24 13:30:31,977 INFO [IPC Server Responder] org.apache.hadoop.ipc.Server: Stopping IPC Server Responder</span><br><span class="line">2021-10-24 13:30:31,978 INFO [IPC Server listener on 0] org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 0</span><br><span class="line">2021-10-24 13:30:31,981 INFO [Thread-75] org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.w.WebAppContext@3cd26422&#123;/,null,UNAVAILABLE&#125;&#123;/mapreduce&#125;</span><br><span class="line">2021-10-24 13:30:31,984 INFO [Thread-75] org.eclipse.jetty.server.AbstractConnector: Stopped ServerConnector@57e388c3&#123;HTTP/1.1,[http/1.1]&#125;&#123;0.0.0.0:0&#125;</span><br><span class="line">2021-10-24 13:30:31,984 INFO [Thread-75] org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@234a8f27&#123;/static,jar:file:/opt/module/ha-hadoop-3.1.3/share/hadoop/yarn/hadoop-yarn-common-3.1.3.jar!/webapps/static,UNAVAILABLE&#125;</span><br><span class="line"></span><br><span class="line">End of LogType:syslog</span><br><span class="line">***********************************************************************</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
<li>查询Container日志：yarn logs -applicationId <ApplicationId> -containerId <ContainerId>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn logs -applicationId application_1635042010561_0007 -containerId container_e06_1635042010561_0007_01_000003 &gt; container_e06_1635042010561_0007_01_000003.log</span><br><span class="line">2021-10-24 13:33:52,284 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">2021-10-24 13:33:52,353 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">[atguigu@hadoop001 ~]$ head container_e06_1635042010561_0007_01_000003.log</span><br><span class="line">Container: container_e06_1635042010561_0007_01_000003 on hadoop001_34161</span><br><span class="line">LogAggregationType: AGGREGATED</span><br><span class="line">========================================================================</span><br><span class="line">LogType:directory.info</span><br><span class="line">LogLastModifiedTime:星期日 十月 24 13:30:33 +0800 2021</span><br><span class="line">LogLength:2011</span><br><span class="line">LogContents:</span><br><span class="line">ls -l:</span><br><span class="line">total 20</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu  129 Oct 24 13:30 container_tokens</span><br><span class="line">[atguigu@hadoop001 ~]$ tail container_e06_1635042010561_0007_01_000003.log</span><br><span class="line">2021-10-24 13:30:25,651 INFO [main] org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16734 bytes</span><br><span class="line">2021-10-24 13:30:25,661 INFO [main] org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: Merged 1 segments, 16746 bytes to disk to satisfy reduce memory <span class="built_in">limit</span></span><br><span class="line">2021-10-24 13:30:25,661 INFO [main] org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: Merging 1 files, 16750 bytes from disk</span><br><span class="line">2021-10-24 13:30:25,661 INFO [main] org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce</span><br><span class="line">2021-10-24 13:30:25,661 INFO [main] org.apache.hadoop.mapred.Merger: Merging 1 sorted segments</span><br><span class="line">2021-10-24 13:30:25,663 INFO [main] org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16734 bytes</span><br><span class="line"></span><br><span class="line">End of LogType:syslog.shuffle</span><br><span class="line">*******************************************************************************</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h3 id="1-5-3-yarn-application-attempt查看尝试运行的任务"><a href="#1-5-3-yarn-application-attempt查看尝试运行的任务" class="headerlink" title="1.5.3 yarn application attempt查看尝试运行的任务"></a>1.5.3 yarn application attempt查看尝试运行的任务</h3></li>
<li>列出所有Application尝试的列表：yarn applicationattempt -list <ApplicationId> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn applicationattempt -list application_1635042010561_0007</span><br><span class="line">Total number of application attempts :1</span><br><span class="line">         ApplicationAttempt-Id	               State	                    AM-Container-Id	                       Tracking-URL</span><br><span class="line">appattempt_1635042010561_0007_000001	            FINISHED	container_e06_1635042010561_0007_01_000001	http://hadoop001:8088/proxy/application_1635042010561_0007/</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
<li>打印ApplicationAttemp状态：yarn applicationattempt -status <ApplicationAttemptId> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn applicationattempt -status appattempt_1635042010561_0007_000001</span><br><span class="line">Application Attempt Report :</span><br><span class="line">	ApplicationAttempt-Id : appattempt_1635042010561_0007_000001</span><br><span class="line">	State : FINISHED</span><br><span class="line">	AMContainer : container_e06_1635042010561_0007_01_000001</span><br><span class="line">	Tracking-URL : http://hadoop001:8088/proxy/application_1635042010561_0007/</span><br><span class="line">	RPC Port : 33721</span><br><span class="line">	AM Host : hadoop002</span><br><span class="line">	Diagnostics :</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="1-5-4-yarn-container查看容器"><a href="#1-5-4-yarn-container查看容器" class="headerlink" title="1.5.4 yarn container查看容器"></a>1.5.4 yarn container查看容器</h3><font color ='blue' >注：只有在任务跑的途中才能看到container的状态</font></li>
<li>打印Container状态：    yarn container -status <ContainerId> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn container -list appattempt_1635042010561_0010_000001</span><br><span class="line">Total number of containers :3</span><br><span class="line">                  Container-Id	          Start Time	         Finish Time	               State	                Host	   Node Http Address	                            LOG-URL</span><br><span class="line">container_e06_1635042010561_0010_01_000001	星期日 十月 24 13:45:46 +0800 2021	                 N/A	             RUNNING	     hadoop002:35693	http://hadoop002:8042	http://hadoop002:8042/node/containerlogs/container_e06_1635042010561_0010_01_000001/atguigu</span><br><span class="line">container_e06_1635042010561_0010_01_000003	星期日 十月 24 13:45:50 +0800 2021	                 N/A	             RUNNING	     hadoop005:37776	http://hadoop005:8042	http://hadoop005:8042/node/containerlogs/container_e06_1635042010561_0010_01_000003/atguigu</span><br><span class="line">container_e06_1635042010561_0010_01_000005	星期日 十月 24 13:46:33 +0800 2021	                 N/A	             RUNNING	     hadoop002:35693	http://hadoop002:8042	http://hadoop002:8042/node/containerlogs/container_e06_1635042010561_0010_01_000005/atguigu</span><br><span class="line">[atguigu@hadoop001 ~]$ yarn container -status container_e06_1635042010561_0010_01_000001</span><br><span class="line">Container Report :</span><br><span class="line">	Container-Id : container_e06_1635042010561_0010_01_000001</span><br><span class="line">	Start-Time : 1635054346921</span><br><span class="line">	Finish-Time : 0</span><br><span class="line">	State : RUNNING</span><br><span class="line">	Execution-Type : GUARANTEED</span><br><span class="line">	LOG-URL : http://hadoop002:8042/node/containerlogs/container_e06_1635042010561_0010_01_000001/atguigu</span><br><span class="line">	Host : hadoop002:35693</span><br><span class="line">	NodeHttpAddress : http://hadoop002:8042</span><br><span class="line">	Diagnostics : null</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
<li>列出所有Container：yarn container -list <ApplicationAttemptId> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn applicationattempt -list application_1635042010561_0009</span><br><span class="line">Total number of application attempts :1</span><br><span class="line">         ApplicationAttempt-Id	               State	                    AM-Container-Id	                       Tracking-URL</span><br><span class="line">appattempt_1635042010561_0009_000001	             RUNNING	container_e06_1635042010561_0009_01_000001	http://hadoop001:8088/proxy/application_1635042010561_0009/</span><br><span class="line">[atguigu@hadoop001 ~]$ yarn container -list appattempt_1635042010561_0009_000001</span><br><span class="line">Total number of containers :4</span><br><span class="line">                  Container-Id	          Start Time	         Finish Time	               State	                Host	   Node Http Address	                            LOG-URL</span><br><span class="line">container_e06_1635042010561_0009_01_000003	星期日 十月 24 13:43:31 +0800 2021	                 N/A	             RUNNING	     hadoop005:37776	http://hadoop005:8042	http://hadoop005:8042/node/containerlogs/container_e06_1635042010561_0009_01_000003/atguigu</span><br><span class="line">container_e06_1635042010561_0009_01_000002	星期日 十月 24 13:43:31 +0800 2021	                 N/A	             RUNNING	     hadoop005:37776	http://hadoop005:8042	http://hadoop005:8042/node/containerlogs/container_e06_1635042010561_0009_01_000002/atguigu</span><br><span class="line">container_e06_1635042010561_0009_01_000001	星期日 十月 24 13:43:27 +0800 2021	                 N/A	             RUNNING	     hadoop005:37776	http://hadoop005:8042	http://hadoop005:8042/node/containerlogs/container_e06_1635042010561_0009_01_000001/atguigu</span><br><span class="line">container_e06_1635042010561_0009_01_000004	星期日 十月 24 13:43:31 +0800 2021	                 N/A	             RUNNING	     hadoop005:37776	http://hadoop005:8042	http://hadoop005:8042/node/containerlogs/container_e06_1635042010561_0009_01_000004/atguigu</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h3 id="1-5-5-yarn-node查看节点状态"><a href="#1-5-5-yarn-node查看节点状态" class="headerlink" title="1.5.5 yarn node查看节点状态"></a>1.5.5 yarn node查看节点状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn node -list -all</span><br><span class="line">Total Nodes:5</span><br><span class="line">         Node-Id	     Node-State	Node-Http-Address	Number-of-Running-Containers</span><br><span class="line"> hadoop004:38785	        RUNNING	   hadoop004:8042	                           0</span><br><span class="line"> hadoop005:37776	        RUNNING	   hadoop005:8042	                           0</span><br><span class="line"> hadoop003:39188	        RUNNING	   hadoop003:8042	                           0</span><br><span class="line"> hadoop002:35693	        RUNNING	   hadoop002:8042	                           0</span><br><span class="line"> hadoop001:34161	        RUNNING	   hadoop001:8042	                           0</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h3 id="1-5-6-yarn-rmadmin更新配置"><a href="#1-5-6-yarn-rmadmin更新配置" class="headerlink" title="1.5.6 yarn rmadmin更新配置"></a>1.5.6 yarn rmadmin更新配置</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn rmadmin -refreshQueues</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h3 id="1-5-7-yarn-queue查看队列"><a href="#1-5-7-yarn-queue查看队列" class="headerlink" title="1.5.7 yarn queue查看队列"></a>1.5.7 yarn queue查看队列</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$  yarn queue -status default</span><br><span class="line">Queue Information :</span><br><span class="line">Queue Name : default</span><br><span class="line">	State : RUNNING</span><br><span class="line">	Capacity : 40.0%</span><br><span class="line">	Current Capacity : .0%</span><br><span class="line">	Maximum Capacity : 60.0%</span><br><span class="line">	Default Node Label expression : &lt;DEFAULT_PARTITION&gt;</span><br><span class="line">	Accessible Node Labels : *</span><br><span class="line">	Preemption : disabled</span><br><span class="line">	Intra-queue Preemption : disabled</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h2 id="1-6-Yarn生产环境核心参数"><a href="#1-6-Yarn生产环境核心参数" class="headerlink" title="1.6 Yarn生产环境核心参数"></a>1.6 Yarn生产环境核心参数</h2><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16350552254539.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ol>
<table>
<thead>
<tr>
<th>类型</th>
<th>配置</th>
<th>描述</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>ResourceManager相关</td>
<td>yarn.resourcemanager.scheduler.class</td>
<td>配置调度器，默认容量</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.resourcemanager.scheduler.client.thread-count</td>
<td>ResourceManager处理调度器请求的线程数量，默认50</td>
<td></td>
</tr>
<tr>
<td>NodeManager相关</td>
<td>yarn.nodemanager.resource.detect-hardware-capabilities</td>
<td>是否让yarn自己检测硬件进行配置，默认false</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.resource.count-logical-processors-as-core</td>
<td>是否将虚拟核数当作CPU核数，默认false</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.resource.pcores-vcores-multiplier</td>
<td>虚拟核数和物理核数乘数，例如：4核8线程，该参数就应设为2，默认1.0</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.resource.memory-mb</td>
<td>NodeManager使用内存，默认8G</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.resource.system-reserved-memory-mb</td>
<td>NodeManager为系统保留多少内存</td>
<td>以上二个参数配置一个即可</td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.resource.cpu-vcores</td>
<td>NodeManager使用CPU核数，默认8个</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.pmem-check-enabled</td>
<td>是否开启物理内存检查限制container，默认打开</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.vmem-check-enabled</td>
<td>是否开启虚拟内存检查限制container，默认打开</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.vmem-pmem-ratio</td>
<td>虚拟内存物理内存比例，默认2.1</td>
<td></td>
</tr>
<tr>
<td>Container相关</td>
<td>yarn.scheduler.minimum-allocation-mb</td>
<td>容器最最小内存，默认1G</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.scheduler.maximum-allocation-mb</td>
<td>容器最最大内存，默认8G</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.scheduler.minimum-allocation-vcores</td>
<td>容器最小CPU核数，默认1个</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.scheduler.maximum-allocation-vcores</td>
<td>容器最大CPU核数，默认4个</td>
<td></td>
</tr>
</tbody></table>
<ol>
<li> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">	            </span><br><span class="line">yarn.resourcemanager.scheduler.client.thread-count     ResourceManager处理调度器请求的线程数量，默认50</span><br></pre></td></tr></table></figure>
<h1 id="二、容量调度器多队列提交案例"><a href="#二、容量调度器多队列提交案例" class="headerlink" title="二、容量调度器多队列提交案例"></a>二、容量调度器多队列提交案例</h1><h3 id="4-5-1-需求"><a href="#4-5-1-需求" class="headerlink" title="4.5.1 需求"></a>4.5.1 需求</h3></li>
</ol>
<ul>
<li>Yarn默认的容量调度器是一条单队列的调度器，在实际使用中会出现单个任务阻塞整个队列的情况。同时，随着业务的增长，公司需要分业务限制集群使用率。这就需要我们按照业务种类配置多条任务队列。</li>
</ul>
<h3 id="4-5-2-配置多队列的容量调度器"><a href="#4-5-2-配置多队列的容量调度器" class="headerlink" title="4.5.2 配置多队列的容量调度器"></a>4.5.2 配置多队列的容量调度器</h3><ul>
<li>默认Yarn的配置下，容量调度器只有一条Default队列。在capacity-scheduler.xml中可以配置多条队列，并降低default队列资源占比：  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定多队列，增加hive队列 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.queues<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>default,hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      The queues at the this level (root is the root queue).</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 降低default队列资源额定容量为40%，默认100% --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.default.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>40<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 降低default队列资源最大容量为60%，默认100% --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.default.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>同时为新加队列添加必要属性：  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定hive队列的资源额定容量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.user-limit-factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定hive队列的资源最大容量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.state<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>RUNNING<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_submit_applications<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_administer_queue<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_application_max_priority<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.maximum-application-lifetime<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.default-application-lifetime<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>在配置完成后，重启Yarn或者执行yarn rmadmin -refreshQueues刷新队列，就可以看到两条队列：</li>
</ul>
<h1 id="问答"><a href="#问答" class="headerlink" title="问答"></a>问答</h1><h3 id="一、MR中的一个Job是如何提交的？"><a href="#一、MR中的一个Job是如何提交的？" class="headerlink" title="一、MR中的一个Job是如何提交的？"></a>一、MR中的一个Job是如何提交的？</h3><ol>
<li>作业提交阶段<ol>
<li>client调用job.waitForCompletion()方法，提交任务到集群</li>
<li>client想ResourceManager申请一个作业id</li>
<li>ResourceManager返回改job资源的提交路径和作业id</li>
<li>client提交jar，切片信息，配置文件到指定的资源提交路径</li>
<li>client提交资源后，向ResourceManager申请运行MRAppMaster</li>
</ol>
</li>
<li>作业初始化<ol>
<li>ResourceManager收到请求后，将job添加到任务队列</li>
<li>某一个空闲的NodeManager领取到任务后，创建Container，运行MRAppMaster</li>
<li>下载client提交的资源文件到本地</li>
</ol>
</li>
<li>任务分配<ol>
<li>MrAppMaster向ResourceManager申请MapTask运行资源</li>
<li>ResourceManager将MapTask分配给其他NodeManager</li>
<li>领取到任务的NodeManager创建MapTask容器</li>
</ol>
</li>
<li>任务运行<ol>
<li>MRAppMaster向领取到任务的NodeManager发送启动程序脚本</li>
<li>NodeManeger启动MapTask,执行map逻辑，输出分区排序后的结果</li>
<li>MRAppMaster等待所有MapTask运行结束，或者指定数量的MapTask运行结束，向ResourceManager申请容器，运行ReduceTask</li>
<li>ReduceTask从MapTask的输出文件中获取响应分区数据，执行Reduce逻辑</li>
<li>程序运行结束，MRAppMaster会向ResourceManager申请注销自己</li>
</ol>
</li>
<li>更新运行状态和进度<ol>
<li>Yarn中的任务进度和状态返回给应用管理器，客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求更新进度，展示给用户</li>
</ol>
</li>
<li>作业完成<ol>
<li>除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。</li>
</ol>
</li>
</ol>
<h3 id="二、描述一下YARN的工作机制。"><a href="#二、描述一下YARN的工作机制。" class="headerlink" title="二、描述一下YARN的工作机制。"></a>二、描述一下YARN的工作机制。</h3><ol>
<li>MR程序在客户端通过job.submit()方法提交任务到本地或者远程hadoop集群，创建YranRunner</li>
<li>YarnRunner向ResourceManager申请一个Application</li>
<li>ResourceManager将程序运行的资源路径(资源提交路径及application_id)返回给Yarnrunner</li>
<li>程序将运行所需资源提交到hdfs上（jar包，配置文件，split信息）</li>
<li>程序资源提交之后申请运行MRAppMaster</li>
<li>ResourceManager将用户请求初始化为一个Task，该task会被放到任务队列中，等待调度器分配资源</li>
<li>NodeManager领取task任务</li>
<li>该NodeManager创建container，并启动MRAppMaster</li>
<li>container从hdfs拷贝资源到本地</li>
<li>MRAppMaster向RM申请运行MapTask资源</li>
<li>ResourceManager将MapTask任务分配给NodeManager，领取到任务的NodeManager创建容器</li>
<li>MRAppMaster向接收到任务的NodeManager发送启动程序脚本，NodeManeger启动MapTask,执行map逻辑，输出分区排序后的结果</li>
<li>MRAppMaster等待所有MapTak运行结束，或者指定数量的MapTask运行结束后，向ResourceManager申请容器，运行ReduceTask.</li>
<li>ReduceTask从MapTask输出中取对应分区的数据，执行reduce逻辑</li>
<li>程序运行结束，MRAppMaster会向ResourceManager申请注销自己</li>
</ol>
<h3 id="三、YARN中有几种资源调度实现策略？分别阐述一下各自的特点"><a href="#三、YARN中有几种资源调度实现策略？分别阐述一下各自的特点" class="headerlink" title="三、YARN中有几种资源调度实现策略？分别阐述一下各自的特点"></a>三、YARN中有几种资源调度实现策略？分别阐述一下各自的特点</h3><ol>
<li>FIFO调度器：<ul>
<li>单队列，根据作业提交的顺序，先到先分配资源</li>
<li>优点：简单易懂</li>
<li>缺点：不支持多队列，无法确定优先级，不够灵活</li>
</ul>
</li>
<li>CapacityScheduler容量调度区：<ul>
<li>多队列：每个队列可以配置一定比例的资源，每个队列内部采用FIFO调度策略</li>
<li>容量保证：可以设置每个队列的资源最低保证和资源使用上限，所有提交到该队列的应用程序共享这些资源</li>
<li>灵活性：如果一个队列中资源有空闲，可以暂时共享给需要资源的队列；一旦空闲队列有新的应用程序提交，借调资源的队列会归还资源，通过这种方式提高了资源的利用率</li>
<li>多重租赁：支持多用户共享集群和多应用程序同时运行，可以对用户，程序添加多重约束，避免产生用户或者程序独占集群资源</li>
<li>安全保证：每个队列有严格的ACL列表限制访问用户，可指用户对应用程序的查看权限，控制权限</li>
<li>动态更新配置文件：管理员可以根据需要动态修改参数配置，实现在线集群管理</li>
</ul>
</li>
</ol>
<h3 id="四、简述一下如何配置一个自定的-队列？"><a href="#四、简述一下如何配置一个自定的-队列？" class="headerlink" title="四、简述一下如何配置一个自定的 队列？"></a>四、简述一下如何配置一个自定的 队列？</h3><ol>
<li>配置多队列</li>
<li>调整队列资源分配</li>
<li>配置新队列属性</li>
<li>刷新队列</li>
</ol>
]]></content>
      <categories>
        <category>Yarn</category>
      </categories>
      <tags>
        <tag>Yarn</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell</title>
    <url>/2021/11/07/Shell/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Shell编程"><a href="#Shell编程" class="headerlink" title="Shell编程"></a>Shell编程</h1><h3 id="初识Shell"><a href="#初识Shell" class="headerlink" title="初识Shell"></a>初识Shell</h3><h4 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h4><p>shell是一个命令行解释器，它接收应用程序/用户命令，然后调用操作系统内核。shell还是一个功能强大的编程语言，易编写、易调试、灵活性强</p>
<p>Linux提供的shell解释器：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ cat /etc/shells </span><br><span class="line">/bin/sh</span><br><span class="line">/bin/bash</span><br><span class="line">/sbin/nologin</span><br><span class="line">/usr/bin/sh</span><br><span class="line">/usr/bin/bash</span><br><span class="line">/usr/sbin/nologin</span><br><span class="line">/bin/tcsh</span><br><span class="line">/bin/csh</span><br></pre></td></tr></table></figure>

<p>Centos默认的解析器是bash</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ echo $SHELL</span><br><span class="line">/bin/bash</span><br></pre></td></tr></table></figure>

<p>bash和sh的关系</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ ll /bin/ | grep bash</span><br><span class="line">-rwxr-xr-x. 1 root root     964544 4月  11 2018 bash</span><br><span class="line">lrwxrwxrwx. 1 root root         10 7月  28 00:28 bashbug -&gt; bashbug-64</span><br><span class="line">-rwxr-xr-x. 1 root root       6964 4月  11 2018 bashbug-64</span><br><span class="line">lrwxrwxrwx. 1 root root          4 7月  28 00:28 sh -&gt; bash</span><br></pre></td></tr></table></figure>

<h4 id="2、入门"><a href="#2、入门" class="headerlink" title="2、入门"></a>2、入门</h4><p>注意：shell脚本后缀为.sh文件，脚本以<code>#!/bin/bash</code>开头，目的是为了指定解析器</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &#x27;hello word!&#x27;</span><br></pre></td></tr></table></figure>

<p>运行脚本的方式：</p>
<ol>
<li><p>第一种：采用bash或sh+脚本的相对路径或绝对路径（不用赋予脚本+x权限）</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> sh ./helloword.sh </span><br><span class="line">hello word!</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> sh /home/atguigu/study/helloword.sh </span><br><span class="line">hello word!</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> bash ./helloword.sh </span><br><span class="line">hello word!</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> bash /home/atguigu/study/helloword.sh </span><br><span class="line">hello word!</span><br></pre></td></tr></table></figure></li>
<li><p>采用输入脚本的绝对路径或相对路径执行脚本（必须具有可执行权限+x）</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> ll</span><br><span class="line">总用量 <span class="number">4</span></span><br><span class="line"><span class="literal">-rw</span><span class="literal">-rw</span><span class="literal">-r</span>--. <span class="number">1</span> atguigu atguigu <span class="number">32</span> <span class="number">8</span>月   <span class="number">6</span> <span class="number">18</span>:<span class="number">29</span> helloword.sh</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> chmod u+x helloword.sh </span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> ll</span><br><span class="line">总用量 <span class="number">4</span></span><br><span class="line"><span class="literal">-rwxrw</span><span class="literal">-r</span>--. <span class="number">1</span> atguigu atguigu <span class="number">32</span> <span class="number">8</span>月   <span class="number">6</span> <span class="number">18</span>:<span class="number">29</span> helloword.sh</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> ./helloword.sh </span><br><span class="line">hello word!</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> /home/atguigu/study/helloword.sh hello word!</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><h4 id="1、系统变量"><a href="#1、系统变量" class="headerlink" title="1、系统变量"></a>1、系统变量</h4><p><code>$PATH</code>、<code>$HOME</code>、<code>$PWD</code>、<code>$SHELL</code>、<code>$USER</code></p>
<p>显示shell中所有变量：set</p>
<p>说明：在终端输入sh进入到shell交互式界面，当然也可以直接在黑屏终端下操作测试，使用exit回退到终端</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ echo $PATH</span><br><span class="line">/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/atguigu/.local/bin:/home/atguigu/bin</span><br><span class="line">[atguigu@centos7_base study]$ sh</span><br><span class="line">sh-4.2$ echo $PATH</span><br><span class="line">/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/atguigu/.local/bin:/home/atguigu/bin</span><br><span class="line">sh-4.2$ exit</span><br><span class="line">exit</span><br><span class="line">[atguigu@centos7_base study]$sh</span><br><span class="line">sh-4.2$ set</span><br></pre></td></tr></table></figure>

<h4 id="2、自定义变量"><a href="#2、自定义变量" class="headerlink" title="2、自定义变量"></a>2、自定义变量</h4><p>定义变量：<code>变量=值</code></p>
<p>撤销变量：<code>unset 变量</code></p>
<p>声明静态变量：<code>readonly 变量</code>，静态变量不能unset</p>
<p>定义规则：</p>
<ol>
<li>变量名称可以由字母、数字和下划线组成，但是不能以数字开头，环境变量名建议大写</li>
<li>等号两侧不能有空格</li>
<li>在bash中，变量默认类型都是字符串类型，无法直接进行数值运算</li>
<li>变量的值如果有空格，需要使用双引号或单引号括起来</li>
</ol>
<p>单引号与双引号的区别：</p>
<ol>
<li>以单引号包围变量的值时，单引号里面是什么就输出什么，即使内容中有变量和命令（命令需要反引起来）也会把它们原样输出。这种方式比较适合定义显示纯字符串的情况，即不希望解析变量、命令等的场景</li>
<li>以双引号包围变量的值时，输出时会先解析里面的变量和命令，而不是把双引号中的变量名和命令原样输出。这种方式比较适合字符串中附带有变量和命令并且想将其解析后再输出的变量定义</li>
</ol>
<p>基本使用：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ sh</span><br><span class="line">sh-4.2$ a = 1</span><br><span class="line">sh: a: 未找到命令</span><br><span class="line">sh-4.2$ a= 1</span><br><span class="line">sh: 1: 未找到命令</span><br><span class="line">sh-4.2$ a =1</span><br><span class="line">sh: a: 未找到命令</span><br><span class="line">sh-4.2$ a=1</span><br><span class="line">sh-4.2$ echo $a</span><br><span class="line">1</span><br><span class="line">sh-4.2$ a=11</span><br><span class="line">sh-4.2$ echo $a</span><br><span class="line">11</span><br><span class="line">sh-4.2$ unset a</span><br><span class="line">sh-4.2$ echo $a</span><br><span class="line"></span><br><span class="line">sh-4.2$ readonly b=2</span><br><span class="line">sh-4.2$ echo $b</span><br><span class="line">2</span><br><span class="line">sh-4.2$ b=22</span><br><span class="line">sh: b: 只读变量</span><br><span class="line">sh-4.2$ unset b</span><br><span class="line">sh: unset: b: 无法反设定: 只读 variable</span><br><span class="line">sh-4.2$ c=1+2</span><br><span class="line">sh-4.2$ echo $c</span><br><span class="line">1+2</span><br><span class="line">sh-4.2$ d=sunck is a good man</span><br><span class="line">sh: is: 未找到命令</span><br><span class="line">sh-4.2$ d=&quot;sunck is a good man&quot;</span><br><span class="line">sh-4.2$ echo $d</span><br><span class="line">sunck is a good man</span><br><span class="line">sh-4.2$ e=&#x27;sunck is a good man&#x27;</span><br><span class="line">sh-4.2$ echo $e</span><br><span class="line">sunck is a good man</span><br><span class="line">sh-4.2$ f=200</span><br><span class="line">sh-4.2$ echo &#x27;变量f的值为：$f&#x27;</span><br><span class="line">变量f的值为：$g</span><br><span class="line">sh-4.2$ echo &quot;变量f的值为：$f&quot;</span><br><span class="line">变量f的值为：200</span><br></pre></td></tr></table></figure>

<p>将变量提升为全局变量：export</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ G=100</span><br><span class="line">sh-4.2$ echo $G</span><br><span class="line">100</span><br><span class="line">sh-4.2$ vim helloword.sh </span><br><span class="line">sh-4.2$ cat helloword.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &#x27;hello word!&#x27;</span><br><span class="line">echo &quot;变量G的值为：$G&quot;</span><br><span class="line">sh-4.2$ ./helloword.sh </span><br><span class="line">hello word!</span><br><span class="line">变量G的值为：</span><br><span class="line">sh-4.2$ export G</span><br><span class="line">sh-4.2$ ./helloword.sh </span><br><span class="line">hello word!</span><br><span class="line">变量G的值为：100</span><br></pre></td></tr></table></figure>

<h4 id="3、特殊变量"><a href="#3、特殊变量" class="headerlink" title="3、特殊变量"></a>3、特殊变量</h4><p><strong>$n</strong></p>
<p>功能描述：n为数字，$0代表该脚本名称，$1-$9代表第一到第九个参数，十以上的参数需要用大括号包含，如${10}</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ vim parameter1.sh</span><br><span class="line">sh-4.2$ cat parameter1.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &quot;命令：$0&quot;</span><br><span class="line">echo &quot;第1个参数：$1&quot;</span><br><span class="line">echo &quot;第2个参数：$2&quot;</span><br><span class="line">echo &quot;第3个参数：$3&quot;</span><br><span class="line">echo &quot;第4个参数：$4&quot;</span><br><span class="line">echo &quot;第5个参数：$5&quot;</span><br><span class="line">echo &quot;第6个参数：$6&quot;</span><br><span class="line">echo &quot;第7个参数：$7&quot;</span><br><span class="line">echo &quot;第8个参数：$8&quot;</span><br><span class="line">echo &quot;第9个参数：$9&quot;</span><br><span class="line">echo &quot;第10个参数：$&#123;10&#125;&quot;</span><br><span class="line">sh-4.2$ sh ./parameter1.sh 1 2 3 4 5 6 7 8 9 a</span><br><span class="line">命令：./parameter1.sh</span><br><span class="line">第1个参数：1</span><br><span class="line">第2个参数：2</span><br><span class="line">第3个参数：3</span><br><span class="line">第4个参数：4</span><br><span class="line">第5个参数：5</span><br><span class="line">第6个参数：6</span><br><span class="line">第7个参数：7</span><br><span class="line">第8个参数：8</span><br><span class="line">第9个参数：9</span><br><span class="line">第10个参数：a</span><br></pre></td></tr></table></figure>

<p><strong>$#</strong></p>
<p>功能描述：获取所有输入参数个数，常用于循环</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ vim parameter2.sh</span><br><span class="line">sh-4.2$ cat parameter2.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &quot;参数个数：$#&quot;</span><br><span class="line">sh-4.2$ sh ./parameter2.sh </span><br><span class="line">参数个数：0</span><br><span class="line">sh-4.2$ sh ./parameter2.sh 1</span><br><span class="line">参数个数：1</span><br><span class="line">sh-4.2$ sh ./parameter2.sh 1 2 3</span><br><span class="line">参数个数：3</span><br></pre></td></tr></table></figure>

<p>**$<em>与$@</em>*</p>
<p>$* 功能描述：这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体</p>
<p>$@ 功能描述：这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待</p>
<p>说明：如果想让$*和$@ 体现区别必须用双引号括起来，并使用循环变量才能看到效果</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ vim parameter3.sh</span><br><span class="line">sh-4.2$ cat parameter3.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo $*</span><br><span class="line">echo $@</span><br><span class="line">sh-4.2$ sh ./parameter3.sh 1 2 3</span><br><span class="line">1 2 3</span><br><span class="line">1 2 3</span><br></pre></td></tr></table></figure>

<p><strong>$?</strong></p>
<p>功能描述：最后一次执行的命令的返回状态。如果这个变量的值为0，证明上一个命令正确执行；如果这个变量的值为非0（具体是哪个数，由命令自己来决定），则证明上一个命令执行不正确了</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ sh ./parameter3.sh 1 2 3</span><br><span class="line">1 2 3</span><br><span class="line">1 2 3</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure>

<h4 id="4、运算符"><a href="#4、运算符" class="headerlink" title="4、运算符"></a>4、运算符</h4><p>基本语法：<code>$((运算式))</code>或<code>$[运算式]</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ h=$(((2+3)*4))</span><br><span class="line">sh-4.2$ i=$[(2+3)*4]</span><br><span class="line">sh-4.2$ echo $h $i</span><br><span class="line">20 20</span><br></pre></td></tr></table></figure>

<h4 id="5、控制台输入"><a href="#5、控制台输入" class="headerlink" title="5、控制台输入"></a>5、控制台输入</h4><p>基本语法：read (选项) (参数)</p>
<table>
<thead>
<tr>
<th>选项值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-p</td>
<td>指定读取值时的提示符</td>
</tr>
<tr>
<td>-t</td>
<td>指定读取值时等待的时间（秒）</td>
</tr>
</tbody></table>
<p>参数：一般为变量，指定读取值的变量名</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ vim read.sh</span><br><span class="line">sh-4.2$ cat read.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -p &quot;请输入一个数字：&quot; num1</span><br><span class="line">echo &quot;你第一次输入的数字为：$num1&quot;</span><br><span class="line"></span><br><span class="line">read -t 5 -p &quot;请在5秒内输入一个数字：&quot; num2</span><br><span class="line">echo &quot;你第二次输入的数字为：$num2&quot;</span><br><span class="line">sh-4.2$ sh ./read.sh </span><br><span class="line">请输入一个数字：1</span><br><span class="line">你第一次输入的数字为：1</span><br><span class="line">请在5秒内输入一个数字：2</span><br><span class="line">你第二次输入的数字为：2</span><br><span class="line">sh-4.2$ sh ./read.sh </span><br><span class="line">请输入一个数字：1</span><br><span class="line">你第一次输入的数字为：1</span><br><span class="line">请在5秒内输入一个数字：你第二次输入的数字为：</span><br><span class="line">sh-4.2$</span><br></pre></td></tr></table></figure>

<h4 id="6、条件判断"><a href="#6、条件判断" class="headerlink" title="6、条件判断"></a>6、条件判断</h4><p><strong>基本语法</strong>：</p>
<ul>
<li><code>test condition</code></li>
<li><code>[ condition ]</code>（注意condition前后要有空格）</li>
</ul>
<p>注意：条件非空即为true，例如[ atguigu ]与[ 0 ]返回true，[] 返回false</p>
<p><strong>常用判断条件</strong>：</p>
<ul>
<li><p>字符串比较是否相等</p>
<p><code>==</code></p>
</li>
<li><p>整数比较</p>
<p><code>-lt</code>：小于（less than）</p>
<p><code>-le</code>：小于等于（less equal）</p>
<p><code>-eq</code>：等于（equal）</p>
<p><code>-ne</code>：不等于（not equal）</p>
<p><code>-gt</code>：大于（greater than）</p>
<p><code>-ge</code>：大于等于（greater equal）</p>
</li>
<li><p>按照文件权限判断</p>
<p><code>-r</code>：有读的权限（read）</p>
<p><code>-w</code>：有写的权限（write）</p>
<p><code>-x</code>：-x 有执行的权限（execute）</p>
</li>
<li><p>按照文件类型判断</p>
<p><code>-f</code>：文件存在并且是一个常规的文件（file）</p>
<p><code>-d</code>：文件存在并是一个目录（directory）</p>
<p><code>-e</code>：文件存在（existence）</p>
</li>
<li><p>与或非</p>
<p><code>-a</code>、<code>-o</code>、<code>!</code>：在中括号内使用</p>
<p><code>&amp;&amp;</code>、<code>||</code>、<code>!</code>：在中括号外使用，计算多个中括号中的条件判断式</p>
</li>
</ul>
<p>实操：</p>
<ol>
<li><p>判断两个字符相等</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ [ &quot;sunck&quot; == &quot;sunck&quot; ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure></li>
<li><p>2是否大于等于1</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ [ 2 -gt 1 ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure></li>
<li><p>helloword.sh是否具有读权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ [ -r ./helloword.sh ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure></li>
<li><p>/home/atguigu/study/a.txt文件是否存在</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ [ -e ./a.txt ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">1</span><br><span class="line">sh-4.2$ [ -e ./helloword.sh ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure></li>
<li><p>helloword.sh是普通文件并且可读</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ [ -f ./helloword.sh -a -r ./helloword.sh ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br><span class="line">sh-4.2$ [ -d ./helloword.sh -a -r ./helloword.sh ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">1</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ [ -d ./helloword.sh ] &amp;&amp; [ -r ./helloword.sh ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">1</span><br><span class="line">sh-4.2$ [ -f ./helloword.sh ] &amp;&amp; [ -r ./helloword.sh ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="选择判断语句"><a href="#选择判断语句" class="headerlink" title="选择判断语句"></a>选择判断语句</h3><h4 id="1、if-语句"><a href="#1、if-语句" class="headerlink" title="1、if 语句"></a>1、if 语句</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if [ 条件判断式 ]</span><br><span class="line">then</span><br><span class="line">	语句</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>逻辑：当程序执行到if语句时，首先计算”条件判断式“，如果”条件判断式“成立，则执行”语句“，否则结束整个if语句继续向下执行</p>
<p>需求：定义一个变量，如果变量的值大于10则输出“sunck is a good man”</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -p &quot;输入一个数字：&quot; num</span><br><span class="line">if [ $num -gt 10 ]</span><br><span class="line">then</span><br><span class="line">	echo &quot;sunck is a good man&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<h4 id="2、if-else语句"><a href="#2、if-else语句" class="headerlink" title="2、if-else语句"></a>2、if-else语句</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if [ 条件判断式 ]</span><br><span class="line">then</span><br><span class="line">	语句1</span><br><span class="line">else</span><br><span class="line">	语句2</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>逻辑：当程序执行到if语句时，首先计算”条件判断式“，如果”条件判断式“成立，则执行”语句1“，否则执行“语句2”</p>
<p>需求：定义一个变量，如果变量的值大于10则输出“sunck is a good man”，否则输出“sunck is a nice man”</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -p &quot;输入一个数字：&quot; num</span><br><span class="line">if [ $num -gt 10 ]</span><br><span class="line">then</span><br><span class="line">	echo &quot;sunck is a good man&quot;</span><br><span class="line">else</span><br><span class="line">	echo &quot;sunck is a nice man&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<h4 id="3、if-elif-else语句"><a href="#3、if-elif-else语句" class="headerlink" title="3、if-elif-else语句"></a>3、if-elif-else语句</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if [ 条件判断式1 ];then</span><br><span class="line">	语句1</span><br><span class="line">elif [ 条件判断式2 ];then</span><br><span class="line">	语句2</span><br><span class="line">elif [ 条件判断式3 ];then</span><br><span class="line">	语句3</span><br><span class="line">……</span><br><span class="line">else</span><br><span class="line">	语句e</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>逻辑：当程序执行到if-elif-else语句时，首先计算“条件判断式1”，如果“条件判断式1”成立则执行“语句1”，否则计算“条件判断式2”。如果“条件判断式2”成立则执行“语句2”，否则计算“条件判断式3”，直到某个“条件判断式”成立为止。如果所有的“条件判断式”都不成立，且有else语句，则执行“语句e”，否则结束整个if-elif-else语句继续向下运行</p>
<p>需求：定义一个age变量，如果age的值小于等于0则输出“age有误”，如果age的值大于0小于等于3则输出“婴儿”，如果age的值大于3小于等于6则输出“幼儿”，如果age的值大于6小于等于12则输出“童年”，如果age的值大于12小于等于18则输出“少年”，如果age的值大于18小于等于30则输出“青年”，如果age的值大于30小于等于40则输出“壮年”，如果age的值大于40小于等于50则输出“中年”，如果age的值大于50小于等于150则输出“老年”，其余则输出“妖怪”</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -p &quot;输入一个数字：&quot; age</span><br><span class="line">if [ $age -le 0 ];then</span><br><span class="line">	echo &quot;age有误！&quot;</span><br><span class="line">elif [ $age -gt 0 ] &amp;&amp; [ $age -le 3 ];then</span><br><span class="line">	echo &quot;婴儿&quot;</span><br><span class="line">elif [ $age -gt 3 ] &amp;&amp; [ $age -le 6 ];then</span><br><span class="line">	echo &quot;幼儿&quot;</span><br><span class="line">elif [ $age -gt 6 ] &amp;&amp; [ $age -le 12 ];then</span><br><span class="line">	echo &quot;童年&quot;</span><br><span class="line">elif [ $age -gt 12 ] &amp;&amp; [ $age -le 18 ];then</span><br><span class="line">	echo &quot;少年&quot;</span><br><span class="line">elif [ $age -gt 18 ] &amp;&amp; [ $age -le 30 ];then</span><br><span class="line">	echo &quot;青年&quot;</span><br><span class="line">elif [ $age -gt 30 ] &amp;&amp; [ $age -le 40 ];then</span><br><span class="line">	echo &quot;壮年&quot;</span><br><span class="line">elif [ $age -gt 40 ] &amp;&amp; [ $age -le 50 ];then</span><br><span class="line">	echo &quot;中年&quot;</span><br><span class="line">elif [ $age -gt 50 ] &amp;&amp; [ $age -le 150 ];then</span><br><span class="line">	echo &quot;老年&quot;</span><br><span class="line">else</span><br><span class="line">	echo &quot;妖怪&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>精髓：每一个else都是对它上面所有表达式的否定</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -p &quot;输入一个数字：&quot; age</span><br><span class="line">if [ $age -le 0 ];then</span><br><span class="line">	echo &quot;age有误！&quot;</span><br><span class="line">elif [ $age -le 3 ];then</span><br><span class="line">	echo &quot;婴儿&quot;</span><br><span class="line">elif [ $age -le 6 ];then</span><br><span class="line">	echo &quot;幼儿&quot;</span><br><span class="line">elif [ $age -le 12 ];then</span><br><span class="line">	echo &quot;童年&quot;</span><br><span class="line">elif [ $age -le 18 ];then</span><br><span class="line">	echo &quot;少年&quot;</span><br><span class="line">elif [ $age -le 30 ];then</span><br><span class="line">	echo &quot;青年&quot;</span><br><span class="line">elif [ $age -le 40 ];then</span><br><span class="line">	echo &quot;壮年&quot;</span><br><span class="line">elif [ $age -le 50 ];then</span><br><span class="line">	echo &quot;中年&quot;</span><br><span class="line">elif [ $age -le 150 ];then</span><br><span class="line">	echo &quot;老年&quot;</span><br><span class="line">else</span><br><span class="line">	echo &quot;妖怪&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<h4 id="4、case语句"><a href="#4、case语句" class="headerlink" title="4、case语句"></a>4、case语句</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">case $变量名 in</span><br><span class="line">&quot;值1&quot;)</span><br><span class="line">	语句1</span><br><span class="line">	;;</span><br><span class="line">&quot;值2&quot;)</span><br><span class="line">	语句2</span><br><span class="line">	;;</span><br><span class="line">……</span><br><span class="line">*)</span><br><span class="line">	语句*</span><br><span class="line">	;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>逻辑：当程序执行到case语句时，匹配“变量”的值，匹配上哪个“值”就执行对应的“语句”，执行完“语句”后结束整个case变量。如果没有匹配的“值”则执行“语句*”</p>
<ol>
<li>case行尾必须为单词“in”，每一个模式匹配必须以右括号“）”结束</li>
<li>双分号“**;;**”表示命令序列结束，相当于java中的break</li>
<li>最后的“*）”表示默认模式，相当于java中的default</li>
</ol>
<p>需求：执行脚本时输入一个数字，输入数字1则打印“星期1”，输入数字2则打印“星期2”，输入数字3则打印“星期3”，输入数字4则打印“星期4”，输入数字5则打印“星期5”，输入数字6则打印“星期6”，输入数字7则打印“星期7”，其他输出“输入有误”</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -p &quot;请输入一个数字：&quot; num</span><br><span class="line"></span><br><span class="line">case $num in</span><br><span class="line">&quot;1&quot;)</span><br><span class="line">	echo &quot;星期1&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;2&quot;)</span><br><span class="line">	echo &quot;星期2&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;3&quot;)</span><br><span class="line">	echo &quot;星期3&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;4&quot;)</span><br><span class="line">	echo &quot;星期4&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;5&quot;)</span><br><span class="line">	echo &quot;星期5&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;6&quot;)</span><br><span class="line">	echo &quot;星期6&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;7&quot;)</span><br><span class="line">	echo &quot;星期7&quot;</span><br><span class="line">	;;</span><br><span class="line">*)</span><br><span class="line">	echo &quot;输入有误&quot;</span><br><span class="line">	;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<h3 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h3><h4 id="1、while循环"><a href="#1、while循环" class="headerlink" title="1、while循环"></a>1、while循环</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">while [ 条件判断式 ]</span><br><span class="line">do</span><br><span class="line">	语句</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>逻辑：当程序执行到while语句时，首先计算“条件判断式”的值，如果“条件判断式”不成立则结束整个while语句继续向下执行，如果“条件判断式”成立则执行“语句”，执行完“语句”再去计算“条件判断式”的值。如果“条件判断式”不成立则结束整个while语句继续向下执行，如果“条件判断式”成立则执行“语句”，执行完“语句”再去计算“条件判断式”的值。如此循环往复，直到“条件判断式”不成立才终止while语句。</p>
<p>需求：计算1加到100的和</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">ret=0</span><br><span class="line">index=1</span><br><span class="line"></span><br><span class="line">while [ $index -le 100 ]</span><br><span class="line">do</span><br><span class="line">	ret=$[$ret+$index]</span><br><span class="line">	index=$[$index+1]</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">echo &quot;1加到100的和为：$ret&quot;</span><br></pre></td></tr></table></figure>

<h4 id="2、for语句"><a href="#2、for语句" class="headerlink" title="2、for语句"></a>2、for语句</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for ((变量初始化;循环控制条件;变量变化))</span><br><span class="line">do</span><br><span class="line">	语句</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>逻辑：当程序执行到for语句时，首先初始化一个“变量”的值，再去计算”循环控制条件”，如果“循环控制条件”不成立则结束整个for语句继续向下运行，如果“循环控制条件”成立则执行“语句”，执行完“语句”在执行“变量变化”修改“变量的值”，再去计算“循环控制条件”。如此循环往复，直到“循环控制条件”不成立才停止整个for语句。</p>
<p>需求：计算1加到100的和</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">ret=0</span><br><span class="line">for ((i=0;i&lt;=100;i++))</span><br><span class="line">do</span><br><span class="line">	ret=$[$ret+$i]</span><br><span class="line">done</span><br><span class="line">echo &quot;1加到100的和为：$ret&quot;</span><br></pre></td></tr></table></figure>

<p>注意：只有在for里可以这么写</p>
<h4 id="3、for-in语句"><a href="#3、for-in语句" class="headerlink" title="3、for-in语句"></a>3、for-in语句</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for 变量 in 计算值1 计算值2 计算值3 ……</span><br><span class="line">do</span><br><span class="line">	语句</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>逻辑：按顺序便利in后面的所有“计算值”</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">a=1</span><br><span class="line">b=2</span><br><span class="line"></span><br><span class="line">for i in &#x27;good&#x27; &#x27;cool&#x27; $a $b $[1+2]</span><br><span class="line">do</span><br><span class="line">	echo &quot;i的值为：$i&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<h4 id="4、-与"><a href="#4、-与" class="headerlink" title="4、$*与$@"></a>4、$*与$@</h4><p>$*和$@都表示传递给函数或脚本的所有参数，不被双引号“”包含时，都以$1 $2 …$n的形式输出所有参数。当它们被双引号“”包含时，“$*”会将所有的参数作为一个整体，以“$1 $2 …$n”的形式输出所有参数；“$@”会将各个参数分开，以“$1” “$2”…”$n”的形式输出所有参数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &#x27;------------$*--------------&#x27;</span><br><span class="line">for i in $*</span><br><span class="line">do</span><br><span class="line">	echo &quot;i的值为：$i&quot;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">echo &#x27;------------$@--------------&#x27;</span><br><span class="line">for j in $@</span><br><span class="line">do</span><br><span class="line">	echo &quot;j的值为：$i&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ sh ./test.sh 1 2 3</span><br><span class="line"><span class="meta">------------$</span><span class="bash">*--------------</span></span><br><span class="line">i的值为：1</span><br><span class="line">i的值为：2</span><br><span class="line">i的值为：3</span><br><span class="line"><span class="meta">------------$</span><span class="bash">@--------------</span></span><br><span class="line">j的值为：3</span><br><span class="line">j的值为：3</span><br><span class="line">j的值为：3</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &#x27;------------$*--------------&#x27;</span><br><span class="line">for i in &quot;$*&quot;</span><br><span class="line">do</span><br><span class="line">	echo &quot;i的值为：$i&quot;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">echo &#x27;------------$@--------------&#x27;</span><br><span class="line">for j in &quot;$@&quot;</span><br><span class="line">do</span><br><span class="line">	echo &quot;j的值为：$i&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ sh ./test.sh 1 2 3</span><br><span class="line"><span class="meta">------------$</span><span class="bash">*--------------</span></span><br><span class="line">i的值为：1 2 3</span><br><span class="line"><span class="meta">------------$</span><span class="bash">@--------------</span></span><br><span class="line">j的值为：1 2 3</span><br><span class="line">j的值为：1 2 3</span><br><span class="line">j的值为：1 2 3</span><br></pre></td></tr></table></figure>

<h4 id="5、break与continue"><a href="#5、break与continue" class="headerlink" title="5、break与continue"></a>5、break与continue</h4><p>break：调出距离最近的那一层循环</p>
<p>continue：跳过距离最近的那一层循环，继续下一次循环继</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">echo &quot;----------------break-------------------&quot;</span><br><span class="line">for ((i=1;i&lt;=10;i++))</span><br><span class="line">do</span><br><span class="line">	if [ $i -eq 5 ]</span><br><span class="line">	then</span><br><span class="line">		break</span><br><span class="line">	fi</span><br><span class="line">	echo &quot;i的值为：$i&quot;</span><br><span class="line">done</span><br><span class="line">echo &quot;----------------continue-------------------&quot;</span><br><span class="line">for ((j=1;j&lt;=10;j++))</span><br><span class="line">do</span><br><span class="line">	if [ $j -eq 5 ]</span><br><span class="line">	then</span><br><span class="line">		continue</span><br><span class="line">	fi</span><br><span class="line">	echo &quot;j的值为：$j&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ sh ./test.sh </span><br><span class="line">----------------break-------------------</span><br><span class="line">i的值为：1</span><br><span class="line">i的值为：2</span><br><span class="line">i的值为：3</span><br><span class="line">i的值为：4</span><br><span class="line">----------------continue-------------------</span><br><span class="line">j的值为：1</span><br><span class="line">j的值为：2</span><br><span class="line">j的值为：3</span><br><span class="line">j的值为：4</span><br><span class="line">j的值为：6</span><br><span class="line">j的值为：7</span><br><span class="line">j的值为：8</span><br><span class="line">j的值为：9</span><br><span class="line">j的值为：10</span><br></pre></td></tr></table></figure>

<h3 id="系统函数"><a href="#系统函数" class="headerlink" title="系统函数"></a>系统函数</h3><h4 id="1、basename"><a href="#1、basename" class="headerlink" title="1、basename"></a>1、basename</h4><p>基本语法：<code>basename [string/pathname] [suffix]</code></p>
<p>功能描述：basename命令会删掉所有的前缀包括最后一个（‘/’）字符，然后将字符串显示出来</p>
<p>选项suffix：如果suffix被指定了，basename会将pathname或string中的suffix去掉</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> basename /a/b/c/d/e.txt</span><br><span class="line">e.txt</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> basename /a/b/c/d/e.txt txt</span><br><span class="line">e.</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> basename /a/b/c/d/e.txt .txt</span><br><span class="line">e</span><br></pre></td></tr></table></figure>

<h4 id="2、dirname"><a href="#2、dirname" class="headerlink" title="2、dirname"></a>2、dirname</h4><p>基本语法：<code>dirname 文件绝对路径</code></p>
<p>功能描述：从给定的包含绝对路径的文件名中去除文件名（非目录的部分），然后返回剩下的路径（目录的部分）</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> dirname /a/b/c/d/e.txt</span><br><span class="line">/a/b/c/d</span><br></pre></td></tr></table></figure>

<h3 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h3><h4 id="1、定义"><a href="#1、定义" class="headerlink" title="1、定义"></a>1、定义</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[function] funname [()]</span><br><span class="line">&#123;</span><br><span class="line">    action;</span><br><span class="line"></span><br><span class="line">    [return int;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>结构</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>function</td>
<td>说明定义函数，可以省略</td>
</tr>
<tr>
<td>funname</td>
<td>自定义函数名，遵循标识符规则</td>
</tr>
<tr>
<td>()</td>
<td>可以省略，和function不能同时省略</td>
</tr>
<tr>
<td>action</td>
<td>封装的功能</td>
</tr>
<tr>
<td>return int</td>
<td>函数返回值，只能通过$?系统变量来获取，可以显示添加。如果不添加，将以最后一条命令运行的结果作为返回值。注意return后的数值在[0~255]之间，如果超过255，将返回该值与256的余数。</td>
</tr>
<tr>
<td>;</td>
<td>语句结束标志，可以不写</td>
</tr>
</tbody></table>
<h4 id="2、调用"><a href="#2、调用" class="headerlink" title="2、调用"></a>2、调用</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">funname [……]</span><br><span class="line">echo &quot;函数的返回值为：$?&quot;</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>结果</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>funname</td>
<td>要调用的函数名</td>
</tr>
<tr>
<td>……</td>
<td>传递参数，如果无需传参即省略</td>
</tr>
<tr>
<td>$?</td>
<td>获取函数的返回值</td>
</tr>
<tr>
<td>“”</td>
<td>需要使用双引号，可以使用特殊变量</td>
</tr>
</tbody></table>
<p>注意：要在定义函数后再调用函数</p>
<h4 id="3、无参无返回值的函数"><a href="#3、无参无返回值的函数" class="headerlink" title="3、无参无返回值的函数"></a>3、无参无返回值的函数</h4><p>需求：打印“hello word”</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 定义函数</span></span><br><span class="line">function func()</span><br><span class="line">&#123;</span><br><span class="line">	echo &quot;hello word&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo &quot;-----函数开始执行------&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 调用函数</span></span><br><span class="line">func</span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取函数的返回值的值</span></span><br><span class="line">echo &quot;返回值为：$?&quot;</span><br><span class="line">echo &quot;-----函数执行完毕------&quot;</span><br></pre></td></tr></table></figure>

<h4 id="4、使用参数"><a href="#4、使用参数" class="headerlink" title="4、使用参数"></a>4、使用参数</h4><p>需求：计算两个数据的和</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">function func()</span><br><span class="line">&#123;</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 使用传递过来的参数值</span></span><br><span class="line">	ret=$[$1+$2]</span><br><span class="line">	echo &quot;$1+$2的结果为：$ret&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo &quot;-----函数开始执行------&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 调用函数，传递参数</span></span><br><span class="line">func 3 5</span><br><span class="line">echo &quot;返回值为：$?&quot;</span><br><span class="line">echo &quot;-----函数执行完毕------&quot;</span><br></pre></td></tr></table></figure>

<h4 id="5、返回值"><a href="#5、返回值" class="headerlink" title="5、返回值"></a>5、返回值</h4><p>说明：return后的数值在[0~255]之间，如果超过255，将返回该值与256的余数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">function func()</span><br><span class="line">&#123;</span><br><span class="line">	ret=$[$1+$2]</span><br><span class="line">	return $ret</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo &quot;-----函数开始执行------&quot;</span><br><span class="line">func 3 5</span><br><span class="line">echo &quot;返回值为：$?&quot;</span><br><span class="line">echo &quot;-----函数执行完毕------&quot;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>需求：在执行脚本时传递两个数字求和</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">function func()</span><br><span class="line">&#123;</span><br><span class="line">	ret=$[$1+$2]</span><br><span class="line">	return $ret</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo &quot;-----函数开始执行------&quot;</span><br><span class="line">func $1 $2</span><br><span class="line">echo &quot;返回值为：$?&quot;</span><br><span class="line">echo &quot;-----函数执行完毕------&quot;</span><br></pre></td></tr></table></figure></li>
<li><p>需求：在执行脚本后从控制台输入两个数字求和</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">function func()</span><br><span class="line">&#123;</span><br><span class="line">	ret=$[$1+$2]</span><br><span class="line">	return $ret</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo &quot;-----函数开始执行------&quot;</span><br><span class="line">read -p &#x27;请输入第一个数字：&#x27; num1</span><br><span class="line">read -p &#x27;请输入第二个数字：&#x27; num2</span><br><span class="line">func $num1 $num2</span><br><span class="line">echo &quot;返回值为：$?&quot;</span><br><span class="line">echo &quot;-----函数执行完毕------&quot;</span><br></pre></td></tr></table></figure></li>
<li><p>返回值超过255，但是就想使用真实结果</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">function func()</span><br><span class="line">&#123;</span><br><span class="line">	ret=$[$1+$2]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo &quot;-----函数开始执行------&quot;</span><br><span class="line">read -p &#x27;请输入第一个数字：&#x27; num1</span><br><span class="line">read -p &#x27;请输入第二个数字：&#x27; num2</span><br><span class="line">func $num1 $num2</span><br><span class="line">echo &quot;结果为：$ret&quot;</span><br><span class="line">echo &quot;-----函数执行完毕------&quot;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h3><h4 id="1、常规匹配"><a href="#1、常规匹配" class="headerlink" title="1、常规匹配"></a>1、常规匹配</h4><p>一串不包含特殊字符的正则表达式匹配它自己</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ cat test.txt </span><br><span class="line">sunck is a good man</span><br><span class="line">kaige is a good man</span><br><span class="line">sunyz is a niee man</span><br><span class="line"></span><br><span class="line">sunck is a nice man!</span><br><span class="line"></span><br><span class="line">kaishen is a cool man!</span><br><span class="line">sunds is a handsome man</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep sunck</span><br><span class="line">sunck is a good man</span><br><span class="line">sunck is a nice man!</span><br></pre></td></tr></table></figure>

<p>查看test.txt文件中包含”sunck”的所有的行，匹配规则就是”sunck”</p>
<p>问题：匹配规则单一</p>
<h4 id="2、匹配单个字符与数字"><a href="#2、匹配单个字符与数字" class="headerlink" title="2、匹配单个字符与数字"></a>2、匹配单个字符与数字</h4><table>
<thead>
<tr>
<th>匹配</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.</td>
<td>匹配任意字符</td>
</tr>
<tr>
<td>[]</td>
<td>里面是字符集合，匹配[]里任意一个字符</td>
</tr>
<tr>
<td>[0123456789]</td>
<td>匹配任意一个数字字符</td>
</tr>
<tr>
<td>[0-9]</td>
<td>匹配任意一个数字字符</td>
</tr>
<tr>
<td>[a-z]</td>
<td>匹配任意一个小写英文字母字符</td>
</tr>
<tr>
<td>[A-Z]</td>
<td>匹配任意一个大写英文字母字符</td>
</tr>
<tr>
<td>[A-Za-z]</td>
<td>匹配任意一个英文字母字符</td>
</tr>
<tr>
<td>[A-Za-z0-9]</td>
<td>匹配任意一个数字或英文字母字符</td>
</tr>
<tr>
<td>[^sunck]</td>
<td>[]里的^称为脱字符，表示非，匹配不在[]内的任意一个字符</td>
</tr>
<tr>
<td>\d</td>
<td>匹配任意一个数字字符，相当于[0-9]</td>
</tr>
<tr>
<td>\D</td>
<td>匹配任意一个非数字字符，相当于<code>[^0-9]</code></td>
</tr>
<tr>
<td>\w</td>
<td>匹配字母、下划线、数字中的任意一个字符，相当于[0-9A-Za-z_]</td>
</tr>
<tr>
<td>\W</td>
<td>匹配非字母、下划线、数字中的任意一个字符，相当于<code>[^0-9A-Za-z_]</code></td>
</tr>
<tr>
<td>\s</td>
<td>匹配空白符(空格、换页、换行、回车、制表)，相当于[ \f\n\r\t]</td>
</tr>
<tr>
<td>\S</td>
<td>匹配非空白符(空格、换页、换行、回车、制表)，相当于<code>[^ \f\n\r\t]</code></td>
</tr>
</tbody></table>
<p>注意：在使用\w、\W、\s、\S时需要使用引号包裹，最好使用单引号</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo &quot;sunck.www.wang.wwwqqww1fww\nffwweeeww3ssssww gege&quot; | grep ww.</span><br><span class="line"></span><br><span class="line">echo &quot;sunck.www.wang.wwwqqww1fww\nffwweeeww3ssssww gege&quot; | grep ww[^0-9]</span><br><span class="line"></span><br><span class="line">echo &quot;sunck.www.wang.wwwqqww1fww\nffwweeeww3ssssww gege&quot; | grep &#x27;ww\w&#x27;</span><br><span class="line"></span><br><span class="line">echo &quot;sunck.www.wang.wwwqqww1fww\nffwweeeww3ssssww gege&quot; | grep &#x27;ww\s&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="3、匹配锚字符"><a href="#3、匹配锚字符" class="headerlink" title="3、匹配锚字符"></a>3、匹配锚字符</h4><table>
<thead>
<tr>
<th>匹配</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>^</td>
<td>行首匹配，和[]里的^不是一个意思</td>
</tr>
<tr>
<td>$</td>
<td>行尾匹配</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ cat test.txt </span><br><span class="line">sunck is a good man</span><br><span class="line">kaige is a good man</span><br><span class="line">sunyz is a niee man</span><br><span class="line"></span><br><span class="line">sunck is a nice man!</span><br><span class="line"></span><br><span class="line">kaishen is a cool man!</span><br><span class="line">sunds is a handsome man</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep ^sun</span><br><span class="line">sunck is a good man</span><br><span class="line">sunyz is a niee man</span><br><span class="line">sunck is a nice man!</span><br><span class="line">sunds is a handsome man</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep ^good</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep man$</span><br><span class="line">sunck is a good man</span><br><span class="line">kaige is a good man</span><br><span class="line">sunyz is a niee man</span><br><span class="line">sunds is a handsome man</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep ^$</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[atguigu@centos7_base study]$</span><br></pre></td></tr></table></figure>

<h4 id="4、匹配边界字符"><a href="#4、匹配边界字符" class="headerlink" title="4、匹配边界字符"></a>4、匹配边界字符</h4><table>
<thead>
<tr>
<th>匹配</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>\b</td>
<td>匹配一个单词的边界，指单词和空格的位置</td>
</tr>
<tr>
<td>\B</td>
<td>匹配非单词边界</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ cat test.txt </span><br><span class="line">sunck is a good man!</span><br><span class="line">ack is a nice ma</span><br><span class="line">sfeck,nckab</span><br><span class="line">aaackge</span><br><span class="line">bbbck geg</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep &#x27;ck\b&#x27;</span><br><span class="line">sunck is a good man!</span><br><span class="line">ack is a nice ma</span><br><span class="line">sfeck,nckab</span><br><span class="line">bbbck geg</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep &#x27;ck\B&#x27;</span><br><span class="line">sfeck,nckab</span><br><span class="line">aaackge</span><br></pre></td></tr></table></figure>

<h4 id="5、匹配多个字符"><a href="#5、匹配多个字符" class="headerlink" title="5、匹配多个字符"></a>5、匹配多个字符</h4><table>
<thead>
<tr>
<th>匹配</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>(xyz)</td>
<td>匹配括号内的xyz，作为一个整体去匹配</td>
</tr>
<tr>
<td>x?</td>
<td>匹配0个或者1个x，非贪婪匹配</td>
</tr>
<tr>
<td>x*</td>
<td>匹配0个或任意多个x</td>
</tr>
<tr>
<td>x+</td>
<td>匹配至少一个x</td>
</tr>
<tr>
<td>x{n}</td>
<td>确定匹配n个x，n是非负数</td>
</tr>
<tr>
<td>x{n,}</td>
<td>至少匹配n个x</td>
</tr>
<tr>
<td>x{n,m}</td>
<td>匹配至少n个最多m个x</td>
</tr>
<tr>
<td>x|y</td>
<td>|表示或的意思，匹配x或y</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo &quot;11111a2222aa3333aaa44444aaaa55555&quot; | grep &#x27;a*&#x27;</span><br><span class="line"></span><br><span class="line">echo &quot;12340567085465046567&quot; | grep &#x27;[0-9]*0&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="6、转义字符"><a href="#6、转义字符" class="headerlink" title="6、转义字符"></a>6、转义字符</h4><p><code>\ </code>表示转义，并不会单独使用。由于所有特殊字符都有其特定匹配模式，当我们想匹配某一特殊字符本身时（例如，我想找出所有包含 ‘$’ 的行），就会碰到困难。此时我们就要将转义字符和特殊字符连用，来表示特殊字符本身</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ cat test.txt </span><br><span class="line">sunck is a go$od man</span><br><span class="line">kaige is a goaod man</span><br><span class="line">sunyz is a nice mano</span><br><span class="line">sunck is a nice man!</span><br><span class="line">kaishen is a co$ol man!</span><br><span class="line">sunds is a handsome man</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep o$</span><br><span class="line">sunyz is a nice mano</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep o\$</span><br><span class="line">sunyz is a nice mano</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep &quot;o\$&quot;</span><br><span class="line">sunyz is a nice mano</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep &#x27;o\$&#x27;</span><br><span class="line">sunck is a go$od man</span><br><span class="line">kaishen is a co$ol man!</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep &#x27;o$&#x27;</span><br><span class="line">sunyz is a nice mano</span><br><span class="line">[atguigu@centos7_base study]$</span><br></pre></td></tr></table></figure>

<h4 id="7、常用正则表达式"><a href="#7、常用正则表达式" class="headerlink" title="7、常用正则表达式"></a>7、常用正则表达式</h4><p>详见《正则匹配示例.docx》</p>
<h3 id="shell工具"><a href="#shell工具" class="headerlink" title="shell工具"></a>shell工具</h3><p>ETL工程师做的一些数据清洗的工作，我们可以使用cut、awk、sort来对一些基本数据进行处理</p>
<h4 id="1、cut"><a href="#1、cut" class="headerlink" title="1、cut"></a>1、cut</h4><p>cut的工作就是“剪”，具体的说就是在文件中负责剪切数据用的。cut 命令从文件的每一行剪切字节、字符和字段并将这些字节、字符和字段输出</p>
<p>格式：cut [选项参数]  filename</p>
<p>说明：默认分隔符是制表符</p>
<table>
<thead>
<tr>
<th>选项参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-f</td>
<td>列号，提取第几列</td>
</tr>
<tr>
<td>-d</td>
<td>分隔符，按照指定分隔符分割列</td>
</tr>
<tr>
<td>-c</td>
<td>指定具体的字符</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">de hua good man</span><br><span class="line">hui mei nice women</span><br><span class="line">xue you cool man</span><br><span class="line">ruo tong good women</span><br><span class="line">dao lang cool man</span><br></pre></td></tr></table></figure>

<ul>
<li><p>切割cut.txt文件第一列</p>
<p><code>cut -d &quot; &quot; -f 1 cut.txt</code></p>
</li>
<li><p>切割cut.txt文件第二、三列</p>
<p><code>cut -d &quot; &quot; -f 2,3 cut.txt</code></p>
</li>
<li><p>切割cut.txt文件第二到四列</p>
<p><code>cut -d &quot; &quot; -f 2,3,4 cut.txt</code></p>
<p><code>cut -d &quot; &quot; -f 2-4 cut.txt</code></p>
</li>
<li><p>切割cut.txt文件第二到最后列</p>
<p><code>cut -d &quot; &quot; -f 2- cut.txt</code></p>
</li>
<li><p>在cut.txt文件中切割出hua</p>
<p><code>cat cut.txt | grep hua | cut -d &quot; &quot; -f 2</code></p>
</li>
<li><p>选取系统PATH变量值，第二个”:”开始后的所有路径</p>
<p><code>echo $PATH | cut -d : -f 3-</code></p>
</li>
<li><p>切割ifconfig后打印的IP地址</p>
<p><code>ifconfig | grep &quot;netmask&quot; | cut -d i -f 2- | cut -d &quot; &quot; -f 2</code></p>
</li>
</ul>
<h4 id="2、awk"><a href="#2、awk" class="headerlink" title="2、awk"></a>2、awk</h4><p>作用：一个强大的文本分析工具，把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行分析处理</p>
<p>语法：awk [选项参数] ‘pattern1{action1} pattern2{action2}…’ filename</p>
<table>
<thead>
<tr>
<th>结构</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>pattern</td>
<td>表示AWK在数据中查找的内容，就是匹配模式</td>
</tr>
<tr>
<td>action</td>
<td>在找到匹配内容时所执行的一系列命令</td>
</tr>
<tr>
<td>filename</td>
<td>文件路径</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>选项参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-F</td>
<td>指定输入文件折分隔符</td>
</tr>
<tr>
<td>-v</td>
<td>赋值一个用户定义变量</td>
</tr>
</tbody></table>
<ul>
<li><p>搜索passwd文件以root关键字开头的所有行，并输出该行的第7列<br><code>awk -F : &#39;/^root/&#123;print $7&#125;&#39; passwd.txt</code></p>
</li>
<li><p>搜索passwd文件以root关键字开头的所有行，并输出该行的第1列和第7列，中间以“，”号分割</p>
<p><code>awk -F : &#39;/^root/&#123;print $1 &quot;,&quot; $7&#125;&#39; passwd.txt</code></p>
</li>
<li><p>只显示/etc/passwd的第一列和第七列，以逗号分割，且在所有行前面添加列名user，shell在最后一行添加”dahaige，/bin/zuishuai”</p>
<p><code>awk -F : &#39;BEGIN&#123;print &quot;user,shell&quot;&#125;&#123;print $1 &quot;,&quot; $7&#125;END&#123;print &quot;sunck,/bin/bash&quot;&#125;&#39; passwd.txt</code></p>
</li>
<li><p>将passwd文件中的用户id增加数值1并输出</p>
<p><code>awk -v i=1 -F : &#39;&#123;print $3+i &quot;,&quot; $4&#125;&#39; passwd.txt</code></p>
</li>
</ul>
<table>
<thead>
<tr>
<th>awk内置变量</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>FILENAME</td>
<td>文件名</td>
</tr>
<tr>
<td>NR</td>
<td>已读的记录数（行数）</td>
</tr>
<tr>
<td>NF</td>
<td>浏览记录的域的个数（切割后，列的个数）</td>
</tr>
</tbody></table>
<ul>
<li><p>统计passwd文件名，每行的行号，每行的列数</p>
<p><code>awk -F : &#39;&#123;print &quot;filename:&quot; FILENAME &quot;,linenumber:&quot; NR &quot;,columns:&quot; NF&#125;&#39; passwd.txt</code></p>
</li>
<li><p>切割IP</p>
<p><code>ifconfig | grep netmask | awk -F &quot;inet &quot; &#39;&#123;print $2&#125;&#39; | awk -F &quot; &quot; &#39;&#123;print $1&#125;&#39;</code></p>
</li>
<li><p>查询cut.txt中空行所在的行号</p>
<p><code>awk &#39;/^$/&#123;print NR&#125;&#39; cut.txt</code></p>
</li>
</ul>
<h4 id="3、sort"><a href="#3、sort" class="headerlink" title="3、sort"></a>3、sort</h4><p>作用：sort命令是在Linux里非常有用，它将文件进行排序，并将排序结果标准输出</p>
<p>格式：sort (选项) (参数)</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-n</td>
<td>依照数值的大小排序</td>
</tr>
<tr>
<td>-r</td>
<td>以相反的顺序来排序</td>
</tr>
<tr>
<td>-t</td>
<td>设置排序时所用的分隔字符</td>
</tr>
<tr>
<td>-k</td>
<td>指定需要排序的列</td>
</tr>
</tbody></table>
<p>参数：指定待排序的文件列表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ cat sort.txt </span><br><span class="line">bb:40:5.4</span><br><span class="line">bd:20:4.2</span><br><span class="line">xz:50:2.3</span><br><span class="line">cls:10:3.5</span><br><span class="line">ss:30:1.6</span><br></pre></td></tr></table></figure>

<p>需求：按照“：”分割后的第三列倒序排序</p>
<p><code>sort -t : -nrk 3 sort.txt</code></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Mysql安装</title>
    <url>/2021/11/07/MySQL/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Mysql安装"><a href="#Mysql安装" class="headerlink" title="Mysql安装"></a>Mysql安装</h1><h2 id="一、卸载Mysql"><a href="#一、卸载Mysql" class="headerlink" title="一、卸载Mysql"></a>一、卸载Mysql</h2><h4 id="1-查询Linux上是否安装Mariadb"><a href="#1-查询Linux上是否安装Mariadb" class="headerlink" title="1. 查询Linux上是否安装Mariadb"></a>1. 查询Linux上是否安装Mariadb</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@mysql ~]# rpm -qa | grep maria</span><br><span class="line">mariadb-libs-5.5.68-1.el7.x86_64</span><br><span class="line">[root@mysql ~]# rpm -e --nodeps mariadb-libs-5.5.68-1.el7.x86_64</span><br><span class="line">[root@mysql ~]# rpm -qa | grep maria</span><br><span class="line">[root@mysql ~]#</span><br></pre></td></tr></table></figure>

<h5 id="2-如果安装过需要卸载Mysql-一步到位"><a href="#2-如果安装过需要卸载Mysql-一步到位" class="headerlink" title="2. 如果安装过需要卸载Mysql 一步到位"></a>2. 如果安装过需要卸载Mysql 一步到位</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo rpm -qa | grep mysql | xargs -n2 sudo rpm -e --nodeps</span><br></pre></td></tr></table></figure>

<h5 id="3-查看-etc-my-cnf-文件，其中-datadir-var-lib-mysql-指向的目录下内容全部删除"><a href="#3-查看-etc-my-cnf-文件，其中-datadir-var-lib-mysql-指向的目录下内容全部删除" class="headerlink" title="3.查看 /etc/my.cnf 文件，其中 datadir=/var/lib/mysql 指向的目录下内容全部删除"></a>3.查看 /etc/my.cnf 文件，其中 datadir=/var/lib/mysql 指向的目录下内容全部删除</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo rm -rf /var/lib/mysql/*</span><br></pre></td></tr></table></figure>

<p>注意：如果首次安装MySQL，此目录下没有内容！而且如果有内容有可能普通用户即便加上sudo 也无法删除，那就切换root用户删除！！！</p>
<h2 id="二、安装MySql"><a href="#二、安装MySql" class="headerlink" title="二、安装MySql"></a>二、安装MySql</h2><h5 id="1-把Mysql安装包上传到Linux指定目录"><a href="#1-把Mysql安装包上传到Linux指定目录" class="headerlink" title="1.把Mysql安装包上传到Linux指定目录"></a>1.把Mysql安装包上传到Linux指定目录</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@mysql software]<span class="comment"># pwd</span></span><br><span class="line">/opt/software</span><br><span class="line">[root@mysql software]<span class="comment"># ls</span></span><br><span class="line">jdk-8u212-linux-x64.tar.gz  mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar</span><br><span class="line">[root@mysql software]<span class="comment">#</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="2-将Mysql安装包进行解包操作"><a href="#2-将Mysql安装包进行解包操作" class="headerlink" title="2. 将Mysql安装包进行解包操作"></a>2. 将Mysql安装包进行解包操作</h5><p>​注意：由于Mysql解包后会有多个文件，为了方便管理最好新建一个文件夹放入</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@mysql software]# mkdir mysql</span><br><span class="line">[root@mysql software]# tar -xvf mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar -C mysql</span><br><span class="line">mysql-community-embedded-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-embedded-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-libs-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-test-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-common-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-embedded-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-client-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-server-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">[root@mysql software]# ll mysql/</span><br><span class="line">total 595272</span><br><span class="line">-rw-r--r--. 1 7155 31415  45109364 Sep 30  2019 mysql-community-client-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415    318768 Sep 30  2019 mysql-community-common-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415   7037096 Sep 30  2019 mysql-community-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415  49329100 Sep 30  2019 mysql-community-embedded-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415  23354908 Sep 30  2019 mysql-community-embedded-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415 136837816 Sep 30  2019 mysql-community-embedded-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415   4374364 Sep 30  2019 mysql-community-libs-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415   1353312 Sep 30  2019 mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415 208694824 Sep 30  2019 mysql-community-server-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415 133129992 Sep 30  2019 mysql-community-test-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">[root@mysql software]#</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>注意：mysql解包后的所有rpm并非都要安装 我们可以把需要安装的通过改名的方式编号，方便后续的顺序安装，严格安装编号的顺序安装</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@mysql mysql]<span class="comment"># ll</span></span><br><span class="line">total 595272</span><br><span class="line">-rw-r--r--. 1 7155 31415    318768 Sep 30  2019 01-mysql-community-common-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415   4374364 Sep 30  2019 02-mysql-community-libs-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415   1353312 Sep 30  2019 03-mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415  45109364 Sep 30  2019 04-mysql-community-client-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415 208694824 Sep 30  2019 05-mysql-community-server-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415   7037096 Sep 30  2019 mysql-community-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415  49329100 Sep 30  2019 mysql-community-embedded-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415  23354908 Sep 30  2019 mysql-community-embedded-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415 136837816 Sep 30  2019 mysql-community-embedded-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415 133129992 Sep 30  2019 mysql-community-test-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">[root@mysql mysql]<span class="comment">#</span></span><br></pre></td></tr></table></figure>

<h5 id="3-按照编号顺序安装"><a href="#3-按照编号顺序安装" class="headerlink" title="3. 按照编号顺序安装"></a>3. 按照编号顺序安装</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@mysql mysql]<span class="comment"># rpm -ivh 0*.rpm</span></span><br><span class="line">warning: 01-mysql-community-common-5.7.28-1.el7.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-common-5.7.28-1.e<span class="comment">################################# [ 20%]</span></span><br><span class="line">   2:mysql-community-libs-5.7.28-1.el7<span class="comment">################################# [ 40%]</span></span><br><span class="line">   3:mysql-community-client-5.7.28-1.e<span class="comment">################################# [ 60%]</span></span><br><span class="line">   4:mysql-community-server-5.7.28-1.e<span class="comment">################################# [ 80%]</span></span><br><span class="line">   5:mysql-community-libs-compat-5.7.2<span class="comment">################################# [100%]</span></span><br><span class="line">[root@mysql mysql]<span class="comment">#</span></span><br></pre></td></tr></table></figure>
<h4 id="4-异常情况"><a href="#4-异常情况" class="headerlink" title="4. 异常情况"></a>4. 异常情况</h4><p>注意：安装第5个包时，如果首次安装会报错，原因是缺少依赖，通过yum安装的方式把依赖安装即可，然后再重新安装第5个包！！！<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20210624154408190.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20210624154408190"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install -y libaio</span><br></pre></td></tr></table></figure>

<h4 id="5-到此Mysql安装就完成了，可以通过一下命令检测是否安装完成"><a href="#5-到此Mysql安装就完成了，可以通过一下命令检测是否安装完成" class="headerlink" title="5. 到此Mysql安装就完成了，可以通过一下命令检测是否安装完成"></a>5. 到此Mysql安装就完成了，可以通过一下命令检测是否安装完成</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@mysql mysql]<span class="comment"># rpm -qa | grep mysql</span></span><br><span class="line">mysql-community-libs-5.7.28-1.el7.x86_64</span><br><span class="line">mysql-community-common-5.7.28-1.el7.x86_64</span><br><span class="line">mysql-community-client-5.7.28-1.el7.x86_64</span><br><span class="line">mysql-community-libs-compat-5.7.28-1.el7.x86_64</span><br><span class="line">mysql-community-server-5.7.28-1.el7.x86_64</span><br><span class="line">[root@mysql mysql]<span class="comment">#</span></span><br></pre></td></tr></table></figure>


<h2 id="三、配置MySql"><a href="#三、配置MySql" class="headerlink" title="三、配置MySql"></a>三、配置MySql</h2><h5 id="1-初始化数据库"><a href="#1-初始化数据库" class="headerlink" title="1. 初始化数据库"></a>1. 初始化数据库</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@mysql mysql]# mysqld --initialize --user=mysql</span><br><span class="line">[root@mysql mysql]#</span><br></pre></td></tr></table></figure>

<h5 id="2-启动Mysql服务"><a href="#2-启动Mysql服务" class="headerlink" title="2.启动Mysql服务"></a>2.启动Mysql服务</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@mysql mysql]# mysqld --initialize --user=mysql</span><br><span class="line">[root@mysql mysql]#</span><br><span class="line">[root@mysql mysql]# systemctl status mysqld</span><br><span class="line">● mysqld.service - MySQL Server</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: man:mysqld(8)</span><br><span class="line">           http://dev.mysql.com/doc/refman/en/using-systemd.html</span><br><span class="line">[root@mysql mysql]# systemctl is-enabled mysqld</span><br><span class="line">enabled</span><br><span class="line">[root@mysql mysql]# systemctl start mysqld</span><br><span class="line">[root@mysql mysql]#</span><br></pre></td></tr></table></figure>


<h5 id="3-查看Mysql的临时密码"><a href="#3-查看Mysql的临时密码" class="headerlink" title="3.查看Mysql的临时密码"></a>3.查看Mysql的临时密码</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@mysql mysql]<span class="comment"># cat /var/log/mysqld.log | grep password</span></span><br><span class="line">2021-10-20T04:26:51.462143Z 1 [Note] A temporary password is generated <span class="keyword">for</span> root@localhost: -NGjuzi5&amp;qlt</span><br><span class="line">[root@mysql mysql]<span class="comment">#</span></span><br></pre></td></tr></table></figure>


<h5 id="4-利用临时密码登录Mysql"><a href="#4-利用临时密码登录Mysql" class="headerlink" title="4. 利用临时密码登录Mysql"></a>4. 利用临时密码登录Mysql</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@mysql mysql]# mysql -uroot -p</span><br><span class="line">Enter password:</span><br><span class="line"><span class="meta">#</span><span class="bash">输入密码-NGjuzi5&amp;qlt</span></span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 2</span><br><span class="line">Server version: 5.7.28</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">mysql&gt;</span></span><br></pre></td></tr></table></figure>


<h5 id="5-修改密码"><a href="#5-修改密码" class="headerlink" title="5.修改密码"></a>5.修改密码</h5><p>注意：首次登录进来以后，必须要先修改指定自定义的密码，不然后续操作都会报错！</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show databases;</span></span><br><span class="line">ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement.</span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> <span class="built_in">set</span> password = password(<span class="string">&quot;123456&quot;</span>);</span></span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.00 sec)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show databases;</span></span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">| mysql              |</span><br><span class="line">| performance_schema |</span><br><span class="line">| sys                |</span><br><span class="line">+--------------------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">mysql&gt;</span></span><br></pre></td></tr></table></figure>

<h6 id="6-修改允许root远程登录"><a href="#6-修改允许root远程登录" class="headerlink" title="6. 修改允许root远程登录"></a>6. 修改允许root远程登录</h6><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; use mysql;</span><br><span class="line">Reading table information <span class="keyword">for</span> completion of table and column names</span><br><span class="line">You can turn off this feature to get a quicker startup with -A</span><br><span class="line"></span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; update mysql.user <span class="built_in">set</span> host=<span class="string">&#x27;%&#x27;</span> <span class="built_in">where</span> user=<span class="string">&#x27;root&#x27;</span>;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">mysql&gt; flush privileges;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL面试题</title>
    <url>/2021/11/07/MySQL%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h1><ol>
<li><p>为什么互联网公司一般选择 MySQL 而不是 Oracle?</p>
<ul>
<li>免费、流行、够用。</li>
</ul>
</li>
<li><p>数据库的三范式是什么？什么是反模式？</p>
<ul>
<li>第一范式，强调属性的原子性约束，要求属性具有原子性，不可再分解。</li>
<li>第二范式，强调记录的唯一性约束，表必须有一个主键，并且没有包含在主键中的列必须完全依赖于主键，而不能只依赖于主键的一部分。</li>
<li>第三范式，强调属性冗余性的约束，即非主键列必须直接依赖于主键。</li>
<li>反1空间换取时间，采取数据冗余的方式避免表之间的关联查询</li>
<li>使存储数据尽可能达到用户一致，保证系统经过一段较短的时间的自我恢复和修正，数据最终达到一致</li>
</ul>
</li>
<li><p>MySQL 有哪些数据类型？</p>
<ul>
<li>数值、日期/时间和字符串(字符)类型</li>
</ul>
</li>
<li><p>MySQL 中 varchar 与 char 的区别？varchar(50) 中的 50 代表的涵义？</p>
<ul>
<li>1、varchar 与 char 的区别，char 是一种固定长度的类型，varchar 则是一种可变长度的类型。</li>
<li>varchar(50) 中 50 的涵义最多存放 50 个字符。varchar(50) 和 (200) 存储 hello 所占空间一样，但后者在排序时会消耗更多内存，因为 ORDER BY col 采用 fixed_length 计算 col 长度(memory引擎也一样)。</li>
</ul>
</li>
<li><p>int(11) 中的 11 代表什么涵义？</p>
<ul>
<li>int(11) 中的 11 ，不影响字段存储的范围，只影响展示效果</li>
</ul>
</li>
<li><p>金额(金钱)相关的数据，选择什么数据类型？</p>
<ul>
<li>方式一，使用 int 或者 bigint 类型。如果需要存储到分的维度，需要 *100 进行放大。</li>
<li>方式二，使用 decimal 类型，避免精度丢失。如果使用 Java 语言时，需要使用 BigDecimal 进行对应。</li>
</ul>
</li>
<li><p>一张表，里面有 ID 自增主键，当 insert 了 17 条记录之后，删除了第 15,16,17 条记录，再把 MySQL 重启，再 insert 一条记录，这条记录的 ID 是 18 还是 15？</p>
<ul>
<li>一般情况下，我们创建的表的类型是 InnoDB ，如果新增一条记录（不重启 MySQL 的情况下），这条记录的 ID 是18 ；但是如果重启 MySQL 的话，这条记录的 ID 是 15 。因为 InnoDB 表只把自增主键的最大 ID 记录到内存中，所以重启数据库或者对表 OPTIMIZE 操作，都会使最大 ID 丢失。</li>
<li>但是，如果我们使用表的类型是 MyISAM ，那么这条记录的 ID 就是 18 。因为 MyISAM 表会把自增主键的最大 ID 记录到数据文件里面，重启 MYSQL 后，自增主键的最大 ID 也不会丢失。</li>
</ul>
</li>
<li><p>表中有大字段 X(例如：text 类型)，且字段 X 不会经常更新，以读为为主，请问您是选择拆成子表，还是继续放一起?写出您这样选择的理由</p>
<ul>
<li>拆带来的问题：连接消耗 + 存储拆分空间。<ul>
<li>如果能容忍拆分带来的空间问题，拆的话最好和经常要查询的表的主键在物理结构上放置在一起(分区) 顺序 IO ，减少连接消耗，最后这是一个文本列再加上一个全文索引来尽量抵消连接消耗。</li>
</ul>
</li>
<li>不拆可能带来的问题：查询性能。<ul>
<li>如果能容忍不拆分带来的查询性能损失的话，上面的方案在某个极致条件下肯定会出现问题，那么不拆就是最好的选择。</li>
</ul>
</li>
</ul>
</li>
<li><p>MySQL 有哪些存储引擎？</p>
<ul>
<li>InnoDB<ul>
<li>InnoDB是一个健壮的<strong>事务型</strong>存储引擎</li>
<li>更新密集的表。InnoDB存储引擎特别适合处理多重并发的更新请求。</li>
<li>事务。InnoDB存储引擎是支持事务的标准MySQL存储引擎。</li>
<li>自动灾难恢复。与其它存储引擎不同，InnoDB表能够自动从灾难中恢复。</li>
<li>外键约束。MySQL支持外键的存储引擎只有InnoDB。</li>
<li>支持自动增加列AUTO_INCREMENT属性。</li>
<li>从5.7开始innodb存储引擎成为默认的存储引擎。</li>
</ul>
</li>
<li>MyISAM<ul>
<li>MyISAM表无法处理事务，这就意味着有事务处理需求的表，不能使用MyISAM存储引擎。MyISAM存储引擎特别适合在以下几种情况下使用：</li>
<li>选择密集型的表。MyISAM存储引擎在筛选大量数据时非常迅速，这是它最突出的优点。</li>
<li>插入密集型的表。MyISAM的并发插入特性允许同时选择和插入数据。例如：MyISAM存储引擎很适合管理邮件或Web服务器日志数据。</li>
</ul>
</li>
</ul>
</li>
<li><p>如何选择合适的存储引擎？</p>
<ul>
<li>是否需要支持事务。</li>
<li>对索引和缓存的支持。</li>
<li>是否需要使用热备。</li>
<li>崩溃恢复，能否接受崩溃。</li>
<li>存储的限制。</li>
<li>MySQL 默认的存储引擎是 InnoDB ，并且也是最主流的选择。主要原因如下:<ul>
<li>【最重要】支持事务。</li>
<li>支持行级锁和表级锁，能支持更多的并发量。</li>
<li>查询不加锁，完全不影响查询。</li>
<li>支持崩溃后恢复。</li>
</ul>
</li>
<li>在 MySQL5.1 以及之前的版本，默认的存储引擎是 MyISAM ，但是目前已经不再更新，且它有几个比较关键的缺点：</li>
<li>不支持事务。</li>
<li>使用表级锁，如果数据量大，一个插入操作锁定表后，其他请求都将阻塞。</li>
</ul>
</li>
<li><p>请说明 InnoDB 和 MyISAM 的区别</p>
<table>
<thead>
<tr>
<th>对比项</th>
<th>InnoDB</th>
<th>MyISAM</th>
</tr>
</thead>
<tbody><tr>
<td>事务</td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td>存储限制</td>
<td>64TB</td>
<td>无</td>
</tr>
<tr>
<td>锁粒度</td>
<td>行锁</td>
<td>表锁</td>
</tr>
<tr>
<td>崩溃后的恢复</td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td>外键</td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td>全文检索</td>
<td>5.7 版本后支持</td>
<td>支持</td>
</tr>
</tbody></table>
</li>
<li><p>为什么 <code>SQL SELECT COUNT(*) FROM table </code> 在 InnoDB 比 MyISAM 慢？</p>
<ul>
<li>对于 SELECT COUNT(*) FROM table 语句，在没有 WHERE 条件的情况下，InnoDB 比 MyISAM 可能会慢很多，尤其在大表的情况下。因为，InnoDB 是去实时统计结果，会全表扫描；而 MyISAM 内部维持了一个计数器，预存了结果，所以直接返回即可。</li>
</ul>
</li>
</ol>
<h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><ol start="13">
<li><p>什么是索引？</p>
<ul>
<li>索引，类似于书籍的目录，想找到一本书的某个特定的主题，需要先找到书的目录，定位对应的页码。</li>
<li>MySQL 中存储引擎使用类似的方式进行查询，先去索引中查找对应的值，然后根据匹配的索引找到对应的数据行。</li>
</ul>
</li>
<li><p>索引有什么好处？</p>
<ul>
<li>提高数据的检索速度，降低数据库IO成本：使用索引的意义就是通过缩小表中需要查询的记录的数目从而加快搜索的速度。</li>
<li>降低数据排序的成本，降低CPU消耗：索引之所以查的快，是因为先将数据排好序，若该字段正好需要排序，则正好降低了排序的成本。</li>
</ul>
</li>
<li><p>索引有什么坏处？</p>
<ul>
<li>占用存储空间：索引实际上也是一张表，记录了主键与索引字段，一般以索引文件的形式存储在磁盘上。</li>
<li>降低更新表的速度：表的数据发生了变化，对应的索引也需要一起变更，从而减低的更新速度。否则索引指向的物理数据可能不对，这也是索引失效的原因之一。 </li>
</ul>
</li>
<li><p>索引的使用场景？</p>
<ul>
<li>对非常小的表，大部分情况下全表扫描效率更高。</li>
<li>对中大型表，索引非常有效。</li>
<li>特大型的表，建立和使用索引的代价随着增长，可以使用分区技术来解决。 </li>
</ul>
</li>
<li><p>索引的类型？</p>
<ul>
<li>索引，都是实现在存储引擎层的。主要有六种类型：<ul>
<li>普通索引：最基本的索引，没有任何约束。</li>
<li>唯一索引：与普通索引类似，但具有唯一性约束。</li>
<li>主键索引：特殊的唯一索引，不允许有空值。</li>
<li>复合索引：将多个列组合在一起创建索引，可以覆盖多个列。</li>
<li>外键索引：只有InnoDB类型的表才可以使用外键索引，保证数据的一致性、完整性和实现级联操作。</li>
<li>全文索引：MySQL 自带的全文索引只能用于 InnoDB、MyISAM ，并且只能对英文进行全文检索，一般使用全文索引引擎。</li>
</ul>
</li>
</ul>
</li>
<li><p>MySQL 索引的“创建”原则？</p>
<ul>
<li>最适合索引的列是出现在 WHERE 子句中的列，或连接子句中的列，而不是出现在 SELECT 关键字后的列。</li>
<li>索引列的基数越大，索引效果越好。</li>
<li>因为复合索引的基数会更大，根据情况创建复合索引可以提高查询效率。</li>
<li>避免创建过多的索引，索引会额外占用磁盘空间，降低写操作效率。</li>
<li>主键尽可能选择较短的数据类型，可以有效减少索引的磁盘占用提高查询效率。</li>
<li>对字符串进行索引，应该定制一个前缀长度，可以节省大量的索引空间。</li>
</ul>
</li>
<li><p>MySQL 索引的“使用”注意事项？</p>
<ul>
<li>应尽量避免在 WHERE 子句中使用 != 或 &lt;&gt; 操作符，否则将引擎放弃使用索引而进行全表扫描。优化器将无法通过索引来确定将要命中的行数,因此需要搜索该表的所有行。</li>
<li>应尽量避免在 WHERE 子句中使用 OR 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如：SELECT id FROM t WHERE num = 10 OR num = 20 。</li>
<li>应尽量避免在 WHERE 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。</li>
<li>应尽量避免在 WHERE 子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描</li>
<li>不要在 WHERE 子句中的 = 左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。</li>
<li>复合索引遵循前缀原则。</li>
<li>如果 MySQL 评估使用索引比全表扫描更慢，会放弃使用索引。如果此时想要索引，可以在语句中添加强制索引。</li>
<li>列类型是字符串类型，查询时一定要给值加引号，否则索引失效。</li>
<li>LIKE 查询，% 不能在前，因为无法使用索引。如果需要模糊匹配，可以使用全文索引。</li>
</ul>
</li>
<li><p>以下三条 SQL 如何建索引，只建一条怎么建？</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">WHERE</span> a <span class="operator">=</span> <span class="number">1</span> <span class="keyword">AND</span> b <span class="operator">=</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">WHERE</span> b <span class="operator">=</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">WHERE</span> b <span class="operator">=</span> <span class="number">1</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="type">time</span> <span class="keyword">DESC</span></span><br></pre></td></tr></table></figure>
<ul>
<li>以顺序 b , a, time 建立复合索引，CREATE INDEX table1_b_a_time ON index_test01(b, a, time)。</li>
<li>对于第一条 SQL ，因为最新 MySQL 版本会优化 WHERE 子句后面的列顺序，以匹配复合索引顺序。</li>
</ul>
</li>
<li><p>想知道一个查询用到了哪个索引，如何查看?</p>
<ul>
<li>EXPLAIN 显示了 MYSQL 如何使用索引来处理 SELECT 语句以及连接表,可以帮助选择更好的索引和写出更优化的查询语句。</li>
<li><code>explain select * from table;</code><ul>
<li>id: id代表执行select子句或操作表的顺序</li>
<li>select_type: 查询的类型,用于区别普通查询,联合查询,子查询等复杂查询<ul>
<li>simple: 简单的select查询,查询中不包含子查询或union查询</li>
<li>primary: 查询中若包含任何复杂的子部分,最外层查询则被标记为primary</li>
<li>subquery 在select 或where 列表中包含了子查询</li>
<li>derived 在from列表中包含的子查询被标记为derived,mysql会递归这些子查询,把结果放在临时表里</li>
<li>union 做第二个select出现在union之后,则被标记为union,若union包含在from子句的子查询中,外层select将被标记为derived</li>
<li>union result 从union表获取结果的select</li>
</ul>
</li>
<li>table: 显示一行的数据时关于哪张表的</li>
<li>type: 查询类型从最好到最差依次是:system&gt;const&gt;eq_ref&gt;ref&gt;range&gt;index&gt;All,一般情况下,得至少保证达到range级别,最好能达到ref<ul>
<li>system:表只有一行记录,这是const类型的特例,平时不会出现</li>
<li>const:表示通过索引一次就找到了,const即常量,它用于比较primary key或unique索引,因为只匹配一行数据,所以效率很快,如将主键置于where条件中,mysql就能将该查询转换为一个常量 </li>
<li>eq_ref:唯一性索引扫描,对于每个索引键,表中只有一条记录与之匹配,常见于主键或唯一索引扫描</li>
<li>ref:非唯一性索引扫描,返回匹配某个单独值的行,它可能会找到多个符合条件的行,所以他应该属于查找和扫描的混合体</li>
<li>range:只检索给定范围的行,使用一个索引来选择行,如where语句中出现了between,&lt;,&gt;,in等查询,这种范围扫描索引比全表扫描要好，因为它只需要开始于索引的某一点，而结束于另一点，不用扫描全部索引。</li>
<li>index:index类型只遍历索引树,这通常比All快,因为索引文件通常比数据文件小,index是从索引中读取,all从硬盘中读取</li>
<li>all:全表扫描,是最差的一种查询类型</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>MySQL 有哪些索引方法？</p>
<ul>
<li>B-Tree 索引。</li>
<li>Hash 索引。</li>
</ul>
</li>
<li><p>什么是 B-Tree 索引？</p>
<ul>
<li>B-Tree 是为磁盘等外存储设备设计的一种平衡查找树。</li>
</ul>
</li>
<li><p>为什么索引结构默认使用B-Tree，而不是hash，二叉树，红黑树？</p>
<ul>
<li>hash：虽然可以快速定位，但是没有顺序，IO复杂度高。</li>
<li>二叉树：树的高度不均匀，不能自平衡，查找效率跟数据有关（树的高度），并且IO代价高。</li>
<li>红黑树：树的高度随着数据量增加而增加，IO代价高。</li>
</ul>
</li>
<li><p>为什么官方建议使用自增长主键作为索引。</p>
<ul>
<li><p>结合B+Tree的特点，自增主键是连续的，在插入过程中尽量减少页分裂，即使要进行页分裂，也只会分裂很少一部分。并且能减少数据的移动，每次插入都是插入到最后。总之就是减少分裂和移动的频率。</p>
</li>
<li><p>插入连续的数据：</p>
<ul>
<li><ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15848449061730.gif?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
</ul>
</li>
<li><p>插入非连续的数据</p>
<ul>
<li><ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15848449240023.gif?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>简述索引定位数据的过程</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15852273948834.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>根节点常驻内存</li>
<li>根据值确定节点位置，二分查找，最终确定叶子节点</li>
</ul>
</li>
<li><p>B-Tree 有哪些索引类型？</p>
<ul>
<li>主键索引的叶子节点存的数据是整行数据( 即具体数据 )。在 InnoDB 里，主键索引也被称为聚集索引（clustered index）。</li>
<li>非主键索引的叶子节点存的数据是整行数据的主键，键值是索引。在 InnoDB 里，非主键索引也被称为辅助索引（secondary index）。</li>
</ul>
</li>
<li><p>聚簇索引的注意点有哪些？</p>
<ul>
<li>聚簇索引表最大限度地提高了 I/O 密集型应用的性能，但它也有以下几个限制：<ul>
<li>插入速度严重依赖于插入顺序，按照主键的顺序插入是最快的方式，否则将会出现页分裂，严重影响性能。因此，对于 InnoDB 表，我们一般都会定义一个自增的 ID 列为主键。</li>
<li>更新主键的代价很高，因为将会导致被更新的行移动。因此，对于InnoDB 表，我们一般定义主键为不可更新。</li>
<li>二级索引访问需要两次索引查找，第一次找到主键值，第二次根据主键值找到行数据。</li>
<li>主键 ID 建议使用整型。因为，每个主键索引的 B+Tree 节点的键值可以存储更多主键 ID ，每个非主键索引的 B+Tree 节点的数据可以存储更多主键 ID 。</li>
</ul>
</li>
</ul>
</li>
<li><p>什么是索引的最左匹配特性？</p>
<ul>
<li>当 B+Tree 的数据项是复合的数据结构，比如索引 (name, age, sex) 的时候，B+Tree 是按照从左到右的顺序来建立搜索树的。<ul>
<li>比如当 (张三, 20, F) 这样的数据来检索的时候，B+Tree 会优先比较 name 来确定下一步的所搜方向，如果 name 相同再依次比较 age 和 sex ，最后得到检索的数据。</li>
<li>但当 (20, F) 这样的没有 name 的数据来的时候，B+Tree 就不知道下一步该查哪个节点，因为建立搜索树的时候 name 就是第一个比较因子，必须要先根据 name 来搜索才能知道下一步去哪里查询。</li>
<li>比如当 (张三, F) 这样的数据来检索时，B+Tree 可以用 name 来指定搜索方向，但下一个字段 age 的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是 F 的数据了。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h3><ol>
<li>事务的特性指的是？<ul>
<li>事务就是对一系列的数据库操作（比如插入多条数据）进行统一的提交或回滚操作，如果插入成功，那么一起成功，如果中间有一条出现异常，那么回滚之前的所有操作。</li>
<li>ACID ，如下图所示：<ul>
<li><ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15848457672689.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
<li>原子性 Atomicity ：一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被恢复（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。</li>
<li>一致性 Consistency ：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器)、级联回滚等。</li>
<li>隔离性 Isolation ：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。</li>
<li>持久性 Durability ：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。</li>
</ul>
</li>
</ul>
</li>
<li>事务的并发问题？<ul>
<li>脏读：事务 A 读取了事务 B 更新的数据，然后 B 回滚操作，那么 A 读取到的数据是脏数据。</li>
<li>不可重复读：事务 A 多次读取同一数据，事务 B 在事务 A 多次读取的过程中，对数据作了更新并提交，导致事务 A 多次读取同一数据时，结果不一致。</li>
<li>幻读：系统管理员 A 将数据库中所有学生的成绩从具体分数改为 ABCDE 等级，但是系统管理员 B 就在这个时候插入了一条具体分数的记录，当系统管理员 A 改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。</li>
</ul>
</li>
<li>MySQL 事务隔离级别会产生的并发问题？<ul>
<li>不同的隔离级别有不同的现象，并有不同的锁定/并发机制，隔离级别越高，数据库的并发性就越差。</li>
<li></li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>面试宝典</tag>
      </tags>
  </entry>
  <entry>
    <title>MapReduce</title>
    <url>/2021/11/07/MapReduce/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h1><h1 id="一、MapReduce概述"><a href="#一、MapReduce概述" class="headerlink" title="一、MapReduce概述"></a>一、MapReduce概述</h1><h2 id="1-1-MapReduce定义"><a href="#1-1-MapReduce定义" class="headerlink" title="1.1 MapReduce定义"></a>1.1 MapReduce定义</h2><ul>
<li>MapReduce是一个<font color ='red' >分布式运算程序的编程框架</font>，是开发“基于Hadoop的数据分析应用”的核心框架。</li>
<li>MapReduce核心功能是将<font color ='red' >用户编写的业务逻辑代码和自带默认组件</font>整合成一个完整的<font color ='red' >分布式运算程序</font>，并<font color ='red' >运行在一个Hadoop集群上</font>。</li>
</ul>
<hr>
<h2 id="1-2-MapReduce优缺点"><a href="#1-2-MapReduce优缺点" class="headerlink" title="1.2 MapReduce优缺点"></a>1.2 MapReduce优缺点</h2><h3 id="1-2-1-优点"><a href="#1-2-1-优点" class="headerlink" title="1.2.1 优点"></a>1.2.1 优点</h3><ol>
<li>MapReduce 易于编程<br> 它简单的<font color ='red' >实现一些接口，就可以完成一个分布式程序</font>，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。</li>
<li>良好的扩展性<br> 当你的计算资源不能得到满足的时候，你可以<font color ='red' >通过简单的增加机器来扩展它的计算能力</font>。</li>
<li>高容错性<br> MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如<font color ='red' >其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败</font>，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的。</li>
<li>适合PB级以上海量数据的离线处理<br> 可以实现上千台服务器集群并发工作，提供数据处理能力。</li>
</ol>
<h3 id="1-2-2-缺点"><a href="#1-2-2-缺点" class="headerlink" title="1.2.2 缺点"></a>1.2.2 缺点</h3><ol>
<li>不擅长实时计算<br> MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果。</li>
<li>不擅长流式计算<br> 流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。</li>
<li>不擅长DAG（有向无环图）计算<br> 多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。</li>
</ol>
<hr>
<h2 id="1-3-MapReduce核心思想"><a href="#1-3-MapReduce核心思想" class="headerlink" title="1.3 MapReduce核心思想"></a>1.3 MapReduce核心思想</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340999252814.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>数据输入：分片， Map阶段根据逻辑分片的理念对要计算的文件进行分片读取（128M）</li>
<li>Map阶段： 将文件进行逻辑划分后，进行分割处理 </li>
<li>Reduce阶段：将Map阶段处理好的数据进行汇总</li>
<li>数据输出：将Reduce阶段的输出结果保存至结果文件</li>
</ul>
<hr>
<h2 id="1-4-MapReduce进程"><a href="#1-4-MapReduce进程" class="headerlink" title="1.4 MapReduce进程"></a>1.4 MapReduce进程</h2><p>一个完整的MapReduce程序在分布式运行时有三类实例进程：</p>
<ol>
<li>MrAppMaster：负责整个程序的过程调度及状态协调。</li>
<li>MapTask：负责Map阶段的整个数据处理流程。</li>
<li>ReduceTask：负责Reduce阶段的整个数据处理流程。</li>
</ol>
<hr>
<h2 id="1-5-MR程序的编程规范"><a href="#1-5-MR程序的编程规范" class="headerlink" title="1.5 MR程序的编程规范"></a>1.5 MR程序的编程规范</h2><ul>
<li>驱动类（负责程序的job提交）</li>
<li>自定义 Mapper类型，并且继承Hadoop提供的Mapper<ul>
<li>重写map方法，在该方法中实现业务逻辑的编写</li>
</ul>
</li>
<li>自定义 Reducer类型，并且继承Hadoop提供的Reducer<ul>
<li>重写reduce方法，在该方法中实现业务逻辑的编写</li>
</ul>
</li>
</ul>
<h1 id="二、Hadoop序列化"><a href="#二、Hadoop序列化" class="headerlink" title="二、Hadoop序列化"></a>二、Hadoop序列化</h1><h2 id="2-1-序列化概述"><a href="#2-1-序列化概述" class="headerlink" title="2.1 序列化概述"></a>2.1 序列化概述</h2><h3 id="2-1-1-什么是序列化"><a href="#2-1-1-什么是序列化" class="headerlink" title="2.1.1 什么是序列化"></a>2.1.1 什么是序列化</h3><ul>
<li>序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。 </li>
<li>反序列化就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象。</li>
</ul>
<h3 id="2-1-2-为什么要序列化"><a href="#2-1-2-为什么要序列化" class="headerlink" title="2.1.2 为什么要序列化"></a>2.1.2 为什么要序列化</h3><ul>
<li>一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。</li>
</ul>
<h3 id="2-1-3-为什么不用Java的序列化"><a href="#2-1-3-为什么不用Java的序列化" class="headerlink" title="2.1.3 为什么不用Java的序列化"></a>2.1.3 为什么不用Java的序列化</h3><ul>
<li>Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。所以，Hadoop自己开发了一套序列化机制（Writable）。</li>
<li>Hadoop序列化特点：<ul>
<li>紧凑 ：高效使用存储空间。</li>
<li>快速：读写数据的额外开销小。</li>
<li>可扩展：随着通信协议的升级而可升级</li>
<li>互操作：支持多语言的交互</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-2-自定义bean对象实现序列化接口（Writable）"><a href="#2-2-自定义bean对象实现序列化接口（Writable）" class="headerlink" title="2.2 自定义bean对象实现序列化接口（Writable）"></a>2.2 自定义bean对象实现序列化接口（Writable）</h2><ul>
<li>在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口。</li>
<li>具体实现bean对象序列化步骤如下7步。</li>
</ul>
<ol>
<li>必须实现Writable接口</li>
<li>反序列化时，需要反射调用空参构造函数，所以必须有空参构造 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="keyword">super</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>重写序列化方法 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">	out.writeLong(upFlow);</span><br><span class="line">	out.writeLong(downFlow);</span><br><span class="line">	out.writeLong(sumFlow);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>重写反序列化方法 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">	upFlow = in.readLong();</span><br><span class="line">	downFlow = in.readLong();</span><br><span class="line">	sumFlow = in.readLong();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>注意反序列化的顺序和序列化的顺序完全一致</li>
<li>要想把结果显示在文件中，需要重写toString()，可用”\t”分开，方便后续用。</li>
<li>如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框中的Shuffle过程要求对key必须能排序。详见后面排序案例。 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line">	<span class="comment">// 倒序排列，从大到小</span></span><br><span class="line">	<span class="keyword">return</span> <span class="keyword">this</span>.sumFlow &gt; o.getSumFlow() ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<hr>
<h1 id="三、MapReduce框架原理"><a href="#三、MapReduce框架原理" class="headerlink" title="三、MapReduce框架原理"></a>三、MapReduce框架原理</h1><h2 id="3-1-MapReduce的工作流程"><a href="#3-1-MapReduce的工作流程" class="headerlink" title="3.1 MapReduce的工作流程"></a>3.1 MapReduce的工作流程</h2><h3 id="3-1-1-MapReduce的数据流"><a href="#3-1-1-MapReduce的数据流" class="headerlink" title="3.1.1 MapReduce的数据流"></a>3.1.1 MapReduce的数据流</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341977072186.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<hr>
<h3 id="3-1-2-MapReduce的执行流程"><a href="#3-1-2-MapReduce的执行流程" class="headerlink" title="3.1.2 MapReduce的执行流程"></a>3.1.2 MapReduce的执行流程</h3><ul>
<li>简易版：InputFormat –&gt; Mapper –&gt; Reduce –&gt; OutputFormat</li>
<li>详细版：InputFormat –&gt; map sort –&gt; copy sort group reduce –&gt; OutputFormat</li>
<li>MR执行的整体大致分为两个阶段<ul>
<li>提交Job<ul>
<li>对当前MR进行初始化工作以及对MT和RT进行规划</li>
</ul>
</li>
<li>执行Job<ul>
<li>执行的就是每一个MT和RT</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="3-2-InputFormat数据输入"><a href="#3-2-InputFormat数据输入" class="headerlink" title="3.2 InputFormat数据输入"></a>3.2 InputFormat数据输入</h2><h3 id="3-3-1-数据切片与MapTask并行度决定机制"><a href="#3-3-1-数据切片与MapTask并行度决定机制" class="headerlink" title="3.3.1 数据切片与MapTask并行度决定机制"></a>3.3.1 数据切片与MapTask并行度决定机制</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341008360509.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>切片的概念：切片是计算数据是从逻辑上文件的进行划分，切块存储数据时从物理上将文件进行切分</li>
<li>一个切片对应一个MapTask来处理</li>
<li>切片大小默认情况等于切块大小128M（这样做的目的是为了计算时读取读取数据效率更高，避免了跨机器读取）</li>
<li>切片的时候不考虑数据整体集，默认情况下对单个文件的进行切片</li>
</ol>
<h3 id="3-3-2-InputFormat类的体系结构"><a href="#3-3-2-InputFormat类的体系结构" class="headerlink" title="3.3.2 InputFormat类的体系结构"></a>3.3.2 InputFormat类的体系结构</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341011423965.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>InputFormat 抽象类的子实现类是 FileInputFormat </li>
<li>FileInputFormat 类的子实现类是 TextInputFormat </li>
<li>InputFormat抽象类中有两个抽象方法 <ul>
<li>getSplits() –&gt; 在 FileInputFormat 做了具体实现<br> 具体的切片的逻辑！！！</li>
<li>createRecordReader() –&gt; TextInputFormat 做了具体实现<br>  创建RecordReader对象， RecordReader最终帮助我们读取待处理的文件的数据，<br>  读取规则就是按行读取由LineRecordReader实现！！！</li>
</ul>
</li>
<li>遇到小文件计算的场景：<br> CombineTextInputFormat –&gt; FileInputFormat的子实现类（用于处理小文件场景）<h4 id="3-3-2-1-分析MapReduce中InputFormat的默认实现"><a href="#3-3-2-1-分析MapReduce中InputFormat的默认实现" class="headerlink" title="3.3.2.1 分析MapReduce中InputFormat的默认实现"></a>3.3.2.1 分析MapReduce中InputFormat的默认实现</h4>InputFormat-&gt;FileInputFormat-&gt;TextInputFormat</li>
<li>定位 驱动类的中的<code>job.waitForCompletion(true);</code></li>
<li>跟进 <code>org.apache.hadoop.mapreduce.Job#waitForCompletion</code></li>
<li>跟进 <code>org.apache.hadoop.mapreduce.Job#submit</code></li>
<li>定位 <code>return submitter.submitJobInternal(Job.this, cluster);</code></li>
<li>跟进 <code>org.apache.hadoop.mapreduce.JobSubmitter#submitJobInternal</code></li>
<li>关注 <code>JobSubmitter.java:200 int maps = writeSplits(job, submitJobDir);</code></li>
<li>跟进 <code>org.apache.hadoop.mapreduce.JobSubmitter#writeSplits</code></li>
<li>跟进 <code>org.apache.hadoop.mapreduce.JobSubmitter#writeNewSplits</code></li>
<li>关注 根据Job中设置的InputFormatClass，然后通过反射的手段获取 InputFormat 的实例  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">InputFormat&lt;?, ?&gt; input = ReflectionUtils.newInstance(job.getInputFormatClass(), conf);</span><br></pre></td></tr></table></figure></li>
<li>跟进 <code>job.getInputFormatClass()</code>  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> Class&lt;? extends InputFormat&lt;?,?&gt;&gt; getInputFormatClass() </span><br><span class="line">    <span class="keyword">throws</span> ClassNotFoundException;</span><br></pre></td></tr></table></figure></li>
<li>定位到 JobContextImpl 实现类  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> Class&lt;? extends InputFormat&lt;?,?&gt;&gt; getInputFormatClass() <span class="keyword">throws</span> ClassNotFoundException &#123;</span><br><span class="line">    <span class="comment">// 先根据 Job中配置信息中的 mapreduce.job.inputformat.class 获取配置，如果获取不到</span></span><br><span class="line">    <span class="comment">// 走默认的 TextInputFormat.class</span></span><br><span class="line">    <span class="keyword">return</span> (Class&lt;? extends InputFormat&lt;?,?&gt;&gt;) </span><br><span class="line">    conf.getClass(INPUT_FORMAT_CLASS_ATTR, TextInputFormat.class);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>核对无默认配置<code>mapreduce.job.inputformat.class</code><a href="media/16340187305977/core-default.xml">hadoop-common-3.1.3.jar!/core-default.xml</a></li>
</ul>
<h4 id="3-3-2-2-分析Hadoop默认的切片规则"><a href="#3-3-2-2-分析Hadoop默认的切片规则" class="headerlink" title="3.3.2.2 分析Hadoop默认的切片规则"></a>3.3.2.2 分析Hadoop默认的切片规则</h4><ul>
<li>定位 <code>org.apache.hadoop.mapreduce.InputFormat#getSplits</code>抽象方法</li>
<li>定位 <code>org.apache.hadoop.mapreduce.lib.input.FileInputFormat#getSplits</code>实现  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Generate the list of files and make them into FileSplits.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> job the job context</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;InputSplit&gt; <span class="title">getSplits</span><span class="params">(JobContext job)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="comment">// 计时器，负责记录当前切片的所花费的时间 最后记录日志中</span></span><br><span class="line">  StopWatch sw = <span class="keyword">new</span> StopWatch().start();</span><br><span class="line">  <span class="comment">// minSize默认情况为1  但是可以通过配置 mapreduce.input.fileinputformat.split.minsize 改变它的值</span></span><br><span class="line">  <span class="keyword">long</span> minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));</span><br><span class="line">  <span class="comment">// maxSize默认情况为Long.MAX_VALUE   但是可以通过配置 mapreduce.input.fileinputformat.split.maxsize 改变它的值</span></span><br><span class="line">  <span class="keyword">long</span> maxSize = getMaxSplitSize(job);</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// generate splits</span></span><br><span class="line">  List&lt;InputSplit&gt; splits = <span class="keyword">new</span> ArrayList&lt;InputSplit&gt;();</span><br><span class="line">  List&lt;FileStatus&gt; files = listStatus(job);</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// ignoreDirs 默认是false</span></span><br><span class="line">  <span class="keyword">boolean</span> ignoreDirs = !getInputDirRecursive(job)</span><br><span class="line">          &amp;&amp; job.getConfiguration().getBoolean(INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS, <span class="keyword">false</span>);</span><br><span class="line">  <span class="keyword">for</span> (FileStatus file : files) &#123;</span><br><span class="line">    <span class="comment">// 默认情况下，对Job中设置的输入路径中的文件以及子目录中的文件全都处理</span></span><br><span class="line">    <span class="comment">// 如果考虑只处理当前设置的路径的子文件，而不管子目录中的文件需要自行定义</span></span><br><span class="line">    <span class="comment">// INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS 为 true</span></span><br><span class="line">    <span class="keyword">if</span> (ignoreDirs &amp;&amp; file.isDirectory()) &#123;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 获取当前文件的大小</span></span><br><span class="line">    Path path = file.getPath();</span><br><span class="line">    <span class="comment">// 对当前文件进行非空判断</span></span><br><span class="line">    <span class="keyword">long</span> length = file.getLen();</span><br><span class="line">    <span class="keyword">if</span> (length != <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// 获取当前文件的对应数据块</span></span><br><span class="line">      BlockLocation[] blkLocations;</span><br><span class="line">      <span class="keyword">if</span> (file <span class="keyword">instanceof</span> LocatedFileStatus) &#123;</span><br><span class="line">        blkLocations = ((LocatedFileStatus) file).getBlockLocations();</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        FileSystem fs = path.getFileSystem(job.getConfiguration());</span><br><span class="line">        blkLocations = fs.getFileBlockLocations(file, <span class="number">0</span>, length);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 判断当前文件是否可以进行切片</span></span><br><span class="line">      <span class="comment">// 主要考虑的是压缩文件这种情况</span></span><br><span class="line">      <span class="keyword">if</span> (isSplitable(job, path)) &#123;</span><br><span class="line">        <span class="comment">// 获取当前文件的块大小</span></span><br><span class="line">        <span class="keyword">long</span> blockSize = file.getBlockSize();</span><br><span class="line">        <span class="comment">// 计算切片大小计算切片逻辑如下</span></span><br><span class="line">        <span class="comment">// 切片大小是否可变？ 可变</span></span><br><span class="line">        <span class="comment">// 如果想调大：改变minSize</span></span><br><span class="line">        <span class="comment">// 如果想调小：改变maxSize</span></span><br><span class="line">        <span class="keyword">long</span> splitSize = computeSplitSize(blockSize, minSize, maxSize);</span><br><span class="line">        <span class="comment">// 当前文件的剩余大小</span></span><br><span class="line">        <span class="keyword">long</span> bytesRemaining = length;</span><br><span class="line">        <span class="comment">// 判断剩余文件是否要继续切分  剩余大小 / 切片大小 是否大于 1.1</span></span><br><span class="line">        <span class="comment">// 目的：就是为了更合理的使用资源计算数据</span></span><br><span class="line">        <span class="keyword">while</span> (((<span class="keyword">double</span>) bytesRemaining) / splitSize &gt; SPLIT_SLOP) &#123;</span><br><span class="line">          <span class="keyword">int</span> blkIndex = getBlockIndex(blkLocations, length - bytesRemaining);</span><br><span class="line">          splits.add(makeSplit(path, length - bytesRemaining, splitSize,</span><br><span class="line">                  blkLocations[blkIndex].getHosts(),</span><br><span class="line">                  blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">          bytesRemaining -= splitSize;</span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> (bytesRemaining != <span class="number">0</span>) &#123;</span><br><span class="line">          <span class="keyword">int</span> blkIndex = getBlockIndex(blkLocations, length - bytesRemaining);</span><br><span class="line">          splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,</span><br><span class="line">                  blkLocations[blkIndex].getHosts(),</span><br><span class="line">                  blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123; <span class="comment">// not splitable</span></span><br><span class="line">        <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">          <span class="comment">// Log only if the file is big enough to be splitted</span></span><br><span class="line">          <span class="keyword">if</span> (length &gt; Math.min(file.getBlockSize(), minSize)) &#123;</span><br><span class="line">            LOG.debug(<span class="string">&quot;File is not splittable so no parallelization &quot;</span></span><br><span class="line">                    + <span class="string">&quot;is possible: &quot;</span> + file.getPath());</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        splits.add(makeSplit(path, <span class="number">0</span>, length, blkLocations[<span class="number">0</span>].getHosts(),</span><br><span class="line">                blkLocations[<span class="number">0</span>].getCachedHosts()));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">//Create empty hosts array for zero length files</span></span><br><span class="line">      splits.add(makeSplit(path, <span class="number">0</span>, length, <span class="keyword">new</span> String[<span class="number">0</span>]));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Save the number of input files for metrics/loadgen</span></span><br><span class="line">  job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());</span><br><span class="line">  sw.stop();</span><br><span class="line">  <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">    LOG.debug(<span class="string">&quot;Total # of splits generated by getSplits: &quot;</span> + splits.size()</span><br><span class="line">            + <span class="string">&quot;, TimeTaken: &quot;</span> + sw.now(TimeUnit.MILLISECONDS));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> splits;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>总结</li>
</ul>
<h4 id="3-3-2-3-CombineTextInputFormat切片机制"><a href="#3-3-2-3-CombineTextInputFormat切片机制" class="headerlink" title="3.3.2.3 CombineTextInputFormat切片机制"></a>3.3.2.3 CombineTextInputFormat切片机制</h4><ol>
<li>框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。</li>
<li>应用场景：CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理。</li>
<li>虚拟存储切片最大值设置CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m<br> 注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</li>
<li>CombineTextInputFormat切片机制<ol>
<li>用户设置切片大小</li>
<li>虚拟过程：根据和切片大小进行比较 <ul>
<li>如果当前文件 &gt; 设置的大小 且 小于2倍的设置的大小就一分为2</li>
<li>如果当前文件大于2倍的设置的大小就先切分出设置大小的块，然后重复步骤2虚拟切分</li>
</ul>
</li>
<li>切片过程：根据虚拟后的结果 把每个虚拟文件和 设置的大小比较<ul>
<li>如果大于等于设置的大小就单独形成一个切片</li>
<li>如果小于设置大小就和下一个虚拟文件进行合并，重复执行</li>
<li>如果合并后大于设置大小就单独形成一个切片</li>
</ul>
</li>
</ol>
</li>
</ol>
<hr>
<h2 id="3-3-Shuffle机制"><a href="#3-3-Shuffle机制" class="headerlink" title="3.3 Shuffle机制"></a>3.3 Shuffle机制</h2><p>Shuffle：Map方法之后，Reduce方法之前的数据处理过程<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341965884201.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="3-3-1-Shuffle机制"><a href="#3-3-1-Shuffle机制" class="headerlink" title="3.3.1 Shuffle机制"></a>3.3.1 Shuffle机制</h3><h4 id="3-3-1-1-Shuffle过程实现的作用"><a href="#3-3-1-1-Shuffle过程实现的作用" class="headerlink" title="3.3.1.1 Shuffle过程实现的作用"></a>3.3.1.1 Shuffle过程实现的作用</h4><ol>
<li>分区：由业务决定<ol>
<li>决定当前的Key交给那个Reduce进行处理</li>
<li>相同的key必须由同一个Reduce进行处理</li>
<li>默认根据Key的Hash值，对Reduce的个数取模</li>
</ol>
</li>
<li>分组：<ol>
<li>将相同Key的value进行合并</li>
<li>相同Key的value分到同一个组</li>
</ol>
</li>
<li>排序<ol>
<li>对key的index进行排序</li>
<li>排序算法为快排，顺序为字典顺序</li>
</ol>
</li>
<li>合并<ol>
<li>相同key溢写的文件合并成一个文件</li>
<li>保证1个MapTask结果输出一个文件</li>
</ol>
</li>
</ol>
<h4 id="3-3-1-2-map端Shuffle"><a href="#3-3-1-2-map端Shuffle" class="headerlink" title="3.3.1.2 map端Shuffle"></a>3.3.1.2 map端Shuffle</h4><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341966144406.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>Partition：将map的结果发送到相应的reduce端，总的partition的数目等于reducer的数量。<ol>
<li>map输出的key-value结果由Partitioner#getPartition方法决定交由那个reducer处理</li>
<li>默认由HashPartitioner实现，对key取hash值后进行取模运算 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">  <span class="comment">/** Use &#123;<span class="doctag">@link</span> Object#hashCode()&#125; to partition. */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value,<span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>将数据写入到内存缓冲区，缓冲区的作用是批量收集map结果，减少磁盘IO的影响。key-value对以及Partition的结果都会被写入缓冲区。在写入之前，key与value值都会被序列化成字节数组</li>
</ol>
</li>
<li>spill: 溢写，sort &amp; combiner<ol>
<li>把内存缓冲区中的数据写入到本地磁盘，在写入本地磁盘时先按照partition、再按照key进行排序（quick sort）</li>
<li>spill是由另外单独的线程来完成，不影响往缓冲区写map结果的线程；</li>
<li>内存缓冲区默认大小限制为100MB，它有个溢写比例（spill.percent），默认为0.8，当缓冲区的数据达到阈值时，溢写线程就会启动，先锁定这80MB的内存，执行溢写过程，maptask的输出结果还可以往剩下的20MB内存中写，互不影响。然后再重新利用这块缓冲区，因此Map的内存缓冲区又叫做环形缓冲区</li>
<li>sort：在将数据写入磁盘之前，先要对要写入磁盘的数据进行一次排序操作，先按&lt;key,value,partition&gt;中的partition分区号排序，然后再按key排序，最后溢出的小文件是分区的，且同一个分区内是保证key有序的；</li>
<li>combine：执行combine操作要求程序中通过job.setCombinerClass(myCombine.class)自定义combine操作<ul>
<li>程序中有两个阶段可能会执行combine操作：<ol>
<li>map输出数据根据分区排序完成后，在写入文件之前会执行一次combine操作（前提是作业中设置了这个操作）；</li>
<li>如果map输出比较大，溢出文件个数大于3（此值可以通过属性min.num.spills.for.combine配置）时，在merge的过程（多个spill文件合并为一个大文件）中还会执行combine操作；</li>
</ol>
</li>
<li>combine主要是把形如&lt;aa,1&gt;,&lt;aa,2&gt;这样的key值相同的数据进行计算，计算规则与reduce一致，比如：当前计算是求key对应的值求和，则combine操作后得到&lt;aa,3&gt;这样的结果。</li>
<li>注意事项：不是每种作业都可以做combine操作的，只有满足以下条件才可以：<ul>
<li>reduce的输入输出类型都一样，因为combine本质上就是reduce操作；</li>
<li>计算逻辑上，combine操作后不会影响计算结果，像求和就不会影响；</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li>merge: 当map很大时，每次溢写会产生一个spill_file，这样会有多个spill_file，而最终的一个map task输出只有一个文件，因此，最终的结果输出之前会对多个中间过程进行多次溢写文件（spill_file）的合并，此过程就是merge过程。也即是，待Map Task任务的所有数据都处理完后，会对任务产生的所有中间数据文件做一次合并操作，以确保一个Map Task最终只生成一个中间数据文件。<ol>
<li>如果生成的文件太多，可能会执行多次合并，每次最多能合并的文件数默认为10，可以通过属性min.num.spills.for.combine配置；</li>
<li>多个溢写文件合并时，会进行一次排序，排序算法是<font color ='red' >多路归并排序</font>；</li>
<li>是否还需要做combine操作，一是看是否设置了combine，二是看溢出的文件数是否大于等于3；</li>
<li>最终生成的文件格式与单个溢出文件一致，也是按分区顺序存储，并且输出文件会有一个对应的索引文件，记录每个分区数据的起始位置，长度以及压缩长度，这个索引文件名叫做file.out.index。</li>
</ol>
</li>
<li>内存缓冲区<ol>
<li>在MapTask任务的业务处理方法map()中，最后一步通过<code>OutputCollector.collect(key,value)</code>或<code>context.write(key,value)</code>输出Map Task的中间处理结果，在<code>collect(key,value)</code>方法中，会调用<code>Partitioner.getPartition(K2 key, V2 value, int numPartitions)</code>方法获得输出的key-value对应的分区号(分区号可以认为对应着一个要执行Reduce Task的节点)，然后将&lt;key,value,partition&gt;暂时保存在内存中的MapOutputBuffe内部的环形数据缓冲区，该缓冲区的默认大小是100MB，可以通过参数io.sort.mb来调整其大小</li>
<li>当缓冲区中的数据使用率达到一定阀值后，触发一次Spill操作，将环形缓冲区中的部分数据写到磁盘上，生成一个临时的本地数据的spill文件；然后在缓冲区的使用率再次达到阀值后，再次生成一个spill文件。直到数据处理完毕，在磁盘上会生成很多的临时文件</li>
<li>触发spill操作时，map输出还会接着往剩下的20%的内存空间写入，但是写满的80%的内存空间会被锁定，数据溢出写入磁盘。当这部分溢出的数据写完后，空出的内存空间可以接着被使用，形成像环一样的被循环使用的效果，所以又叫做环形内存缓冲区</li>
<li>MapOutputBuffe内部存数的数据采用了两个索引结构，涉及三个环形内存缓冲区。两级索引结构如下：<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341959070345.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><ol>
<li>写入到缓冲区的数据会进行压缩，由CompressionCodec提供实现</li>
<li>kvoffsets缓冲区：也叫偏移量索引数组，用于保存key-value信息在位置索引 kvindices 中的偏移量。当 kvoffsets 的使用率超过io.sort.spill.percent (默认为80%)后，便会触发一次 SpillThread 线程的“溢写”操作，也就是开始一次 Spill 阶段的操作。</li>
<li>kvindices缓冲区：也叫位置索引数组，用于保存 key-value 在数据缓冲区 kvbuffer 中的起始位置。</li>
<li>kvbuffer数据缓冲区：用于保存实际的 key-value 的值。默认情况下该缓冲区最多可以使用io.sort.mb的95%，当kvbuffer使用率超过io.sort.spill.percent(默认为80%)后，便会出发一次 SpillThread 线程的“溢写”操作，也就是开始一次 Spill 阶段的操作。</li>
<li>写入到本地磁盘时，对数据进行排序，实际上是对kvoffsets这个偏移量索引数组进行排序。</li>
</ol>
</li>
</ol>
</li>
<li>MapTask结束，通知appmaster,appmaster通过Reduce拉取数据</li>
</ol>
<h4 id="3-3-1-3-reduce端Shuffle"><a href="#3-3-1-3-reduce端Shuffle" class="headerlink" title="3.3.1.3 reduce端Shuffle"></a>3.3.1.3 reduce端Shuffle</h4><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341965290492.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>copy过程<ul>
<li>作用：拉取MapTask处理完成的数据</li>
<li>过程：Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求MapTask所在的TaskTracker获取MapTask的输出文件。因为这时MapTask已经结束，这些文件就由TaskTracker管理在本地磁盘中。</li>
<li>默认情况下，当整个MapReduce作业的所有已执行完成的MapTask任务数超过MapTask总数的5%后，JobTracker便会开始调度执行ReduceTask任务。然后ReduceTask任务默认启动mapred.reduce.parallel.copies(默认为5）个MapOutputCopier线程到已完成的MapTask任务节点上分别copy一份属于自己的数据。 这些copy的数据会首先保存的内存缓冲区中，当内冲缓冲区的使用率达到一定阀值后，触发溢写写到磁盘上。</li>
<li>内存缓冲区<ol>
<li>内存缓冲区大小通过mapred.job.shuffle.input.buffer.percent（default 0.7）参数来设置，控制shuffle在reduce内存中的数据最多使用内存量为：0.7 × maxHeap of ReduceTask。</li>
<li>ReduceTask使用最大heap的一定比例用来缓存数据（最大heap通常通过mapred.child.java.opts来设置，比如设置为-Xmx1024m）。默认情况下，reduce会使用其heapsize的70%来在内存中缓存数据。如果reduce的heap由于业务原因调整的比较大，相应的缓存大小也会变大，这也是为什么reduce用来做缓存的参数是一个百分比，而不是一个固定的值了。</li>
</ol>
</li>
</ul>
</li>
<li>merge过程<ul>
<li>merge 有三种形式：1)内存到内存 2)内存到磁盘 3)磁盘到磁盘。默认情况下第一种形式是不启用的。</li>
<li>当内存中的数据量到达一定阈值，就启动内存到磁盘的 merge（图中的第一个merge，之所以进行merge是因为reduce端在从多个map端copy数据的时候，并没有进行sort，只是把它们加载到内存，当达到阈值写入磁盘时，需要进行merge） 。这和map端的很类似，这实际上就是溢写的过程，在这个过程中如果你设置有Combiner，它也是会启用的，然后在磁盘中生成了众多的溢写文件，这种merge方式一直在运行，直到没有 map 端的数据时才结束，然后才会启动第三种磁盘到磁盘的 merge （图中的第二个merge）方式生成最终的那个文件。</li>
<li>在远程copy数据的同时，ReduceTask在后台启动了两个后台线程对内存和磁盘上的数据文件做合并操作，以防止内存使用过多或磁盘生的文件过多。</li>
</ul>
</li>
<li>reducer的输入文件<ul>
<li>merge的最后会生成一个文件，大多数情况下存在于磁盘中，但是需要将其放入内存中。当reducer 输入文件已定，整个 Shuffle 阶段才算结束。然后就是 Reducer 执行，把结果放到 HDFS 上。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="3-3-2-Partition分区"><a href="#3-3-2-Partition分区" class="headerlink" title="3.3.2 Partition分区"></a>3.3.2 Partition分区</h3><h4 id="3-3-2-1-分区使用场景"><a href="#3-3-2-1-分区使用场景" class="headerlink" title="3.3.2.1 分区使用场景"></a>3.3.2.1 分区使用场景</h4><ul>
<li>要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区）</li>
<li>分区是由需求决定的，而分区编号的产生是由ReducerTask的数量控制的。<h4 id="3-3-2-2-Hadoop默认的分区规则"><a href="#3-3-2-2-Hadoop默认的分区规则" class="headerlink" title="3.3.2.2 Hadoop默认的分区规则"></a>3.3.2.2 Hadoop默认的分区规则</h4></li>
<li>根据key的hashCode对ReduceTasks个数取模得到的。无法控制哪个key存储到哪个分区。</li>
<li>默认分区规则源码分析<ul>
<li>定位Mapper逻辑中的 context.write(outk, outv);</li>
<li>跟进 write(outk, outv);  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(KEYOUT key, VALUEOUT value)</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br></pre></td></tr></table></figure></li>
<li>具体实现 TaskInputOutputContextImpl#write  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(KEYOUT key, VALUEOUT value</span></span></span><br><span class="line"><span class="params"><span class="function">                )</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    output.write(key, value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进 output.write(key, value)  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(K key, V value)</span> <span class="keyword">throws</span> IOException, </span></span><br><span class="line"><span class="function">    InterruptedException</span>;</span><br></pre></td></tr></table></figure></li>
<li>具体实现实现类RecordWriterWithCounter#write  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Object key, Object value)</span> </span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    context.getCounter(COUNTERS_GROUP, counterName).increment(<span class="number">1</span>);</span><br><span class="line">    writer.write(key, value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进 writer.write(key, value);具体实现 NewOutputCollector#write<ul>
<li>collector-&gt;MapOutputCollector ： 此对象就是环形缓冲区对象<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(K key, V value)</span> <span class="keyword">throws</span> IOException,InterruptedException </span>&#123;</span><br><span class="line">    collector.collect(key, value,</span><br><span class="line">        partitioner.getPartition(key, value, partitions));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>跟进 partitioner.getPartition(key, value, partitions)<br>  注意：跟进后发现来到了一个 叫做 Partitioner的抽象类中，如果想知道<br>  Hadoop默认的分区规则，必须得知道 当前Partitioner的默认实现类！<ul>
<li>查找Partitioner的默认实现类<ul>
<li>关注：partitioner赋值发生在NewOutputCollector构造方法中   <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">NewOutputCollector(org.apache.hadoop.mapreduce.JobContext jobContext,</span><br><span class="line">                JobConf job,</span><br><span class="line">                TaskUmbilicalProtocol umbilical,</span><br><span class="line">                TaskReporter reporter</span><br><span class="line">                ) <span class="keyword">throws</span> IOException, ClassNotFoundException &#123;</span><br><span class="line">    <span class="comment">//获取环形缓冲区对象</span></span><br><span class="line">    collector = createSortingCollector(job, reporter);</span><br><span class="line">    <span class="comment">//获取ReduceTask数量作为分区数量</span></span><br><span class="line">    partitions = jobContext.getNumReduceTasks();</span><br><span class="line">    <span class="comment">//分区大于1个</span></span><br><span class="line">    <span class="keyword">if</span> (partitions &gt; <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="comment">//根据 jobContext.getPartitionerClass() 获取Partitioner实现类</span></span><br><span class="line">        partitioner = (org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;)</span><br><span class="line">            ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        partitioner = <span class="keyword">new</span> org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> partitions - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进 JobContext#getPartitionerClass 实现类 JobContextImpl#getPartitionerClass  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">            <span class="comment">// 根据PARTITIONER_CLASS_ATTR枚举值对一个mapreduce.job.partitioner.class配置项</span></span><br><span class="line"><span class="comment">// 获取 Partitioner 的实现类，发现默认没有配置，那就使用后面默认的HashPartitioner.class</span></span><br><span class="line">            <span class="keyword">public</span> Class&lt;? extends Partitioner&lt;?,?&gt;&gt; getPartitionerClass() </span><br><span class="line">                    <span class="keyword">throws</span> ClassNotFoundException &#123;</span><br><span class="line">                <span class="keyword">return</span> (Class&lt;? extends Partitioner&lt;?,?&gt;&gt;) </span><br><span class="line">                    conf.getClass(PARTITIONER_CLASS_ATTR, HashPartitioner.class);</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进 HashPartitioner<ul>
<li>根据以上分析 Partitioner 的实现类是 HashPartitioner.class，以下就是Hadoop的默认分区规则  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/** Use &#123;<span class="doctag">@link</span> Object#hashCode()&#125; to partition. */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value,</span></span></span><br><span class="line"><span class="params"><span class="function">                            <span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>结束：根据当前的key的hashcode值和 ReduceTask的数量取模操作得到当前key的所属分区编号</li>
<li>在MR中使用分区，通常要结合业务去做自定义分区规则！</li>
</ul>
</li>
</ul>
<h4 id="3-3-2-3-自定义Partitioner步骤"><a href="#3-3-2-3-自定义Partitioner步骤" class="headerlink" title="3.3.2.3 自定义Partitioner步骤"></a>3.3.2.3 自定义Partitioner步骤</h4><ol>
<li>自定义类继承Partitioner，重写getPartition()方法 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MobileModPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowDTO</span>&gt;</span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, FlowDTO flowDTO, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> mobileNum = Long.parseLong(flowDTO.getMobile());</span><br><span class="line">        <span class="keyword">int</span> partitionNum = (<span class="keyword">int</span>) (mobileNum % numPartitions);</span><br><span class="line">        log.info(<span class="string">&quot;执行分区操作 key:&#123;&#125; partition;&#123;&#125;&quot;</span>, mobileNum, partitionNum);</span><br><span class="line">        <span class="keyword">return</span> partitionNum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>在Job驱动中，设置自定义Partitioner <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">job.setPartitionerClass(CustomPartitioner.class);</span><br></pre></td></tr></table></figure></li>
<li>自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure></li>
<li>分区器使用时注意事项<ul>
<li>当ReduceTask的数量设置 &gt; 实际用到的分区数 此时会生成空的分区文件</li>
<li>当ReduceTask的数量设置 &lt; 实际用到的分区数 此时会报错</li>
<li>当ReduceTask的数量设置 = 1 结果文件会输出到一个文件中，由以下源码可以论证：<ul>
<li>位置 NewOutputCollector#NewOutputCollector <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">NewOutputCollector(org.apache.hadoop.mapreduce.JobContext jobContext,</span><br><span class="line">                    JobConf job,</span><br><span class="line">                    TaskUmbilicalProtocol umbilical,</span><br><span class="line">                    TaskReporter reporter</span><br><span class="line">                    ) <span class="keyword">throws</span> IOException, ClassNotFoundException &#123;</span><br><span class="line">    collector = createSortingCollector(job, reporter);</span><br><span class="line">    <span class="comment">// 获取当前ReduceTask的数量</span></span><br><span class="line">    partitions = jobContext.getNumReduceTasks();</span><br><span class="line">    <span class="comment">// 判断ReduceTask的数量 是否大于1，找指定分区器对象</span></span><br><span class="line">    <span class="keyword">if</span> (partitions &gt; <span class="number">1</span>) &#123;</span><br><span class="line">    partitioner = (org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;)</span><br><span class="line">        ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 执行默认的分区规则，最终返回一个唯一的0号分区</span></span><br><span class="line">        partitioner = <span class="keyword">new</span> org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> partitions - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>分区编号生成的规则：根据指定的ReduceTask的数量 从0开始，依次累加。</li>
</ul>
</li>
</ol>
<h3 id="3-3-3-WritableComparable排序"><a href="#3-3-3-WritableComparable排序" class="headerlink" title="3.3.3 WritableComparable排序"></a>3.3.3 WritableComparable排序</h3><h4 id="3-3-3-1-排序概述"><a href="#3-3-3-1-排序概述" class="headerlink" title="3.3.3.1 排序概述"></a>3.3.3.1 排序概述</h4><ul>
<li>MapTask和ReduceTask均会对数据<font color ='red' >按照key进行排序</font>。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。</li>
<li><font color ='red' >默认排序是按照字典顺序升序排序，且实现该排序的方法是快速排序</font>。</li>
<li>对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序</li>
<li>对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序</li>
</ul>
<h4 id="3-3-3-2-排序分类"><a href="#3-3-3-2-排序分类" class="headerlink" title="3.3.3.2 排序分类"></a>3.3.3.2 排序分类</h4><ol>
<li>部分排序：分区内排序<ul>
<li>MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序</li>
</ul>
</li>
<li>全排序<ul>
<li>最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构</li>
</ul>
</li>
<li>辅助排序：GroupingComparator分组<ul>
<li>在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部字段比较不相同）的key进入到同一个reduce方法时，可以采用分组排序。</li>
</ul>
</li>
<li>二次排序<ul>
<li>在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序</li>
</ul>
</li>
</ol>
<h4 id="3-3-3-3-实现排序比较的方式"><a href="#3-3-3-3-实现排序比较的方式" class="headerlink" title="3.3.3.3 实现排序比较的方式"></a>3.3.3.3 实现排序比较的方式</h4><ol>
<li>直接让参与比较的对象实现WritableComparable 接口，并在该类中实现compareTo，在compareTo中定义自己的比较规则。这种情况 当运行的的时候Hadoop会自动生成比较器对象WritableComparator <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CompareFlowDTO</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">CompareFlowDTO</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String mobile;</span><br><span class="line">    <span class="keyword">private</span> Long upFlow;</span><br><span class="line">    <span class="keyword">private</span> Long downFlow;</span><br><span class="line">    <span class="keyword">private</span> Long sumFlow;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        out.writeUTF(mobile);</span><br><span class="line">        out.writeLong(upFlow);</span><br><span class="line">        out.writeLong(downFlow);</span><br><span class="line">        out.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        mobile = in.readUTF();</span><br><span class="line">        upFlow = in.readLong();</span><br><span class="line">        downFlow = in.readLong();</span><br><span class="line">        sumFlow = in.readLong();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(CompareFlowDTO o)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> o.getSumFlow().compareTo(getSumFlow());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">&quot;\t&quot;</span> + downFlow + <span class="string">&quot;\t&quot;</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>继承Hadoop提供的WritableComparator类，重写该类compare() 方法，在该方法中定义比较规则，注意在自定义的比较器对象中通过调用父类的super方法将自定义的比较器对象和要参与比较的对象进行关联。最后再Driver类中指定自定义的比较器对象。 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CompareFlowDTOComparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">CompareFlowDTOComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(CompareFlowDTO.class, <span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line">        CompareFlowDTO aCompareFlowDTO = (CompareFlowDTO) a;</span><br><span class="line">        CompareFlowDTO bCompareFlowDTO = (CompareFlowDTO) b;</span><br><span class="line">        <span class="keyword">return</span> aCompareFlowDTO.getSumFlow().compareTo(bCompareFlowDTO.getSumFlow());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>继承Hadoop提供的WritableComparator类，重写该类compare() 方法，在该方法中定义比较规则，注意在自定义的比较器对象中通过调用父类的super方法将自定义的比较器对象和要参与比较的对象进行关联。在比较对象的类定义中添加静态代码块，主动注册比较器 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//主动注册</span></span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">    WritableComparator.define(CompareFlowDTO.class, <span class="keyword">new</span> CompareFlowDTOComparator());</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="3-3-3-4-Hadoop中获取比较器对象的规则源码分析"><a href="#3-3-3-4-Hadoop中获取比较器对象的规则源码分析" class="headerlink" title="3.3.3.4 Hadoop中获取比较器对象的规则源码分析"></a>3.3.3.4 Hadoop中获取比较器对象的规则源码分析</h4><ul>
<li>入口 <code>org.apache.hadoop.mapred.MapTask.MapOutputBuffer#init</code></li>
<li>定位MapTask.java:1018  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">comparator = job.getOutputKeyComparator();</span><br></pre></td></tr></table></figure></li>
<li>跟进 org.apache.hadoop.mapred.JobConf#getOutputKeyComparator  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> RawComparator <span class="title">getOutputKeyComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 配置文件查找对应配置JobContext.KEY_COMPARATOR-&gt;mapreduce.job.output.key.comparator.class </span></span><br><span class="line">  <span class="comment">// 如果配置存在且实现了Comparator接口，返回配置的比较器</span></span><br><span class="line">  <span class="comment">// 配置存在但是没有实现Comparator接口，抛出异常</span></span><br><span class="line">  <span class="comment">// 配置不存在，取默认值null</span></span><br><span class="line">  Class&lt;? extends RawComparator&gt; theClass = getClass(</span><br><span class="line">    JobContext.KEY_COMPARATOR, <span class="keyword">null</span>, RawComparator.class);</span><br><span class="line">  <span class="keyword">if</span> (theClass != <span class="keyword">null</span>)</span><br><span class="line">      <span class="comment">// 如果通过JobContext.KEY_COMPARATOR 获取到了 直接通过反射的形式实例化对象</span></span><br><span class="line">      <span class="keyword">return</span> ReflectionUtils.newInstance(theClass, <span class="keyword">this</span>);</span><br><span class="line">  <span class="comment">// 如果 JobContext.KEY_COMPARATOR 没获取到，就走一下流程获取参与排序的对象的比较器对象</span></span><br><span class="line">  <span class="comment">// 首先会检验 当前Map端输出的key是否实现WritableComparable接口</span></span><br><span class="line">  <span class="keyword">return</span> WritableComparator.get(getMapOutputKeyClass().asSubclass(WritableComparable.class), <span class="keyword">this</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进WritableComparator#get  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> WritableComparator <span class="title">get</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      Class&lt;? extends WritableComparable&gt; c, Configuration conf)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 通过当前比较的对象的class类对象到comparators这个Map容器中获取比较器对象</span></span><br><span class="line">    <span class="comment">// 凡是在comparators能获取到的比较器对象，那当前参与比较的对象一定Hadoop自身的数据类型。</span></span><br><span class="line">    WritableComparator comparator = comparators.get(c);</span><br><span class="line">    <span class="keyword">if</span> (comparator == <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="comment">// 考虑到加载的类可能遇到内存的一些错误，导致GC,所以再强制加载一次 已过时</span></span><br><span class="line">      forceInit(c);</span><br><span class="line">      <span class="comment">// 强制加载后再获取</span></span><br><span class="line">      comparator = comparators.get(c);</span><br><span class="line">      <span class="comment">// 如果还没有获取到，那当前参与比较的对象就不是Hadoop自身的数据类型</span></span><br><span class="line">      <span class="keyword">if</span> (comparator == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">//如果到这还没获取到，那就是我们自定义的数据类型，此时Hadoop创建一个比较器</span></span><br><span class="line">        comparator = <span class="keyword">new</span> WritableComparator(c, conf, <span class="keyword">true</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Newly passed Configuration objects should be used.</span></span><br><span class="line">    ReflectionUtils.setConf(comparator, conf);</span><br><span class="line">    <span class="keyword">return</span> comparator;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-3-3-5-Hadoop中获取自身数据类型的比较器对象源码分析"><a href="#3-3-3-5-Hadoop中获取自身数据类型的比较器对象源码分析" class="headerlink" title="3.3.3.5 Hadoop中获取自身数据类型的比较器对象源码分析"></a>3.3.3.5 Hadoop中获取自身数据类型的比较器对象源码分析</h4></li>
<li>以org.apache.hadoop.io.Text为例</li>
<li>实现了org.apache.hadoop.io.WritableComparable接口</li>
<li>在Text本类中已经声明了比较器对象 并且做了关联  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Comparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Comparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    #关联比较器和比较对象</span><br><span class="line">    <span class="keyword">super</span>(Text.class);</span><br><span class="line">  &#125;</span><br><span class="line">  #具体比较实现</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(<span class="keyword">byte</span>[] b1, <span class="keyword">int</span> s1, <span class="keyword">int</span> l1,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">byte</span>[] b2, <span class="keyword">int</span> s2, <span class="keyword">int</span> l2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n1 = WritableUtils.decodeVIntSize(b1[s1]);</span><br><span class="line">    <span class="keyword">int</span> n2 = WritableUtils.decodeVIntSize(b2[s2]);</span><br><span class="line">    <span class="keyword">return</span> compareBytes(b1, s1+n1, l1-n1, b2, s2+n2, l2-n2);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>注册Comparator  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">  <span class="comment">// register this comparator</span></span><br><span class="line">  WritableComparator.define(Text.class, <span class="keyword">new</span> Comparator());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进 define()方法   <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">define</span><span class="params">(Class c, WritableComparator comparator)</span> </span>&#123;</span><br><span class="line">  comparators.put(c, comparator);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>破案：当Text类加载的时候，会将当前Text.class 做为key  它的比较器对象作为value会放入comparators Map容器中。</li>
</ul>
<hr>
<h3 id="3-3-4-Combiner流程"><a href="#3-3-4-Combiner流程" class="headerlink" title="3.3.4 Combiner流程"></a>3.3.4 Combiner流程</h3><ol>
<li>Combiner组件的父类就是Reducer。</li>
<li>Combiner和Reducer的区别在于运行的位置<ol>
<li>Combiner是在每一个MapTask所在的节点运行</li>
<li>Reducer是接收全局所有Mapper的输出结果</li>
</ol>
</li>
<li>Combiner的使用场景：总的来说，为了提升MR程序的运行效率，为了减轻ReduceTask的压力，另外减少IO的开销。</li>
<li>使用Combiner<ol>
<li>自定一个Combiner类 继承Hadoop提供的Reducer</li>
<li>在Job中指定自定义的Combiner类</li>
<li>Combiner的输出kv应该跟Reducer的输入kv类型要对应起来 </li>
</ol>
</li>
<li>Combiner能够应用的前提是不能影响最终的业务逻辑</li>
<li>Combiner不适用的场景：Reduce端处理的数据考虑到多个MapTask的数据的整体集时就不能提前合并了。</li>
<li>示例 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> IntWritable valueOut = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String word = key.toString();</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (values.iterator().hasNext()) &#123;</span><br><span class="line">            sum += values.iterator().next().get();</span><br><span class="line">        &#125;</span><br><span class="line">        valueOut.set(sum);</span><br><span class="line">        log.info(<span class="string">&quot;combine-word: &#123;&#125; 累计出现次数:&#123;&#125;&quot;</span>, word, sum);</span><br><span class="line">        context.write(key, valueOut);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h2 id="3-5-OutputFormat"><a href="#3-5-OutputFormat" class="headerlink" title="3.5 OutputFormat"></a>3.5 OutputFormat</h2><p>OutputFormat主要负责最终数据的写出</p>
<h3 id="3-5-1-OutputFormat实现类"><a href="#3-5-1-OutputFormat实现类" class="headerlink" title="3.5.1 OutputFormat实现类"></a>3.5.1 OutputFormat实现类</h3><ol>
<li>探索OutputFormat的默认实现<ul>
<li>OutputFormat的实现的功能中有一个检验输出路径的方法org.apache.hadoop.mapreduce.OutputFormat#checkOutputSpecs</li>
<li>考虑到检验输出路径应该在Job提交流程中完成(不设置会报错)  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   org.apache.hadoop.mapred.InvalidJobConfException: Output directory not set.</span><br><span class="line">at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:<span class="number">156</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:<span class="number">277</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:<span class="number">143</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at org.apache.hadoop.mapreduce.Job$<span class="number">11.</span>run(Job.java:<span class="number">1570</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at org.apache.hadoop.mapreduce.Job$<span class="number">11.</span>run(Job.java:<span class="number">1567</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at java.security.AccessController.doPrivileged(Native Method) ~[?:<span class="number">1.8</span><span class="number">.0_291</span>]</span><br><span class="line">at javax.security.auth.Subject.doAs(Subject.java:<span class="number">422</span>) ~[?:<span class="number">1.8</span><span class="number">.0_291</span>]</span><br><span class="line">at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:<span class="number">1729</span>) ~[hadoop-common-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at org.apache.hadoop.mapreduce.Job.submit(Job.java:<span class="number">1567</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:<span class="number">1588</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at sgg.hadoop.mapreduce.combiner.WordCountCombinerDriver.main(WordCountCombinerDriver.java:<span class="number">53</span>) [classes/:?]</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>跟进Job提交流程org.apache.hadoop.mapreduce.Job#waitForCompletion</li>
<li>跟进org.apache.hadoop.mapreduce.Job#submit</li>
<li>跟进org.apache.hadoop.mapreduce.JobSubmitter#submitJobInternal</li>
<li>跟进org.apache.hadoop.mapreduce.JobSubmitter#checkSpecs  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">checkSpecs</span><span class="params">(Job job)</span> <span class="keyword">throws</span> ClassNotFoundException, </span></span><br><span class="line"><span class="function">    InterruptedException, IOException </span>&#123;</span><br><span class="line">      JobConf jConf = (JobConf)job.getConfiguration();</span><br><span class="line">      <span class="comment">// Check the output specification</span></span><br><span class="line">      <span class="keyword">if</span> (jConf.getNumReduceTasks() == <span class="number">0</span> ? </span><br><span class="line">          jConf.getUseNewMapper() : jConf.getUseNewReducer()) &#123;</span><br><span class="line">          <span class="comment">//获取OutputFormat</span></span><br><span class="line">        org.apache.hadoop.mapreduce.OutputFormat&lt;?, ?&gt; output =</span><br><span class="line">          ReflectionUtils.newInstance(job.getOutputFormatClass(),</span><br><span class="line">            job.getConfiguration());</span><br><span class="line">        output.checkOutputSpecs(job);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        jConf.getOutputFormat().checkOutputSpecs(jtFs, jConf);</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进org.apache.hadoop.mapreduce.task.JobContextImpl#getOutputFormatClass  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> Class&lt;? extends OutputFormat&lt;?,?&gt;&gt; getOutputFormatClass() </span><br><span class="line">     <span class="keyword">throws</span> ClassNotFoundException &#123;</span><br><span class="line">    <span class="keyword">return</span> (Class&lt;? extends OutputFormat&lt;?,?&gt;&gt;) </span><br><span class="line">      conf.getClass(OUTPUT_FORMAT_CLASS_ATTR, TextOutputFormat.class);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>破案：OutputFormat默认实现就是TextOutputFormat</li>
</ul>
</li>
<li>OutputFormat 类的体系结构<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16342996424249.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><ul>
<li>FileOutputFormat 是 OutputFormat的子类（实现类）<ul>
<li>对 checkOutputSpecs() 做了具体的实现</li>
</ul>
</li>
<li>TextOutputFormat 是 FileOutputFormat的子类<ul>
<li>对 getRecordWriter 做了具体实现</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="3-5-2-OutputFormat的使用场景"><a href="#3-5-2-OutputFormat的使用场景" class="headerlink" title="3.5.2 OutputFormat的使用场景"></a>3.5.2 OutputFormat的使用场景</h3><ul>
<li>当我们对MR最终的结果有个性化制定的需求，就可以通过自定义OutputFormat来实现</li>
</ul>
<h3 id="3-5-3-自定义OutputFormat"><a href="#3-5-3-自定义OutputFormat" class="headerlink" title="3.5.3 自定义OutputFormat"></a>3.5.3 自定义OutputFormat</h3><ul>
<li>自定一个 OutputFormat 类，继承Hadoop提供的OutputFormat，在该类中实现getRecordWriter() ,返回一个RecordWriter</li>
<li>自定义一个 RecordWriter 并且继承Hadoop提供的RecordWriter类，在该类中重写 write()  和 close()  在这些方法中完成自定义输出。</li>
<li>示例  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountOutputFormat</span> <span class="keyword">extends</span> <span class="title">FileOutputFormat</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RecordWriter&lt;Text, IntWritable&gt; <span class="title">getRecordWriter</span><span class="params">(TaskAttemptContext job)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> WordCountRecordWriter(job);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountRecordWriter</span> <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> FileSystem fileSystem;</span><br><span class="line">    HashMap&lt;Integer, FSDataOutputStream&gt; fsDataOutputStreamHashMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">WordCountRecordWriter</span><span class="params">(TaskAttemptContext job)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            fileSystem = FileSystem.get(job.getConfiguration());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            log.error(<span class="string">&quot;WordCountRecordWriter创建失败&quot;</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Text key, IntWritable value)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String word = key.toString();</span><br><span class="line">        Integer first = word.length();</span><br><span class="line">        FSDataOutputStream fsDataOutputStream = fsDataOutputStreamHashMap.get(first);</span><br><span class="line">        <span class="keyword">if</span> (fsDataOutputStream == <span class="keyword">null</span>) &#123;</span><br><span class="line">            fsDataOutputStream = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">&quot;/Users/zhenan/atguigu/project/sgg-big-data/sgg-hadoop/sgg-hadoop-mapreduce/src/main/resources/outputformat/&quot;</span> + first + <span class="string">&quot;.txt&quot;</span>));</span><br><span class="line">            fsDataOutputStreamHashMap.put(first, fsDataOutputStream);</span><br><span class="line">        &#125;</span><br><span class="line">        fsDataOutputStream.write((word + <span class="string">&quot;\t&quot;</span> + value.get()+<span class="string">&quot;\n&quot;</span>).getBytes());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        IOUtils.closeStream(fileSystem);</span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;Integer, FSDataOutputStream&gt; entry : fsDataOutputStreamHashMap.entrySet()) &#123;</span><br><span class="line">            IOUtils.closeStream(entry.getValue());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h2 id="3-6-Join多种应用"><a href="#3-6-Join多种应用" class="headerlink" title="3.6 Join多种应用"></a>3.6 Join多种应用</h2><h3 id="3-6-1-Reduce-Join"><a href="#3-6-1-Reduce-Join" class="headerlink" title="3.6.1 Reduce Join"></a>3.6.1 Reduce Join</h3><ol>
<li>概念：在MR程序中计算数据的时候，出现输入文件是多个且文件之间存在关联性，需要在计算过程中通过两个文件之间相互关联才能获取最终的计算结果。</li>
<li>ReduceJoin的思想：<ol>
<li>分析文件之间的关系，然后定位关联字段</li>
<li>在Map阶段对多个文件进行数据整合，并且让关联字段作为输出数据的key </li>
<li>当一组相同key的values进入Reduce阶段的reduce方法中第一步：先把两个文件数据分离出来，分别放到各自的对象中维护。</li>
<li>把当前一组维护好的数据进行关联操作，得到想要的数据结果。</li>
</ol>
</li>
</ol>
<h3 id="3-6-2-Map-Join"><a href="#3-6-2-Map-Join" class="headerlink" title="3.6.2 Map Join"></a>3.6.2 Map Join</h3><ol>
<li>概念：考虑MR整体的执行效率，且业务场景是一个大文件和一个小文件进行关联操作，可以使用MapJoin来实现。另外MapJoin也是解决ReduceJoin数据倾斜问题很有效的办法。</li>
<li>MapJoin的思想：<ol>
<li>分析文件之间的关系，然后定位关联字段</li>
<li>将小文件的数据映射到内存中的一个容器维护起来。 </li>
<li>当MapTask处理大文件的数据时，每读取一行数据，就根据当前行中的关联字段到内存的容器里获取对象的信息。</li>
<li>封装结果将其输出</li>
</ol>
</li>
<li>具体办法：采用DistributedCache<ol>
<li>在Mapper的setup阶段，将文件读取到缓存集合中。</li>
<li>在Driver驱动类中加载缓存 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//缓存普通文件到Task运行节点。</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">&quot;file:///e:/cache/pd.txt&quot;</span>));</span><br><span class="line"><span class="comment">//如果是集群运行,需要设置HDFS路径</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9820/cache/pd.txt&quot;</span>));</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<hr>
<h2 id="3-7-计数器"><a href="#3-7-计数器" class="headerlink" title="3.7 计数器"></a>3.7 计数器</h2><ul>
<li>Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量</li>
<li>计数器API<ol>
<li>采用枚举的方式统计计数</li>
<li>采用计数器组、计数器名称的方式统计</li>
<li>计数结果在程序运行后的控制台上查看</li>
</ol>
</li>
</ul>
<hr>
<h2 id="3-8-数据清洗（ETL）"><a href="#3-8-数据清洗（ETL）" class="headerlink" title="3.8 数据清洗（ETL）"></a>3.8 数据清洗（ETL）</h2><ul>
<li>在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序。</li>
<li>Mapper程序不符合规则直接return</li>
</ul>
<hr>
<h2 id="3-9-MapReduce工作流程梳理"><a href="#3-9-MapReduce工作流程梳理" class="headerlink" title="3.9 MapReduce工作流程梳理"></a>3.9 MapReduce工作流程梳理</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16344606281489.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16344606667460.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>MapTask收集map()方法输出的kv对，放到内存缓冲区中</li>
<li>从内存缓冲区不断溢写本地磁盘文件，可能会溢写多个文件</li>
<li>多个溢出文件会被合并成大的溢写文件</li>
<li>在溢写过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序</li>
<li>ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据</li>
<li>ReduceTask会抓取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序）</li>
<li>合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法）</li>
<li>注意：<ol>
<li>Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。</li>
<li>缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb默认100M。</li>
</ol>
</li>
<li>MapTask源码分析<ul>
<li>定位 map方法输出结果的位置，TaskInputOutputContext#write<ul>
<li>实现类TaskInputOutputContextImpl#write</li>
</ul>
</li>
<li>跟进org.apache.hadoop.mapred.MapTask.NewOutputCollector#write<ul>
<li>获取分区编号org.apache.hadoop.mapreduce.Partitioner#getPartition</li>
<li>k-v放入环形缓冲区org.apache.hadoop.mapred.MapOutputCollector#collect</li>
<li>map端所有的kv全部写出后会执行org.apache.hadoop.mapred.MapTask.NewOutputCollector#close<ul>
<li>执行溢写org.apache.hadoop.mapred.MapTask.MapOutputBuffer#flush </li>
<li>排序溢写org.apache.hadoop.mapred.MapTask.MapOutputBuffer#sortAndSpill<ul>
<li>执行combiner org.apache.hadoop.mapred.Task.CombinerRunner#combine</li>
<li>初始化combiner org.apache.hadoop.mapred.Task.CombinerRunner#create </li>
</ul>
</li>
<li>合并文件org.apache.hadoop.mapred.MapTask.MapOutputBuffer#mergeParts</li>
<li>结束</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>ReduceTask源码分析<ul>
<li>ReduceTask执行入口org.apache.hadoop.mapred.ReduceTask#run<ul>
<li>初始化ReduceTask org.apache.hadoop.mapred.Task#initialize<ul>
<li>获取OutputFormat org.apache.hadoop.mapred.Task:605</li>
<li>获取Shuffer Consumer  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  Class&lt;? extends ShuffleConsumerPlugin&gt; clazz =</span><br><span class="line">job.getClass(MRConfig.SHUFFLE_CONSUMER_PLUGIN, Shuffle.class, ShuffleConsumerPlugin.class);</span><br></pre></td></tr></table></figure></li>
<li>初始化shuffle consumer org.apache.hadoop.mapred.ShuffleConsumerPlugin#init<ul>
<li>创建shuffle实现类org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl<ul>
<li>获取MapTask数量 <code>totalMaps = job.getNumMapTasks();</code></li>
</ul>
</li>
<li>创建合并管理器org.apache.hadoop.mapreduce.task.reduce.Shuffle#createMergeManager<ul>
<li>实现类 org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl#MergeManagerImpl<ul>
<li>ReduceTask内存最大值  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="comment">// Allow unit tests to fix Runtime memory</span></span><br><span class="line"><span class="keyword">this</span>.memoryLimit = (<span class="keyword">long</span>)(jobConf.getLong(</span><br><span class="line">    MRJobConfig.REDUCE_MEMORY_TOTAL_BYTES,</span><br><span class="line">    Runtime.getRuntime().maxMemory()) * maxInMemCopyUse);</span><br></pre></td></tr></table></figure></li>
<li>创建内存合并器 org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl#createInMemoryMerger<ul>
<li>实现类 org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.InMemoryMerger</li>
<li>合并方法org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.InMemoryMerger#merge</li>
<li>Combiner + 溢写org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl#combineAndSpill</li>
</ul>
</li>
<li>创建磁盘合并器 org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.OnDiskMerger</li>
</ul>
</li>
</ul>
</li>
<li>开始抓取数据org.apache.hadoop.mapreduce.task.reduce.Shuffle:107  <code>eventFetcher.start();</code></li>
<li>抓取数据结束org.apache.hadoop.mapreduce.task.reduce.Shuffle:141 <code>eventFetcher.shutDown();</code></li>
<li>copy阶段完成，启动下一个阶段sort org.apache.hadoop.mapreduce.task.reduce.Shuffle:151 <code>// copyPhase.complete();</code></li>
<li>标记进入sort阶段 org.apache.hadoop.mapreduce.task.reduce.Shuffle:152 <code>taskStatus.setPhase(TaskStatus.Phase.SORT);</code></li>
</ul>
</li>
<li>sort阶段完成 开启下一阶段reduce org.apache.hadoop.mapred.ReduceTask:382 <code>sortPhase.complete();</code></li>
</ul>
</li>
<li>reduce();  //reduce阶段调用的就是我们自定义的reduce方法，会被调用多次</li>
<li>cleanup(context); //reduce完成之前，会最后调用一次Reducer里面的cleanup方法</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="3-9-1分析Job提交流程的源码"><a href="#3-9-1分析Job提交流程的源码" class="headerlink" title="3.9.1分析Job提交流程的源码"></a>3.9.1分析Job提交流程的源码</h3><ul>
<li>定位提交入口 org.apache.hadoop.mapreduce.Job#waitForCompletion</li>
<li>跟进 <code>org.apache.hadoop.mapreduce.Job#waitForCompletion</code>  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">waitForCompletion</span><span class="params">(<span class="keyword">boolean</span> verbose</span></span></span><br><span class="line"><span class="params"><span class="function">                              )</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">  <span class="comment">// 判断当前Job的状态是否为定义阶段</span></span><br><span class="line">  <span class="keyword">if</span> (state == JobState.DEFINE) &#123;</span><br><span class="line">    <span class="comment">//提交方法</span></span><br><span class="line">    submit();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (verbose) &#123;</span><br><span class="line">    monitorAndPrintJob();</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// get the completion poll interval from the client.</span></span><br><span class="line">    <span class="keyword">int</span> completionPollIntervalMillis = </span><br><span class="line">      Job.getCompletionPollInterval(cluster.getConf());</span><br><span class="line">    <span class="keyword">while</span> (!isComplete()) &#123;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(completionPollIntervalMillis);</span><br><span class="line">          &#125; <span class="keyword">catch</span> (InterruptedException ie) &#123;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> isSuccessful();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进<code>org.apache.hadoop.mapreduce.Job#submit</code>  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">submit</span><span class="params">()</span> </span></span><br><span class="line"><span class="function">       <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">  <span class="comment">//判断当前为定义阶段</span></span><br><span class="line">  ensureState(JobState.DEFINE);</span><br><span class="line">  <span class="comment">//兼容老版本API</span></span><br><span class="line">  setUseNewAPI();</span><br><span class="line">  <span class="comment">//连接集群（如果是本地模式结果就是LocalRunner, 如果Yarn集群结果就是YARNRuuner）</span></span><br><span class="line">  connect();</span><br><span class="line">  <span class="comment">// 开始提交Job</span></span><br><span class="line">  <span class="keyword">final</span> JobSubmitter submitter = </span><br><span class="line">      getJobSubmitter(cluster.getFileSystem(), cluster.getClient());</span><br><span class="line">  status = ugi.doAs(<span class="keyword">new</span> PrivilegedExceptionAction&lt;JobStatus&gt;() &#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> JobStatus <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, </span></span><br><span class="line"><span class="function">    ClassNotFoundException </span>&#123;</span><br><span class="line">      <span class="comment">//执行提交动作</span></span><br><span class="line">      <span class="keyword">return</span> submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">  state = JobState.RUNNING;</span><br><span class="line">  LOG.info(<span class="string">&quot;The url to track the job: &quot;</span> + getTrackingURL());</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进<code>org.apache.hadoop.mapreduce.JobSubmitter#submitJobInternal</code>  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Internal method for submitting jobs to the system.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * &lt;p&gt;The job submission process involves:</span></span><br><span class="line"><span class="comment"> * &lt;ol&gt;</span></span><br><span class="line"><span class="comment"> *   &lt;li&gt;</span></span><br><span class="line"><span class="comment"> *   检测输入输出路径的合法性</span></span><br><span class="line"><span class="comment"> *   Checking the input and output specifications of the job.</span></span><br><span class="line"><span class="comment"> *   &lt;/li&gt;</span></span><br><span class="line"><span class="comment"> *   &lt;li&gt;</span></span><br><span class="line"><span class="comment"> *   给当前Job计算切片信息</span></span><br><span class="line"><span class="comment"> *   Computing the &#123;<span class="doctag">@link</span> InputSplit&#125;s for the job.</span></span><br><span class="line"><span class="comment"> *   &lt;/li&gt;</span></span><br><span class="line"><span class="comment"> *   &lt;li&gt;</span></span><br><span class="line"><span class="comment"> *   添加分布式缓存文件</span></span><br><span class="line"><span class="comment"> *   Setup the requisite accounting information for the </span></span><br><span class="line"><span class="comment"> *   &#123;<span class="doctag">@link</span> DistributedCache&#125; of the job, if necessary.</span></span><br><span class="line"><span class="comment"> *   &lt;/li&gt;</span></span><br><span class="line"><span class="comment"> *   &lt;li&gt;</span></span><br><span class="line"><span class="comment"> *   将必要的内容都拷贝到 job执行的临时目录（jar包、切片信息、配置文件）</span></span><br><span class="line"><span class="comment"> *   Copying the job&#x27;s jar and configuration to the map-reduce system</span></span><br><span class="line"><span class="comment"> *   directory on the distributed file-system. </span></span><br><span class="line"><span class="comment"> *   &lt;/li&gt;</span></span><br><span class="line"><span class="comment"> *   &lt;li&gt;</span></span><br><span class="line"><span class="comment"> *   提交Job</span></span><br><span class="line"><span class="comment"> *   Submitting the job to the &lt;code&gt;JobTracker&lt;/code&gt; and optionally</span></span><br><span class="line"><span class="comment"> *   monitoring it&#x27;s status.</span></span><br><span class="line"><span class="comment"> *   &lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;/ol&gt;&lt;/p&gt;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> job the configuration to submit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> cluster the handle to the Cluster</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> ClassNotFoundException</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">JobStatus <span class="title">submitJobInternal</span><span class="params">(Job job, Cluster cluster)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> ClassNotFoundException, InterruptedException, IOException </span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h2 id="3-8-MapReduce开发总结"><a href="#3-8-MapReduce开发总结" class="headerlink" title="3.8 MapReduce开发总结"></a>3.8 MapReduce开发总结</h2><ol>
<li>输入数据接口：InputFormat<ul>
<li>默认使用的实现类是：TextInputFormat</li>
<li>TextInputFormat 的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为 value 返回。</li>
<li>CombineTextInputFormat 可以把多个小文件合并成一个切片处理，提高处理效率。</li>
</ul>
</li>
<li>map逻辑处理接口：Mapper <ul>
<li>用户根据业务需求实现其中三个方法：map() setup() cleanup () </li>
</ul>
</li>
<li>Partitioner 分区<ul>
<li>有默认实现 HashPartitioner，逻辑是根据 key 的哈希值和 numReduces 来返回一个分区号；key.hashCode()&amp;Integer.MAXVALUE % numReduces</li>
<li>如果业务上有特别的需求，可以自定义分区。</li>
</ul>
</li>
<li>Comparable 排序<ul>
<li>当我们用自定义的对象作为 key 来输出时，就必须要实现 WritableComparable 接口，重写其中的 compareTo()方法。</li>
<li>部分排序：对最终输出的每一个文件进行内部排序。</li>
<li>全排序：对所有数据进行排序，通常只有一个 Reduce。 （4）二次排序：排序的条件有两个。</li>
</ul>
</li>
<li>Combiner 合并<ul>
<li>Combiner 合并可以提高程序执行效率，减少 IO 传输。但是使用时必须不能影响原有的业务处理结果。</li>
</ul>
</li>
<li>reduce逻辑处理接口：Reducer<ul>
<li>用户根据业务需求实现其中三个方法：reduce() setup() cleanup () </li>
</ul>
</li>
<li>输出数据接口：OutputFormat<ul>
<li>默认实现类是 TextOutputFormat，功能逻辑是：将每一个 KV 对，向目标文本文件输出一行。</li>
<li>用户还可以自定义 OutputFormat。</li>
</ul>
</li>
</ol>
<h2 id="问答"><a href="#问答" class="headerlink" title="问答"></a>问答</h2><h4 id="一、描述一下手写MR的大概流程和规范"><a href="#一、描述一下手写MR的大概流程和规范" class="headerlink" title="一、描述一下手写MR的大概流程和规范"></a>一、描述一下手写MR的大概流程和规范</h4><ol>
<li>继承Mapper重写map方法</li>
<li>继承Reducer重写reduce方法 </li>
<li>编写Driver配置Job参数</li>
<li>提交Job</li>
</ol>
<h4 id="二、如何实现Hadoop中的序列化，以及Hadoop的序列化和Java的序列化有什么区别？"><a href="#二、如何实现Hadoop中的序列化，以及Hadoop的序列化和Java的序列化有什么区别？" class="headerlink" title="二、如何实现Hadoop中的序列化，以及Hadoop的序列化和Java的序列化有什么区别？"></a>二、如何实现Hadoop中的序列化，以及Hadoop的序列化和Java的序列化有什么区别？</h4><ol>
<li>实现Writeable接口</li>
<li>无参构造</li>
<li>重写序列化方法write</li>
<li>重写反序列化方法readFields</li>
<li>write 方法和readFields方法保持一致</li>
</ol>
<h4 id="三、概述一下MR程序的执行流程"><a href="#三、概述一下MR程序的执行流程" class="headerlink" title="三、概述一下MR程序的执行流程"></a>三、概述一下MR程序的执行流程</h4><ol>
<li>数据读取阶段：InputFormat进行切片读取</li>
<li>map阶段：执行map方法业务逻辑，输出处理后的kv数据</li>
<li>shuffle阶段：对map阶段输出的kv进行分区，排序，分组，通知reduce取数据</li>
<li>reduce阶段：执行reduce方法业务逻辑，输出数据</li>
<li>输出阶段：OutputFormat处理输出数据，写入文件</li>
</ol>
<h4 id="四、InputFormat负责数据写的时候要进行切片，为什么切片大小默认是128M"><a href="#四、InputFormat负责数据写的时候要进行切片，为什么切片大小默认是128M" class="headerlink" title="四、InputFormat负责数据写的时候要进行切片，为什么切片大小默认是128M"></a>四、InputFormat负责数据写的时候要进行切片，为什么切片大小默认是128M</h4><ol>
<li>HDFS默认的Block大小为128M</li>
<li>默认128M切片可以从单个数据块读取到全部数据</li>
<li>避免了跨机器读取导致大量IO</li>
</ol>
<h4 id="五、描述一下切片的逻辑（从源码角度描述）"><a href="#五、描述一下切片的逻辑（从源码角度描述）" class="headerlink" title="五、描述一下切片的逻辑（从源码角度描述）"></a>五、描述一下切片的逻辑（从源码角度描述）</h4><ol>
<li>定位入口InputFormat#getSplits</li>
<li>由FileInputFormat#getSplits具体实现</li>
<li>确定最小切片大小默认1，最大切片大小默认Long.MAX_VALUE</li>
<li>获取是否对输入路径递归执行的参数默认false，递归处理输入路径下的所有文件</li>
<li>判断是否能够切分，压缩文件不进行切分</li>
<li>获取文件大小和块大小</li>
<li>计算切片大小max(最小切块大小，min(块大小，最大切块大小))</li>
<li>判断剩余文件大小是否可以继续切分，大于1.1倍的切片大小则继续切分</li>
</ol>
<h4 id="六、CombineTextInputFormat机制是怎么实现的"><a href="#六、CombineTextInputFormat机制是怎么实现的" class="headerlink" title="六、CombineTextInputFormat机制是怎么实现的"></a>六、CombineTextInputFormat机制是怎么实现的</h4><ol>
<li>CombineTextInputFormat默认切片大小为4m</li>
<li>虚拟切片过程：文件和切片大小进行比较<ol>
<li>当前文件&gt;切片大小 且 小于2倍的切片大小，就切成2片</li>
<li>当前文件&gt;大于2倍的切片大小，直接切出切片大小的文件，重复执行虚拟切片过程</li>
</ol>
</li>
<li>实际切片过程：比较虚拟切片的结果文件大小和设置切片大小<ol>
<li>如果大于等于切片大小就单独行程一个切片</li>
<li>如果小于设置切片大小就和下一个虚拟文件进行合并，重复执行至大于切片大小</li>
<li>合并后大于设置切片大小单独就形成一个切片</li>
</ol>
</li>
</ol>
<h4 id="七、阐述一下-Shuffle机制-流程？"><a href="#七、阐述一下-Shuffle机制-流程？" class="headerlink" title="七、阐述一下 Shuffle机制 流程？"></a>七、阐述一下 Shuffle机制 流程？</h4><ol>
<li>Shuffle机制处于Map过程和Reduce过程的中间阶段</li>
<li>具体实现的功能包括，分区，分组，排序，合并</li>
<li>map端的Shuffle<ol>
<li>partition: 获取分区编号保存到元数据中，数据写入环形缓冲区</li>
<li>spill: 环形缓冲区默认100M，到达80%时触发spill溢写，剩余20%继续执行写入</li>
<li>sort: spill溢写过程根据分区编号，Key比较规则进行排序（升序，快排），溢写文件保证分区内有序</li>
<li>combine: 触发spill进行sort之后，写入文件之前会进行combine操作；溢写文件大于3个时merge的过程中也会执行combine</li>
<li>merge: 对多个溢写文件进行合并，算法为多路归并排序，最终生成一个文件作为map阶段的输出</li>
<li>通知reduce拉取数据</li>
</ol>
</li>
<li>reduce端的Shuffle<ol>
<li>copy过程：拉取MapTask处理完的数据，使用0.7 × maxHeap大小的堆内存空间作为内存缓冲区</li>
<li>merge过程：内存中数据到达阈值会触发内存到磁盘的合并；map端数据读取完成后触发磁盘到磁盘的合并，算法为归并排序</li>
<li>reduce输入：merge阶段合并为一个大文件作Reduce数据文件执行Reduce逻辑，Shuffle阶段结束</li>
</ol>
</li>
</ol>
<h4 id="八、在MR程序中由谁来决定分区的数量，哪个阶段环节会开始往分区中写数据？"><a href="#八、在MR程序中由谁来决定分区的数量，哪个阶段环节会开始往分区中写数据？" class="headerlink" title="八、在MR程序中由谁来决定分区的数量，哪个阶段环节会开始往分区中写数据？"></a>八、在MR程序中由谁来决定分区的数量，哪个阶段环节会开始往分区中写数据？</h4><ol>
<li>分区由业务逻辑决定</li>
<li>分区规则有ReduceTask数量控制</li>
<li>Shuffle阶段确定分区编号</li>
<li>溢写阶段进行分区</li>
<li>Reduce执行结束，写入分区文件</li>
</ol>
<h4 id="九、阐述MR中实现分区的思路（从源码角度分析）"><a href="#九、阐述MR中实现分区的思路（从源码角度分析）" class="headerlink" title="九、阐述MR中实现分区的思路（从源码角度分析）"></a>九、阐述MR中实现分区的思路（从源码角度分析）</h4><ol>
<li>Shuffle阶段确定分区编号</li>
<li>溢写阶段根据分区编号生成不同的溢写文件</li>
<li>Reduce从多个map输出的文件中取自己分区的数据，处理后生成该分区的结果文件</li>
<li>默认分区规则为根据Key.hashcode 取模，作为分区编号</li>
</ol>
<h4 id="十、描述一下Hadoop中实现排序比较的规则"><a href="#十、描述一下Hadoop中实现排序比较的规则" class="headerlink" title="十、描述一下Hadoop中实现排序比较的规则"></a>十、描述一下Hadoop中实现排序比较的规则</h4><ol>
<li>Hadoop中实现排序依赖Comparator#compare方法</li>
<li>Comparator获取逻辑如下<ol>
<li>首先从jobContext中获取配置比较器的类名</li>
<li>如果获取到直接通过反射创建比较器，流程结束</li>
<li>如果未配置比较器类名，先从比较器缓存Map中根据输出key类对象获取比较器，如果获取到，直接返回比较器，流程结束</li>
<li>如果比较器缓存中未获取到比较器，强制加载后重新获取</li>
<li>如果还获取不到，Hadoop会使用输出key的class对象向创建一个比较器（要求必须实现了WritableComparable）</li>
</ol>
</li>
</ol>
<h4 id="十一、Hadoop中实现排序的两种方案分别是什么？"><a href="#十一、Hadoop中实现排序的两种方案分别是什么？" class="headerlink" title="十一、Hadoop中实现排序的两种方案分别是什么？"></a>十一、Hadoop中实现排序的两种方案分别是什么？</h4><ol>
<li>比较对象实现WritableComparable接口，重写compareTo方法</li>
<li>自定义Comparator，继承WritableComparator，重写compare方法，在Driver中指定</li>
<li>自定义Comparator，继承WritableComparator，重写compare方法，静态代码注册比较器</li>
</ol>
<h4 id="十二、编写MR的时候什么情况下使用Combiner-具体实现流程是什么？"><a href="#十二、编写MR的时候什么情况下使用Combiner-具体实现流程是什么？" class="headerlink" title="十二、编写MR的时候什么情况下使用Combiner 具体实现流程是什么？"></a>十二、编写MR的时候什么情况下使用Combiner 具体实现流程是什么？</h4><ol>
<li>为了提高MR运行效率，减轻ReduceTask压力，减少copy环节IO开销</li>
<li>Combiner执行不影响最终的业务逻辑</li>
<li>Reduce端对MaoTask数据的整体性没有要求</li>
<li>Combiner实现流程<ol>
<li>自定义Combiner类，继承Reducer，重写reduce方法</li>
<li>Job中指定Combiner</li>
<li>输入k-v为map的输出，输出k-v为reduce的输入</li>
</ol>
</li>
</ol>
<h4 id="十三、OutputFormat自定义实现流程描述一下"><a href="#十三、OutputFormat自定义实现流程描述一下" class="headerlink" title="十三、OutputFormat自定义实现流程描述一下"></a>十三、OutputFormat自定义实现流程描述一下</h4><ol>
<li>自定义OutputFormat类，继承OutputFormat，实现getRecordWriter抽象方法，返回自定义RecordWriter</li>
<li>自定义RecordWriter类，继承RecordWriter，重写write实现数据写出的逻辑，重写close方法对资源进行关闭</li>
<li>Job中指定OutputFormat处理类</li>
</ol>
<h4 id="十四、MR实现-ReduceJoin-的思路，以及ReduceJoin方案有哪些不足？"><a href="#十四、MR实现-ReduceJoin-的思路，以及ReduceJoin方案有哪些不足？" class="headerlink" title="十四、MR实现 ReduceJoin 的思路，以及ReduceJoin方案有哪些不足？"></a>十四、MR实现 ReduceJoin 的思路，以及ReduceJoin方案有哪些不足？</h4><ul>
<li>思路<ol>
<li>分析文件关联，确定关联字段</li>
<li>定义统一对象，包含关联字段和数据来源</li>
<li>map端参与文件输出统一对象，key为关联字段</li>
<li>reduce端以关联字段为key，统一对象为value，从统一对象中根据数据来源拆分对象</li>
<li>根据拆分对象进行关联</li>
</ol>
</li>
<li>不足<ol>
<li>耗费性能，需要参与数据全部遍历才能进行join</li>
<li>无法解决数据倾斜问题</li>
<li>海量数据容易造成reduce崩溃，任务失败</li>
</ol>
</li>
</ul>
<h4 id="十五、MR实现-MapJoin-的思路，以及MapJoin的局限性是什么？"><a href="#十五、MR实现-MapJoin-的思路，以及MapJoin的局限性是什么？" class="headerlink" title="十五、MR实现 MapJoin 的思路，以及MapJoin的局限性是什么？"></a>十五、MR实现 MapJoin 的思路，以及MapJoin的局限性是什么？</h4><ul>
<li>思路：<ol>
<li>分析文件关联，确定关联字段</li>
<li>小文件使用DistributedCache加载到缓存中</li>
<li>map端每读取一行数据，都根据关联字段，在缓存中获取对应关联数据</li>
<li>输出包换关联信息的完整数据给reduce</li>
</ol>
</li>
<li>局限<ol>
<li>适用数据量差异比较大的两个数据集</li>
</ol>
</li>
</ul>
]]></content>
      <categories>
        <category>MR</category>
      </categories>
      <tags>
        <tag>MR</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux</title>
    <url>/2021/11/07/Linux/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h1><h3 id="1、Linux简介"><a href="#1、Linux简介" class="headerlink" title="1、Linux简介"></a>1、Linux简介</h3><p><strong>什么是操作系统？</strong></p>
<p>操作系统是管理计算机硬件与软件资源的计算机程序，同时也是计算机系统的内核与基石。操作系统需要处理如管理与配置内存、决定系统资源供需的优先次序、控制输入设备与输出设备、操作网络与管理文件系统等基本事务。操作系统也提供一个让用户与系统交互的操作界面</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211120630067.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211120630067"></p>
<p><strong>常见的操作系统</strong></p>
<ul>
<li>2</li>
<li>MAC OS</li>
<li>Android</li>
<li>iOS</li>
</ul>
<p><strong>操作系统的发展史</strong></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211120911148.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211120911148"></p>
<ul>
<li><p>Unix</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211120956539.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211120956539"></p>
<p>1965年之前的时候，电脑并不像现在一样普遍，它可不是一般人能碰的起的，除非是军事或者学院的研究机构，而且当时大型主机至多能提供30台终端（30个键盘、显示器)，连接一台电脑</p>
<p>为了解决数量不够的问题：</p>
<ul>
<li><p>1965年左后由贝尔实验室、麻省理工学院 以及 通用电气共同发起了Multics项目，想让大型主机支持300台终端</p>
</li>
<li><p>1969年前后这个项目进度缓慢，资金短缺，贝尔实验室退出了研究</p>
</li>
<li><p>1969年从这个项目中退出的Ken Thompson当时在实验室无聊时，为了让一台空闲的电脑上能够运行“星际旅行”游行，在8月份左右趁着其妻子探亲的时间，用了1个月的时间 编写出了 Unix操作系统的原型</p>
</li>
<li><p>1970年，美国贝尔实验室的 Ken Thompson，以 BCPL语言 为基础，设计出很简单且很接近硬件的 B语言（取BCPL的首字母），并且他用B语言写了第一个UNIX操作系统</p>
</li>
<li><p>因为B语言的跨平台性较差，为了能够在其他的电脑上也能够运行这个非常棒的Unix操作系统，Dennis Ritchie和Ken Thompson 从B语言的基础上准备研究一个更好的语言</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/jie-ping20200211121129.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="截屏2020-02-1112.11.29"></p>
</li>
<li><p>1972年，美国贝尔实验室的 Dennis Ritchie在B语言的基础上最终设计出了一种新的语言，他取了BCPL的第二个字母作为这种语言的名字，这就是C语言</p>
</li>
<li><p>1973年初，C语言的主体完成。Thompson和Ritchie迫不及待地开始用它完全重写了现在大名鼎鼎的Unix操作系统</p>
</li>
</ul>
</li>
<li><p>Minix</p>
<p>简介：因为AT&amp;T(通用电气)的政策改变，在Version 7 Unix推出之后，发布新的使用条款，将UNIX源代码私有化，在大学中不再能使用UNIX源代码。Andrew S. Tanenbaum(塔能鲍姆)教授为了能在课堂上教授学生操作系统运作的实务细节，决定在不使用任何AT&amp;T的源代码前提下，自行开发与UNIX兼容的操作系统，以避免版权上的争议。他以小型UNIX（mini-UNIX）之意，将它称为MINIX</p>
<p>没有火的原因：Minix的创始人说，MINIX 3没有统治世界是源于他在1992年犯下的一个错误，当时他认为BSD必然会一统天下，因为它是一个更稳定和更成熟的系统，其它操作系统难以与之竞争。因此他的MINIX的重心集中在教育上。四名BSD开发者已经成立了一家公司销售BSD系统，他们甚至还有一个有趣的电话号码1-800-ITS-UNIX。然而他们正因为这个电话号码而惹火上身。美国电话电报公司因电话号码而提起诉讼。官司打了三年才解决。在此期间，BSD陷于停滞，而Linux则借此一飞冲天。他的错误在于没有意识官司竟然持续了如此长的时间，以及BSD会因此受到削弱。如果美国电话电报公司没有起诉，Linux永远不会流行起来，BSD将统治世界</p>
</li>
<li><p>Linux</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211121238635.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211121238635"></p>
<p>因为Minix只是教学使用，因此功能并不强，因此Torvalds利用GNU的bash当做开发环境，gcc当做编译工具，编写了Linux内核-v0.02，但是一开始Linux并不能兼容Unix，即Unix上跑的应用程序不能在Linux上跑，即应用程序与内核之间的接口不一致，因为Unix是遵循POSIX规范的，因此Torvalds修改了Linux，并遵循POSIX（Portable Operating System Interface，他规范了应用程序与内核的接口规范）； 一开始Linux只适用于386，后来经过全世界的网友的帮助，最终能够兼容多种硬件</p>
<p>Linux发展的重要里程碑：</p>
<ul>
<li>1990, Linus Torvalds 首次接触 MINIX</li>
<li>1991, Linus Torvalds 开始在 MINIX 上编写各种驱动程序等操作系统内核组件</li>
<li>1991 底, Linus Torvalds 公开了 Linux 内核</li>
<li>1993, Linux 1.0 版发行，Linux 转向 GPL 版权协议</li>
<li>1994, Linux 的第一个商业发行版 Slackware 问世</li>
<li>1996, 美国国家标准技术局的计算机系统实验室确认 Linux 版本1.2.13（由 Open Linux 公司打包）符合 POSIX 标准</li>
<li>1999, Linux 的简体中文发行版相继问世</li>
</ul>
</li>
</ul>
<p><strong>Linux版本</strong></p>
<ul>
<li><p>Linux内核版本</p>
<p>内核(kernel)是系统的心脏，是运行程序和管理像磁盘和打印机等硬件设备的核心程序，它提供了一个在裸设备与应用程序间的抽象层</p>
<p>Linux内核版本又分为稳定版和开发版，两种版本是相互关联，相互循环</p>
<table>
<thead>
<tr>
<th align="left">版本</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">稳定版</td>
<td align="left">具有工业级强度，可以广泛地应用和部署。新的稳定版相对于较旧的只是修正一些bug或加入一些新的驱动程序</td>
</tr>
<tr>
<td align="left">开发版</td>
<td align="left">由于要试验各种解决方案，所以变化很快</td>
</tr>
</tbody></table>
<p>内核源码网址[<a href="http://www.kernel.org]，所有来自全世界的对Linux源码的修改最终都会汇总到这个网站，由Linus领导的开源社区对其进行甄别和修改最终决定是否进入到Linux主线内核源码中">http://www.kernel.org]，所有来自全世界的对Linux源码的修改最终都会汇总到这个网站，由Linus领导的开源社区对其进行甄别和修改最终决定是否进入到Linux主线内核源码中</a></p>
</li>
<li><p>Linux发行版本</p>
<p>通常包含了包括桌面环境、办公套件、媒体播放器、数据库等应用软件</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211121551282.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211121551282"></p>
<p>常见发行版本：</p>
<ul>
<li>Fedora</li>
<li>Redhat</li>
<li>Ubuntu</li>
<li>CentOS</li>
</ul>
</li>
</ul>
<p><strong>Linux应用领域</strong></p>
<table>
<thead>
<tr>
<th>领域</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>个人桌面领域</td>
<td>此领域是传统linux应用最薄弱的环节，传统linux由于界面简单、操作复杂、应用软件少的缺点，一直被windows所压制，但近些年来随着ubuntu、fedora等优秀桌面环境的兴起，同时各大硬件厂商对其支持的加大，linux在个人桌面领域的占有率在逐渐的提高</td>
</tr>
<tr>
<td>服务器领域</td>
<td>linux免费、稳定、高效等特点在这里得到了很好的体现，但早期因为维护、运行等原因同样受到了很大的限制，但近些年来linux服务器市场得到了飞速的提升，尤其在一些高端领域尤为广泛</td>
</tr>
<tr>
<td>嵌入式领域</td>
<td>linux运行稳定、对网络的良好支持性、低成本，且可以根据需要进行软件裁剪，内核最小可以达到几百KB等特点，使其近些年来在嵌入式领域的应用得到非常大的提高</td>
</tr>
</tbody></table>
<p><strong>Linux和Windows区别</strong></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/bu-huo.PNG?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="捕获"></p>
<p><strong>CentOS下载地址</strong></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/bu-huo1627694182903.PNG?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="捕获"></p>
<h3 id="2、安装VMware虚拟机"><a href="#2、安装VMware虚拟机" class="headerlink" title="2、安装VMware虚拟机"></a>2、安装VMware虚拟机</h3><h3 id="3、安装linux系统"><a href="#3、安装linux系统" class="headerlink" title="3、安装linux系统"></a>3、安装linux系统</h3><h3 id="4、文件和目录"><a href="#4、文件和目录" class="headerlink" title="4、文件和目录"></a>4、文件和目录</h3><p><strong>Windows文件系统</strong></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211134106179.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211134106179"></p>
<p>在 windows 平台下，打开“计算机”，我们看到的是一个个的驱动器盘符</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211134123743.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211134123743"></p>
<p>每个驱动器都有自己的根目录结构，这样形成了多个树并列的情形</p>
<p><strong>Linux文件系统</strong></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211145837589.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211145837589"></p>
<p>centos没有盘符这个概念，只有一个根目录/，所有文件都在它下面</p>
<p><strong>目录</strong></p>
<table>
<thead>
<tr>
<th>路径</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>/</td>
<td>根目录，一般根目录下只存放目录，在Linux下有且只有一个根目录。所有的东西都是从这里开始。当你在终端里输入“/home”，你其实是在告诉电脑，先从/（根目录）开始，再进入到home目录</td>
</tr>
<tr>
<td>/bin<br />/usr/bin</td>
<td>可执行二进制文件的目录，如常用的命令ls、tar、mv、cat等</td>
</tr>
<tr>
<td>/boot</td>
<td>放置linux系统启动时用到的一些文件，如Linux的内核文件：/boot/vmlinuz，系统引导管理器：/boot/grub</td>
</tr>
<tr>
<td>/dev</td>
<td>存放linux系统下的设备文件，访问该目录下某个文件，相当于访问某个设备，常用的是挂载光驱 mount /dev/cdrom /mnt</td>
</tr>
<tr>
<td>/etc</td>
<td>系统配置文件存放的目录，不建议在此目录下存放可执行文件，重要的配置文件有 /etc/inittab、/etc/fstab、/etc/init.d、/etc/X11、/etc/sysconfig、/etc/xinetd.d</td>
</tr>
<tr>
<td>/home</td>
<td>系统默认的用户家目录，新增用户账号时，用户的家目录都存放在此目录下，<del>表示当前用户的家目录，</del>edu 表示用户 edu 的家目录</td>
</tr>
<tr>
<td>/lib<br />/usr/lib<br />/usr/local/lib</td>
<td>系统使用的函数库的目录，程序在执行过程中，需要调用一些额外的参数时需要函数库的协助</td>
</tr>
<tr>
<td>/lost+fount</td>
<td>系统异常产生错误时，会将一些遗失的片段放置于此目录下</td>
</tr>
<tr>
<td>/mnt</td>
<td>/media：光盘默认挂载点，通常光盘挂载于 /mnt/cdrom 下，也不一定，可以选择任意位置进行挂载</td>
</tr>
<tr>
<td>/opt</td>
<td>给主机额外安装软件所摆放的目录</td>
</tr>
<tr>
<td>/proc</td>
<td>此目录的数据都在内存中，如系统核心，外部设备，网络状态，由于数据都存放于内存中，所以不占用磁盘空间，比较重要的目录有 /proc/cpuinfo、/proc/interrupts、/proc/dma、/proc/ioports、/proc/net/* 等</td>
</tr>
<tr>
<td>/root</td>
<td>系统管理员root的家目录</td>
</tr>
<tr>
<td>/sbin<br />/usr/sbin<br />/usr/local/sbin</td>
<td>放置系统管理员使用的可执行命令，如fdisk、shutdown、mount 等。与 /bin 不同的是，这几个目录是给系统管理员 root使用的命令，一般用户只能”查看”而不能设置和使用</td>
</tr>
<tr>
<td>/tmp</td>
<td>一般用户或正在执行的程序临时存放文件的目录，任何人都可以访问，重要数据不可放置在此目录下</td>
</tr>
<tr>
<td>/srv</td>
<td>服务启动之后需要访问的数据目录，如 www 服务需要访问的网页数据存放在 /srv/www 内</td>
</tr>
<tr>
<td>/usr</td>
<td>应用程序存放目录，/usr/bin 存放应用程序，/usr/share 存放共享数据，/usr/lib 存放不能直接运行的，却是许多程序运行所必需的一些函数库文件。/usr/local: 存放软件升级包。/usr/share/doc: 系统说明文件存放目录。/usr/share/man: 程序说明文件存放目录</td>
</tr>
<tr>
<td>/var</td>
<td>放置系统执行过程中经常变化的文件，如随时更改的日志文件 /var/log，/var/log/message：所有的登录文件存放目录，/var/spool/mail：邮件存放的目录，/var/run:程序或服务启动后，其PID存放在该目录下</td>
</tr>
</tbody></table>
<p><strong>路径</strong></p>
<ul>
<li><p>绝对路径</p>
<p>从/目录开始描述的路径为绝对路径</p>
</li>
<li><p>相对路径</p>
<p>从当前位置开始描述的路径为相对路径</p>
</li>
<li><p>.</p>
<p>代表当前目录</p>
</li>
<li><p>..</p>
<p>表示上一级目录</p>
<p>注意：根目录下的.和..都代表当前目录</p>
</li>
</ul>
<h3 id="5、命令概述"><a href="#5、命令概述" class="headerlink" title="5、命令概述"></a>5、命令概述</h3><p>Linux 提供了大量的命令，利用它可以有效地完成大量的工作，如磁盘操作、文件存取、目录操作、进程管理、文件权限设定等。Linux 发行版本最少的命令也有 200 多个，这里只介绍比较重要和使用频率最多的命令</p>
<ul>
<li><p>命令的使用方法</p>
<p>格式：command  [-options]  [parameter1]  …</p>
<p>command：命令名，相应功能的英文单词或单词的缩写</p>
<p>[-options]：选项，可用来对命令进行控制，也可以省略</p>
<p>[parameter1]  …：传给命令的参数，可以是零个一个或多个</p>
</li>
<li><p>查看帮助文档</p>
<ul>
<li><p>man</p>
<p>man是linux提供的一个手册，包含了绝大部分的命令、函数使用说明。该手册分成很多章节（section），使用man时可以指定不同的章节来浏览</p>
<img src="Linux.assets/image-20200211174846722.png" alt="image-20200211174846722" style="zoom:150%;" />

<p>功能键<img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211174857959.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211174857959"></p>
</li>
<li><p>help</p>
<p>获得shell内置命令的帮助信息</p>
<p>语法：help 命令</p>
<p><code>help cd</code></p>
</li>
<li><p>–help</p>
<p>一般是linux命令自带的帮助信息</p>
<p>例如：ls  –help</p>
</li>
</ul>
</li>
<li><p>自动补全</p>
<p>在敲出命令的前几个字母的同时，按下tab键，系统会自动帮我们补全命令</p>
</li>
<li><p>历史命令</p>
<p>当系统执行过一些命令后，可按上下键翻看以前的命令，history将执行过的命令列举出来</p>
</li>
</ul>
<h3 id="6、常用快捷键"><a href="#6、常用快捷键" class="headerlink" title="6、常用快捷键"></a>6、常用快捷键</h3><table>
<thead>
<tr>
<th>常用快捷键</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>ctrl + c</td>
<td>停止进程</td>
</tr>
<tr>
<td>ctrl+l</td>
<td>清屏:clear；彻底清屏是：reset</td>
</tr>
<tr>
<td>ctrl + q</td>
<td>退出</td>
</tr>
<tr>
<td>善于用tab键</td>
<td>提示(更重要的是可以防止敲错)</td>
</tr>
<tr>
<td>上下键</td>
<td>查找执行过的命令</td>
</tr>
<tr>
<td>ctrl +alt</td>
<td>linux和Windows之间切换</td>
</tr>
</tbody></table>
<h3 id="7、文件管理"><a href="#7、文件管理" class="headerlink" title="7、文件管理"></a>7、文件管理</h3><ul>
<li><p><code>ls</code></p>
<p>作用：显示指定目录下所有的文件和目录</p>
<p>选项：</p>
<ul>
<li><p>-a</p>
<p>显示指定目录下所有子目录与文件，包括隐藏文件。Linux文件或者目录名称最长可以有265个字符，“.”代表当前目录，“..”代表上一级目录，以“.”开头的文件为隐藏文件，需要用 -a 参数才能显示</p>
</li>
<li><p>-l</p>
<p>以列表方式显示文件的详细信息</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211181530984.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211181530984"></p>
<p>文件类型：</p>
<table>
<thead>
<tr>
<th>符号</th>
<th>类型</th>
</tr>
</thead>
<tbody><tr>
<td>-</td>
<td>普通文件</td>
</tr>
<tr>
<td>d</td>
<td>目录文件</td>
</tr>
<tr>
<td>l</td>
<td>链接文件</td>
</tr>
<tr>
<td>c</td>
<td>字符设备</td>
</tr>
<tr>
<td>b</td>
<td>块设备</td>
</tr>
</tbody></table>
</li>
<li><p>-h</p>
<p>配合 -l 以人性化的方式显示文件大小</p>
</li>
</ul>
<p>通配符：</p>
<table>
<thead>
<tr>
<th>符号</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td>*</td>
<td align="left">文件代表文件名中所有字符</td>
</tr>
<tr>
<td>ls te*</td>
<td align="left">查找以te开头的文件</td>
</tr>
<tr>
<td>ls *html</td>
<td align="left">查找结尾为html的文件</td>
</tr>
<tr>
<td>？</td>
<td align="left">代表文件名中任意一个字符</td>
</tr>
<tr>
<td>ls ?.c</td>
<td align="left">只找第一个字符任意，后缀为.c的文件</td>
</tr>
<tr>
<td>ls a.?</td>
<td align="left">只找只有3个字符，前2字符为a.，最后一个字符任意的文件</td>
</tr>
<tr>
<td>[]</td>
<td align="left">[”和“]”将字符组括起来，表示可以匹配字符组中的任意一个。“-”用于表示字符范围</td>
</tr>
<tr>
<td>[abc]</td>
<td align="left">匹配a、b、c中的任意一个</td>
</tr>
<tr>
<td>[a-f]</td>
<td align="left">匹配从a到f范围内的的任意一个字符</td>
</tr>
<tr>
<td>ls [a-f]*</td>
<td align="left">找到从a到f范围内的的任意一个字符开头的文件</td>
</tr>
<tr>
<td>ls a-f</td>
<td align="left">查找文件名为a-f的文件,当“-”处于方括号之外失去通配符的作用</td>
</tr>
<tr>
<td>\</td>
<td align="left">如果要使通配符作为普通字符使用，可以在其前面加上转义字符。“?”和“*”处于方括号内时不用使用转义字符就失去通配符的作用</td>
</tr>
<tr>
<td><code>ls \*a</code></td>
<td align="left">查找文件名为*a的文件</td>
</tr>
</tbody></table>
</li>
<li><p><code>pwd</code></p>
<p>作用：显示当前的工作目录</p>
</li>
<li><p><code>cd</code></p>
<p>作用：切换工作目录</p>
<p>注意：cd后面可跟绝对路径，也可以跟相对路径</p>
<p>特殊写法：</p>
<table>
<thead>
<tr>
<th>写法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>cd</td>
<td>切换到当前用户的主目录(/home/用户目录)，用户登陆的时候，默认的目录就是用户的主目录</td>
</tr>
<tr>
<td>cd ~</td>
<td>切换到当前用户的主目录(/home/用户目录)</td>
</tr>
<tr>
<td>cd .</td>
<td>切换到当前目录</td>
</tr>
<tr>
<td>cd ..</td>
<td>切换到上级目录</td>
</tr>
<tr>
<td>cd -</td>
<td>可进入上次所在的目录</td>
</tr>
<tr>
<td>cd -P</td>
<td>跳转到实际物理路径，而非快捷方式路径</td>
</tr>
</tbody></table>
</li>
<li><p><code>&gt;</code></p>
<p>作用：输出重定向，Linux允许将命令执行结果重定向到一个文件，本应显示在终端上的内容保存到指定文件中</p>
<p>示例：<code>ls &gt; test.txt</code></p>
<p>注意：如果文件不存在，则创建，存在则覆盖其内容</p>
</li>
<li><p><code>&gt;&gt;</code></p>
<p>作用：输出重定向，Linux允许将命令执行结果重定向到一个文件，本应显示在终端上的内容保存到指定文件中</p>
<p>示例：<code>ls &gt;&gt; test.txt</code></p>
<p>注意：如果文件不存在，则创建，存在则追加到文件的尾部</p>
</li>
<li><p><code>cat</code></p>
<p>作用：查看或者合并文件内容</p>
<p>合并文件示例：<code>cat  test1.txt  test2.txt &gt; test.txt</code></p>
</li>
<li><p><code>head</code></p>
<p>作用：查看文件</p>
<p>默认显示前10行：<code>head  test.txt</code></p>
<p>显示前n行：<code>head  -n  test.txt</code></p>
</li>
<li><p><code>tail</code></p>
<p>作用：查看文件  </p>
<p>默认显示后10行：<code>tail  test.txt</code></p>
<p>显示后n行：<code>tail -n  test.txt</code></p>
<p>监控文件变化：<code>tail -f  test.txt</code></p>
</li>
<li><p><code>more</code></p>
<p>作用：分屏显示，查看内容时，在信息过长无法在一屏上显示时，会出现快速滚屏，使得用户无法看清文件的内容，此时可以使用more命令，每次只显示一页，按下空格键可以显示下一页，按下q键退出显示，按下h键可以获取帮助</p>
</li>
<li><p><code>less</code></p>
<p>作用：分屏查看文件，它的功能与more指令类似，但是比more指令更加强大，支持各种显示终端。less指令在显示文件内容时，并不是一次将整个文件加载之后才显示，而是根据显示需要加载内容，对于显示大型文件具有较高的效率。</p>
<p>说明：敲enter键往下走一行，敲空格键，往下走一页，可以向上翻页，键盘上的pageup，pagedown</p>
</li>
<li><p><code>wc</code></p>
<p>作用：一次显示文件行数、字节数、文件名信息</p>
</li>
<li><p><code>echo</code></p>
<p>使用：echo [选项] [输出内容]</p>
<p>作用：输出内容</p>
<p>-e： 支持反斜线控制的字符转换</p>
<table>
<thead>
<tr>
<th>控制字符</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>\</td>
<td>输出\本身</td>
</tr>
<tr>
<td>\n</td>
<td>换行符</td>
</tr>
<tr>
<td>\t</td>
<td>制表符，也就是Tab键</td>
</tr>
</tbody></table>
<p><code>echo “hello\tworld”</code></p>
<p><code>echo -e “hello\tworld”</code></p>
</li>
<li><p><code>clear</code></p>
<p>作用：清除终端上的显示清除终端上的显示</p>
</li>
<li><p><code>mkdir</code></p>
<p>作用：创建一个新的目录</p>
<p>注意：新建目录的名称不能与当前目录中已有的目录或文件同名，并且目录创建者必须对当前目录具有写权限</p>
<p>递归创建目录：mkdir  -p  a/b/c/d  </p>
</li>
<li><p><code>touch</code></p>
<p>作用：创建一个新的普通文件</p>
</li>
<li><p><code>rmdir</code></p>
<p>作用：删除一个目录</p>
<p>注意：目录必须为空目录</p>
</li>
<li><p><code>rm</code></p>
<p>使用：rm  [选项]  deleteFile</p>
<p>作用：删除文件或目录，删除的文件不能恢复</p>
<p>参数：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-i</td>
<td>以进行交互式方式执行</td>
</tr>
<tr>
<td>-f</td>
<td>强制删除，忽略不存在的文件，无需提示</td>
</tr>
<tr>
<td>-r</td>
<td>递归地删除目录下的内容，删除文件夹时必须加此参数</td>
</tr>
<tr>
<td>-v</td>
<td>显示指令的详细执行过程</td>
</tr>
</tbody></table>
</li>
<li><p><code>cp</code></p>
<p>作用：将给出的文件或目录复制到另一个文件或目录中</p>
<p>格式：cp  文件名 目标目录</p>
<p>选项：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>该选项通常在复制目录时使用，它保留链接、文件属性，并递归地复制目录，简单而言，保持文件原有属性</td>
</tr>
<tr>
<td>-f</td>
<td>已经存在的目标文件而不提示</td>
</tr>
<tr>
<td>-i</td>
<td>交互式复制，在覆盖目标文件之前将给出提示要求用户确认</td>
</tr>
<tr>
<td>-r</td>
<td>若给出的源文件是目录文件，则cp将递归复制该目录下的所有子目录和文件，目标文件必须为一个目录名</td>
</tr>
<tr>
<td>-v</td>
<td>显示拷贝进度</td>
</tr>
</tbody></table>
</li>
<li><p><code>mv</code></p>
<p>作用:</p>
<table>
<thead>
<tr>
<th>说明</th>
<th>使用格式</th>
</tr>
</thead>
<tbody><tr>
<td>移动文件或目录</td>
<td>mv 文件  目标目录</td>
</tr>
<tr>
<td>重命名</td>
<td>mv  文件名  文件名</td>
</tr>
</tbody></table>
<p>参数：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-f</td>
<td>禁止交互式操作，如有覆盖也不会给出提示</td>
</tr>
<tr>
<td>-i</td>
<td>确认交互方式操作，如果mv操作将导致对已存在的目标文件的覆盖，系统会询问是否重写，要求用户回答以避免误覆盖文件</td>
</tr>
<tr>
<td>-v</td>
<td>显示移动进度</td>
</tr>
</tbody></table>
</li>
<li><p><code>ln</code></p>
<p>作用：建立链接文件，Linux链接文件类似于Windows下的快捷方式</p>
<table>
<thead>
<tr>
<th>链接文件分类</th>
<th>说明</th>
<th>创建格式</th>
<th>注意事项</th>
</tr>
</thead>
<tbody><tr>
<td>软连接</td>
<td>软链接不占用磁盘空间，源文件删除则软链接失效</td>
<td>ln  -s  源文件  链接文件</td>
<td>如果软链接文件和源文件不在同一个目录，源文件要使用绝对路径，不能使用相对路径</td>
</tr>
<tr>
<td>硬链接</td>
<td>硬链接只能链接普通文件，不能链接目录</td>
<td>ln  源文件  链接文件</td>
<td>两个文件占用相同大小的硬盘空间，即使删除了源文件，链接文件还是存在，所以-s选项是更常见的形式</td>
</tr>
</tbody></table>
<p>注意：删除软链接： rm -rf 软链接名，而不是rm -rf 软链接名/。如果使用 rm -rf 软链接名/ 删除，会把软链接对应的真实目录下内容删掉。</p>
</li>
</ul>
<h3 id="8、文件查找"><a href="#8、文件查找" class="headerlink" title="8、文件查找"></a>8、文件查找</h3><ul>
<li><p><code>find</code></p>
<p>作用：查找文件</p>
<p>格式：find [搜索范围] [选项]</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>find  ./  -name  test.sh</td>
<td>查找当前目录下所有名为test.sh的文件</td>
</tr>
<tr>
<td>find  ./  -name  ‘*.sh’</td>
<td>查找当前目录下所有后缀为.sh的文件</td>
</tr>
<tr>
<td>find  ./  -name  “[A-Z]*”</td>
<td>查找当前目录下所有以大写字母开头的文件</td>
</tr>
<tr>
<td>find  /tmp  -size  2M</td>
<td>查找在/tmp 目录下等于2M的文件</td>
</tr>
<tr>
<td>find  /tmp  -size  +2M</td>
<td>查找在/tmp 目录下大于2M的文件</td>
</tr>
<tr>
<td>find  /tmp  -size  -2M</td>
<td>查找在/tmp 目录下小于2M的文件</td>
</tr>
<tr>
<td>find  ./  -size  +4k  -size  -5M</td>
<td>查找当前目录下大于4k，小于5M的文件</td>
</tr>
<tr>
<td>find  ./  -perm  0777</td>
<td>查找当前目录下权限为 777 的文件或目录</td>
</tr>
<tr>
<td>find ./ -uers atguigu</td>
<td>查找当前目录下属于atguigu用户的所有文件</td>
</tr>
</tbody></table>
<p>-size单位：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">b —— 块（512字节）</span><br><span class="line">c —— 字节</span><br><span class="line">w —— 字（2字节）</span><br><span class="line">k —— 千字节</span><br><span class="line">M —— 兆字节</span><br><span class="line">G —— 吉字节</span><br></pre></td></tr></table></figure></li>
<li><p>locate</p>
<p>作用：快速定位文件路径，locate指令利用事先建立的系统中所有文件名称及路径的locate数据库实现快速定位给定的文件。Locate指令无需遍历整个文件系统，查询速度较快。为了保证查询结果的准确度，管理员必须定期更新locate时刻。</p>
<p>格式：locate 搜索文件</p>
<p>注意：由于locate指令基于数据库进行查询，所以第一次运行前，必须使用updatedb指令创建locate数据库。<br>注意：tmp目录不会简历索引</p>
</li>
<li><p><code>which</code></p>
<p>作用：查看命令位置</p>
</li>
<li><p><code>|</code></p>
<p>名称：管道</p>
<p>说明：一个命令的输出可以通过管道做为另一个命令的输入</p>
<p>简述：管道我们可以理解现实生活中的管子，管子的一头塞东西进去，另一头取出来，这里“ | ”的左右分为两端，左端塞东西(写)，右端取东西(读)</p>
</li>
<li><p><code>grep</code></p>
<p>作用：文本搜索，强大的文本搜索工具，grep允许对文本文件进行模式查找，如果找到匹配模式， grep打印包含模式的所有行</p>
<p>格式：grep  [-选项]  ‘搜索内容串’  文件名</p>
<p>注意：搜索内容串可以是正则表达式</p>
<p>选项：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-v</td>
<td>显示不包含匹配文本的所有行（相当于求反）</td>
</tr>
<tr>
<td>-n</td>
<td>显示匹配行及行号</td>
</tr>
<tr>
<td>-i</td>
<td>忽略大小写</td>
</tr>
</tbody></table>
<p>示例：<code>ps -aux | grep java</code> </p>
</li>
</ul>
<h3 id="9、解压和压缩"><a href="#9、解压和压缩" class="headerlink" title="9、解压和压缩"></a>9、解压和压缩</h3><ul>
<li><p><code>tar</code></p>
<p>作用：归档管理，计算机中的数据经常需要备份，tar是Unix/Linux中最常用的备份工具，此命令可以把一系列文件归档到一个大文件中，也可以把档案文件解开以恢复数据</p>
<p>格式：tar  [参数]  打包文件名  文件</p>
<p>参数：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-c</td>
<td>生成档案文件，创建打包文件</td>
</tr>
<tr>
<td>-v</td>
<td>列出归档解档的详细过程，显示进度</td>
</tr>
<tr>
<td>-f</td>
<td>指定档案文件名称，f后面一定是.tar文件，所以必须放选项最后</td>
</tr>
<tr>
<td>-t</td>
<td>列出档案中包含的文件</td>
</tr>
<tr>
<td>-x</td>
<td>解开档案文件</td>
</tr>
</tbody></table>
<p>注意：</p>
<p>​        参数前面可以使用“-”，也可以不使用</p>
<p>​        除了f需要放在参数的最后，其它参数的顺序任意</p>
</li>
<li><p><code>gzip</code></p>
<p>作用：tar与gzip命令结合使用实现文件打包、压缩。 tar只负责打包文件，但不压缩，用gzip压缩tar打包后的文件，其扩展名一般用xxxx.tar.gz</p>
<p>解压格式：gzip  [选项]  待解压文件</p>
<p>压缩格式：gzip  [选项]  被压缩文件  压缩后文件名</p>
<p>选项：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td>-r</td>
<td>压缩所有子目录</td>
<td>gzip  -r  1.tar  1.tar.gz</td>
</tr>
<tr>
<td>-d</td>
<td>解压</td>
<td>gzip  -d  1.tar.gz</td>
</tr>
</tbody></table>
<p>注意：tar这个命令并没有压缩的功能，它只是一个打包的命令，但是在tar命令中增加一个选项(-z)可以调用gzip实现了一个压缩的功能，实行一个先打包后压缩的过程</p>
<p>结合tar使用：</p>
<p>​        压缩：tar  -cvzf  1.tar.gz  *</p>
<p>​        解压：</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>tar  -xvzf  1.tar.gz</td>
<td>解压到当前目录</td>
</tr>
<tr>
<td>tar  -xvzf  1.tar.gz  -C  /temp</td>
<td>解压到指定目录</td>
</tr>
</tbody></table>
</li>
<li><p><code>bzip2</code></p>
<p>作用：tar与bzip2命令结合使用实现文件打包、压缩(用法和gzip一样)。tar只负责打包文件，但不压缩，用bzip2压缩tar打包后的文件，其扩展名一般用xxxx.tar.bz2。在tar命令中增加一个选项(-j)可以调用bzip2实现了一个压缩的功能，实行一个先打包后压缩的过程</p>
<p>结合tar使用：</p>
<p>压缩：tar  -jcvf  压缩包包名  文件…(tar  jcvf  bk.tar.bz2  *.c)</p>
<p>解压：tar  -jxvf  压缩包包名  (tar  jxvf  bk.tar.bz2)</p>
</li>
<li><p><code>zip、unzip</code></p>
<p>作用：通过zip压缩文件的目标文件不需要指定扩展名，默认扩展名为zip</p>
<p>压缩：zip  [-r]  目标文件(没有扩展名)  源文件</p>
<p>解压：unzip  -d  解压后目录文件  压缩文件</p>
</li>
</ul>
<h3 id="10、vi编辑器"><a href="#10、vi编辑器" class="headerlink" title="10、vi编辑器"></a>10、vi编辑器</h3><p><code>gedit</code>：是一个Linux环境下的文本编辑器，类似windows下的写字板程序，在不需要特别复杂的编程环境下，作为基本的文本编辑器比较合适</p>
<ul>
<li><p>作用</p>
<p>打开文件编辑并保存退出文件</p>
</li>
<li><p>打开文件</p>
<p>格式：vim  文件名</p>
<p>说明：如果文件不存在则先打开，当关闭保存时自动创建该文件</p>
<p>示例：</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>vim  sunck.txt</td>
<td>打开文件，光标在第一行</td>
</tr>
<tr>
<td>vim  sunck.txt  +5</td>
<td>打开文件，将光标移动到第四行<br />如果文件不存在则没有效果</td>
</tr>
<tr>
<td>vim sunck.txt +</td>
<td>打开文件，将光标移动到最后一行<br />如果文件不存在则没有效果</td>
</tr>
</tbody></table>
</li>
<li><p>模式</p>
<ul>
<li><p>命令模式</p>
<p>作用：可以实行特定命令，快速操作文本</p>
<p>进入与退出命令模式：打开文件即进入命令模式，按ESC即退出</p>
<p>移动光标命令：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>上、下、左、右方向键</td>
<td>移动光标</td>
</tr>
<tr>
<td>G</td>
<td>光标快速的定位到末行行首</td>
</tr>
<tr>
<td>$</td>
<td>光标快速定位到该行行尾</td>
</tr>
<tr>
<td>^</td>
<td>光标快速定位到该行行首</td>
</tr>
<tr>
<td>gg</td>
<td>光标快速定位到第一行行首</td>
</tr>
<tr>
<td>ngg</td>
<td>光标快速定位到第n行行首</td>
</tr>
<tr>
<td>M</td>
<td>光标移动到中间行</td>
</tr>
<tr>
<td>L</td>
<td>光标移动到屏幕最后一行行首</td>
</tr>
<tr>
<td>w</td>
<td>向后一次移动一个字</td>
</tr>
<tr>
<td>b</td>
<td>向前一次移动一个字</td>
</tr>
<tr>
<td>ctr+d、ctr+u</td>
<td>向下、上翻半屏</td>
</tr>
<tr>
<td>ctr+f、ctr+b</td>
<td>向下、上翻一屏</td>
</tr>
<tr>
<td>h、j、k、l</td>
<td>左、下、上、右移动光标</td>
</tr>
</tbody></table>
<p>删除命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>x</td>
<td>删除光标后一个字符</td>
</tr>
<tr>
<td>X</td>
<td>删除光标前一个字符</td>
</tr>
<tr>
<td>dd</td>
<td>删除光标所在行</td>
</tr>
<tr>
<td>ndd</td>
<td>删除指定的行数</td>
</tr>
<tr>
<td>d0</td>
<td>删除光标前本行所有内容,不包含光标所在字符</td>
</tr>
<tr>
<td>dw</td>
<td>删除光标开始位置的字,包含光标所在字符</td>
</tr>
</tbody></table>
<p>撤销命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>u</td>
<td>一步一步撤销</td>
</tr>
<tr>
<td>ctr+r</td>
<td>反撤销</td>
</tr>
</tbody></table>
<p>重复命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.</td>
<td>重复上一次操作的命令</td>
</tr>
</tbody></table>
<p>文本行移动命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>shift+&gt;&gt;</td>
<td>文本行右移</td>
</tr>
<tr>
<td>shift+&gt;&gt;</td>
<td>问本行左移</td>
</tr>
</tbody></table>
<p>复制粘贴命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>yy</td>
<td>复制当前行</td>
</tr>
<tr>
<td>nyy</td>
<td>复制n行</td>
</tr>
<tr>
<td>p</td>
<td>在光标所在位置向下新开辟一行,粘贴</td>
</tr>
</tbody></table>
<p>剪切粘贴命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>dd、ndd</td>
<td>删除命令相当于剪切</td>
</tr>
<tr>
<td>p</td>
<td>在光标所在位置向下新开辟一行,粘贴</td>
</tr>
</tbody></table>
<p>可视模式命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>v</td>
<td>按字符移动,选中文本</td>
</tr>
<tr>
<td>V</td>
<td>按行移动,选中文本可视模式可以配合 d, y, &gt;&gt;, &lt;&lt; 实现对文本块的删除,复制,左右移动</td>
</tr>
</tbody></table>
</li>
<li><p>输入模式</p>
<p>作用：向文件中输入内容</p>
<table>
<thead>
<tr>
<th>进入方式</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>按ESC后按a</td>
<td>从光标之后开始输入</td>
</tr>
<tr>
<td>按ESC后按A</td>
<td>在光标所在行的末尾开始输入</td>
</tr>
<tr>
<td>按ESC后按i</td>
<td>从光标之前开始输入</td>
</tr>
<tr>
<td>按ESC后按I</td>
<td>从光标所在行第一个非空字符开始输入</td>
</tr>
<tr>
<td>按ESC后按o</td>
<td>在光标所在行下一行，另起一行开始输入</td>
</tr>
<tr>
<td>按ESC后按O</td>
<td>在光标所在行上一行，另起一行开始输入</td>
</tr>
<tr>
<td>按ESC后按s</td>
<td>删除光标所在字符开始输入</td>
</tr>
<tr>
<td>按ESC后按S</td>
<td>删除光标所在行开始输入</td>
</tr>
</tbody></table>
</li>
<li><p>末行模式</p>
<p>作用：可以实行特定命令，可用于查找替换、保存退出等</p>
<p>进入方式：按ESC后按Shift+冒号</p>
<p>光标命令：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>n</td>
<td>光标跳转到第n行</td>
</tr>
</tbody></table>
<p>存储命令：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>w</td>
<td>保存文件</td>
</tr>
<tr>
<td>wq</td>
<td>保存并退出文件</td>
</tr>
<tr>
<td>x</td>
<td>保存并退出文件</td>
</tr>
<tr>
<td>!</td>
<td>表示强制</td>
</tr>
<tr>
<td>w!</td>
<td>强制保存</td>
</tr>
<tr>
<td>q!</td>
<td>强制退出</td>
</tr>
<tr>
<td>wq!</td>
<td>强制保存退出</td>
</tr>
</tbody></table>
<p>查找命令：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td>/</td>
<td>正向查找，按n查看下一个</td>
<td>/sunck</td>
</tr>
<tr>
<td>?</td>
<td>反向查找，按n查看上一个</td>
<td>?sunck</td>
</tr>
</tbody></table>
<p>替换命令：</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>s/sunck/kaige</td>
<td>将光标所在行的第一个sunck替换为kaige</td>
</tr>
<tr>
<td>s/sunck/kaige/g</td>
<td>将光标所在行的所有sunck替换为kaige</td>
</tr>
<tr>
<td>n,s/sunck/kaige</td>
<td>将指定行的第一个sunck替换为kaige</td>
</tr>
<tr>
<td>n,s/sunck/kaige/g</td>
<td>将指定行的所有sunck替换为kaige</td>
</tr>
<tr>
<td>%s/sunck/kaige</td>
<td>将每一行的第一个sunck替换为kaige</td>
</tr>
<tr>
<td>%s/sunck/kaige/g</td>
<td>将每一行的所有sunck替换为kaige</td>
</tr>
</tbody></table>
<p>设置命令：</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>set  nu</td>
<td>显示行号</td>
</tr>
<tr>
<td>set  nonu</td>
<td>取消显示行号</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>转换关系</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200212103923712.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200212103923712"></p>
</li>
<li><p>非法关闭处理</p>
<p>说明：当非法关闭正在编辑的文件的时候，再次打开文件会有提示信息</p>
<p>解决：</p>
<p>​        敲击enter：进入文件</p>
<p>​        保存上次写的内容：vim -r 1.txt</p>
<p>​        将产生的交换文件删除掉：rm .1.txt.swp</p>
</li>
<li><p>配置</p>
<table>
<thead>
<tr>
<th>打开文件</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>vim  ~/.vimrc</td>
<td>修改当前用户配置</td>
</tr>
<tr>
<td>sudo vim /etc/vim/vimrc</td>
<td>修改所有用户配置</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">syntax on</span><br><span class="line">set nu</span><br><span class="line">set autoindent</span><br><span class="line">set smartindent</span><br><span class="line">set tabstop=4</span><br><span class="line">set shiftwidth=4</span><br><span class="line">set showmatch</span><br><span class="line">set ruler</span><br><span class="line">set cindent</span><br><span class="line">set background=dark</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="11、用户、权限管理"><a href="#11、用户、权限管理" class="headerlink" title="11、用户、权限管理"></a>11、用户、权限管理</h3><ul>
<li><p>概述</p>
<ul>
<li>用户是Unix/Linux系统工作中重要的一环，用户管理包括用户与组账号的管理</li>
<li>在Unix/Linux系统中，不论是由本机或是远程登录系统，每个系统都必须拥有一个账号，并且对于不同的系统资源拥有不同的使用权限</li>
<li>Unix/Linux系统中的root账号通常用于系统的维护和管理，它对Unix/Linux操作系统的所有部分具有不受限制的访问权限</li>
<li>在Unix/Linux安装的过程中，系统会自动创建许多用户账号，而这些默认的用户就称为“标准用户”</li>
<li>在大多数版本的Unix/Linux中，都不推荐直接使用root账号登录系统</li>
</ul>
</li>
<li><p><code>whoami</code></p>
<p>作用：查看当前系统当前账号的用户名。可通过cat /etc/passwd查看系统用户信息</p>
</li>
<li><p><code>who</code></p>
<p>作用：查看当前所有登录系统的用户信息</p>
<p>选项：</p>
<table>
<thead>
<tr>
<th>选项值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-m或am  I</td>
<td>只显示运行who命令的用户名、登录终端和登录时间</td>
</tr>
<tr>
<td>-q或–count</td>
<td>只显示用户的登录账号和登录用户的数量</td>
</tr>
<tr>
<td>-u或–heading</td>
<td>显示列标题</td>
</tr>
</tbody></table>
</li>
<li><p><code>exit</code></p>
<table>
<thead>
<tr>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>如果是图形界面，退出当前终端</td>
</tr>
<tr>
<td>如果是使用ssh远程登录，退出登陆账户</td>
</tr>
<tr>
<td>如果是切换后的登陆用户，退出则返回上一个登陆账号</td>
</tr>
</tbody></table>
</li>
<li><p><code>groupadd</code></p>
<p>作用：新建组账号</p>
<p>格式：groupadd 组名</p>
</li>
<li><p><code>groupdel</code></p>
<p>作用：删除组账号</p>
<p>格式：groupdel 组名</p>
</li>
<li><p><code>useradd</code></p>
<p>作用：在Unix/Linux中添加用户账号可以使用adduser或useradd命令，因为adduser命令是指向useradd命令的一个链接，因此，这两个命令的使用格式完全一样</p>
<p>格式：useradd  [参数]  新建用户账号</p>
<table>
<thead>
<tr>
<th>参数值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-d</td>
<td>指定用户登录系统时的主目录，如果不使用该参数，系统自动在/home目录下建立与用户名同名目录为主目录</td>
</tr>
<tr>
<td>-m</td>
<td>自动建立目录</td>
</tr>
<tr>
<td>-g</td>
<td>指定组名称</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>使用说明</th>
</tr>
</thead>
<tbody><tr>
<td>Linux每个用户都要有一个主目录，主目录就是第一次登陆系统，用户的默认当前目录(/home/用户)</td>
</tr>
<tr>
<td>每一个用户必须有一个主目录，所以用useradd创建用户的时候，一定给用户指定一个主目录</td>
</tr>
<tr>
<td>用户的主目录一般要放到根目录的home目录下，用户的主目录和用户名是相同的</td>
</tr>
<tr>
<td>如果创建用户的时候，不指定组名，那么系统会自动创建一个和用户名一样的组名</td>
</tr>
</tbody></table>
</li>
<li><p><code>passwd</code></p>
<p>作用：在Unix/Linux中，超级用户可以使用passwd命令为普通用户设置或修改用户口令。用户也可以直接使用该命令来修改自己的口令，而无需在命令后面使用用户名</p>
</li>
<li><p><code>userdel</code></p>
<p>作用：删除用户</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>userdel  kaige</td>
<td>删除kaige用户，但不会自动删除用户的主目录</td>
</tr>
<tr>
<td>userdel  -r  kaige</td>
<td>删除用户，同时删除用户的主目录</td>
</tr>
</tbody></table>
</li>
<li><p><code>su</code></p>
<p>作用：切换用户</p>
<p>注意：su后面可以加“-”。su和su –命令不同之处在于，su -切换到对应的用户时会将当前的工作目录自动转换到切换后的用户主目录</p>
</li>
<li><p>查看有哪些用户组</p>
<p>cat  /etc/group</p>
<p>groupmod  +  三次tab键</p>
</li>
<li><p><code>groups</code></p>
<p>作用：查看用户在哪些组</p>
<p>示例：groups  sunck</p>
</li>
<li><p><code>usermod</code></p>
<p>作用：修改用户所在组</p>
<p>格式：usermod  选项  用户组  用户名</p>
<table>
<thead>
<tr>
<th>选项值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-g</td>
<td>用来制定这个用户默认的用户组</td>
</tr>
<tr>
<td>-G</td>
<td>一般配合’-a’来完成向其它组添加</td>
</tr>
</tbody></table>
</li>
<li><p>为创建的普通用户添加sudo权限</p>
<p>注意：新创建的用户，默认不能sudo，需要进行一下操作</p>
<p>操作：</p>
<ul>
<li>修改/etc/sudoers<img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200212105606513.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200212105606513"></li>
<li>需要强制保存退出</li>
</ul>
</li>
<li><p><code>chmod</code></p>
<p>作用：修改文件权限</p>
<p>权限：<img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200212105732765.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200212105732765"></p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>r</td>
<td>read 表示可读取，对于一个目录，如果没有r权限，那么就意味着不能通过ls查看这个目录的内容</td>
</tr>
<tr>
<td>w</td>
<td>write 表示可写入，对于一个目录，如果没有w权限，那么就意味着不能在目录下创建新的文件</td>
</tr>
<tr>
<td>x</td>
<td>excute 表示可执行，对于一个目录，如果没有x权限，那么就意味着不能通过cd进入这个目录</td>
</tr>
</tbody></table>
<p>修改:</p>
<ul>
<li><p>字母法</p>
<p>格式：chmod  u/g/o/a  +/-/=  rwx  文件</p>
<table>
<thead>
<tr>
<th>u/g/o/a</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>u</td>
<td>user 表示该文件的所有者</td>
</tr>
<tr>
<td>g</td>
<td>group 表示与该文件的所有者属于同一组( group )者，即用户组</td>
</tr>
<tr>
<td>o</td>
<td>other 表示其他以外的人</td>
</tr>
<tr>
<td>a</td>
<td>all 表示这三者皆是</td>
</tr>
</tbody></table>
<p>[ +-= ]说明：</p>
<table>
<thead>
<tr>
<th>+-=</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>+</td>
<td>增加权限</td>
</tr>
<tr>
<td>-</td>
<td>撤销权限</td>
</tr>
<tr>
<td>=</td>
<td>设定权限</td>
</tr>
</tbody></table>
</li>
<li><p>数字法</p>
<table>
<thead>
<tr>
<th>rwx-</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>r</td>
<td>读取权限，数字代号为 “4”</td>
</tr>
<tr>
<td>w</td>
<td>写入权限，数字代号为 “2”</td>
</tr>
<tr>
<td>x</td>
<td>执行权限，数字代号为 “1”</td>
</tr>
<tr>
<td>-</td>
<td>不具任何权限，数字代号为 “0”</td>
</tr>
</tbody></table>
<p>示例：chmod  751  file</p>
<p>说明：</p>
<p>​        文件所有者：读、写、执行权限</p>
<p>​        同组用户：读、执行的权限</p>
<p>​        其它用户：执行的权限</p>
</li>
</ul>
</li>
<li><p><code>chown</code></p>
<p>作用：修改文件所有者</p>
<p>格式：chown  新用户名  文件名</p>
</li>
<li><p><code>chgrp</code></p>
<p>作用：修改文件所属组</p>
<p>格式：chgrp  新组名  文件名</p>
</li>
</ul>
<h3 id="12、时间日期命令"><a href="#12、时间日期命令" class="headerlink" title="12、时间日期命令"></a>12、时间日期命令</h3><ul>
<li><p><code>cal</code></p>
<p>作用：查看当前日历</p>
<p>显示整年日历：cal  -y</p>
</li>
<li><p>date</p>
<p>作用：显示或设置时间</p>
<p><code>date  [MMDDhhmm[[CC]YY][.ss]] +format</code></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200212110626684.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200212110626684"></p>
<p>显示当前：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date</span></span><br><span class="line"><span class="number">2021</span>年 <span class="number">07</span>月 <span class="number">31</span>日 星期六 <span class="number">19</span>:<span class="number">42</span>:<span class="number">24</span> CST</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date +%Y</span></span><br><span class="line"><span class="number">2021</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date +%Y-%m-%d</span></span><br><span class="line"><span class="number">2021</span><span class="literal">-07</span><span class="literal">-31</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date +&quot;%Y-%m-%d %H:%M:%S&quot;</span></span><br><span class="line"><span class="number">2021</span><span class="literal">-07</span><span class="literal">-31</span> <span class="number">19</span>:<span class="number">43</span>:<span class="number">03</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure>

<p>显示非当前：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date -d &quot;1 days ago&quot;</span></span><br><span class="line"><span class="number">2021</span>年 <span class="number">07</span>月 <span class="number">30</span>日 星期五 <span class="number">19</span>:<span class="number">44</span>:<span class="number">16</span> CST</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date -d &quot;-1 days ago&quot;</span></span><br><span class="line"><span class="number">2021</span>年 <span class="number">08</span>月 <span class="number">01</span>日 星期日 <span class="number">19</span>:<span class="number">44</span>:<span class="number">25</span> CST</span><br></pre></td></tr></table></figure>

<p>设置：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date</span></span><br><span class="line"><span class="number">2021</span>年 <span class="number">07</span>月 <span class="number">31</span>日 星期六 <span class="number">19</span>:<span class="number">45</span>:<span class="number">40</span> CST</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date -s &quot;2020-11-23 20:12:03&quot;</span></span><br><span class="line"><span class="number">2020</span>年 <span class="number">11</span>月 <span class="number">23</span>日 星期一 <span class="number">20</span>:<span class="number">12</span>:<span class="number">03</span> CST</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date</span></span><br><span class="line"><span class="number">2020</span>年 <span class="number">11</span>月 <span class="number">23</span>日 星期一 <span class="number">20</span>:<span class="number">12</span>:<span class="number">05</span> CST</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date -s &quot;2021-07-31 11:46:45&quot;</span></span><br><span class="line"><span class="number">2021</span>年 <span class="number">07</span>月 <span class="number">31</span>日 星期六 <span class="number">11</span>:<span class="number">46</span>:<span class="number">45</span> CST</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date</span></span><br><span class="line"><span class="number">2021</span>年 <span class="number">07</span>月 <span class="number">31</span>日 星期六 <span class="number">11</span>:<span class="number">46</span>:<span class="number">47</span> CST</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="13、磁盘分区命令"><a href="#13、磁盘分区命令" class="headerlink" title="13、磁盘分区命令"></a>13、磁盘分区命令</h3><ul>
<li><p><code>df</code>（disk free）</p>
<p>作用：检测文件系统的磁盘空间占用和空余情况，可以显示所有文件系统对节点和磁盘块的使用情况（查看磁盘空间使用情况）</p>
<p>格式：df  选项</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>显示所有文件系统的磁盘使用情况</td>
</tr>
<tr>
<td>-m</td>
<td>以1024字节为单位显示</td>
</tr>
<tr>
<td>-T</td>
<td>显示文件系统</td>
</tr>
<tr>
<td>-h</td>
<td>以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示；</td>
</tr>
</tbody></table>
</li>
<li><p><code>fdisk</code></p>
<p>作用：查看磁盘分区详情</p>
<p>注意：该命令必须在root用户下才能使用</p>
<p>使用：<code>fdisk -l</code></p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-l</td>
<td>显示所有硬盘的分区列表</td>
</tr>
</tbody></table>
<p>Boot：引导</p>
<p>Start：从X磁柱开始</p>
<p>End：到Y磁柱结束</p>
<p>Blocks：容量</p>
<p>Id：分区类型ID</p>
<p>System：分区类型</p>
</li>
<li><p><code>lsblk</code></p>
<p>作用：查看设备挂载情况</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-f</td>
<td>查看详细的设备挂载情况，显示文件系统信息</td>
</tr>
</tbody></table>
</li>
<li><p><code>mount/umount</code></p>
<p>对于Linux用户来讲，不论有几个分区，分别分给哪一个目录使用，它总归就是一个根目录、一个独立且唯一的文件结构。Linux中每个分区都是用来组成整个文件系统的一部分，它在用一种叫做“挂载”的处理方法，它整个文件系统中包含了一整套的文件和目录，并将一个分区和一个目录联系起来，要载入的那个分区将使它的存储空间在这个目录下获得。</p>
<p>功能：挂载设备</p>
<p>格式：<code>mount [-t vfstype] [-o options] device dir</code></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-t vfstype</td>
<td>指定文件系统的类型，通常不必指定。mount 会自动选择正确的类型。常用类型有：  光盘或光盘镜像：iso9660  DOS fat16文件系统：msdos  <a href="http://blog.csdn.net/hancunai0017/article/details/6995284">Windows</a> 9x fat32文件系统：vfat  Windows NT ntfs文件系统：ntfs  Mount Windows文件<a href="http://blog.csdn.net/hancunai0017/article/details/6995284">网络</a>共享：smbfs  <a href="http://blog.csdn.net/hancunai0017/article/details/6995284">UNIX</a>(LINUX) 文件网络共享：nfs</td>
</tr>
<tr>
<td>-o options</td>
<td>主要用来描述设备或档案的挂接方式。常用的参数有：  loop：用来把一个文件当成硬盘分区挂接上系统  ro：采用只读方式挂接设备  rw：采用读写方式挂接设备  　 iocharset：指定访问文件系统所用字符集</td>
</tr>
<tr>
<td>device</td>
<td>要挂接(mount)的设备</td>
</tr>
<tr>
<td>dir</td>
<td>设备在系统上的挂接点(mount point)</td>
</tr>
</tbody></table>
<p>挂载U盘：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># ls /mnt/</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># mkdir /mnt/upan</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># ls /dev | grep sd</span></span><br><span class="line">sda</span><br><span class="line">sda1</span><br><span class="line">sda2</span><br></pre></td></tr></table></figure>

<p>虚拟机–&gt;可移动设备–&gt;设备名称–&gt;连接</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># ls /dev | grep sd</span></span><br><span class="line">sda</span><br><span class="line">sda1</span><br><span class="line">sda2</span><br><span class="line">sdb</span><br><span class="line">sdb1</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># ls /mnt/upan/</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># mount /dev/sdb1 /mnt/upan/</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># ls /mnt/upan/</span></span><br><span class="line"><span class="number">1</span>?Linux  bigData2105  System Volume Information</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># umount /dev/sdb1</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># ls /mnt/upan/</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># </span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="14、系统管理"><a href="#14、系统管理" class="headerlink" title="14、系统管理"></a>14、系统管理</h3><ul>
<li><p><code>ps</code></p>
<p>作用：查看进程信息</p>
<table>
<thead>
<tr>
<th>选项值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>显示终端上的所有进程，包括其他用户的进程</td>
</tr>
<tr>
<td>-u</td>
<td>显示进程的详细状态</td>
</tr>
<tr>
<td>-x</td>
<td>显示没有控制终端的进程</td>
</tr>
<tr>
<td>-w</td>
<td>显示加宽，以便显示更多的信息</td>
</tr>
<tr>
<td>-r</td>
<td>只显示正在运行的进程</td>
</tr>
</tbody></table>
<p>常用格式：</p>
<p><code>ps -aux</code>：查看系统中所有进程</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">USER：该进程是由哪个用户产生的</span><br><span class="line">PID：进程的ID号</span><br><span class="line">%CPU：该进程占用CPU资源的百分比，占用越高，进程越耗费资源；</span><br><span class="line">%MEM：该进程占用物理内存的百分比，占用越高，进程越耗费资源；</span><br><span class="line">VSZ：该进程占用虚拟内存的大小，单位KB；</span><br><span class="line">RSS：该进程占用实际物理内存的大小，单位KB；</span><br><span class="line">TTY：该进程是在哪个终端中运行的。其中tty1-tty7代表本地控制台终端，tty1-tty6是本地的字符界面终端，tty7是图形终端。pts/0-255代表虚拟终端。</span><br><span class="line">STAT：进程状态。常见的状态有：R：运行、S：睡眠、T：停止状态、s：包含子进程、+：位于后台</span><br><span class="line">START：该进程的启动时间</span><br><span class="line">TIME：该进程占用CPU的运算时间，注意不是系统时间</span><br><span class="line">COMMAND：产生此进程的命令名</span><br></pre></td></tr></table></figure>

<p><code>ps -ef</code>：可以查看子父进程之间的关系</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">UID：用户ID </span><br><span class="line">PID：进程ID </span><br><span class="line">PPID：父进程ID </span><br><span class="line">C：CPU用于计算执行优先级的因子。数值越大，表明进程是CPU密集型运算，执行优先级会降低；数值越小，表明进程是I/O密集型运算，执行优先级会提高 </span><br><span class="line">STIME：进程启动的时间 </span><br><span class="line">TTY：完整的终端名称 </span><br><span class="line">TIME：CPU时间 </span><br><span class="line">CMD：启动进程所用的命令和参数</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p><code>kill</code></p>
<p>作用：终止进程</p>
<p>格式：kill  [-signal]  pid</p>
<p>注意：信号值从0到15，其中9为绝对终止，可以处理一般信号无法终止的进程</p>
</li>
<li><p><code>pstree </code></p>
<p>作用：查看进程树</p>
<p>格式：pstree [选项]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-p</td>
<td>显示进程的PID</td>
</tr>
<tr>
<td>-u</td>
<td>显示进程的所属用户</td>
</tr>
</tbody></table>
</li>
<li><p><code>top</code></p>
<p>作用：动态显示进程</p>
<p>格式：top [选项]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-d 秒数</td>
<td>指定top命令每隔几秒更新。默认是3秒在top命令的交互模式当中可以执行的命令：</td>
</tr>
<tr>
<td>-i</td>
<td>使top不显示任何闲置或者僵死进程。</td>
</tr>
<tr>
<td>-p</td>
<td>通过指定监控进程ID来仅仅监控某个进程的状态。</td>
</tr>
</tbody></table>
<p>说明：能够在运行后，在指定的时间间隔更新显示信息。可以在使用top命令时加上-d 来指定显示信息更新的时间间隔</p>
<table>
<thead>
<tr>
<th>按键</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>M</td>
<td>根据内存使用量来排序</td>
</tr>
<tr>
<td>P</td>
<td>根据CPU占有率来排序</td>
</tr>
<tr>
<td>T</td>
<td>根据进程运行时间的长短来排序</td>
</tr>
<tr>
<td>U</td>
<td>可以根据后面输入的用户名来筛选进程</td>
</tr>
<tr>
<td>k</td>
<td>可以根据后面输入的PID来杀死进程</td>
</tr>
<tr>
<td>q</td>
<td>退出</td>
</tr>
<tr>
<td>h</td>
<td>获得帮助</td>
</tr>
</tbody></table>
<ol>
<li><p>第一行信息为任务队列信息</p>
<table>
<thead>
<tr>
<th align="left">内容</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">12:26:46</td>
<td>系统当前时间</td>
</tr>
<tr>
<td align="left">up 1 day, 13:32</td>
<td>系统的运行时间，本机已经运行1天  13小时32分钟</td>
</tr>
<tr>
<td align="left">2 users</td>
<td>当前登录了两个用户</td>
</tr>
<tr>
<td align="left">load average:   0.00, 0.00, 0.00</td>
<td>系统在之前1分钟，5分钟，15分钟的平均负载。一般认为小于1时，负载较小。如果大于1，系统已经超出负荷。</td>
</tr>
</tbody></table>
</li>
<li><p>第二行信息为任务队列信息</p>
<table>
<thead>
<tr>
<th></th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>Tasks: 95 total</td>
<td>系统中的进程总数</td>
</tr>
<tr>
<td>1 running</td>
<td>正在运行的进程数</td>
</tr>
<tr>
<td>94 sleeping</td>
<td>睡眠的进程</td>
</tr>
<tr>
<td>0 stopped</td>
<td>正在停止的进程</td>
</tr>
<tr>
<td>0 zombie</td>
<td>僵尸进程。如果不是0，需要手工检查僵尸进程</td>
</tr>
</tbody></table>
</li>
<li><p>第三信息为任务队列信息</p>
<table>
<thead>
<tr>
<th>内容</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>Cpu(s): 0.1%us</td>
<td>用户模式占用的CPU百分比</td>
</tr>
<tr>
<td>0.1%sy</td>
<td>系统模式占用的CPU百分比</td>
</tr>
<tr>
<td>0.0%ni</td>
<td>改变过优先级的用户进程占用的CPU百分比</td>
</tr>
<tr>
<td>99.7%id</td>
<td>空闲CPU的CPU百分比</td>
</tr>
<tr>
<td>0.1%wa</td>
<td>等待输入/输出的进程的占用CPU百分比</td>
</tr>
<tr>
<td>0.0%hi</td>
<td>硬中断请求服务占用的CPU百分比</td>
</tr>
<tr>
<td>0.1%si</td>
<td>软中断请求服务占用的CPU百分比</td>
</tr>
<tr>
<td>0.0%st</td>
<td>st（Steal time）虚拟时间百分比。就是当有虚拟机时，虚拟CPU等待实际CPU的时间百分比。</td>
</tr>
</tbody></table>
</li>
<li><p>第四行信息为任务队列信息</p>
<table>
<thead>
<tr>
<th>内容</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>Mem:  625344k total</td>
<td>物理内存的总量，单位KB</td>
</tr>
<tr>
<td>571504k used</td>
<td>已经使用的物理内存数量</td>
</tr>
<tr>
<td>53840k free</td>
<td>空闲的物理内存数量，我们使用的是虚拟机，总共只分配了628MB内存，所以只有53MB的空闲内存了</td>
</tr>
<tr>
<td>65800k buffers</td>
<td>作为缓冲的内存数量</td>
</tr>
</tbody></table>
</li>
<li><p>第五行为交换分区（swap）信息</p>
<table>
<thead>
<tr>
<th>Swap:  524280k total</th>
<th>交换分区（虚拟内存）的总大小</th>
</tr>
</thead>
<tbody><tr>
<td>0k used</td>
<td>已经使用的交互分区的大小</td>
</tr>
<tr>
<td>524280k free</td>
<td>空闲交换分区的大小</td>
</tr>
<tr>
<td>409280k cached</td>
<td>作为缓存的交互分区的大小</td>
</tr>
</tbody></table>
</li>
</ol>
</li>
<li><p><code>netstat</code></p>
<p>作用：显示网络统计信息和端口占用情况</p>
<ul>
<li><p><code>netstat -anp | grep 进程号</code>   </p>
<p>功能描述：查看该进程网络信息</p>
</li>
<li><p><code>netstat –nlp | grep 端口号</code>   </p>
<p>能描述：查看网络端口号占用情况</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-n</td>
<td>拒绝显示别名，能显示数字的全部转化成数字</td>
</tr>
<tr>
<td>-l</td>
<td>仅列出有在listen（监听）的服务状态</td>
</tr>
<tr>
<td>-p</td>
<td>表示显示哪个进程在调用</td>
</tr>
</tbody></table>
<p>查看某端口号是否被占用：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> <span class="type">etc</span>]<span class="comment"># netstat -nltp | grep 22</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">192.168</span>.<span class="number">122.1</span>:<span class="number">53</span>        <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">1384</span>/dnsmasq        </span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">22</span>              <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">1009</span>/sshd           </span><br><span class="line">tcp6       <span class="number">0</span>      <span class="number">0</span> :::<span class="number">22</span>                   :::*                    LISTEN      <span class="number">1009</span>/sshd </span><br></pre></td></tr></table></figure>

<p>通过进程号查看该进程的网络信息：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> <span class="type">etc</span>]<span class="comment"># netstat -anp | grep sshd</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">22</span>              <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">1009</span>/sshd           </span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">192.168</span>.<span class="number">100.5</span>:<span class="number">22</span>        <span class="number">192.168</span>.<span class="number">100.1</span>:<span class="number">52900</span>     ESTABLISHED <span class="number">1775</span>/sshd: root@pts </span><br><span class="line">tcp6       <span class="number">0</span>      <span class="number">0</span> :::<span class="number">22</span>                   :::*                    LISTEN      <span class="number">1009</span>/sshd           </span><br><span class="line">unix  <span class="number">3</span>      [ ]         STREAM     CONNECTED     <span class="number">23573</span>    <span class="number">1009</span>/sshd            </span><br><span class="line">unix  <span class="number">2</span>      [ ]         DGRAM                    <span class="number">31107</span>    <span class="number">1775</span>/sshd: root@pts  </span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p><code>reboot、shutdown、init</code></p>
<p>作用：关机重启</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>reboot</td>
<td>重新启动操作系统</td>
</tr>
<tr>
<td>shutdown –r now</td>
<td>重新启动操作系统，shutdown会给别的用户提示</td>
</tr>
<tr>
<td>shutdown -h now</td>
<td>立刻关机，其中now相当于时间为0的状态</td>
</tr>
<tr>
<td>shutdown -h 20:25</td>
<td>系统在今天的20:25 会关机</td>
</tr>
<tr>
<td>shutdown -h +10</td>
<td>系统再过十分钟后自动关机</td>
</tr>
<tr>
<td>init 0</td>
<td>关机</td>
</tr>
<tr>
<td>init 6</td>
<td>重启</td>
</tr>
</tbody></table>
</li>
</ul>
<h3 id="15、网络配置"><a href="#15、网络配置" class="headerlink" title="15、网络配置"></a>15、网络配置</h3><h3 id="16、系统定时任务"><a href="#16、系统定时任务" class="headerlink" title="16、系统定时任务"></a>16、系统定时任务</h3><ul>
<li><p>crond 服务</p>
<p><code>systemctl status crond</code></p>
<p><code>systemctl stop crond</code></p>
<p><code>systemctl start crond</code></p>
<p><code>systemctl restart crond</code></p>
</li>
<li><p>crontab 定时任务设置</p>
<p>语法：crontab [选项]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-e</td>
<td>编辑crontab定时任务</td>
</tr>
<tr>
<td>-l</td>
<td>查询crontab任务</td>
</tr>
<tr>
<td>-r</td>
<td>删除当前用户所有的crontab任务</td>
</tr>
</tbody></table>
<p>进入crontab编辑界面。会打开vim编辑你的工作</p>
<p>格式：* * * * * 执行的任务</p>
<table>
<thead>
<tr>
<th>项目</th>
<th>含义</th>
<th>范围</th>
</tr>
</thead>
<tbody><tr>
<td>第一个“*”</td>
<td>一小时当中的第几分钟</td>
<td>0-59</td>
</tr>
<tr>
<td>第二个“*”</td>
<td>一天当中的第几小时</td>
<td>0-23</td>
</tr>
<tr>
<td>第三个“*”</td>
<td>一个月当中的第几天</td>
<td>1-31</td>
</tr>
<tr>
<td>第四个“*”</td>
<td>一年当中的第几月</td>
<td>1-12</td>
</tr>
<tr>
<td>第五个“*”</td>
<td>一周当中的星期几</td>
<td>0-7（0和7都代表星期日）</td>
</tr>
</tbody></table>
<p>特殊符号:</p>
<table>
<thead>
<tr>
<th>特殊符号</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>*</td>
<td>代表任何时间。比如第一个“*”就代表一小时中每分钟都执行一次的意思。</td>
</tr>
<tr>
<td>，</td>
<td>代表不连续的时间。比如“0 8,12,16 * * * 命令”，就代表在每天的8点0分，12点0分，16点0分都执行一次命令</td>
</tr>
<tr>
<td>-</td>
<td>代表连续的时间范围。比如“0 5 *   * 1-6命令”，代表在周一到周六的凌晨5点0分执行命令</td>
</tr>
<tr>
<td>*/n</td>
<td>代表每隔多久执行一次。比如“*/10 *   * * * 命令”，代表每隔10分钟就执行一遍命令</td>
</tr>
</tbody></table>
<p>特定时间执行命令:</p>
<ul>
<li><p><code>45 22 * * * 命令</code></p>
<p>在22点45分执行命令</p>
</li>
<li><p><code>0 17 * * 1 命令</code></p>
<p>每周1 的17点0分执行命令</p>
</li>
<li><p><code>0 5 1,15 * * 命令</code></p>
<p>每月1号和15号的凌晨5点0分执行命令</p>
</li>
<li><p><code>40 4 * * 1-5 命令</code></p>
<p>每周一到周五的凌晨4点40分执行命令</p>
</li>
<li><p><code>*/10 4 * * * 命令</code></p>
<p>每天的凌晨4点，每隔10分钟执行一次命令</p>
</li>
<li><p><code>0 0 1,15 * 1 命令</code></p>
<p>每月1号和15号，每周1的0点0分都会执行命令。注意：星期几和几号最好不要同时出现，因为他们定义的都是天。非常容易让管理员混乱。</p>
</li>
</ul>
</li>
</ul>
<h3 id="17、软件包管理"><a href="#17、软件包管理" class="headerlink" title="17、软件包管理"></a>17、软件包管理</h3><h3 id="18、远程连接与拷贝"><a href="#18、远程连接与拷贝" class="headerlink" title="18、远程连接与拷贝"></a>18、远程连接与拷贝</h3><p>通常在工作过程中，公司中使用的真实服务器或者是云服务器，都不允许除运维人员之外的员工直接接触，因此就需要通过远程登录的方式来操作。所以，远程登录工具就是必不可缺的，目前，比较主流的有Xshell, SSH Secure Shell, SecureCRT,FinalShell等，同学们可以根据自己的习惯自行选择</p>
<ul>
<li><p>ssh</p>
<ul>
<li><p>概述</p>
<p>SSH为Secure Shell的缩写，由 IETF 的网络工作小组（Network Working Group）所制定；SSH 为建立在应用层和传输层基础上的安全协议</p>
<p>SSH是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议。常用于远程登录，以及用户之间进行资料拷贝</p>
<p>利用SSH协议可以有效防止远程管理过程中的信息泄露问题。SSH最初是 UNIX 系统上的一个程序，后来又迅速扩展到其他操作平台。SSH 在正确使用时可弥补网络中的漏洞。SSH 客户端适用于多种平台。几乎所有 UNIX 平台—包括 HP-UX、Linux、AIX、Solaris、Digital UNIX、Irix，以及其他平台，都可运行SSH</p>
<p>使用SSH服务，需要安装相应的服务器和客户端。客户端和服务器的关系：如果，A机器想被B机器远程控制，那么，A机器需要安装SSH服务器，B机器需要安装SSH客户端</p>
</li>
<li><p>远程连接</p>
<p>格式：ssh  用户名@IP</p>
<p>示例：ssh  <a href="mailto:&#x72;&#x6f;&#111;&#116;&#64;&#49;&#57;&#x32;&#46;&#49;&#x36;&#x38;&#46;&#48;&#x2e;&#x31;">&#x72;&#x6f;&#111;&#116;&#64;&#49;&#57;&#x32;&#46;&#49;&#x36;&#x38;&#46;&#48;&#x2e;&#x31;</a></p>
<p>注意：第一次连接会出现问题，输入yes后回车，以后ssh不会出现</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200212112656835.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200212112656835"></p>
<p>可能存在的问题：使用ssh访问，如访问出现错误。可查看是否有该文件 ～/.ssh/known_ssh 尝试删除该文件解决</p>
</li>
<li><p>注意</p>
<p>Windows可以通过使用Xshell实现远程连接Linux</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/20200212113130.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="2020-02-12_113130"></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/20200212113220.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="2020-02-12_113220"></p>
</li>
</ul>
</li>
<li><p>scp</p>
<p>作用：远程拷贝文件</p>
<p>本地文件复制到远程：scp  -r  本机文件的绝对路径或者相对路径  目标用户名@目标主机IP地址:目标文件的绝对路径</p>
<p>远程文件复制到本地：scp  -r  目标用户名@目标主机IP地址:目标文件的绝对路径  保存到本机的绝对路径或者相对路径</p>
<p>注意：Windows可以通过winscp实现远程拷贝</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/20200212113738.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="2020-02-12_113738"></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/20200212113751.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="2020-02-12_113751"></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/20200212113807.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="2020-02-12_113807"></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/20200212113832.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="2020-02-12_113832"></p>
</li>
</ul>
<h3 id="19、克隆虚拟机"><a href="#19、克隆虚拟机" class="headerlink" title="19、克隆虚拟机"></a>19、克隆虚拟机</h3><p><strong>创建基础系统(界面)</strong></p>
<ol>
<li><p>创建流程</p>
</li>
<li><p>切换到root用户</p>
</li>
<li><p>配置静态IP</p>
<p><code>vim /etc/sysconfig/network-scripts/ifcfg-ens33</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">TYPE=Ethernet</span><br><span class="line">PROXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=yes</span><br><span class="line">IPV6_AUTOCONF=yes</span><br><span class="line">IPV6_DEFROUTE=yes</span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line">IPV6_ADDR_GEN_MODE=stable-privacy</span><br><span class="line">NAME=ens33</span><br><span class="line">UUID=d901d918-f6af-48cc-845c-dfc0e3e41f71</span><br><span class="line">DEVICE=ens33</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=192.168.100.5</span><br><span class="line">GATEWAY=192.168.100.2</span><br><span class="line">DNS1=192.168.100.2</span><br></pre></td></tr></table></figure></li>
<li><p>修改主机名</p>
<p><code>vim /etc/hostname</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">centos7_base</span><br></pre></td></tr></table></figure></li>
<li><p>关闭防火墙</p>
<p><code>systemctl stop firewalld</code></p>
<p><code>systemctl disable firewalld</code></p>
</li>
<li><p>atguigu用户添加管理员权限</p>
<p><code>vim /etc/sudoers</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">%</span><span class="bash">wheel  ALL=(ALL)       ALL</span></span><br><span class="line">atguigu ALL=(ALL)       ALL</span><br></pre></td></tr></table></figure></li>
<li><p>添加vim配置</p>
<p><code>vim  ~/.vimrc</code>针对当前用户生效</p>
<p><code>vim /etc/vimrc</code>针对所有用户生效</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">syntax on</span><br><span class="line">set nu</span><br><span class="line">set autoindent</span><br><span class="line">set smartindent</span><br><span class="line">set tabstop=4</span><br><span class="line">set shiftwidth=4</span><br><span class="line">set showmatch</span><br><span class="line">set ruler</span><br><span class="line">set cindent</span><br><span class="line">set background=dark</span><br></pre></td></tr></table></figure></li>
<li><p>重启</p>
</li>
<li><p>安装epel-release</p>
<p><code>yum install -y epel-release</code></p>
</li>
</ol>
<p><strong>创建基础系统(最小化)</strong></p>
<ol>
<li><p>创建流程（使用最小化）</p>
</li>
<li><p>root用户登录</p>
</li>
<li><p>配置静态IP</p>
<p><code>vi /etc/sysconfig/network-scripts/ifcfg-ens33</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">TYPE=Ethernet</span><br><span class="line">PROXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=yes</span><br><span class="line">IPV6_AUTOCONF=yes</span><br><span class="line">IPV6_DEFROUTE=yes</span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line">IPV6_ADDR_GEN_MODE=stable-privacy</span><br><span class="line">NAME=ens33</span><br><span class="line">UUID=d901d918-f6af-48cc-845c-dfc0e3e41f71</span><br><span class="line">DEVICE=ens33</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=192.168.100.6</span><br><span class="line">GATEWAY=192.168.100.2</span><br><span class="line">DNS1=192.168.100.2</span><br></pre></td></tr></table></figure></li>
<li><p>修改主机名</p>
<p><code>vi /etc/hostname</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">centos7_mix</span><br></pre></td></tr></table></figure></li>
<li><p>关闭防火墙</p>
<p><code>systemctl stop firewalld</code></p>
<p><code>systemctl disable firewalld</code></p>
</li>
<li><p>atguigu用户添加管理员权限</p>
<p><code>vi /etc/sudoers</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">%</span><span class="bash">wheel  ALL=(ALL)       ALL</span></span><br><span class="line">atguigu ALL=(ALL)       ALL</span><br></pre></td></tr></table></figure></li>
<li><p>重启</p>
<p>reboot</p>
</li>
<li><p>安装epel-release</p>
<p><code>yum install -y epel-release</code></p>
</li>
<li><p>net-tool：工具包集合，包含ifconfig等命令</p>
<p><code>yum install -y net-tools</code></p>
</li>
<li><p>vim：编辑器</p>
<p><code>yum install -y vim</code></p>
</li>
<li><p>添加vim配置</p>
<p><code>vim  ~/.vimrc</code>针对当前用户生效</p>
<p><code>vim /etc/vimrc</code>针对所有用户生效</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">syntax on</span><br><span class="line">set nu</span><br><span class="line">set autoindent</span><br><span class="line">set smartindent</span><br><span class="line">set tabstop=4</span><br><span class="line">set shiftwidth=4</span><br><span class="line">set showmatch</span><br><span class="line">set ruler</span><br><span class="line">set cindent</span><br><span class="line">set background=dark</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>克隆虚拟机</strong></p>
<ol>
<li><code>su - root</code></li>
<li><code>vim /etc/sysconfig/network-scripts/ifcfg-ens33</code></li>
<li><code>vim /etc/hostname</code></li>
<li><code>vim /etc/hosts</code></li>
<li><code>reboot</code></li>
<li><code>C:\Windows\System32\drivers\etc</code></li>
</ol>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive</title>
    <url>/2021/11/07/Hive/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h1><h1 id="一、Hive基本概念"><a href="#一、Hive基本概念" class="headerlink" title="一、Hive基本概念"></a>一、Hive基本概念</h1><h2 id="1-1-什么是Hive"><a href="#1-1-什么是Hive" class="headerlink" title="1.1 什么是Hive"></a>1.1 什么是Hive</h2><p><a href="https://cloud.tencent.com/developer/article/1456786">通俗易懂理解hive是什么</a></p>
<ol>
<li>Hive简介<ul>
<li>Hive：由Facebook开源用于解决海量结构化日志的数据统计工具。</li>
<li>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。</li>
</ul>
</li>
<li>Hive本质：将HQL转化成MapReduce程序<ol>
<li>Hive处理的数据存储在HDFS</li>
<li>Hive分析数据底层的实现是MapReduce</li>
<li>执行程序运行在Yarn上<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346965217937.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ol>
</li>
</ol>
<h2 id="1-2Hive的优缺点"><a href="#1-2Hive的优缺点" class="headerlink" title="1.2Hive的优缺点"></a>1.2Hive的优缺点</h2><h3 id="1-2-1-优点"><a href="#1-2-1-优点" class="headerlink" title="1.2.1 优点"></a>1.2.1 优点</h3><ol>
<li>操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）。</li>
<li>避免了去写MapReduce，减少开发人员的学习成本。</li>
<li>Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合。</li>
<li>Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高。</li>
<li>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。<h3 id="1-2-2-缺点"><a href="#1-2-2-缺点" class="headerlink" title="1.2.2 缺点"></a>1.2.2 缺点</h3></li>
<li>Hive的HQL表达能力有限<ol>
<li>迭代式算法无法表达</li>
<li>数据挖掘方面不擅长，由于MapReduce数据处理流程的限制，效率更高的算法却无法实现。</li>
</ol>
</li>
<li>Hive的效率比较低<ol>
<li>Hive自动生成的MapReduce作业，通常情况下不够智能化</li>
<li>Hive调优比较困难，粒度较粗</li>
</ol>
</li>
</ol>
<h2 id="1-3-Hive架构原理"><a href="#1-3-Hive架构原理" class="headerlink" title="1.3 Hive架构原理"></a>1.3 Hive架构原理</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346968536473.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>用户接口：Client<ul>
<li>CLI（command-line interface）、JDBC/ODBC(jdbc访问hive)、WEBUI（浏览器访问hive）</li>
</ul>
</li>
<li>元数据：Metastore<ul>
<li>元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；</li>
<li>默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore</li>
</ul>
</li>
<li>Hadoop<ul>
<li>使用HDFS进行存储，使用MapReduce进行计算。</li>
</ul>
</li>
<li>驱动器：Driver<ul>
<li>解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</li>
<li>编译器（Physical Plan）：将AST编译生成逻辑执行计划。</li>
<li>优化器（Query Optimizer）：对逻辑执行计划进行优化。</li>
<li>执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。</li>
</ul>
</li>
</ol>
<h2 id="1-4-Hive和-数据库比较"><a href="#1-4-Hive和-数据库比较" class="headerlink" title="1.4 Hive和 数据库比较"></a>1.4 Hive和 数据库比较</h2><ul>
<li>由于 Hive 采用了类似SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。本文将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是Hive 是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。<h3 id="1-4-1-查询语言"><a href="#1-4-1-查询语言" class="headerlink" title="1.4.1 查询语言"></a>1.4.1 查询语言</h3></li>
<li>由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发。<h3 id="1-4-2-数据更新"><a href="#1-4-2-数据更新" class="headerlink" title="1.4.2 数据更新"></a>1.4.2 数据更新</h3></li>
<li>由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO …  VALUES 添加数据，使用 UPDATE … SET修改数据。<h3 id="1-4-3-执行延迟"><a href="#1-4-3-执行延迟" class="headerlink" title="1.4.3 执行延迟"></a>1.4.3 执行延迟</h3></li>
<li>Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce框架。由于MapReduce 本身具有较高的延迟，因此在利用MapReduce 执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。<h3 id="1-4-4-数据规模"><a href="#1-4-4-数据规模" class="headerlink" title="1.4.4 数据规模"></a>1.4.4 数据规模</h3></li>
<li>由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。</li>
</ul>
<h1 id="二、Hive安装"><a href="#二、Hive安装" class="headerlink" title="二、Hive安装"></a>二、Hive安装</h1><h2 id="2-1-Hive安装地址"><a href="#2-1-Hive安装地址" class="headerlink" title="2.1 Hive安装地址"></a>2.1 Hive安装地址</h2><ol>
<li>Hive官网地址<ul>
<li><a href="http://hive.apache.org/">http://hive.apache.org/</a></li>
</ul>
</li>
<li>文档查看地址<ul>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></li>
</ul>
</li>
<li>下载地址<ul>
<li><a href="http://archive.apache.org/dist/hive/">http://archive.apache.org/dist/hive/</a></li>
</ul>
</li>
<li>github地址<ul>
<li><a href="https://github.com/apache/hive">https://github.com/apache/hive</a></li>
</ul>
</li>
</ol>
<h2 id="2-2-MySql安装"><a href="#2-2-MySql安装" class="headerlink" title="2.2 MySql安装"></a>2.2 MySql安装</h2><ul>
<li>参考<a href="mweblib://16347028415943">Mysql安装</a></li>
</ul>
<h2 id="2-3-Hive安装部署"><a href="#2-3-Hive安装部署" class="headerlink" title="2.3 Hive安装部署"></a>2.3 Hive安装部署</h2><ol>
<li>把apache-hive-3.1.2-bin.tar.gz上传到linux的/opt/software目录下  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 software]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/software</span><br><span class="line">[atguigu@hadoop001 software]$ ll</span><br><span class="line">total 835216</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu 312850286 Apr 20  2020 apache-hive-3.1.2-bin.tar.gz</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu   9311744 Apr 20  2020 apache-zookeeper-3.5.7-bin.tar.gz</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu 338075860 Oct  9 12:59 hadoop-3.1.3.tar.gz</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu 195013152 Oct  9 12:59 jdk-8u212-linux-x64.tar.gz</span><br><span class="line">[atguigu@hadoop001 software]$</span><br></pre></td></tr></table></figure></li>
<li>解压apache-hive-3.1.2-bin.tar.gz到/opt/module/目录下面 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf /opt/software/apache-hive-3.1.2-bin.tar.gz -C /opt/module/</span><br><span class="line">···</span><br><span class="line">[atguigu@hadoop001 module]$ ll</span><br><span class="line">total 0</span><br><span class="line">drwxrwxr-x.  9 atguigu atguigu 153 Oct 20 13:55 apache-hive-3.1.2-bin</span><br><span class="line">drwxr-xr-x.  9 atguigu atguigu 149 Oct 19 14:29 hadoop-3.1.3</span><br><span class="line">drwxr-xr-x. 11 atguigu atguigu 173 Oct 19 14:45 ha-hadoop-3.1.3</span><br><span class="line">drwxr-xr-x.  7 atguigu atguigu 245 Apr  2  2019 jdk1.8.0_212</span><br><span class="line">drwxrwxr-x.  8 atguigu atguigu 160 Oct 18 13:44 zookeeper-3.5.7</span><br><span class="line">[atguigu@hadoop001 module]$</span><br></pre></td></tr></table></figure></li>
<li>修改apache-hive-3.1.2-bin.tar.gz的名称为hive<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 module]$ mv apache-hive-3.1.2-bin hive-3.1.2</span><br><span class="line">[atguigu@hadoop001 module]$ ll</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x.  9 atguigu atguigu 149 Oct 19 14:29 hadoop-3.1.3</span><br><span class="line">drwxr-xr-x. 11 atguigu atguigu 173 Oct 19 14:45 ha-hadoop-3.1.3</span><br><span class="line">drwxrwxr-x.  9 atguigu atguigu 153 Oct 20 13:55 hive-3.1.2</span><br><span class="line">drwxr-xr-x.  7 atguigu atguigu 245 Apr  2  2019 jdk1.8.0_212</span><br><span class="line">drwxrwxr-x.  8 atguigu atguigu 160 Oct 18 13:44 zookeeper-3.5.7</span><br><span class="line">[atguigu@hadoop001 module]$</span><br></pre></td></tr></table></figure></li>
<li>修改/etc/profile.d/set_env.sh，添加环境变量 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 module]$ cat /etc/profile.d/set_env.sh</span><br><span class="line"><span class="comment">#JAVA_HOME</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="comment">#HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/ha-hadoop-3.1.3</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line"></span><br><span class="line"><span class="comment">#HIVE_HOME</span></span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/opt/module/hive-3.1.2</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HIVE_HOME</span>/bin</span><br><span class="line">[atguigu@hadoop001 module]$</span><br></pre></td></tr></table></figure></li>
<li>解决日志Jar包冲突 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ ll <span class="variable">$HIVE_HOME</span>/lib/log4j-slf4j-impl-2.10.0.jar</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 24173 Apr 15  2020 /opt/module/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar</span><br><span class="line">[atguigu@hadoop001 ~]$ mv <span class="variable">$HIVE_HOME</span>/lib/log4j-slf4j-impl-2.10.0.jar <span class="variable">$HIVE_HOME</span>/lib/log4j-slf4j-impl-2.10.0.jar_bak</span><br><span class="line">[atguigu@hadoop001 ~]$ ll <span class="variable">$HIVE_HOME</span>/lib/log4j-slf4j-impl-2.10.0.jar_bak</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 24173 Apr 15  2020 /opt/module/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar_bak</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h2 id="2-4-Hive元数据配置到MySql"><a href="#2-4-Hive元数据配置到MySql" class="headerlink" title="2.4 Hive元数据配置到MySql"></a>2.4 Hive元数据配置到MySql</h2><h3 id="2-4-1-拷贝驱动"><a href="#2-4-1-拷贝驱动" class="headerlink" title="2.4.1 拷贝驱动"></a>2.4.1 拷贝驱动</h3></li>
<li>将MySQL的JDBC驱动拷贝到Hive的lib目录下 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 software]$ cp /opt/software/mysql-connector-java-8.0.25.jar <span class="variable">$HIVE_HOME</span>/lib</span><br><span class="line">[atguigu@hadoop001 software]$ ll <span class="variable">$HIVE_HOME</span>/lib/mysql-connector-java-8.0.25.jar</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu 2428320 Oct 20 14:14 /opt/module/hive-3.1.2/lib/mysql-connector-java-8.0.25.jar</span><br><span class="line">[atguigu@hadoop001 software]$</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-4-2-配置Metastore到MySql"><a href="#2-4-2-配置Metastore到MySql" class="headerlink" title="2.4.2 配置Metastore到MySql"></a>2.4.2 配置Metastore到MySql</h3><ol>
<li>在$HIVE_HOME/conf目录下新建hive-site.xml文件添加如下内容 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- jdbc连接的URL --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop102:3306/metastore?useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- jdbc连接的Driver--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- jdbc连接的username--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- jdbc连接的password --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Hive默认在HDFS的工作目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Hive元数据存储的验证 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 元数据存储授权  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.event.db.notification.api.auth<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="2-5-启动Hive"><a href="#2-5-启动Hive" class="headerlink" title="2.5 启动Hive"></a>2.5 启动Hive</h2><h3 id="2-5-1-初始化元数据库"><a href="#2-5-1-初始化元数据库" class="headerlink" title="2.5.1 初始化元数据库"></a>2.5.1 初始化元数据库</h3><ol>
<li>登陆MySQL创建Hive元数据库 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> Database           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> information_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mysql              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> performance_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> sys                <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="number">4</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> database metastore;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> Database           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> information_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> metastore          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mysql              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> performance_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> sys                <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>初始化Hive元数据库 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 conf]$ schematool -initSchema -dbType mysql -verbose</span><br><span class="line">Metastore connection URL:	 jdbc:mysql://mysql:3306/metastore?useSSL=<span class="literal">false</span></span><br><span class="line">Metastore Connection Driver :	 com.mysql.jdbc.Driver</span><br><span class="line">Metastore connection User:	 root</span><br><span class="line">Loading class `com.mysql.jdbc.Driver<span class="string">&#x27;. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver&#x27;</span>. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.</span><br><span class="line">Starting metastore schema initialization to 3.1.0</span><br><span class="line">Initialization script hive-schema-3.1.0.mysql.sql</span><br><span class="line">Connecting to jdbc:mysql://mysql:3306/metastore?useSSL=<span class="literal">false</span></span><br><span class="line">Connected to: MySQL (version 8.0.25)</span><br><span class="line">Driver: MySQL Connector/J (version mysql-connector-java-8.0.25 (Revision: 08be9e9b4cba6aa115f9b27b215887af40b159e0))</span><br><span class="line">Transaction isolation: TRANSACTION_READ_COMMITTED</span><br><span class="line">0: jdbc:mysql://mysql:3306/metastore&gt; !autocommit on</span><br><span class="line">Autocommit status: <span class="literal">true</span></span><br><span class="line">···</span><br><span class="line">···</span><br><span class="line">Closing: 0: jdbc:mysql://mysql:3306/metastore?useSSL=<span class="literal">false</span></span><br><span class="line">beeline&gt;</span><br><span class="line">beeline&gt; Initialization script completed</span><br><span class="line">schemaTool completed</span><br><span class="line">[atguigu@hadoop001 conf]$</span><br></pre></td></tr></table></figure></li>
<li>元数据相关表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">desc</span> dbs;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------+---------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="operator">|</span> Field           <span class="operator">|</span> Type          <span class="operator">|</span> <span class="keyword">Null</span> <span class="operator">|</span> Key <span class="operator">|</span> <span class="keyword">Default</span> <span class="operator">|</span> Extra <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------+---------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="operator">|</span> DB_ID           <span class="operator">|</span> <span class="type">bigint</span>        <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span> PRI <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">DESC</span>            <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">4000</span>) <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> DB_LOCATION_URI <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">4000</span>) <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> NAME            <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">128</span>)  <span class="operator">|</span> YES  <span class="operator">|</span> MUL <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> OWNER_NAME      <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">128</span>)  <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> OWNER_TYPE      <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">10</span>)   <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> CTLG_NAME       <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">256</span>)  <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span> MUL <span class="operator">|</span> hive    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------+---------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="number">7</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">desc</span> tbls;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+--------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="operator">|</span> Field              <span class="operator">|</span> Type         <span class="operator">|</span> <span class="keyword">Null</span> <span class="operator">|</span> Key <span class="operator">|</span> <span class="keyword">Default</span> <span class="operator">|</span> Extra <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+--------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="operator">|</span> TBL_ID             <span class="operator">|</span> <span class="type">bigint</span>       <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span> PRI <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> CREATE_TIME        <span class="operator">|</span> <span class="type">int</span>          <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> DB_ID              <span class="operator">|</span> <span class="type">bigint</span>       <span class="operator">|</span> YES  <span class="operator">|</span> MUL <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> LAST_ACCESS_TIME   <span class="operator">|</span> <span class="type">int</span>          <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> OWNER              <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">767</span>) <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> OWNER_TYPE         <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">10</span>)  <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> RETENTION          <span class="operator">|</span> <span class="type">int</span>          <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> SD_ID              <span class="operator">|</span> <span class="type">bigint</span>       <span class="operator">|</span> YES  <span class="operator">|</span> MUL <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> TBL_NAME           <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">256</span>) <span class="operator">|</span> YES  <span class="operator">|</span> MUL <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> TBL_TYPE           <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">128</span>) <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> VIEW_EXPANDED_TEXT <span class="operator">|</span> mediumtext   <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> VIEW_ORIGINAL_TEXT <span class="operator">|</span> mediumtext   <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> IS_REWRITE_ENABLED <span class="operator">|</span> bit(<span class="number">1</span>)       <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> b<span class="string">&#x27;0&#x27;</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+--------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="number">13</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">desc</span> sds;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+---------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="operator">|</span> Field                     <span class="operator">|</span> Type          <span class="operator">|</span> <span class="keyword">Null</span> <span class="operator">|</span> Key <span class="operator">|</span> <span class="keyword">Default</span> <span class="operator">|</span> Extra <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+---------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="operator">|</span> SD_ID                     <span class="operator">|</span> <span class="type">bigint</span>        <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span> PRI <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> CD_ID                     <span class="operator">|</span> <span class="type">bigint</span>        <span class="operator">|</span> YES  <span class="operator">|</span> MUL <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> INPUT_FORMAT              <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">4000</span>) <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> IS_COMPRESSED             <span class="operator">|</span> bit(<span class="number">1</span>)        <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> IS_STOREDASSUBDIRECTORIES <span class="operator">|</span> bit(<span class="number">1</span>)        <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> LOCATION                  <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">4000</span>) <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> NUM_BUCKETS               <span class="operator">|</span> <span class="type">int</span>           <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> OUTPUT_FORMAT             <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">4000</span>) <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> SERDE_ID                  <span class="operator">|</span> <span class="type">bigint</span>        <span class="operator">|</span> YES  <span class="operator">|</span> MUL <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+---------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="number">9</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-5-2-启动Hive"><a href="#2-5-2-启动Hive" class="headerlink" title="2.5.2 启动Hive"></a>2.5.2 启动Hive</h3></li>
<li>先启动hadoop集群</li>
<li>启动Hive <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 conf]$ hive</span><br><span class="line"><span class="built_in">which</span>: no hbase <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/ha-hadoop-3.1.3/bin:/opt/module/ha-hadoop-3.1.3/sbin:/opt/module/hive-3.1.2/bin:/home/atguigu/.<span class="built_in">local</span>/bin:/home/atguigu/bin)</span><br><span class="line">Hive Session ID = 6e003b50-4b21-418c-9983-d13aea1037a4</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> jar:file:/opt/module/hive-3.1.2/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: <span class="literal">true</span></span><br><span class="line">Loading class `com.mysql.jdbc.Driver<span class="string">&#x27;. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver&#x27;</span>. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.</span><br><span class="line">Hive-on-MR is deprecated <span class="keyword">in</span> Hive 2 and may not be available <span class="keyword">in</span> the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br><span class="line">Hive Session ID = 19720043-aa70-4ba3-8eb9-7ff94beb4379</span><br><span class="line">hive&gt; </span><br></pre></td></tr></table></figure></li>
<li>使用Hive <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line">OK</span><br><span class="line"><span class="keyword">default</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.58</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">show</span> tables;</span><br><span class="line">OK</span><br><span class="line">test</span><br><span class="line">test_2</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.029</span> seconds, Fetched: <span class="number">2</span> <span class="type">row</span>(s)</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> test_3 (id <span class="type">int</span>,name string,age <span class="type">int</span>);</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.346</span> seconds</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">show</span> tables;</span><br><span class="line">OK</span><br><span class="line">test</span><br><span class="line">test_2</span><br><span class="line">test_3</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.034</span> seconds, Fetched: <span class="number">3</span> <span class="type">row</span>(s)</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> test_3 <span class="keyword">values</span> (<span class="number">123</span>,<span class="string">&#x27;zhangsan&#x27;</span>,<span class="number">18</span>);</span><br><span class="line">Query ID <span class="operator">=</span> atguigu_20211020154850_d4a395ef<span class="operator">-</span>b85d<span class="number">-466</span>a<span class="number">-852</span>f<span class="operator">-</span>b6707a260960</span><br><span class="line">Total jobs <span class="operator">=</span> <span class="number">3</span></span><br><span class="line">Launching Job <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">3</span></span><br><span class="line">Number <span class="keyword">of</span> reduce tasks determined <span class="keyword">at</span> compile <span class="type">time</span>: <span class="number">1</span></span><br><span class="line"><span class="keyword">In</span> <span class="keyword">order</span> <span class="keyword">to</span> change the average load <span class="keyword">for</span> a reducer (<span class="keyword">in</span> bytes):</span><br><span class="line">  <span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer<span class="operator">=</span><span class="operator">&lt;</span>number<span class="operator">&gt;</span></span><br><span class="line"><span class="keyword">In</span> <span class="keyword">order</span> <span class="keyword">to</span> limit the maximum number <span class="keyword">of</span> reducers:</span><br><span class="line">  <span class="keyword">set</span> hive.exec.reducers.max<span class="operator">=</span><span class="operator">&lt;</span>number<span class="operator">&gt;</span></span><br><span class="line"><span class="keyword">In</span> <span class="keyword">order</span> <span class="keyword">to</span> <span class="keyword">set</span> a constant number <span class="keyword">of</span> reducers:</span><br><span class="line">  <span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="operator">&lt;</span>number<span class="operator">&gt;</span></span><br><span class="line">Starting Job <span class="operator">=</span> job_1634689350447_0003, Tracking URL <span class="operator">=</span> http:<span class="operator">/</span><span class="operator">/</span>hadoop003:<span class="number">8088</span><span class="operator">/</span>proxy<span class="operator">/</span>application_1634689350447_0003<span class="operator">/</span></span><br><span class="line">Kill Command <span class="operator">=</span> <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>ha<span class="operator">-</span>hadoop<span class="number">-3.1</span><span class="number">.3</span><span class="operator">/</span>bin<span class="operator">/</span>mapred job  <span class="operator">-</span>kill job_1634689350447_0003</span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage<span class="number">-1</span>: number <span class="keyword">of</span> mappers: <span class="number">1</span>; number <span class="keyword">of</span> reducers: <span class="number">1</span></span><br><span class="line"><span class="number">2021</span><span class="number">-10</span><span class="number">-20</span> <span class="number">15</span>:<span class="number">50</span>:<span class="number">00</span>,<span class="number">166</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">0</span><span class="operator">%</span>,  reduce <span class="operator">=</span> <span class="number">0</span><span class="operator">%</span></span><br><span class="line"><span class="number">2021</span><span class="number">-10</span><span class="number">-20</span> <span class="number">15</span>:<span class="number">50</span>:<span class="number">05</span>,<span class="number">319</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>,  reduce <span class="operator">=</span> <span class="number">0</span><span class="operator">%</span>, Cumulative CPU <span class="number">1.37</span> sec</span><br><span class="line"><span class="number">2021</span><span class="number">-10</span><span class="number">-20</span> <span class="number">15</span>:<span class="number">50</span>:<span class="number">10</span>,<span class="number">425</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>,  reduce <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>, Cumulative CPU <span class="number">2.49</span> sec</span><br><span class="line">MapReduce Total cumulative CPU <span class="type">time</span>: <span class="number">2</span> seconds <span class="number">490</span> msec</span><br><span class="line">Ended Job <span class="operator">=</span> job_1634689350447_0003</span><br><span class="line">Stage<span class="number">-4</span> <span class="keyword">is</span> selected <span class="keyword">by</span> <span class="keyword">condition</span> resolver.</span><br><span class="line">Stage<span class="number">-3</span> <span class="keyword">is</span> filtered <span class="keyword">out</span> <span class="keyword">by</span> <span class="keyword">condition</span> resolver.</span><br><span class="line">Stage<span class="number">-5</span> <span class="keyword">is</span> filtered <span class="keyword">out</span> <span class="keyword">by</span> <span class="keyword">condition</span> resolver.</span><br><span class="line">Moving data <span class="keyword">to</span> directory hdfs:<span class="operator">/</span><span class="operator">/</span>mycluster<span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>test_3<span class="operator">/</span>.hive<span class="operator">-</span>staging_hive_2021<span class="number">-10</span><span class="number">-20</span>_15<span class="number">-48</span><span class="number">-50</span>_976_8532078537182566570<span class="number">-1</span><span class="operator">/</span><span class="operator">-</span>ext<span class="number">-10000</span></span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> default.test_3</span><br><span class="line">MapReduce Jobs Launched:</span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-1</span>: Map: <span class="number">1</span>  Reduce: <span class="number">1</span>   Cumulative CPU: <span class="number">2.49</span> sec   HDFS Read: <span class="number">16281</span> HDFS Write: <span class="number">285</span> SUCCESS</span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">2</span> seconds <span class="number">490</span> msec</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">80.881</span> seconds</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> test_3;</span><br><span class="line">OK</span><br><span class="line"><span class="number">123</span>	zhangsan	<span class="number">18</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.109</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br><span class="line">hive<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-5-3-使用独立的元数据服务的方式访问Hive"><a href="#2-5-3-使用独立的元数据服务的方式访问Hive" class="headerlink" title="2.5.3 使用独立的元数据服务的方式访问Hive"></a>2.5.3 使用独立的元数据服务的方式访问Hive</h3><ol>
<li>在hive-site.xml文件中添加如下配置信息 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定存储元数据要连接的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://hadoop102:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>启动metastore <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 conf]$ hive --service metastore</span><br><span class="line">2021-10-20 16:19:53: Starting Hive Metastore Server</span><br><span class="line">Loading class `com.mysql.jdbc.Driver<span class="string">&#x27;. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver&#x27;</span>. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.</span><br></pre></td></tr></table></figure>
 注意: 启动后窗口不能再操作，需打开一个新的shell窗口做别的操作</li>
<li>启动 hive <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ hive</span><br><span class="line"><span class="built_in">which</span>: no hbase <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/ha-hadoop-3.1.3/bin:/opt/module/ha-hadoop-3.1.3/sbin:/opt/module/hive-3.1.2/bin:/home/atguigu/.<span class="built_in">local</span>/bin:/home/atguigu/bin)</span><br><span class="line">Hive Session ID = cc81e23a-657b-47e7-b211-bc4364e64fc2</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> jar:file:/opt/module/hive-3.1.2/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: <span class="literal">true</span></span><br><span class="line">Hive-on-MR is deprecated <span class="keyword">in</span> Hive 2 and may not be available <span class="keyword">in</span> the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br><span class="line">Hive Session ID = 60aa1b01-a2b8-4766-b9fc-4246f8c5fee6</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure>
<h3 id="2-5-4-使用JDBC方式访问Hive"><a href="#2-5-4-使用JDBC方式访问Hive" class="headerlink" title="2.5.4 使用JDBC方式访问Hive"></a>2.5.4 使用JDBC方式访问Hive</h3></li>
<li>在hive-site.xml文件中添加如下配置信息 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">    <span class="comment">&lt;!-- 指定hiveserver2连接的host --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- 指定hiveserver2连接的端口号 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    ```    </span><br><span class="line">2. 启动hiveserver2</span><br><span class="line">    ```bash</span><br><span class="line">    [atguigu@hadoop001 ~]$ hive --service hiveserver2</span><br><span class="line">    which: no hbase in (/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/ha-hadoop-3.1.3/bin:/opt/module/ha-hadoop-3.1.3/sbin:/opt/module/hive-3.1.2/bin:/home/atguigu/.local/bin:/home/atguigu/bin)</span><br><span class="line">    2021-10-20 16:23:25: Starting HiveServer2</span><br><span class="line">    Hive Session ID = 9191577f-34bc-478c-a4ae-8b0f2d20f945</span><br><span class="line">    Hive Session ID = 769ddf72-053a-4f0b-aa48-5723090f513b</span><br></pre></td></tr></table></figure></li>
<li>启动beeline客户端（需要多等待一会） <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ beeline -u jdbc:hive2://hadoop001:10000 -n atguigu</span><br><span class="line">Connecting to jdbc:hive2://hadoop001:10000</span><br><span class="line">Connected to: Apache Hive (version 3.1.2)</span><br><span class="line">Driver: Hive JDBC (version 3.1.2)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version 3.1.2 by Apache Hive</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; show databases;</span><br><span class="line">INFO  : Compiling <span class="built_in">command</span>(queryId=atguigu_20211020162645_fd2a8faf-9209-4bec-b36d-ec4c63bd8149): show databases</span><br><span class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</span><br><span class="line">INFO  : Semantic Analysis Completed (retrial = <span class="literal">false</span>)</span><br><span class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:database_name, <span class="built_in">type</span>:string, comment:from deserializer)], properties:null)</span><br><span class="line">INFO  : Completed compiling <span class="built_in">command</span>(queryId=atguigu_20211020162645_fd2a8faf-9209-4bec-b36d-ec4c63bd8149); Time taken: 0.589 seconds</span><br><span class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</span><br><span class="line">INFO  : Executing <span class="built_in">command</span>(queryId=atguigu_20211020162645_fd2a8faf-9209-4bec-b36d-ec4c63bd8149): show databases</span><br><span class="line">INFO  : Starting task [Stage-0:DDL] <span class="keyword">in</span> serial mode</span><br><span class="line">INFO  : Completed executing <span class="built_in">command</span>(queryId=atguigu_20211020162645_fd2a8faf-9209-4bec-b36d-ec4c63bd8149); Time taken: 0.016 seconds</span><br><span class="line">INFO  : OK</span><br><span class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</span><br><span class="line">+----------------+</span><br><span class="line">| database_name  |</span><br><span class="line">+----------------+</span><br><span class="line">| default        |</span><br><span class="line">+----------------+</span><br><span class="line">1 row selected (0.84 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-5-5-编写启动metastore和hiveserver2脚本"><a href="#2-5-5-编写启动metastore和hiveserver2脚本" class="headerlink" title="2.5.5 编写启动metastore和hiveserver2脚本"></a>2.5.5 编写启动metastore和hiveserver2脚本</h3><ol>
<li><p>前台启动的方式导致需要打开多个shell窗口，可以使用如下方式后台方式启动</p>
<ul>
<li>nohup: 放在命令开头，表示不挂起,也就是关闭终端进程也继续保持运行状态<ul>
<li>0:标准输入</li>
<li>1:标准输出</li>
<li>2:错误输出</li>
<li>2&gt;&amp;1 : 表示将错误重定向到标准输出上</li>
<li>&amp;: 放在命令结尾,表示后台运行</li>
<li>一般会组合使用: nohup  [xxx命令操作]&gt; file  2&gt;&amp;1 &amp;  ， 表示将xxx命令运行的<br>结果输出到file中，并保持命令启动的进程在后台运行。<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ nohup hive --service metastore 2&gt;&amp;1 &amp;</span><br><span class="line">[1] 19488</span><br><span class="line">[atguigu@hadoop001 ~]$ nohup: ignoring input and appending output to ‘nohup.out’</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop001 ~]$</span><br><span class="line">[atguigu@hadoop001 ~]$ nohup hive --service hiveserver2 2&gt;&amp;1 &amp;</span><br><span class="line">[2] 19603</span><br><span class="line">[atguigu@hadoop001 ~]$ nohup: ignoring input and appending output to ‘nohup.out’</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop001 ~]$</span><br><span class="line">[atguigu@hadoop001 ~]$ tail -f nohup.out</span><br><span class="line">2021-10-20 16:46:00: Starting Hive Metastore Server</span><br><span class="line">Loading class `com.mysql.jdbc.Driver<span class="string">&#x27;. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver&#x27;</span>. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.</span><br><span class="line"><span class="built_in">which</span>: no hbase <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/ha-hadoop-3.1.3/bin:/opt/module/ha-hadoop-3.1.3/sbin:/opt/module/hive-3.1.2/bin:/home/atguigu/.<span class="built_in">local</span>/bin:/home/atguigu/bin)</span><br><span class="line">2021-10-20 16:46:07: Starting HiveServer2</span><br><span class="line">Hive Session ID = e305ccdf-dd51-48e1-a740-fbc309de0a31</span><br><span class="line">Hive Session ID = b9a65f04-071e-493d-b89e-774daf7cac0f</span><br><span class="line">Hive Session ID = 985ef9a9-6d00-47f6-a0e4-178eed45c7f8</span><br><span class="line">Hive Session ID = dae41fea-63a9-4bda-84e6-c4e171fc0ecc</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
<li><p>编写脚本hiveservices.sh</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">HIVE_LOG_DIR=<span class="variable">$HIVE_HOME</span>/logs</span><br><span class="line"><span class="keyword">if</span> [ ! -d <span class="variable">$HIVE_LOG_DIR</span> ]; <span class="keyword">then</span></span><br><span class="line">    mkdir -p <span class="variable">$HIVE_LOG_DIR</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="comment">#检查进程是否运行正常，参数1为进程名，参数2为进程端口</span></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">check_process</span></span>() &#123;</span><br><span class="line">    pid=$(ps -ef 2&gt;/dev/null | grep -v grep | grep -i <span class="variable">$1</span> | awk <span class="string">&#x27;&#123;print $2&#125;&#x27;</span>)</span><br><span class="line">    ppid=$(netstat -nltp 2&gt;/dev/null | grep <span class="variable">$2</span> | awk <span class="string">&#x27;&#123;print $7&#125;&#x27;</span> | cut -d <span class="string">&#x27;/&#x27;</span> -f 1)</span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$pid</span></span><br><span class="line">    [[ <span class="string">&quot;<span class="variable">$pid</span>&quot;</span> =~ <span class="string">&quot;<span class="variable">$ppid</span>&quot;</span> ]] &amp;&amp; [ <span class="string">&quot;<span class="variable">$ppid</span>&quot;</span> ] &amp;&amp; <span class="built_in">return</span> 0 || <span class="built_in">return</span> 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">hive_start</span></span>() &#123;</span><br><span class="line">    metapid=$(check_process HiveMetastore 9083)</span><br><span class="line">    cmd=<span class="string">&quot;nohup hive --service metastore &gt;<span class="variable">$HIVE_LOG_DIR</span>/metastore.log 2&gt;&amp;1 &amp;&quot;</span></span><br><span class="line">    cmd=<span class="variable">$cmd</span><span class="string">&quot; sleep 4; hdfs dfsadmin -safemode wait &gt;/dev/null 2&gt;&amp;1&quot;</span></span><br><span class="line">    [ -z <span class="string">&quot;<span class="variable">$metapid</span>&quot;</span> ] &amp;&amp; <span class="built_in">eval</span> <span class="variable">$cmd</span> || <span class="built_in">echo</span> <span class="string">&quot;Metastroe服务已启动&quot;</span></span><br><span class="line">    server2pid=$(check_process HiveServer2 10000)</span><br><span class="line">    cmd=<span class="string">&quot;nohup hive --service hiveserver2 &gt;<span class="variable">$HIVE_LOG_DIR</span>/hiveServer2.log 2&gt;&amp;1 &amp;&quot;</span></span><br><span class="line">    [ -z <span class="string">&quot;<span class="variable">$server2pid</span>&quot;</span> ] &amp;&amp; <span class="built_in">eval</span> <span class="variable">$cmd</span> || <span class="built_in">echo</span> <span class="string">&quot;HiveServer2服务已启动&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">hive_stop</span></span>() &#123;</span><br><span class="line">    metapid=$(check_process HiveMetastore 9083)</span><br><span class="line">    [ <span class="string">&quot;<span class="variable">$metapid</span>&quot;</span> ] &amp;&amp; <span class="built_in">kill</span> <span class="variable">$metapid</span> || <span class="built_in">echo</span> <span class="string">&quot;Metastore服务未启动&quot;</span></span><br><span class="line">    server2pid=$(check_process HiveServer2 10000)</span><br><span class="line">    [ <span class="string">&quot;<span class="variable">$server2pid</span>&quot;</span> ] &amp;&amp; <span class="built_in">kill</span> <span class="variable">$server2pid</span> || <span class="built_in">echo</span> <span class="string">&quot;HiveServer2服务未启动&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">&quot;start&quot;</span>)</span><br><span class="line">    hive_start</span><br><span class="line">    ;;</span><br><span class="line"><span class="string">&quot;stop&quot;</span>)</span><br><span class="line">    hive_stop</span><br><span class="line">    ;;</span><br><span class="line"><span class="string">&quot;restart&quot;</span>)</span><br><span class="line">    hive_stop</span><br><span class="line">    sleep 2</span><br><span class="line">    hive_start</span><br><span class="line">    ;;</span><br><span class="line"><span class="string">&quot;status&quot;</span>)</span><br><span class="line">    check_process HiveMetastore 9083 &gt;/dev/null &amp;&amp; <span class="built_in">echo</span> <span class="string">&quot;Metastore服务运行正常&quot;</span> || <span class="built_in">echo</span> <span class="string">&quot;Metastore服务运行异常&quot;</span></span><br><span class="line">    check_process HiveServer2 10000 &gt;/dev/null &amp;&amp; <span class="built_in">echo</span> <span class="string">&quot;HiveServer2服务运行正常&quot;</span> || <span class="built_in">echo</span> <span class="string">&quot;HiveServer2服务运行异常&quot;</span></span><br><span class="line">    ;;</span><br><span class="line">*)</span><br><span class="line">    <span class="built_in">echo</span> Invalid Args!</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&#x27;Usage: &#x27;</span>$(basename <span class="variable">$0</span>)<span class="string">&#x27; start|stop|restart|status&#x27;</span></span><br><span class="line">    ;;</span><br><span class="line"><span class="keyword">esac</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>添加执行权限</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 bin]$ ll</span><br><span class="line">total 20</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 1723 Oct 20 17:02 hive_server.sh</span><br><span class="line">[atguigu@hadoop001 bin]$ chmod a+x hive_server.sh</span><br><span class="line">[atguigu@hadoop001 bin]$ ll</span><br><span class="line">total 20</span><br><span class="line">-rwxrwxr-x. 1 atguigu atguigu 1723 Oct 20 17:02 hive_server.sh</span><br><span class="line">[atguigu@hadoop001 bin]$</span><br></pre></td></tr></table></figure></li>
<li><p>启动Hive后台服务</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 bin]$ hive_server.sh start</span><br><span class="line">[atguigu@hadoop001 bin]$ hive_server.sh status</span><br><span class="line">Metastore服务运行正常</span><br><span class="line">HiveServer2服务运行正常</span><br><span class="line">[atguigu@hadoop001 bin]$</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="2-6-Hive常用交互命令"><a href="#2-6-Hive常用交互命令" class="headerlink" title="2.6 Hive常用交互命令"></a>2.6 Hive常用交互命令</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hive]$ hive -<span class="built_in">help</span></span><br><span class="line"><span class="built_in">which</span>: no hbase <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/ha-hadoop-3.1.3/bin:/opt/module/ha-hadoop-3.1.3/sbin:/opt/module/hive-3.1.2/bin:/home/atguigu/.<span class="built_in">local</span>/bin:/home/atguigu/bin)</span><br><span class="line">Hive Session ID = ccd1a72b-c24a-4b49-9583-1601fa1f7a34</span><br><span class="line">usage: hive</span><br><span class="line"> -d,--define &lt;key=value&gt;          Variable substitution to apply to Hive</span><br><span class="line">                                  commands. e.g. -d A=B or --define A=B</span><br><span class="line">    --database &lt;databasename&gt;     Specify the database to use</span><br><span class="line"> -e &lt;quoted-query-string&gt;         SQL from <span class="built_in">command</span> line</span><br><span class="line"> -f &lt;filename&gt;                    SQL from files</span><br><span class="line"> -H,--<span class="built_in">help</span>                        Print <span class="built_in">help</span> information</span><br><span class="line">    --hiveconf &lt;property=value&gt;   Use value <span class="keyword">for</span> given property</span><br><span class="line">    --hivevar &lt;key=value&gt;         Variable substitution to apply to Hive</span><br><span class="line">                                  commands. e.g. --hivevar A=B</span><br><span class="line"> -i &lt;filename&gt;                    Initialization SQL file</span><br><span class="line"> -S,--silent                      Silent mode <span class="keyword">in</span> interactive shell</span><br><span class="line"> -v,--verbose                     Verbose mode (<span class="built_in">echo</span> executed SQL to the</span><br><span class="line">                                  console)</span><br><span class="line">[atguigu@hadoop001 hive]$</span><br></pre></td></tr></table></figure>
<ol>
<li>“-e”不进入hive的交互窗口执行sql语句 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hive]$ hive -e <span class="string">&#x27;select count(1) from test_3&#x27;</span></span><br><span class="line"><span class="built_in">which</span>: no hbase <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/ha-hadoop-3.1.3/bin:/opt/module/ha-hadoop-3.1.3/sbin:/opt/module/hive-3.1.2/bin:/home/atguigu/.<span class="built_in">local</span>/bin:/home/atguigu/bin)</span><br><span class="line">Hive Session ID = 8eeed838-5d5e-4ed4-a252-29f85e7f15f3</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> jar:file:/opt/module/hive-3.1.2/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: <span class="literal">true</span></span><br><span class="line">Hive Session ID = c1c58f1c-71c4-4869-9f5e-8dc2a4ac52f4</span><br><span class="line">OK</span><br><span class="line">32</span><br><span class="line">Time taken: 1.972 seconds, Fetched: 1 row(s)</span><br><span class="line">[atguigu@hadoop001 hive]$</span><br></pre></td></tr></table></figure></li>
<li>-f”执行脚本中sql语句 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hive]$ ls</span><br><span class="line">selectall.sql</span><br><span class="line"><span class="comment"># 编写sql脚本</span></span><br><span class="line">[atguigu@hadoop001 hive]$ cat selectall.sql</span><br><span class="line">select count(1) from test_3</span><br><span class="line"><span class="comment"># 使用-f执行脚本</span></span><br><span class="line">[atguigu@hadoop001 hive]$ hive -f selectall.sql</span><br><span class="line"><span class="built_in">which</span>: no hbase <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/ha-hadoop-3.1.3/bin:/opt/module/ha-hadoop-3.1.3/sbin:/opt/module/hive-3.1.2/bin:/home/atguigu/.<span class="built_in">local</span>/bin:/home/atguigu/bin)</span><br><span class="line">Hive Session ID = 2b43ecf4-1fd8-4bd1-be10-60adaad0ffa4</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> jar:file:/opt/module/hive-3.1.2/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: <span class="literal">true</span></span><br><span class="line">Hive Session ID = a07efeea-d038-4ca8-b342-7b001f1493d7</span><br><span class="line">OK</span><br><span class="line">32</span><br><span class="line">Time taken: 1.954 seconds, Fetched: 1 row(s)</span><br><span class="line">[atguigu@hadoop001 hive]$</span><br><span class="line"><span class="comment"># 执行并导出文件</span></span><br><span class="line">[atguigu@hadoop001 hive]$ hive -f selectall.sql &gt; selectall.txt</span><br><span class="line"><span class="built_in">which</span>: no hbase <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/ha-hadoop-3.1.3/bin:/opt/module/ha-hadoop-3.1.3/sbin:/opt/module/hive-3.1.2/bin:/home/atguigu/.<span class="built_in">local</span>/bin:/home/atguigu/bin)</span><br><span class="line">Hive Session ID = 13cd6a52-4af8-4234-86bf-70841d565b98</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> jar:file:/opt/module/hive-3.1.2/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: <span class="literal">true</span></span><br><span class="line">Hive Session ID = 9279ee4e-997c-4a92-b258-11ac8c0beaa6</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.953 seconds, Fetched: 1 row(s)</span><br><span class="line">[atguigu@hadoop001 hive]$ cat selectall.txt</span><br><span class="line">32</span><br><span class="line">[atguigu@hadoop001 hive]$</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="2-7-Hive其他命令操作"><a href="#2-7-Hive其他命令操作" class="headerlink" title="2.7 Hive其他命令操作"></a>2.7 Hive其他命令操作</h2><ol>
<li>退出hive窗口： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive(default)&gt;<span class="built_in">exit</span>;</span><br><span class="line">hive(default)&gt;quit;</span><br></pre></td></tr></table></figure>
<ul>
<li>在新版的hive中没区别了，在以前的版本是有的：</li>
<li>exit:先隐性提交数据，再退出；</li>
<li>quit:不提交数据，退出；</li>
</ul>
</li>
<li>在hive cli命令窗口中如何查看hdfs文件系统 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive&gt; dfs -ls /user;</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-10-20 14:52 /user/hive</span><br><span class="line">hive&gt; dfs -ls /user/hive;</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-10-20 15:48 /user/hive/warehouse</span><br><span class="line">hive&gt; dfs -ls /user/hive/warehouse;</span><br><span class="line">Found 3 items</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-10-20 15:40 /user/hive/warehouse/<span class="built_in">test</span></span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-10-20 15:43 /user/hive/warehouse/test_2</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-10-22 15:06 /user/hive/warehouse/test_3</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure></li>
<li>查看在hive中输入的所有历史命令,用户home目录下.hivehistory文件,~/.hivehistory <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ head -n 10 ~/.hivehistory</span><br><span class="line">show databases;</span><br><span class="line">create table <span class="built_in">test</span> (id int);</span><br><span class="line">show database;</span><br><span class="line">show databases;</span><br><span class="line">ls</span><br><span class="line">show databases;</span><br><span class="line">quit;</span><br><span class="line">show databases;</span><br><span class="line">quit</span><br><span class="line">;</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="2-8-Hive常见属性配置"><a href="#2-8-Hive常见属性配置" class="headerlink" title="2.8 Hive常见属性配置"></a>2.8 Hive常见属性配置</h2><h3 id="2-8-1-hive窗口打印默认库和表头"><a href="#2-8-1-hive窗口打印默认库和表头" class="headerlink" title="2.8.1 hive窗口打印默认库和表头"></a>2.8.1 hive窗口打印默认库和表头</h3><ol>
<li>打印 当前库 和 表头 hive-site.xml中加入如下两个配置:  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> test_2;</span><br><span class="line">OK</span><br><span class="line">test_2.id	test_2.name</span><br><span class="line"><span class="number">123</span>	zhangsan</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.106</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-8-2-Hive运行日志信息配置"><a href="#2-8-2-Hive运行日志信息配置" class="headerlink" title="2.8.2 Hive运行日志信息配置"></a>2.8.2 Hive运行日志信息配置</h3></li>
<li>Hive的log默认存放在/tmp/atguigu/hive.log目录下（当前用户名下）</li>
<li>修改hive的log存放日志到/opt/module/hive-3.1.2/logs<ol>
<li>修改/opt/module/hive/conf/hive-log4j2.properties.template文件名称为hive-log4j2.properties <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hive/conf</span><br><span class="line">[atguigu@hadoop102 conf]$ mv hive-log4j2.properties.template hive-log4j2.properties</span><br></pre></td></tr></table></figure></li>
<li>在hive-log4j.properties文件中修改log存放位置 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">property.hive.log.dir=/opt/module/hive/logs</span><br></pre></td></tr></table></figure>
<h3 id="2-8-2-参数配置方式"><a href="#2-8-2-参数配置方式" class="headerlink" title="2.8.2 参数配置方式"></a>2.8.2 参数配置方式</h3></li>
</ol>
</li>
<li>查看当前所有的配置信息 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span><span class="keyword">set</span>;</span><br></pre></td></tr></table></figure></li>
<li>参数的配置三种方式<ol>
<li>配置文件方式<ul>
<li>默认配置文件：hive-default.xml </li>
<li>用户自定义配置文件：hive-site.xml</li>
<li>注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。</li>
</ul>
</li>
<li>命令行参数方式<ul>
<li>启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。</li>
<li>例如：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>注意：仅对本次hive启动有效。查看参数设置：  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapred.reduce.tasks;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>参数声明方式<ul>
<li>可以在HQL中使用SET关键字设定参数</li>
<li>例如：  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapred.reduce.tasks<span class="operator">=</span><span class="number">100</span>;</span><br></pre></td></tr></table></figure></li>
<li>注意：仅对本次hive启动有效。查看参数设置  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapred.reduce.tasks;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>上述三种设定方式的优先级依次递增。即default配置文件&lt;hive-site.x配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</li>
</ol>
</li>
</ol>
<h1 id="三、Hive数据类型"><a href="#三、Hive数据类型" class="headerlink" title="三、Hive数据类型"></a>三、Hive数据类型</h1><h2 id="3-1-基本数据类型"><a href="#3-1-基本数据类型" class="headerlink" title="3.1 基本数据类型"></a>3.1 基本数据类型</h2><table>
<thead>
<tr>
<th>Hive数据类型</th>
<th>Java数据类型</th>
<th>长度</th>
</tr>
</thead>
<tbody><tr>
<td>TINYINT</td>
<td>byte</td>
<td>1byte有符号整数</td>
</tr>
<tr>
<td>SMALINT</td>
<td>short</td>
<td>2byte有符号整数</td>
</tr>
<tr>
<td>INT</td>
<td>int</td>
<td>4byte有符号整数</td>
</tr>
<tr>
<td>BIGINT</td>
<td>long</td>
<td>8byte有符号整数</td>
</tr>
<tr>
<td>BOOLEAN</td>
<td>boolean</td>
<td>布尔类型</td>
</tr>
<tr>
<td>FLOAT</td>
<td>float</td>
<td>单精度浮点数</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>double</td>
<td>双精度浮点数</td>
</tr>
<tr>
<td>STRING</td>
<td>string</td>
<td>字符系列。可以指定字符集。使用单引号或者双引号</td>
</tr>
<tr>
<td>TIMESTAMP</td>
<td>Dat</td>
<td>时间类型</td>
</tr>
<tr>
<td>BINARY</td>
<td></td>
<td>字节数组</td>
</tr>
</tbody></table>
<ul>
<li>对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数</li>
</ul>
<h2 id="3-2-集合数据类型"><a href="#3-2-集合数据类型" class="headerlink" title="3.2 集合数据类型"></a>3.2 集合数据类型</h2><table>
<thead>
<tr>
<th>数据类型</th>
<th>描述</th>
<th>语法示例</th>
</tr>
</thead>
<tbody><tr>
<td>STRUCT</td>
<td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td>
<td>struct()<br> 例如struct&lt;street:string, city:string&gt;</td>
</tr>
<tr>
<td>MAP</td>
<td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td>
<td>map()<br>例如map&lt;string, int&gt;</td>
</tr>
<tr>
<td>ARRAY</td>
<td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td>
<td>Array()<br>例如array<string></td>
</tr>
</tbody></table>
<ul>
<li>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套</li>
</ul>
<ol>
<li>数据类型实操<ol>
<li>数据示例 <figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;songsong&quot;</span>,</span><br><span class="line">    </span><br><span class="line">    <span class="attr">&quot;friends&quot;</span>: [</span><br><span class="line">        <span class="string">&quot;bingbing&quot;</span>,</span><br><span class="line">        <span class="string">&quot;lili&quot;</span></span><br><span class="line">    ], </span><br><span class="line">    <span class="attr">&quot;children&quot;</span>: &#123; </span><br><span class="line">        <span class="attr">&quot;xiao song&quot;</span>: <span class="number">19</span>,</span><br><span class="line">        <span class="attr">&quot;xiaoxiao song&quot;</span>: <span class="number">18</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;address&quot;</span>: &#123; </span><br><span class="line">        <span class="attr">&quot;street&quot;</span>: <span class="string">&quot;hui long guan&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;city&quot;</span>: <span class="string">&quot;beijing&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;zip&quot;</span>: <span class="number">1000001</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>基于上述数据结构，在Hive里创建对应的表，并导入数据 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">test.txt</span><br><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing_1000001</span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing_1000001</span><br></pre></td></tr></table></figure>
<ul>
<li>注意：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。</li>
</ul>
</li>
<li>Hive上创建测试表user <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> person_info (</span><br><span class="line">    name     string,</span><br><span class="line">    friends  <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>,</span><br><span class="line">    children map<span class="operator">&lt;</span>string, <span class="type">int</span><span class="operator">&gt;</span>,</span><br><span class="line">    address  struct<span class="operator">&lt;</span>street:string, city:string, zip:<span class="type">int</span><span class="operator">&gt;</span></span><br><span class="line">)</span><br><span class="line"><span class="comment">-- 列分隔符</span></span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line"><span class="comment">-- MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</span></span><br><span class="line">collection items terminated <span class="keyword">by</span> <span class="string">&#x27;_&#x27;</span></span><br><span class="line"><span class="comment">-- MAP中的key与value的分隔符</span></span><br><span class="line">map keys terminated <span class="keyword">by</span> <span class="string">&#x27;:&#x27;</span></span><br><span class="line"><span class="comment">-- 行分隔符</span></span><br><span class="line">lines terminated <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>导入数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/user.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> person_info;</span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> default.user_test</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.873</span> seconds</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>查看数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> person_info <span class="keyword">where</span> friends[<span class="number">1</span>]<span class="operator">=</span><span class="string">&#x27;lili&#x27;</span> <span class="keyword">and</span> address.city<span class="operator">=</span><span class="string">&#x27;beijing&#x27;</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+----------------------+--------------------------------------+----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> person_info.name  <span class="operator">|</span> person_info.friends  <span class="operator">|</span>         person_info.children         <span class="operator">|</span>                person_info.address                 <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+----------------------+--------------------------------------+----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> songsong          <span class="operator">|</span> [&quot;bingbing&quot;,&quot;lili&quot;]  <span class="operator">|</span> &#123;&quot;xiao song&quot;:<span class="number">18</span>,&quot;xiaoxiao song&quot;:<span class="number">19</span>&#125;  <span class="operator">|</span> &#123;&quot;street&quot;:&quot;hui long guan&quot;,&quot;city&quot;:&quot;beijing&quot;,&quot;zip&quot;:<span class="number">1000001</span>&#125; <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+----------------------+--------------------------------------+----------------------------------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.076</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h2 id="3-3-类型转化"><a href="#3-3-类型转化" class="headerlink" title="3.3 类型转化"></a>3.3 类型转化</h2><p>Hive的原生数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。</p>
<ol>
<li>隐式类型转换规则如下<ul>
<li>任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT。</li>
<li>所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。</li>
<li>TINYINT、SMALLINT、INT都可以转换为FLOAT。</li>
<li>BOOLEAN类型不可以转换为任何其它的类型。</li>
</ul>
</li>
<li>可以使用CAST操作显示进行数据类型转换<ul>
<li>例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="number">1</span><span class="operator">+</span><span class="string">&#x27;1&#x27;</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> _c0  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2.0</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.098</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="number">1</span><span class="operator">+</span><span class="built_in">cast</span>(<span class="string">&#x27;1&#x27;</span> <span class="keyword">as</span> <span class="type">int</span>);</span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> _c0  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.087</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="number">1</span><span class="operator">+</span><span class="built_in">cast</span>(<span class="string">&#x27;x&#x27;</span> <span class="keyword">as</span> <span class="type">int</span>);</span><br><span class="line"><span class="operator">+</span><span class="comment">-------+</span></span><br><span class="line"><span class="operator">|</span>  _c0  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.096</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h1 id="四、DDL数据库定义操作"><a href="#四、DDL数据库定义操作" class="headerlink" title="四、DDL数据库定义操作"></a>四、DDL数据库定义操作</h1><h2 id="4-1-创建数据库"><a href="#4-1-创建数据库" class="headerlink" title="4.1 创建数据库"></a>4.1 创建数据库</h2><ul>
<li>格式  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> DATABASE [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] database_name</span><br><span class="line">[COMMENT database_comment]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[<span class="keyword">WITH</span> DBPROPERTIES (property_name<span class="operator">=</span>property_value, ...)];</span><br></pre></td></tr></table></figure></li>
</ul>
<ol>
<li>创建一个数据库，数据库在HDFS上的默认存储路径是/user/hive/warehouse/*.db。 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> database hive_id;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.108</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> database_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.044</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>避免要创建的数据库已经存在错误，增加if not exists判断。（标准写法） <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> database_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.044</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> database hive_id;</span><br><span class="line">Error: Error while processing statement: FAILED: Execution Error, <span class="keyword">return</span> code <span class="number">1</span> <span class="keyword">from</span> org.apache.hadoop.hive.ql.exec.DDLTask. Database hive_id already <span class="keyword">exists</span> (state<span class="operator">=</span><span class="number">42000</span>,code<span class="operator">=</span><span class="number">1</span>)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> database if <span class="keyword">not</span> <span class="keyword">exists</span> hive_id;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.031</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> database_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.03</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>创建一个数据库，指定数据库在HDFS上存放的位置 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> database if <span class="keyword">not</span> <span class="keyword">exists</span> database_test location <span class="string">&#x27;/hive/db/test/location/location_test_db.db&#x27;</span> <span class="keyword">with</span> dbproperties (<span class="string">&#x27;author&#x27;</span><span class="operator">=</span><span class="string">&#x27;anzhen&#x27;</span>, <span class="string">&#x27;create_time&#x27;</span><span class="operator">=</span><span class="string">&#x27;2021/10/25&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.057</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="operator">|</span>   database_name   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> location_test_db  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> selected (<span class="number">0.027</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">/</span>hive<span class="operator">/</span>db<span class="operator">/</span>test<span class="operator">/</span>location<span class="operator">/</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">1</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> drwxr<span class="operator">-</span>xr<span class="operator">-</span>x   <span class="operator">-</span> atguigu supergroup          <span class="number">0</span> <span class="number">2021</span><span class="number">-10</span><span class="number">-22</span> <span class="number">17</span>:<span class="number">11</span> <span class="operator">/</span>hive<span class="operator">/</span>db<span class="operator">/</span>test<span class="operator">/</span>location<span class="operator">/</span>location_test_db.db <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.011</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="4-2-查询数据库"><a href="#4-2-查询数据库" class="headerlink" title="4.2 查询数据库"></a>4.2 查询数据库</h2></li>
<li>2.1 显示数据库</li>
<li>显示数据库 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="operator">|</span>   database_name   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> location_test_db  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> selected (<span class="number">0.096</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>过滤显示查询的数据库 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases <span class="keyword">like</span> <span class="string">&#x27;l*&#x27;</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="operator">|</span>   database_name   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="operator">|</span> location_test_db  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.025</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="4-2-2-查看数据库详情"><a href="#4-2-2-查看数据库详情" class="headerlink" title="4.2.2 查看数据库详情"></a>4.2.2 查看数据库详情</h3></li>
<li>显示数据库信息 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> database location_test_db;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+----------+----------------------------------------------------+-------------+-------------+-------------+</span></span><br><span class="line"><span class="operator">|</span>      db_name      <span class="operator">|</span> comment  <span class="operator">|</span>                      location                      <span class="operator">|</span> owner_name  <span class="operator">|</span> owner_type  <span class="operator">|</span> parameters  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+----------+----------------------------------------------------+-------------+-------------+-------------+</span></span><br><span class="line"><span class="operator">|</span> location_test_db  <span class="operator">|</span>          <span class="operator">|</span> hdfs:<span class="operator">/</span><span class="operator">/</span>mycluster<span class="operator">/</span>hive<span class="operator">/</span>db<span class="operator">/</span>test<span class="operator">/</span>location<span class="operator">/</span>location_test_db.db <span class="operator">|</span> atguigu     <span class="operator">|</span> <span class="keyword">USER</span>        <span class="operator">|</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+----------+----------------------------------------------------+-------------+-------------+-------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.037</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>显示数据库详细信息，extended <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> database EXTENDED database_test;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+----------+----------------------------------------------------+-------------+-------------+------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>    db_name     <span class="operator">|</span> comment  <span class="operator">|</span>                      location                      <span class="operator">|</span> owner_name  <span class="operator">|</span> owner_type  <span class="operator">|</span>                parameters                <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+----------+----------------------------------------------------+-------------+-------------+------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> database_test  <span class="operator">|</span>          <span class="operator">|</span> hdfs:<span class="operator">/</span><span class="operator">/</span>mycluster<span class="operator">/</span>hive<span class="operator">/</span>db<span class="operator">/</span>test<span class="operator">/</span>location<span class="operator">/</span>location_test_db.db <span class="operator">|</span> atguigu     <span class="operator">|</span> <span class="keyword">USER</span>        <span class="operator">|</span> &#123;create_time<span class="operator">=</span><span class="number">2021</span><span class="operator">/</span><span class="number">10</span><span class="operator">/</span><span class="number">25</span>, author<span class="operator">=</span>anzhen&#125;  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+----------+----------------------------------------------------+-------------+-------------+------------------------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.019</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="4-2-3-切换当前数据库"><a href="#4-2-3-切换当前数据库" class="headerlink" title="4.2.3 切换当前数据库"></a>4.2.3 切换当前数据库</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> use location_test_db;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.026</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> use hive_id;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.022</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="4-3-修改数据库"><a href="#4-3-修改数据库" class="headerlink" title="4.3 修改数据库"></a>4.3 修改数据库</h2></li>
<li>用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> database location_test_db <span class="keyword">set</span> dbproperties(<span class="string">&#x27;createtile&#x27;</span><span class="operator">=</span><span class="string">&#x27;20211022&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.069</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> database extended location_test_db;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+----------+----------------------------------------------------+-------------+-------------+------------------------+</span></span><br><span class="line"><span class="operator">|</span>      db_name      <span class="operator">|</span> comment  <span class="operator">|</span>                      location                      <span class="operator">|</span> owner_name  <span class="operator">|</span> owner_type  <span class="operator">|</span>       parameters       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+----------+----------------------------------------------------+-------------+-------------+------------------------+</span></span><br><span class="line"><span class="operator">|</span> location_test_db  <span class="operator">|</span>          <span class="operator">|</span> hdfs:<span class="operator">/</span><span class="operator">/</span>mycluster<span class="operator">/</span>hive<span class="operator">/</span>db<span class="operator">/</span>test<span class="operator">/</span>location<span class="operator">/</span>location_test_db.db <span class="operator">|</span> atguigu     <span class="operator">|</span> <span class="keyword">USER</span>        <span class="operator">|</span> &#123;createtile<span class="operator">=</span><span class="number">20211022</span>&#125;  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+----------+----------------------------------------------------+-------------+-------------+------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.023</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="4-4-删除数据库"><a href="#4-4-删除数据库" class="headerlink" title="4.4 删除数据库"></a>4.4 删除数据库</h2><ol>
<li>删除空数据库 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="operator">|</span>   database_name   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> location_test_db  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> selected (<span class="number">0.026</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">drop</span> database location_test_db;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.12</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> database_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.022</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>如果删除的数据库不存在，最好采用 if exists判断数据库是否存在 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> database_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.022</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">drop</span> database location_test_db;</span><br><span class="line">Error: Error while compiling statement: FAILED: SemanticException [Error <span class="number">10072</span>]: Database does <span class="keyword">not</span> exist: location_test_db (state<span class="operator">=</span><span class="number">42000</span>,code<span class="operator">=</span><span class="number">10072</span>)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">drop</span> database if <span class="keyword">exists</span> location_test_db;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.011</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> database_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.02</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>如果数据库不为空，可以采用cascade命令，强制删除 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> database_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.02</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> database location_test_db;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.051</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> use location_test_db;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.02</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> test(id <span class="type">int</span>,name string);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.053</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">drop</span> database location_test_db;</span><br><span class="line">Error: Error while processing statement: FAILED: Execution Error, <span class="keyword">return</span> code <span class="number">1</span> <span class="keyword">from</span> org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database location_test_db <span class="keyword">is</span> <span class="keyword">not</span> empty. <span class="keyword">One</span> <span class="keyword">or</span> more tables exist.) (state<span class="operator">=</span><span class="number">08</span>S01,code<span class="operator">=</span><span class="number">1</span>)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">drop</span> database location_test_db cascade;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.31</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> database_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.02</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="4-5-创建表"><a href="#4-5-创建表" class="headerlink" title="4.5 创建表"></a>4.5 创建表</h2><ol>
<li>建表语法 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name </span><br><span class="line">[(col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[COMMENT table_comment] </span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) </span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span><span class="operator">|</span><span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS] </span><br><span class="line">[<span class="type">ROW</span> FORMAT row_format] </span><br><span class="line">[STORED <span class="keyword">AS</span> file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name<span class="operator">=</span>property_value, ...)]</span><br><span class="line">[<span class="keyword">AS</span> select_statement]</span><br></pre></td></tr></table></figure></li>
<li>字段解释说明 <ol>
<li><code>CREATE TABLE</code>: 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用<code>IF NOT EXISTS</code>选项来忽略这个异常。</li>
<li><code>EXTERNAL</code>: 关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实际数据的路径（<code>LOCATION</code>），<font color ='red' >在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据</font>。</li>
<li><code>COMMENT</code>: 为表和列添加注释。</li>
<li><code>PARTITIONED BY</code>: 指定分区表的规范</li>
<li><code>CLUSTERED BY</code>: 指定分桶表的规范</li>
<li><code>SORTED BY</code>: 指定单签表默认排序规则及粪桶数量</li>
<li><code>ROW FORMAT DELIMITED</code>: 定义每一行中字段之间的分隔符</li>
<li><code>collection items terminated by</code>: 集合数据类型，元素之间的分隔符</li>
<li><code>map keys terminated by</code>: map数据类型key-value分隔符</li>
<li><code>lines terminated by</code>: 每一行的分隔符</li>
<li><code>STORED AS</code>: 指定存储文件类型<ul>
<li>常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）</li>
<li>如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。</li>
</ul>
</li>
<li><code>LOCATION</code>: 指定表在HDFS上的存储位置。</li>
<li><code>AS select_statement</code>: 后跟查询语句，根据查询结果创建表。</li>
<li><code>LIKE</code>: 允许用户复制现有的表结构，但是不复制数据。</li>
</ol>
</li>
</ol>
<h3 id="4-5-1-管理表-内部表"><a href="#4-5-1-管理表-内部表" class="headerlink" title="4.5.1 管理表(内部表)"></a>4.5.1 管理表(内部表)</h3><ol>
<li>理论<ul>
<li>默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。当我们<font color ='red' >删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据</font>。</li>
</ul>
</li>
<li>案例实操<ol>
<li>原始数据 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1001	ss1</span><br><span class="line">1002	ss2</span><br><span class="line">1003	ss3</span><br><span class="line">1004	ss4</span><br><span class="line">1005	ss5</span><br><span class="line">1006	ss6</span><br><span class="line">1007	ss7</span><br><span class="line">1008	ss8</span><br><span class="line">1009	ss9</span><br><span class="line">1010	ss10</span><br><span class="line">1011	ss11</span><br><span class="line">1012	ss12</span><br><span class="line">1013	ss13</span><br><span class="line">1014	ss14</span><br><span class="line">1015	ss15</span><br><span class="line">1016	ss16</span><br></pre></td></tr></table></figure></li>
<li>普通创建表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> student (</span><br><span class="line">    id   <span class="type">int</span>,</span><br><span class="line">    name string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> textfile</span><br><span class="line">location <span class="string">&#x27;/user/hive/warehouse/student&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>导入数据 </li>
<li>根据查询结果创建表（查询的结果会添加到新创建的表中） <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> student_copy <span class="keyword">as</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></li>
<li>根据已经存在的表结构创建表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> student_copy_like <span class="keyword">like</span> student;</span><br></pre></td></tr></table></figure></li>
<li>查询表的类型 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> student;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------------+----------+</span></span><br><span class="line"><span class="operator">|</span> col_name  <span class="operator">|</span> data_type  <span class="operator">|</span> comment  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------------+----------+</span></span><br><span class="line"><span class="operator">|</span> id        <span class="operator">|</span> <span class="type">int</span>        <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> name      <span class="operator">|</span> string     <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------------+----------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.101</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="4-5-2-外部表"><a href="#4-5-2-外部表" class="headerlink" title="4.5.2 外部表"></a>4.5.2 外部表</h3></li>
</ol>
</li>
<li>理论<ul>
<li>因为表是外部表，所以Hive并非认为其完全拥有这份数据。<font color ='red' >删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉</font>。</li>
</ul>
</li>
<li>管理表和外部表的使用场景<ul>
<li>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</li>
</ul>
</li>
<li>案例实操<ul>
<li>分别创建部门和员工外部表，并向表中导入数据。</li>
</ul>
<ol>
<li>原始数据<ul>
<li>dept:  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">10	ACCOUNTING	1700</span><br><span class="line">20	RESEARCH	1800</span><br><span class="line">30	SALES	1900</span><br><span class="line">40	OPERATIONS	1700</span><br></pre></td></tr></table></figure></li>
<li>emp：  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">7369	SMITH	CLERK	7902	1980-12-17	800.00		20</span><br><span class="line">7499	ALLEN	SALESMAN	7698	1981-2-20	1600.00	300.00	30</span><br><span class="line">7521	WARD	SALESMAN	7698	1981-2-22	1250.00	500.00	30</span><br><span class="line">7566	JONES	MANAGER	7839	1981-4-2	2975.00		20</span><br><span class="line">7654	MARTIN	SALESMAN	7698	1981-9-28	1250.00	1400.00	30</span><br><span class="line">7698	BLAKE	MANAGER	7839	1981-5-1	2850.00		30</span><br><span class="line">7782	CLARK	MANAGER	7839	1981-6-9	2450.00		10</span><br><span class="line">7788	SCOTT	ANALYST	7566	1987-4-19	3000.00		20</span><br><span class="line">7839	KING	PRESIDENT		1981-11-17	5000.00		10</span><br><span class="line">7844	TURNER	SALESMAN	7698	1981-9-8	1500.00	0.00	30</span><br><span class="line">7876	ADAMS	CLERK	7788	1987-5-23	1100.00		20</span><br><span class="line">7900	JAMES	CLERK	7698	1981-12-3	950.00		30</span><br><span class="line">7902	FORD	ANALYST	7566	1981-12-3	3000.00		20</span><br><span class="line">7934	MILLER	CLERK	7782	1982-1-23	1300.00		10</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>上传数据到HDFS <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hive]$ hadoop fs -put dep.txt /hive/source_data/</span><br><span class="line">[atguigu@hadoop001 hive]$ hadoop fs -put emp.txt /hive/source_data/</span><br><span class="line">[atguigu@hadoop001 hive]$ hadoop fs -ls /hive/source_data</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 atguigu supergroup         69 2021-10-25 13:15 /hive/source_data/dep.txt</span><br><span class="line">-rw-r--r--   3 atguigu supergroup        657 2021-10-25 13:15 /hive/source_data/emp.txt</span><br><span class="line">[atguigu@hadoop001 hive]$</span><br></pre></td></tr></table></figure></li>
<li>建表语句，创建外部表<ul>
<li>创建部门表  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> dept (</span><br><span class="line">    dept_no <span class="type">int</span>,</span><br><span class="line">    dept_name  string,</span><br><span class="line">    loc    <span class="type">int</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>创建员工表  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> emp (</span><br><span class="line">    emp_no    <span class="type">int</span>,</span><br><span class="line">    emp_name  string,</span><br><span class="line">    job       string,</span><br><span class="line">    mgr       <span class="type">int</span>,</span><br><span class="line">    hire_date string,</span><br><span class="line">    sal       <span class="keyword">double</span>,</span><br><span class="line">    comm      <span class="keyword">double</span>,</span><br><span class="line">    dept_no   <span class="type">int</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>查看表格式化数据  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000&gt; desc formatted dept;</span><br><span class="line">+---------------+------------------+--------+</span><br><span class="line">| col_name      | data_type        | comment|</span><br><span class="line">+---------------+------------------+--------+</span><br><span class="line">···</span><br><span class="line">···</span><br><span class="line">| Table Type:   | EXTERNAL_TABLE   | NULL   |</span><br><span class="line">···</span><br><span class="line">···</span><br><span class="line">+---------------+------------------+--------+</span><br><span class="line">35 rows selected (0.054 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>导入数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000&gt; load data inpath <span class="string">&#x27;/hive/source_data/dep.txt&#x27;</span> into table mock_data.dept;</span><br><span class="line">No rows affected (0.132 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; load data inpath <span class="string">&#x27;/hive/source_data/emp.txt&#x27;</span> into table mock_data.emp;</span><br><span class="line">No rows affected (0.095 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; select count(1) from mock_data.emp;</span><br><span class="line">+------+</span><br><span class="line">| _c0  |</span><br><span class="line">+------+</span><br><span class="line">| 14   |</span><br><span class="line">+------+</span><br><span class="line">1 row selected (14.939 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; select count(1) from mock_data.dept;</span><br><span class="line">+------+</span><br><span class="line">| _c0  |</span><br><span class="line">+------+</span><br><span class="line">| 4    |</span><br><span class="line">+------+</span><br><span class="line">1 row selected (13.836 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br></pre></td></tr></table></figure></li>
<li>删除外部表后，hdfs中的数据还在，但是metadata中dept的元数据已被删除 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">drop</span> <span class="keyword">table</span> mock_data.dept;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.097</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> tables;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span>      tab_name      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> emp                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> person             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> person_info        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student_copy_as    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student_copy_like  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="number">6</span> <span class="keyword">rows</span> selected (<span class="number">0.02</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>dept;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">1</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup         69 2021-10-25 13:15 /user/hive/warehouse/mock_data.db/dept/dep.txt |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.042</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>重建表，再次查询 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> dept (</span><br><span class="line">     dept_no <span class="type">int</span>,</span><br><span class="line">     dept_name  string,</span><br><span class="line">     loc    <span class="type">int</span></span><br><span class="line"> )</span><br><span class="line"> <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.039</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> tables ;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span>      tab_name      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> dept               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> emp                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> person             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> person_info        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student_copy_as    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student_copy_like  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="number">7</span> <span class="keyword">rows</span> selected (<span class="number">0.021</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+-----------------+-----------+</span></span><br><span class="line"><span class="operator">|</span> dept.dept_no  <span class="operator">|</span> dept.dept_name  <span class="operator">|</span> dept.loc  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+-----------------+-----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>            <span class="operator">|</span> ACCOUNTING      <span class="operator">|</span> <span class="number">1700</span>      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>            <span class="operator">|</span> RESEARCH        <span class="operator">|</span> <span class="number">1800</span>      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">30</span>            <span class="operator">|</span> SALES           <span class="operator">|</span> <span class="number">1900</span>      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">40</span>            <span class="operator">|</span> OPERATIONS      <span class="operator">|</span> <span class="number">1700</span>      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+-----------------+-----------+</span></span><br><span class="line"><span class="number">4</span> <span class="keyword">rows</span> selected (<span class="number">0.059</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h3 id="4-5-3-管理表-内部表-与外部表的互相转换"><a href="#4-5-3-管理表-内部表-与外部表的互相转换" class="headerlink" title="4.5.3 管理表(内部表)与外部表的互相转换"></a>4.5.3 管理表(内部表)与外部表的互相转换</h3><ol>
<li>查询表的类型 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hive]$ hive -e <span class="string">&#x27;desc formatted mock_data.student;&#x27;</span>| grep <span class="string">&#x27;Table Type&#x27;</span></span><br><span class="line"><span class="built_in">which</span>: no hbase <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/ha-hadoop-3.1.3/bin:/opt/module/ha-hadoop-3.1.3/sbin:/opt/module/hive-3.1.2/bin:/home/atguigu/.<span class="built_in">local</span>/bin:/home/atguigu/bin)</span><br><span class="line">Hive Session ID = 8eb096f9-31db-49d1-b22b-5eb6e458c0c4</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> file:/opt/module/hive-3.1.2/conf/hive-log4j2.properties Async: <span class="literal">true</span></span><br><span class="line">Hive Session ID = a1d905c5-6885-493a-bd5e-db945af0b3ac</span><br><span class="line">OK</span><br><span class="line">Table Type:         	MANAGED_TABLE</span><br><span class="line">Time taken: 0.679 seconds, Fetched: 32 row(s)</span><br><span class="line">[atguigu@hadoop001 hive]$</span><br></pre></td></tr></table></figure></li>
<li>内部表外部表转换 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 外部表</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> student <span class="keyword">set</span> tblproperties(<span class="string">&#x27;EXTERNAL&#x27;</span><span class="operator">=</span><span class="string">&#x27;TRUE&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.048</span> seconds)</span><br><span class="line"># 内部表</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> student <span class="keyword">set</span> tblproperties(<span class="string">&#x27;EXTERNAL&#x27;</span><span class="operator">=</span><span class="string">&#x27;FALSE&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.053</span> seconds)</span><br></pre></td></tr></table></figure>
<ul>
<li>注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写！</li>
</ul>
</li>
</ol>
<h3 id="4-5-3-内部表外部表使用场景"><a href="#4-5-3-内部表外部表使用场景" class="headerlink" title="4.5.3 内部表外部表使用场景"></a>4.5.3 内部表外部表使用场景</h3><ul>
<li>内部表：不常用，通常一些中间表(运算过程产生)会使用内部表</li>
<li>外部表：常用，主要考虑HDFS源数据安全性，数据分析一般采用外部表</li>
</ul>
<h2 id="4-6-修改表"><a href="#4-6-修改表" class="headerlink" title="4.6 修改表"></a>4.6 修改表</h2><h3 id="4-6-1-重命名表"><a href="#4-6-1-重命名表" class="headerlink" title="4.6.1 重命名表"></a>4.6.1 重命名表</h3><ol>
<li>语法 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name RENAME <span class="keyword">TO</span> new_table_name</span><br></pre></td></tr></table></figure></li>
<li>实操案例 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> tables;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span>      tab_name      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> dept               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> emp                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> person             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> person_info        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student_copy_as    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student_copy_like  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="number">7</span> <span class="keyword">rows</span> selected (<span class="number">0.035</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> student_copy_as rename <span class="keyword">to</span> student_copy_as_rename;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.069</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> tables;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"><span class="operator">|</span>        tab_name         <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"><span class="operator">|</span> dept                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> emp                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> person                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> person_info             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student_copy_as_rename  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student_copy_like       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"><span class="number">7</span> <span class="keyword">rows</span> selected (<span class="number">0.019</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="4-6-2-增加、修改和删除表分区"><a href="#4-6-2-增加、修改和删除表分区" class="headerlink" title="4.6.2 增加、修改和删除表分区"></a>4.6.2 增加、修改和删除表分区</h3> 详见7.1章分区表基本操作。</li>
</ol>
<h3 id="4-6-3-增加-修改-替换列信息"><a href="#4-6-3-增加-修改-替换列信息" class="headerlink" title="4.6.3 增加/修改/替换列信息"></a>4.6.3 增加/修改/替换列信息</h3><ol>
<li>语法<ul>
<li>更新列  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name CHANGE [<span class="keyword">COLUMN</span>] col_old_name col_new_name column_type [COMMENT col_comment] [<span class="keyword">FIRST</span><span class="operator">|</span>AFTER column_name]</span><br></pre></td></tr></table></figure></li>
<li>增加和替换列  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">ADD</span><span class="operator">|</span>REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...) </span><br></pre></td></tr></table></figure>
<ul>
<li>注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。</li>
</ul>
</li>
</ul>
</li>
<li>实操案例<ul>
<li>查询表结构  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> student;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------------+----------+</span></span><br><span class="line"><span class="operator">|</span> col_name  <span class="operator">|</span> data_type  <span class="operator">|</span> comment  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------------+----------+</span></span><br><span class="line"><span class="operator">|</span> id        <span class="operator">|</span> <span class="type">int</span>        <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> name      <span class="operator">|</span> string     <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------------+----------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.027</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>添加列  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> dept <span class="keyword">add</span> columns(dept_desc string, dept_create_time string);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.058</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> dept;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+------------+----------+</span></span><br><span class="line"><span class="operator">|</span>     col_name      <span class="operator">|</span> data_type  <span class="operator">|</span> comment  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+------------+----------+</span></span><br><span class="line"><span class="operator">|</span> dept_no           <span class="operator">|</span> <span class="type">int</span>        <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> dept_name         <span class="operator">|</span> string     <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> loc               <span class="operator">|</span> <span class="type">int</span>        <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> dept_desc         <span class="operator">|</span> string     <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> dept_create_time  <span class="operator">|</span> string     <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+------------+----------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> selected (<span class="number">0.026</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>更新列  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000&gt; alter table dept change column dept_desc desc string;</span><br><span class="line">No rows affected (0.055 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; desc dept;</span><br><span class="line">+-------------------+------------+----------+</span><br><span class="line">|     col_name      | data_type  | comment  |</span><br><span class="line">+-------------------+------------+----------+</span><br><span class="line">| dept_no           | int        |          |</span><br><span class="line">| dept_name         | string     |          |</span><br><span class="line">| loc               | int        |          |</span><br><span class="line">| desc              | string     |          |</span><br><span class="line">| dept_create_time  | string     |          |</span><br><span class="line">+-------------------+------------+----------+</span><br><span class="line">5 rows selected (0.026 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br></pre></td></tr></table></figure></li>
<li>替换列  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> dept replace columns(deptno string, dname string, loc string);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.083</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> dept;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------------+----------+</span></span><br><span class="line"><span class="operator">|</span> col_name  <span class="operator">|</span> data_type  <span class="operator">|</span> comment  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------------+----------+</span></span><br><span class="line"><span class="operator">|</span> deptno    <span class="operator">|</span> string     <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> dname     <span class="operator">|</span> string     <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> loc       <span class="operator">|</span> string     <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------------+----------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> selected (<span class="number">0.027</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h1 id="五、DML数据库操作"><a href="#五、DML数据库操作" class="headerlink" title="五、DML数据库操作"></a>五、DML数据库操作</h1><h2 id="5-1-数据导入"><a href="#5-1-数据导入" class="headerlink" title="5.1 数据导入"></a>5.1 数据导入</h2><h3 id="5-1-1-向表中装载数据（Load）"><a href="#5-1-1-向表中装载数据（Load）" class="headerlink" title="5.1.1 向表中装载数据（Load）"></a>5.1.1 向表中装载数据（Load）</h3><ol>
<li>语法 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">load data [<span class="built_in">local</span>] inpath <span class="string">&#x27;数据的path&#x27;</span> [overwrite] into table student [partition (partcol1=val1,…)];</span><br></pre></td></tr></table></figure>
<ul>
<li>load data:表示加载数据</li>
<li>local:表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</li>
<li>inpath:表示加载数据的路径</li>
<li>overwrite:表示覆盖表中已有数据，否则表示追加</li>
<li>into table:表示加载到哪张表</li>
<li>student:表示具体的表</li>
<li>partition:表示上传到指定分区</li>
</ul>
</li>
<li>实操案例<ul>
<li>加载本地文件到hive  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/student.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> mock_data.student;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.148</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>加载HDFS文件到hive中  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">/</span>hive<span class="operator">/</span>source_data;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">1</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup        150 2021-10-25 15:22 /hive/source_data/student.txt |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.055</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> load data inpath <span class="string">&#x27;/hive/source_data/student.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> mock_data.student;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.121</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>加载数据覆盖表中已有的数据  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000&gt; select count(*) from mock_data.student;</span><br><span class="line">+------+</span><br><span class="line">| _c0  |</span><br><span class="line">+------+</span><br><span class="line">| 48   |</span><br><span class="line">+------+</span><br><span class="line">1 row selected (16.219 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; load data <span class="built_in">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/student.txt&#x27;</span> overwrite into table mock_data.student;</span><br><span class="line">No rows affected (0.104 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; select count(*) from mock_data.student;</span><br><span class="line">+------+</span><br><span class="line">| _c0  |</span><br><span class="line">+------+</span><br><span class="line">| 16   |</span><br><span class="line">+------+</span><br><span class="line">1 row selected (14.727 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>导入顺序<ul>
<li>追加导入数据根据文件名字典序排序</li>
</ul>
</li>
</ol>
<h3 id="5-1-2-通过查询语句向表中插入数据（Insert）"><a href="#5-1-2-通过查询语句向表中插入数据（Insert）" class="headerlink" title="5.1.2 通过查询语句向表中插入数据（Insert）"></a>5.1.2 通过查询语句向表中插入数据（Insert）</h3><ol>
<li>根据单张表查询结果插入 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student_from_select <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
<ul>
<li>insert into：以追加数据的方式插入到表或分区，原有数据不会删除</li>
<li>insert overwrite：会覆盖表中已存在的数据</li>
<li>注意：insert不支持插入部分字段</li>
</ul>
</li>
<li>多表（多分区）插入模式（根据多张表查询结果） <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> student</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student_from_select</span><br><span class="line"><span class="keyword">select</span> id, name <span class="keyword">where</span> id<span class="operator">=</span><span class="number">1001</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student_copy_like</span><br><span class="line"><span class="keyword">select</span> id, name <span class="keyword">where</span> id<span class="operator">=</span><span class="number">1009</span></span><br></pre></td></tr></table></figure>
<h3 id="5-1-3-查询语句中创建表并加载数据（As-Select）"><a href="#5-1-3-查询语句中创建表并加载数据（As-Select）" class="headerlink" title="5.1.3 查询语句中创建表并加载数据（As Select）"></a>5.1.3 查询语句中创建表并加载数据（As Select）</h3></li>
<li>根据查询结果创建表（查询的结果会添加到新创建的表中） <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> student3 <span class="keyword">as</span> <span class="keyword">select</span> id, name <span class="keyword">from</span> student; </span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="5-1-4-创建表时通过Location指定加载数据路径"><a href="#5-1-4-创建表时通过Location指定加载数据路径" class="headerlink" title="5.1.4 创建表时通过Location指定加载数据路径"></a>5.1.4 创建表时通过Location指定加载数据路径</h3><ol>
<li>上传数据到hdfs上 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>put <span class="operator">/</span>home<span class="operator">/</span>atguigu<span class="operator">/</span>hive<span class="operator">/</span>load_student.txt <span class="operator">/</span>hive<span class="operator">/</span>db<span class="operator">/</span>mock_data<span class="operator">/</span>load_student<span class="operator">/</span>load_student.txt;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+</span></span><br><span class="line"><span class="operator">|</span> DFS Output  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+</span></span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> selected (<span class="number">0.019</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>创建表，并指定在hdfs上的位置 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> load_student (</span><br><span class="line">    id   <span class="type">int</span>,</span><br><span class="line">    name string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> textfile</span><br><span class="line">location <span class="string">&#x27;/hive/db/mock_data/load_student&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>查询数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> mock_data.load_student limit <span class="number">10</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------------------+--------------------+</span></span><br><span class="line"><span class="operator">|</span> load_student.id  <span class="operator">|</span> load_student.name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------+--------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852294</span>         <span class="operator">|</span> 司寇俊                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852295</span>         <span class="operator">|</span> 安投悬                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852296</span>         <span class="operator">|</span> 申燎赢                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852297</span>         <span class="operator">|</span> 洪谎                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852298</span>         <span class="operator">|</span> 丰觅                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852299</span>         <span class="operator">|</span> 拓跋卷                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852300</span>         <span class="operator">|</span> 汪舶                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852301</span>         <span class="operator">|</span> 涂钦羡                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852302</span>         <span class="operator">|</span> 濮崭                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852303</span>         <span class="operator">|</span> 阚撑                 <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------+--------------------+</span></span><br><span class="line"><span class="number">10</span> <span class="keyword">rows</span> selected (<span class="number">0.057</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="5-1-5-Import数据到指定Hive表中"><a href="#5-1-5-Import数据到指定Hive表中" class="headerlink" title="5.1.5 Import数据到指定Hive表中"></a>5.1.5 Import数据到指定Hive表中</h3><ul>
<li>注意：先用export导出后，再将数据导入。多用于小规模迁移，会自动创建表。<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> export <span class="keyword">table</span> mock_data.person <span class="keyword">to</span> <span class="string">&#x27;/hive/source_data/person/export&#x27;</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">17.558</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> import <span class="keyword">table</span> mock_data.import_person <span class="keyword">from</span> <span class="string">&#x27;/hive/source_data/person/export&#x27;</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">4.487</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> mock_data.import_person;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+</span></span><br><span class="line"><span class="operator">|</span>    _c0    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">24307000</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">19.358</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="5-2-数据导出"><a href="#5-2-数据导出" class="headerlink" title="5.2 数据导出"></a>5.2 数据导出</h2><h3 id="5-2-1-Insert导出"><a href="#5-2-1-Insert导出" class="headerlink" title="5.2.1 Insert导出"></a>5.2.1 Insert导出</h3><ol>
<li>将查询的结果导出到本地 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/home/atguigu/hive/export/person&#x27;</span> <span class="keyword">select</span> id,name <span class="keyword">from</span> mock_data.person;</span><br></pre></td></tr></table></figure></li>
<li>将查询的结果格式化导出到本地 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/home/atguigu/hive/export/person&#x27;</span> <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span> <span class="keyword">select</span> id,name <span class="keyword">from</span> mock_data.person;</span><br></pre></td></tr></table></figure></li>
<li>将查询的结果导出到HDFS上(没有local) <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite directory <span class="string">&#x27;/hive/source_data/person&#x27;</span> <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span> <span class="keyword">select</span> id,name <span class="keyword">from</span> mock_data.person;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">21.398</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">/</span>hive<span class="operator">/</span>source_data<span class="operator">/</span>person;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">5</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup  115767751 2021-10-25 19:13 /hive/source_data/person/000000_0 |</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup  115656456 2021-10-25 19:13 /hive/source_data/person/000001_0 |</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup  111382098 2021-10-25 19:13 /hive/source_data/person/000002_0 |</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup   55441386 2021-10-25 19:13 /hive/source_data/person/000003_0 |</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup   27719097 2021-10-25 19:13 /hive/source_data/person/000004_0 |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">6</span> <span class="keyword">rows</span> selected (<span class="number">0.012</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="5-2-2-Export导出到HDFS上"><a href="#5-2-2-Export导出到HDFS上" class="headerlink" title="5.2.2 Export导出到HDFS上"></a>5.2.2 Export导出到HDFS上</h3><p>export和import主要用于两个Hadoop平台集群之间Hive表迁移。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">export <span class="keyword">table</span> mock_data.person <span class="keyword">to</span> <span class="string">&#x27;/hive/source_data/person/export&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h3 id="5-2-3-Hive-Shell-命令导出"><a href="#5-2-3-Hive-Shell-命令导出" class="headerlink" title="5.2.3 Hive Shell 命令导出"></a>5.2.3 Hive Shell 命令导出</h3><ul>
<li>基本语法：（hive -f/-e 执行语句或者脚本 &gt; file）  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -e <span class="string">&#x27;select * from default.student;&#x27;</span> &gt;</span><br><span class="line"> /opt/module/hive/datas/<span class="built_in">export</span>/student4.txt;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="5-2-4-Export导出到HDFS上"><a href="#5-2-4-Export导出到HDFS上" class="headerlink" title="5.2.4 Export导出到HDFS上"></a>5.2.4 Export导出到HDFS上</h3><ul>
<li>export和import主要用于两个Hadoop平台集群之间Hive表迁移。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">(defahiveult)<span class="operator">&gt;</span> export <span class="keyword">table</span> default.student <span class="keyword">to</span></span><br><span class="line"> <span class="string">&#x27;/user/hive/warehouse/export/student&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="5-2-5-清除表中数据（Truncate）"><a href="#5-2-5-清除表中数据（Truncate）" class="headerlink" title="5.2.5 清除表中数据（Truncate）"></a>5.2.5 清除表中数据（Truncate）</h3><ul>
<li>注意：Truncate只能删除管理表，不能删除外部表中数据<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">truncate</span> <span class="keyword">table</span> student;</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="六、查询"><a href="#六、查询" class="headerlink" title="六、查询"></a>六、查询</h1><h3 id="6-1-1-全表和特定列查询"><a href="#6-1-1-全表和特定列查询" class="headerlink" title="6.1.1 全表和特定列查询"></a>6.1.1 全表和特定列查询</h3><ol>
<li>数据准备 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dept:</span><br><span class="line">10	ACCOUNTING	1700</span><br><span class="line">20	RESEARCH	1800</span><br><span class="line">30	SALES	1900</span><br><span class="line">40	OPERATIONS	1700</span><br><span class="line">emp：</span><br><span class="line">7369	SMITH	CLERK	7902	1980-12-17	800.00		20</span><br><span class="line">7499	ALLEN	SALESMAN	7698	1981-2-20	1600.00	300.00	30</span><br><span class="line">7521	WARD	SALESMAN	7698	1981-2-22	1250.00	500.00	30</span><br><span class="line">7566	JONES	MANAGER	7839	1981-4-2	2975.00		20</span><br><span class="line">7654	MARTIN	SALESMAN	7698	1981-9-28	1250.00	1400.00	30</span><br><span class="line">7698	BLAKE	MANAGER	7839	1981-5-1	2850.00		30</span><br><span class="line">7782	CLARK	MANAGER	7839	1981-6-9	2450.00		10</span><br><span class="line">7788	SCOTT	ANALYST	7566	1987-4-19	3000.00		20</span><br><span class="line">7839	KING	PRESIDENT		1981-11-17	5000.00		10</span><br><span class="line">7844	TURNER	SALESMAN	7698	1981-9-8	1500.00	0.00	30</span><br><span class="line">7876	ADAMS	CLERK	7788	1987-5-23	1100.00		20</span><br><span class="line">7900	JAMES	CLERK	7698	1981-12-3	950.00		30</span><br><span class="line">7902	FORD	ANALYST	7566	1981-12-3	3000.00		20</span><br><span class="line">7934	MILLER	CLERK	7782	1982-1-23	1300.00		10</span><br></pre></td></tr></table></figure></li>
<li>创建表结构 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 部门表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> dept(</span><br><span class="line">    deptno <span class="type">int</span>,</span><br><span class="line">    dname string,</span><br><span class="line">    loc <span class="type">int</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"><span class="comment">-- 员工表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> emp(</span><br><span class="line">    empno <span class="type">int</span>,</span><br><span class="line">    ename string,</span><br><span class="line">    job string,</span><br><span class="line">    mgr <span class="type">int</span>,</span><br><span class="line">    hiredate string, </span><br><span class="line">    sal <span class="keyword">double</span>, </span><br><span class="line">    comm <span class="keyword">double</span>,</span><br><span class="line">    deptno <span class="type">int</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>导入数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/dept.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span></span><br><span class="line">dept;	</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/emp.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> emp;</span><br></pre></td></tr></table></figure></li>
<li>全表查询 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> empno,ename,job,mgr,hiredate,sal,comm,deptno <span class="keyword">from</span> emp ;</span><br></pre></td></tr></table></figure></li>
<li>选择特定列查询 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> empno, ename <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure></li>
<li>注意：<ul>
<li>（1）SQL 语言大小写不敏感。 </li>
<li>（2）SQL 可以写在一行或者多行</li>
<li>（3）关键字不能被缩写也不能分行</li>
<li>（4）各子句一般要分行写。</li>
<li>（5）使用缩进提高语句的可读性。</li>
</ul>
</li>
</ol>
<h3 id="6-1-2-列别名"><a href="#6-1-2-列别名" class="headerlink" title="6.1.2 列别名"></a>6.1.2 列别名</h3><ol>
<li>用途<ol>
<li>重命名一个列</li>
<li>便于计算</li>
<li>紧跟列名，也可以在列名和别名之间加入关键字‘AS’ </li>
<li>案例实操 </li>
</ol>
</li>
<li>查询名称和部门 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> ename <span class="keyword">AS</span> name, deptno dn <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="6-1-3-算术运算符"><a href="#6-1-3-算术运算符" class="headerlink" title="6.1.3 算术运算符"></a>6.1.3 算术运算符</h3><table>
<thead>
<tr>
<th>运算符</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>A+B</td>
<td>A和B 相加</td>
</tr>
<tr>
<td>A-B</td>
<td>A减去B</td>
</tr>
<tr>
<td>A*B</td>
<td>A和B 相乘</td>
</tr>
<tr>
<td>A/B</td>
<td>A除以B</td>
</tr>
<tr>
<td>A%B</td>
<td>A对B取余</td>
</tr>
<tr>
<td>A&amp;B</td>
<td>A和B按位取与</td>
</tr>
<tr>
<td>A</td>
<td>B</td>
</tr>
<tr>
<td>A^B</td>
<td>A和B按位取异或</td>
</tr>
<tr>
<td>~A</td>
<td>A按位取反</td>
</tr>
</tbody></table>
<ul>
<li>案例实操：查询出所有员工的薪水后加1显示。<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> sal <span class="operator">+</span><span class="number">1</span> <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>
<h3 id="6-1-4-常用函数"><a href="#6-1-4-常用函数" class="headerlink" title="6.1.4 常用函数"></a>6.1.4 常用函数</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 求总行数（count）</span></span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) cnt <span class="keyword">from</span> emp;</span><br><span class="line"><span class="comment">-- 求工资的最大值（max）</span></span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">max</span>(sal) max_sal <span class="keyword">from</span> emp;</span><br><span class="line"><span class="comment">-- 求工资的最小值（min）</span></span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">min</span>(sal) min_sal <span class="keyword">from</span> emp;</span><br><span class="line"><span class="comment">-- 求工资的总和（sum）</span></span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">sum</span>(sal) sum_sal <span class="keyword">from</span> emp; </span><br><span class="line"><span class="comment">-- 求工资的平均值（avg）</span></span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">avg</span>(sal) avg_sal <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="6-1-5-Limit语句"><a href="#6-1-5-Limit语句" class="headerlink" title="6.1.5 Limit语句"></a>6.1.5 Limit语句</h3><ul>
<li>LIMIT子句用于限制返回的行数。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp limit <span class="number">5</span>;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp limit <span class="number">2</span>,<span class="number">3</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="6-1-6-Where语句"><a href="#6-1-6-Where语句" class="headerlink" title="6.1.6 Where语句"></a>6.1.6 Where语句</h3><ol>
<li>使用WHERE子句，将不满足条件的行过滤掉</li>
<li>WHERE子句紧随FROM子句</li>
<li>案例实操<ul>
<li>查询出薪水大于1000的所有员工  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="operator">&gt;</span><span class="number">1000</span>;</span><br></pre></td></tr></table></figure></li>
<li>注意：where子句中不能使用字段别名。</li>
</ul>
</li>
</ol>
<h3 id="6-1-7-比较运算符（Between-In-Is-Null）"><a href="#6-1-7-比较运算符（Between-In-Is-Null）" class="headerlink" title="6.1.7 比较运算符（Between/In/ Is Null）"></a>6.1.7 比较运算符（Between/In/ Is Null）</h3><ol>
<li><p>下面表中描述了谓词操作符，这些操作符同样可以用于JOIN…ON和HAVING语句中。</p>
<table>
<thead>
<tr>
<th>操作符</th>
<th>支持的数据类型</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>A=B</td>
<td>基本数据类型</td>
<td>如果A等于B则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&lt;=&gt;B</td>
<td>基本数据类型</td>
<td>如果A和B都为NULL，则返回TRUE，如果一边为NULL，返回False</td>
</tr>
<tr>
<td>A&lt;&gt;B, A!=B</td>
<td>基本数据类型</td>
<td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&lt;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&lt;=B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&gt;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&gt;=B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A [NOT] BETWEEN B AND C</td>
<td>基本数据类型</td>
<td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。</td>
</tr>
<tr>
<td>A IS NULL</td>
<td>所有数据类型</td>
<td>如果A等于NULL，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A IS NOT NULL</td>
<td>所有数据类型</td>
<td>如果A不等于NULL，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>IN(数值1, 数值2)</td>
<td>所有数据类型</td>
<td>使用 IN运算显示列表中的值</td>
</tr>
<tr>
<td>A [NOT] LIKE B</td>
<td>STRING 类型</td>
<td>B是一个SQL下的简单正则表达式，也叫通配符模式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td>
</tr>
<tr>
<td>A RLIKE B, A REGEXP B</td>
<td>STRING 类型</td>
<td>B是基于java的正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td>
</tr>
</tbody></table>
</li>
<li><p>案例实操</p>
<ul>
<li>查询出薪水等于5000的所有员工  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp <span class="built_in">where</span> sal =5000;</span><br></pre></td></tr></table></figure></li>
<li>查询工资在500到1000的员工信息  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">between</span> <span class="number">500</span> <span class="keyword">and</span> <span class="number">1000</span>;</span><br></pre></td></tr></table></figure></li>
<li>查询comm为空的所有员工信息  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> comm <span class="keyword">is</span> <span class="keyword">null</span>;</span><br></pre></td></tr></table></figure></li>
<li>查询工资是1500或5000的员工信息  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">IN</span> (<span class="number">1500</span>, <span class="number">5000</span>);</span><br></pre></td></tr></table></figure>
<h3 id="6-1-8-Like和RLike"><a href="#6-1-8-Like和RLike" class="headerlink" title="6.1.8 Like和RLike"></a>6.1.8 Like和RLike</h3></li>
</ul>
</li>
<li><p>使用LIKE运算选择类似的值</p>
</li>
<li><p>选择条件可以包含字符或数字:</p>
<ul>
<li>% 代表零个或多个字符(任意个字符)。</li>
<li>_ 代表一个字符。</li>
</ul>
</li>
<li><p>RLIKE子句</p>
<ul>
<li>RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。</li>
</ul>
</li>
<li><p>案例实操</p>
<ol>
<li>查找名字以A开头的员工信息 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> ename <span class="keyword">LIKE</span> <span class="string">&#x27;A%&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>查找名字中第二个字母为A的员工信息 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> ename <span class="keyword">LIKE</span> <span class="string">&#x27;_A%&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>查找名字中带有A-N的员工信息 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> ename  RLIKE <span class="string">&#x27;[A-N]&#x27;</span>;</span><br></pre></td></tr></table></figure>
<h3 id="6-1-9-逻辑运算符（And-Or-Not）"><a href="#6-1-9-逻辑运算符（And-Or-Not）" class="headerlink" title="6.1.9 逻辑运算符（And/Or/Not）"></a>6.1.9 逻辑运算符（And/Or/Not）</h3><table>
<thead>
<tr>
<th>操作符</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>AND</td>
<td>逻辑并</td>
</tr>
<tr>
<td>OR</td>
<td>逻辑或</td>
</tr>
<tr>
<td>NOT</td>
<td>逻辑否</td>
</tr>
</tbody></table>
</li>
</ol>
</li>
<li><p>案例实操</p>
<ol>
<li>查询薪水大于1000，部门是30 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal<span class="operator">&gt;</span><span class="number">1000</span> <span class="keyword">and</span> deptno<span class="operator">=</span><span class="number">30</span>;</span><br></pre></td></tr></table></figure></li>
<li>查询薪水大于1000，或者部门是30 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal<span class="operator">&gt;</span><span class="number">1000</span> <span class="keyword">or</span> deptno<span class="operator">=</span><span class="number">30</span>;</span><br></pre></td></tr></table></figure></li>
<li>查询除了20部门和30部门以外的员工信息 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> deptno <span class="keyword">not</span> <span class="keyword">IN</span>(<span class="number">30</span>, <span class="number">20</span>);</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h2 id="6-2-分组"><a href="#6-2-分组" class="headerlink" title="6.2 分组"></a>6.2 分组</h2><h3 id="6-2-1-Group-By语句"><a href="#6-2-1-Group-By语句" class="headerlink" title="6.2.1 Group By语句"></a>6.2.1 Group By语句</h3><p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。</p>
<ol>
<li>案例实操：<ol>
<li>计算emp表每个部门的平均工资 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> t.deptno, <span class="built_in">avg</span>(t.sal) avg_sal <span class="keyword">from</span> emp t <span class="keyword">group</span> <span class="keyword">by</span> t.deptno;</span><br></pre></td></tr></table></figure></li>
<li>计算emp每个部门中每个岗位的最高薪水 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> t.deptno, t.job, <span class="built_in">max</span>(t.sal) max_sal <span class="keyword">from</span> emp t <span class="keyword">group</span> <span class="keyword">by</span> t.deptno, t.job;</span><br></pre></td></tr></table></figure>
<h3 id="6-2-2-Having语句"><a href="#6-2-2-Having语句" class="headerlink" title="6.2.2 Having语句"></a>6.2.2 Having语句</h3></li>
</ol>
</li>
<li>having与where不同点<ol>
<li>where后面不能写分组函数，而having后面可以使用分组函数。</li>
<li>having只用于group by分组统计语句。</li>
</ol>
</li>
<li>案例实操<ol>
<li>求每个部门的平均薪水大于2000的部门<ol>
<li>求每个部门的平均工资 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> deptno, <span class="built_in">avg</span>(sal) <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure></li>
<li>求每个部门的平均薪水大于2000的部门  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> deptno, <span class="built_in">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno <span class="keyword">having</span> avg_sal <span class="operator">&gt;</span> <span class="number">2000</span>;</span><br></pre></td></tr></table></figure>
<h2 id="6-3-Join语句"><a href="#6-3-Join语句" class="headerlink" title="6.3 Join语句"></a>6.3 Join语句</h2><h3 id="6-3-1-等值Join"><a href="#6-3-1-等值Join" class="headerlink" title="6.3.1 等值Join"></a>6.3.1 等值Join</h3>Hive支持通常的SQL JOIN语句。 </li>
</ol>
</li>
</ol>
</li>
<li>案例实操<ol>
<li>根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门名称； <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> e.empno, e.ename, d.deptno, d.dname <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h3 id="6-3-2-表的别名"><a href="#6-3-2-表的别名" class="headerlink" title="6.3.2 表的别名"></a>6.3.2 表的别名</h3><ol>
<li>好处<ol>
<li>使用别名可以简化查询。</li>
<li>使用表名前缀可以提高执行效率。</li>
</ol>
</li>
<li>案例实操<ol>
<li>合并员工表和部门表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>
<h3 id="6-3-3-内连接"><a href="#6-3-3-内连接" class="headerlink" title="6.3.3 内连接"></a>6.3.3 内连接</h3></li>
</ol>
</li>
</ol>
<ul>
<li>只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="6-3-4-左外连接"><a href="#6-3-4-左外连接" class="headerlink" title="6.3.4 左外连接"></a>6.3.4 左外连接</h3><ul>
<li>JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="6-3-5-右外连接"><a href="#6-3-5-右外连接" class="headerlink" title="6.3.5 右外连接"></a>6.3.5 右外连接</h3><ul>
<li>JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="6-3-6-满外连接"><a href="#6-3-6-满外连接" class="headerlink" title="6.3.6 满外连接"></a>6.3.6 满外连接</h3><ul>
<li>将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">full</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>
<h3 id="6-3-7-多表连接"><a href="#6-3-7-多表连接" class="headerlink" title="6.3.7 多表连接"></a>6.3.7 多表连接</h3></li>
<li>注意：连接 n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。</li>
<li>数据准备  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">location</span><br><span class="line">1700	Beijing</span><br><span class="line">1800	London</span><br><span class="line">1900	Tokyo</span><br></pre></td></tr></table></figure></li>
</ul>
<ol>
<li>创建位置表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> location(</span><br><span class="line">    loc <span class="type">int</span>,</span><br><span class="line">    loc_name string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>导入数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/location.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> location;</span><br></pre></td></tr></table></figure></li>
<li>多表连接查询 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">SELECT</span></span><br><span class="line">       e.emp_name,</span><br><span class="line">       d.dept_name,</span><br><span class="line">       l.loc_name</span><br><span class="line"><span class="keyword">FROM</span> emp e</span><br><span class="line"><span class="keyword">JOIN</span> dept d <span class="keyword">ON</span> d.dept_no <span class="operator">=</span> e.dept_no</span><br><span class="line"><span class="keyword">JOIN</span> location l <span class="keyword">ON</span> d.loc <span class="operator">=</span> l.loc;</span><br></pre></td></tr></table></figure>
<ul>
<li>大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l;进行连接操作。</li>
<li>注意：为什么不是表d和表l先进行连接操作呢？这是因为Hive总是按照从左到右的顺序执行的。</li>
<li>优化：当对3个或者更多表进行join连接时，如果每个on子句都使用相同的连接键的话，那么只会产生一个MapReduce job。</li>
</ul>
</li>
</ol>
<h3 id="6-3-8-笛卡尔积"><a href="#6-3-8-笛卡尔积" class="headerlink" title="6.3.8 笛卡尔积"></a>6.3.8 笛卡尔积</h3><ol>
<li>笛卡尔积会在下面条件下产生<ol>
<li>省略连接条件</li>
<li>连接条件无效</li>
<li>所有表中的所有行互相连接</li>
</ol>
</li>
<li>案例实操 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> emp_name, dept_name <span class="keyword">from</span> emp, dept;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="6-4-排序"><a href="#6-4-排序" class="headerlink" title="6.4 排序"></a>6.4 排序</h2><h3 id="6-4-1-全局排序（Order-By）"><a href="#6-4-1-全局排序（Order-By）" class="headerlink" title="6.4.1 全局排序（Order By）"></a>6.4.1 全局排序（Order By）</h3><p>Order By：全局排序，只有一个Reducer</p>
<ol>
<li>使用 ORDER BY 子句排序<ul>
<li>ASC（ascend）: 升序（默认）</li>
<li>DESC（descend）: 降序</li>
</ul>
</li>
<li>ORDER BY 子句在SELECT语句的结尾</li>
<li>案例实操 <ol>
<li>查询员工信息按工资升序排列 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal;</span><br></pre></td></tr></table></figure></li>
<li>查询员工信息按工资降序排列 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>
<h3 id="6-4-2-按照别名排序"><a href="#6-4-2-按照别名排序" class="headerlink" title="6.4.2 按照别名排序"></a>6.4.2 按照别名排序</h3></li>
</ol>
</li>
</ol>
<ul>
<li>按照员工薪水的2倍排序  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> ename, sal<span class="operator">*</span><span class="number">2</span> twosal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> twosal;</span><br></pre></td></tr></table></figure>
<h3 id="6-4-3-多个列排序"><a href="#6-4-3-多个列排序" class="headerlink" title="6.4.3 多个列排序"></a>6.4.3 多个列排序</h3></li>
<li>按照部门和工资升序排序  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> ename, deptno, sal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> deptno, sal ;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="6-4-4-Sort-By-每个Reduce内部排序"><a href="#6-4-4-Sort-By-每个Reduce内部排序" class="headerlink" title="6.4.4 Sort By 每个Reduce内部排序"></a>6.4.4 Sort By 每个Reduce内部排序</h3><ul>
<li>Sort By：对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用sort by。</li>
<li>Sort by为每个reducer产生一个排序文件。每个Reducer内部进行排序，对全局结果集来说不是排序。</li>
</ul>
<ol>
<li>设置reduce个数 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">2</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.005</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.job.reduces;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------------+</span></span><br><span class="line"><span class="operator">|</span>           <span class="keyword">set</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------------+</span></span><br><span class="line"><span class="operator">|</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">2</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.204</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>根据部门编号降序查看员工信息 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> mock_data.emp sort <span class="keyword">by</span> dept_no <span class="keyword">desc</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+---------------+------------+----------+----------------+----------+-----------+--------------+</span></span><br><span class="line"><span class="operator">|</span> emp.emp_no  <span class="operator">|</span> emp.emp_name  <span class="operator">|</span>  emp.job   <span class="operator">|</span> emp.mgr  <span class="operator">|</span> emp.hire_date  <span class="operator">|</span> emp.sal  <span class="operator">|</span> emp.comm  <span class="operator">|</span> emp.dept_no  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+---------------+------------+----------+----------------+----------+-----------+--------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7521</span>        <span class="operator">|</span> WARD          <span class="operator">|</span> SALESMAN   <span class="operator">|</span> <span class="number">7698</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-2</span><span class="number">-22</span>      <span class="operator">|</span> <span class="number">1250.0</span>   <span class="operator">|</span> <span class="number">500.0</span>     <span class="operator">|</span> <span class="number">30</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7900</span>        <span class="operator">|</span> JAMES         <span class="operator">|</span> CLERK      <span class="operator">|</span> <span class="number">7698</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-12</span><span class="number">-3</span>      <span class="operator">|</span> <span class="number">950.0</span>    <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">30</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7844</span>        <span class="operator">|</span> TURNER        <span class="operator">|</span> SALESMAN   <span class="operator">|</span> <span class="number">7698</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-9</span><span class="number">-8</span>       <span class="operator">|</span> <span class="number">1500.0</span>   <span class="operator">|</span> <span class="number">0.0</span>       <span class="operator">|</span> <span class="number">30</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7654</span>        <span class="operator">|</span> MARTIN        <span class="operator">|</span> SALESMAN   <span class="operator">|</span> <span class="number">7698</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-9</span><span class="number">-28</span>      <span class="operator">|</span> <span class="number">1250.0</span>   <span class="operator">|</span> <span class="number">1400.0</span>    <span class="operator">|</span> <span class="number">30</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7876</span>        <span class="operator">|</span> ADAMS         <span class="operator">|</span> CLERK      <span class="operator">|</span> <span class="number">7788</span>     <span class="operator">|</span> <span class="number">1987</span><span class="number">-5</span><span class="number">-23</span>      <span class="operator">|</span> <span class="number">1100.0</span>   <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">20</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7902</span>        <span class="operator">|</span> FORD          <span class="operator">|</span> ANALYST    <span class="operator">|</span> <span class="number">7566</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-12</span><span class="number">-3</span>      <span class="operator">|</span> <span class="number">3000.0</span>   <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">20</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7788</span>        <span class="operator">|</span> SCOTT         <span class="operator">|</span> ANALYST    <span class="operator">|</span> <span class="number">7566</span>     <span class="operator">|</span> <span class="number">1987</span><span class="number">-4</span><span class="number">-19</span>      <span class="operator">|</span> <span class="number">3000.0</span>   <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">20</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7369</span>        <span class="operator">|</span> SMITH         <span class="operator">|</span> CLERK      <span class="operator">|</span> <span class="number">7902</span>     <span class="operator">|</span> <span class="number">1980</span><span class="number">-12</span><span class="number">-17</span>     <span class="operator">|</span> <span class="number">800.0</span>    <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">20</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7566</span>        <span class="operator">|</span> JONES         <span class="operator">|</span> MANAGER    <span class="operator">|</span> <span class="number">7839</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-4</span><span class="number">-2</span>       <span class="operator">|</span> <span class="number">2975.0</span>   <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">20</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7499</span>        <span class="operator">|</span> ALLEN         <span class="operator">|</span> SALESMAN   <span class="operator">|</span> <span class="number">7698</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-2</span><span class="number">-20</span>      <span class="operator">|</span> <span class="number">1600.0</span>   <span class="operator">|</span> <span class="number">300.0</span>     <span class="operator">|</span> <span class="number">30</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7698</span>        <span class="operator">|</span> BLAKE         <span class="operator">|</span> MANAGER    <span class="operator">|</span> <span class="number">7839</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-5</span><span class="number">-1</span>       <span class="operator">|</span> <span class="number">2850.0</span>   <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">30</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7934</span>        <span class="operator">|</span> MILLER        <span class="operator">|</span> CLERK      <span class="operator">|</span> <span class="number">7782</span>     <span class="operator">|</span> <span class="number">1982</span><span class="number">-1</span><span class="number">-23</span>      <span class="operator">|</span> <span class="number">1300.0</span>   <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">10</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7839</span>        <span class="operator">|</span> KING          <span class="operator">|</span> PRESIDENT  <span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-11</span><span class="number">-17</span>     <span class="operator">|</span> <span class="number">5000.0</span>   <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">10</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7782</span>        <span class="operator">|</span> CLARK         <span class="operator">|</span> MANAGER    <span class="operator">|</span> <span class="number">7839</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-6</span><span class="number">-9</span>       <span class="operator">|</span> <span class="number">2450.0</span>   <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">10</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+---------------+------------+----------+----------------+----------+-----------+--------------+</span></span><br><span class="line"><span class="number">14</span> <span class="keyword">rows</span> selected (<span class="number">16.877</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>将查询结果导入到文件中（按照部门编号降序排序） <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">2</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.037</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/home/atguigu/hive/emp&#x27;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> mock_data.emp sort <span class="keyword">by</span> dept_no <span class="keyword">desc</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">17.73</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="operator">!</span>quit</span><br><span class="line">Closing: <span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span></span><br><span class="line">[atguigu<span class="variable">@hadoop001</span> emp]$ ls</span><br><span class="line"><span class="number">000000</span>_0  <span class="number">000001</span>_0</span><br><span class="line">[atguigu<span class="variable">@hadoop001</span> emp]$</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="6-4-5-分区（Distribute-By）"><a href="#6-4-5-分区（Distribute-By）" class="headerlink" title="6.4.5 分区（Distribute By）"></a>6.4.5 分区（Distribute By）</h3><ul>
<li>Distribute By： 在有些情况下，我们需要控制某个特定行应该到哪个reducer，通常是为了进行后续的聚集操作。distribute by 子句可以做这件事。</li>
<li>Distribute by类似MR中partition（自定义分区），进行分区，结合sort by使用。 </li>
<li>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</li>
</ul>
<ol>
<li>案例实操：<ol>
<li>先按照部门编号分区，再按照员工编号降序排序。 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">3</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.035</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/home/atguigu/hive/emp/distribute&#x27;</span> <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> mock_data.emp distribute <span class="keyword">by</span> dept_no sort <span class="keyword">by</span> emp_no <span class="keyword">desc</span> ;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">18.69</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="operator">!</span>quit</span><br><span class="line">Closing: <span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span></span><br><span class="line">[atguigu<span class="variable">@hadoop001</span> distribute]$ ls</span><br><span class="line"><span class="number">000000</span>_0  <span class="number">000001</span>_0  <span class="number">000002</span>_0</span><br><span class="line">[atguigu<span class="variable">@hadoop001</span> distribute]$ cat <span class="number">000000</span>_0</span><br><span class="line"><span class="number">7900</span>	JAMES	CLERK	<span class="number">7698</span>	<span class="number">1981</span><span class="number">-12</span><span class="number">-3</span>	<span class="number">950.0</span>	\N	<span class="number">30</span></span><br><span class="line"><span class="number">7844</span>	TURNER	SALESMAN	<span class="number">7698</span>	<span class="number">1981</span><span class="number">-9</span><span class="number">-8</span>	<span class="number">1500.0</span>	<span class="number">0.0</span>	<span class="number">30</span></span><br><span class="line"><span class="number">7698</span>	BLAKE	MANAGER	<span class="number">7839</span>	<span class="number">1981</span><span class="number">-5</span><span class="number">-1</span>	<span class="number">2850.0</span>	\N	<span class="number">30</span></span><br><span class="line"><span class="number">7654</span>	MARTIN	SALESMAN	<span class="number">7698</span>	<span class="number">1981</span><span class="number">-9</span><span class="number">-28</span>	<span class="number">1250.0</span>	<span class="number">1400.0</span>	<span class="number">30</span></span><br><span class="line"><span class="number">7521</span>	WARD	SALESMAN	<span class="number">7698</span>	<span class="number">1981</span><span class="number">-2</span><span class="number">-22</span>	<span class="number">1250.0</span>	<span class="number">500.0</span>	<span class="number">30</span></span><br><span class="line"><span class="number">7499</span>	ALLEN	SALESMAN	<span class="number">7698</span>	<span class="number">1981</span><span class="number">-2</span><span class="number">-20</span>	<span class="number">1600.0</span>	<span class="number">300.0</span>	<span class="number">30</span></span><br><span class="line">[atguigu<span class="variable">@hadoop001</span> distribute]$ cat <span class="number">000001</span>_0</span><br><span class="line"><span class="number">7934</span>	MILLER	CLERK	<span class="number">7782</span>	<span class="number">1982</span><span class="number">-1</span><span class="number">-23</span>	<span class="number">1300.0</span>	\N	<span class="number">10</span></span><br><span class="line"><span class="number">7839</span>	KING	PRESIDENT	\N	<span class="number">1981</span><span class="number">-11</span><span class="number">-17</span>	<span class="number">5000.0</span>	\N	<span class="number">10</span></span><br><span class="line"><span class="number">7782</span>	CLARK	MANAGER	<span class="number">7839</span>	<span class="number">1981</span><span class="number">-6</span><span class="number">-9</span>	<span class="number">2450.0</span>	\N	<span class="number">10</span></span><br><span class="line">[atguigu<span class="variable">@hadoop001</span> distribute]$ cat <span class="number">000002</span>_0</span><br><span class="line"><span class="number">7902</span>	FORD	ANALYST	<span class="number">7566</span>	<span class="number">1981</span><span class="number">-12</span><span class="number">-3</span>	<span class="number">3000.0</span>	\N	<span class="number">20</span></span><br><span class="line"><span class="number">7876</span>	ADAMS	CLERK	<span class="number">7788</span>	<span class="number">1987</span><span class="number">-5</span><span class="number">-23</span>	<span class="number">1100.0</span>	\N	<span class="number">20</span></span><br><span class="line"><span class="number">7788</span>	SCOTT	ANALYST	<span class="number">7566</span>	<span class="number">1987</span><span class="number">-4</span><span class="number">-19</span>	<span class="number">3000.0</span>	\N	<span class="number">20</span></span><br><span class="line"><span class="number">7566</span>	JONES	MANAGER	<span class="number">7839</span>	<span class="number">1981</span><span class="number">-4</span><span class="number">-2</span>	<span class="number">2975.0</span>	\N	<span class="number">20</span></span><br><span class="line"><span class="number">7369</span>	SMITH	CLERK	<span class="number">7902</span>	<span class="number">1980</span><span class="number">-12</span><span class="number">-17</span>	<span class="number">800.0</span>	\N	<span class="number">20</span></span><br><span class="line">[atguigu<span class="variable">@hadoop001</span> distribute]$</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>注意：<ul>
<li>distribute by的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。</li>
<li>Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。</li>
</ul>
</li>
</ol>
<h3 id="6-4-6-Cluster-By"><a href="#6-4-6-Cluster-By" class="headerlink" title="6.4.6 Cluster By"></a>6.4.6 Cluster By</h3><ul>
<li>当distribute by和sort by字段相同时，可以使用cluster by方式。</li>
<li>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</li>
<li>以下两种写法等价  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp distribute <span class="keyword">by</span> deptno sort <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure></li>
<li>注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</li>
</ul>
<h1 id="七、分区表和分桶表"><a href="#七、分区表和分桶表" class="headerlink" title="七、分区表和分桶表"></a>七、分区表和分桶表</h1><h2 id="7-1-分区表"><a href="#7-1-分区表" class="headerlink" title="7.1 分区表"></a>7.1 分区表</h2><ul>
<li>概念：<ol>
<li>出于对数据查询的优化，考虑Hive常规查询会进行全表扫描，效率低下。在建表的时候使用分区表规划更合理的存储结构</li>
<li>分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。</li>
<li>Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。</li>
</ol>
</li>
</ul>
<h3 id="7-1-1-分区表基本操作"><a href="#7-1-1-分区表基本操作" class="headerlink" title="7.1.1 分区表基本操作"></a>7.1.1 分区表基本操作</h3><ol>
<li><p>创建分区表语法</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition (</span><br><span class="line">    deptno <span class="type">int</span>,</span><br><span class="line">    dname  string,</span><br><span class="line">    loc    string</span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">day</span> string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>注意：分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。</li>
</ul>
</li>
<li><p>加载数据到分区表中</p>
<ul>
<li>数据准备  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dept_20200401.log</span><br><span class="line">10	ACCOUNTING	1700</span><br><span class="line">20	RESEARCH	1800</span><br><span class="line"></span><br><span class="line">dept_20200402.log</span><br><span class="line">30	SALES	1900</span><br><span class="line">40	OPERATIONS	1700</span><br><span class="line"></span><br><span class="line">dept_20200403.log</span><br><span class="line">50	TEST	2000</span><br><span class="line">60	DEV	1900</span><br></pre></td></tr></table></figure></li>
<li>加载数据  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000&gt; load data local inpath &#x27;/home/atguigu/hive/emp/partitioned/dept_20200401.log&#x27; into table mock_data.dept_partition partition(day=&#x27;20200401&#x27;);</span><br><span class="line">No rows affected (1.197 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; load data local inpath &#x27;/home/atguigu/hive/emp/partitioned/dept_20200402.log&#x27; into table mock_data.dept_partition partition(day=&#x27;20200402&#x27;);</span><br><span class="line">No rows affected (0.363 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; load data local inpath &#x27;/home/atguigu/hive/emp/partitioned/dept_20200403.log&#x27; into table mock_data.dept_partition partition(day=&#x27;20200403&#x27;);</span><br><span class="line">No rows affected (0.307 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; dfs -ls /user/hive/warehouse/mock_data.db/dept_partition;</span><br><span class="line">+----------------------------------------------------+</span><br><span class="line">|                     DFS Output                     |</span><br><span class="line">+----------------------------------------------------+</span><br><span class="line">| Found 3 items                                      |</span><br><span class="line">| drwxr-xr-x   - atguigu supergroup          0 2021-10-26 14:18 /user/hive/warehouse/mock_data.db/dept_partition/day=20200401 |</span><br><span class="line">| drwxr-xr-x   - atguigu supergroup          0 2021-10-26 14:18 /user/hive/warehouse/mock_data.db/dept_partition/day=20200402 |</span><br><span class="line">| drwxr-xr-x   - atguigu supergroup          0 2021-10-26 14:18 /user/hive/warehouse/mock_data.db/dept_partition/day=20200403 |</span><br><span class="line">+----------------------------------------------------+</span><br><span class="line">4 rows selected (0.01 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br></pre></td></tr></table></figure></li>
<li>注意：分区表加载数据时，必须指定分区</li>
</ul>
</li>
<li><p>查询分区表中数据</p>
<ul>
<li>单分区查询  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>多分区联合查询  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200402&#x27;</span></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200403&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span> <span class="keyword">in</span> (<span class="string">&#x27;20200401&#x27;</span>,<span class="string">&#x27;20200402&#x27;</span>,<span class="string">&#x27;20200403&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span> <span class="keyword">between</span> <span class="number">20200401</span> <span class="keyword">and</span> <span class="number">20200403</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> (<span class="keyword">day</span> <span class="operator">=</span> <span class="string">&#x27;20200401&#x27;</span> <span class="keyword">or</span> <span class="keyword">day</span> <span class="operator">=</span><span class="string">&#x27;20200402&#x27;</span> <span class="keyword">or</span> <span class="keyword">day</span> <span class="operator">=</span> <span class="string">&#x27;20200403&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>增加分区</p>
<ul>
<li>创建单个分区  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> mock_data.dept_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200404&#x27;</span>) ;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.099</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> partitions mock_data.dept_partition;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span>   <span class="keyword">partition</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200402</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200403</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200404</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="number">4</span> <span class="keyword">rows</span> selected (<span class="number">0.075</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>同时创建多个分区  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> mock_data.dept_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200405&#x27;</span>) <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200406&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.135</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> partitions mock_data.dept_partition;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span>   <span class="keyword">partition</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200402</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200403</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200404</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200405</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200406</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="number">6</span> <span class="keyword">rows</span> selected (<span class="number">0.094</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>删除分区</p>
<ul>
<li>删除单个分区  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> mock_data.dept_partition <span class="keyword">drop</span> <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200404&#x27;</span>) ;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.154</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> partitions mock_data.dept_partition;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span>   <span class="keyword">partition</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200402</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200403</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200405</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200406</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> selected (<span class="number">0.065</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>同时删除多个分区  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> mock_data.dept_partition <span class="keyword">drop</span> <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200405&#x27;</span>), <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200406&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.223</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> partitions mock_data.dept_partition;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span>   <span class="keyword">partition</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200402</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200403</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> selected (<span class="number">0.068</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>查看分区表有多少分区</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> partitions mock_data.dept_partition;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span>   <span class="keyword">partition</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200402</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200403</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> selected (<span class="number">0.068</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li><p>查看分区表结构(关注Partition Information.numPartitions)</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> formatted mock_data.dept_partition;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------------+----------------------------------------------------+-----------------------+</span></span><br><span class="line"><span class="operator">|</span>           col_name            <span class="operator">|</span>                     data_type                      <span class="operator">|</span>        comment        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------------+----------------------------------------------------+-----------------------+</span></span><br><span class="line"><span class="operator">|</span> # col_name                    <span class="operator">|</span> data_type                                          <span class="operator">|</span> comment               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> deptno                        <span class="operator">|</span> <span class="type">int</span>                                                <span class="operator">|</span>                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> dname                         <span class="operator">|</span> string                                             <span class="operator">|</span>                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> loc                           <span class="operator">|</span> string                                             <span class="operator">|</span>                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> <span class="keyword">NULL</span>                                               <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> # <span class="keyword">Partition</span> Information       <span class="operator">|</span> <span class="keyword">NULL</span>                                               <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> # col_name                    <span class="operator">|</span> data_type                                          <span class="operator">|</span> comment               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span>                           <span class="operator">|</span> string                                             <span class="operator">|</span>                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> <span class="keyword">NULL</span>                                               <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> # Detailed <span class="keyword">Table</span> Information  <span class="operator">|</span> <span class="keyword">NULL</span>                                               <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Database:                     <span class="operator">|</span> mock_data                                          <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> OwnerType:                    <span class="operator">|</span> <span class="keyword">USER</span>                                               <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Owner:                        <span class="operator">|</span> atguigu                                            <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> CreateTime:                   <span class="operator">|</span> Tue Oct <span class="number">26</span> <span class="number">14</span>:<span class="number">10</span>:<span class="number">53</span> CST <span class="number">2021</span>                       <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> LastAccessTime:               <span class="operator">|</span> <span class="literal">UNKNOWN</span>                                            <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Retention:                    <span class="operator">|</span> <span class="number">0</span>                                                  <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Location:                     <span class="operator">|</span> hdfs:<span class="operator">/</span><span class="operator">/</span>mycluster<span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>dept_partition <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">Table</span> Type:                   <span class="operator">|</span> MANAGED_TABLE                                      <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">Table</span> Parameters:             <span class="operator">|</span> <span class="keyword">NULL</span>                                               <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> bucketing_version                                  <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> numFiles                                           <span class="operator">|</span> <span class="number">3</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> numPartitions                                      <span class="operator">|</span> <span class="number">3</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> numRows                                            <span class="operator">|</span> <span class="number">0</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> rawDataSize                                        <span class="operator">|</span> <span class="number">0</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> totalSize                                          <span class="operator">|</span> <span class="number">95</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> transient_lastDdlTime                              <span class="operator">|</span> <span class="number">1635228653</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> <span class="keyword">NULL</span>                                               <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> # Storage Information         <span class="operator">|</span> <span class="keyword">NULL</span>                                               <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> SerDe Library:                <span class="operator">|</span> org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> InputFormat:                  <span class="operator">|</span> org.apache.hadoop.mapred.TextInputFormat           <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> OutputFormat:                 <span class="operator">|</span> org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Compressed:                   <span class="operator">|</span> <span class="keyword">No</span>                                                 <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Num Buckets:                  <span class="operator">|</span> <span class="number">-1</span>                                                 <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Bucket Columns:               <span class="operator">|</span> []                                                 <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Sort Columns:                 <span class="operator">|</span> []                                                 <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Storage <span class="keyword">Desc</span> Params:          <span class="operator">|</span> <span class="keyword">NULL</span>                                               <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> field.delim                                        <span class="operator">|</span> \t                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> serialization.format                               <span class="operator">|</span> \t                    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------------+----------------------------------------------------+-----------------------+</span></span><br><span class="line"><span class="number">38</span> <span class="keyword">rows</span> selected (<span class="number">0.151</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="7-1-2-二级分区"><a href="#7-1-2-二级分区" class="headerlink" title="7.1.2 二级分区"></a>7.1.2 二级分区</h3><p>思考: 如何一天的日志数据量也很大，如何再将数据拆分?</p>
<ol>
<li>创建多级级分区表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition_level (</span><br><span class="line">    deptno <span class="type">int</span>,</span><br><span class="line">    dname  string,</span><br><span class="line">    loc    string</span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">year</span> string,<span class="keyword">month</span> string, <span class="keyword">day</span> string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>正常的加载数据<ul>
<li>加载数据到分区表中  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/emp/partitioned/dept_20200401.log&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> mock_data.dept_partition_level <span class="keyword">partition</span>(<span class="keyword">year</span><span class="operator">=</span><span class="string">&#x27;2020&#x27;</span>, <span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;04&#x27;</span>,<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;01&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.292</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/emp/partitioned/dept_20200402.log&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> mock_data.dept_partition_level <span class="keyword">partition</span>(<span class="keyword">year</span><span class="operator">=</span><span class="string">&#x27;2020&#x27;</span>, <span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;04&#x27;</span>,<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;02&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.295</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/emp/partitioned/dept_20200403.log&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> mock_data.dept_partition_level <span class="keyword">partition</span>(<span class="keyword">year</span><span class="operator">=</span><span class="string">&#x27;2020&#x27;</span>, <span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;04&#x27;</span>,<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;03&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.279</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>查询分区数据  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">    <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> dept_partition_level</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">year</span> <span class="operator">=</span> <span class="number">2020</span></span><br><span class="line">  <span class="keyword">and</span> <span class="keyword">month</span> <span class="operator">=</span> <span class="number">04</span></span><br><span class="line">  <span class="keyword">and</span> <span class="keyword">day</span> <span class="operator">=</span> <span class="number">01</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式<ul>
<li>方式一：上传数据后修复<ul>
<li>上传数据  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 partitioned]$ hadoop fs -mkdir -p /user/hive/warehouse/mock_data.db/dept_partition_level/year=2021/month=06/day=09</span><br><span class="line">[atguigu@hadoop001 partitioned]$ hadoop fs -put dept_20210609.log /user/hive/warehouse/mock_data.db/dept_partition_level/year=2021/month=06/day=09</span><br><span class="line">2021-10-26 15:36:57,908 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">[atguigu@hadoop001 partitioned]$</span><br></pre></td></tr></table></figure></li>
<li>查询数据（查询不到刚上传的数据）  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000&gt; select *</span><br><span class="line"> from mock_data.dept_partition_level</span><br><span class="line"> <span class="built_in">where</span> year = 2021;</span><br><span class="line">+------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span><br><span class="line">| dept_partition_level.deptno  | dept_partition_level.dname  | dept_partition_level.loc  | dept_partition_level.year  | dept_partition_level.month  | dept_partition_level.day  |</span><br><span class="line">+------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span><br><span class="line">| 60                           | ACCOUNTING                  | 1700                      | 2021                       | 04                          | 01                        |</span><br><span class="line">+------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span><br><span class="line">1 row selected (0.118 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br></pre></td></tr></table></figure></li>
<li>执行修复命令再次查询数据  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> msck repair <span class="keyword">table</span> mock_data.dept_partition_level;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.139</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span></span><br><span class="line"> <span class="keyword">from</span> mock_data.dept_partition_level</span><br><span class="line"> <span class="keyword">where</span> <span class="keyword">year</span> <span class="operator">=</span> <span class="number">2021</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span></span><br><span class="line"><span class="operator">|</span> dept_partition_level.deptno  <span class="operator">|</span> dept_partition_level.dname  <span class="operator">|</span> dept_partition_level.loc  <span class="operator">|</span> dept_partition_level.year  <span class="operator">|</span> dept_partition_level.month  <span class="operator">|</span> dept_partition_level.day  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">60</span>                           <span class="operator">|</span> ACCOUNTING                  <span class="operator">|</span> <span class="number">1700</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">04</span>                          <span class="operator">|</span> <span class="number">01</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>                           <span class="operator">|</span> ACCOUNTING                  <span class="operator">|</span> <span class="number">1700</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">06</span>                          <span class="operator">|</span> <span class="number">09</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>                           <span class="operator">|</span> RESEARCH                    <span class="operator">|</span> <span class="number">1800</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">06</span>                          <span class="operator">|</span> <span class="number">09</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> selected (<span class="number">0.114</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>方式二：上传数据后添加分区<ul>
<li>上传数据  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 partitioned]$ hadoop fs -mkdir -p /user/hive/warehouse/mock_data.db/dept_partition_level/year=2021/month=10/day=26</span><br><span class="line">[atguigu@hadoop001 partitioned]$ hadoop fs -put dept_20211026.log /user/hive/warehouse/mock_data.db/dept_partition_level/year=2021/month=10/day=26</span><br><span class="line">2021-10-26 15:51:40,926 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br></pre></td></tr></table></figure></li>
<li>执行添加分区  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> mock_data.dept_partition_level <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">year</span><span class="operator">=</span><span class="string">&#x27;2021&#x27;</span>, <span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;10&#x27;</span>, <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;26&#x27;</span>) ;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.062</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>查询数据  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span></span><br><span class="line"> <span class="keyword">from</span> mock_data.dept_partition_level</span><br><span class="line"> <span class="keyword">where</span> <span class="keyword">year</span> <span class="operator">=</span> <span class="number">2021</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span></span><br><span class="line"><span class="operator">|</span> dept_partition_level.deptno  <span class="operator">|</span> dept_partition_level.dname  <span class="operator">|</span> dept_partition_level.loc  <span class="operator">|</span> dept_partition_level.year  <span class="operator">|</span> dept_partition_level.month  <span class="operator">|</span> dept_partition_level.day  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">60</span>                           <span class="operator">|</span> ACCOUNTING                  <span class="operator">|</span> <span class="number">1700</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">04</span>                          <span class="operator">|</span> <span class="number">01</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>                           <span class="operator">|</span> ACCOUNTING                  <span class="operator">|</span> <span class="number">1700</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">06</span>                          <span class="operator">|</span> <span class="number">09</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>                           <span class="operator">|</span> RESEARCH                    <span class="operator">|</span> <span class="number">1800</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">06</span>                          <span class="operator">|</span> <span class="number">09</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>                           <span class="operator">|</span> ACCOUNTING                  <span class="operator">|</span> <span class="number">1700</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">10</span>                          <span class="operator">|</span> <span class="number">26</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>                           <span class="operator">|</span> RESEARCH                    <span class="operator">|</span> <span class="number">1800</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">10</span>                          <span class="operator">|</span> <span class="number">26</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> selected (<span class="number">0.094</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>方式三：创建文件夹后load数据到分区<ul>
<li>创建目录  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>mkdir <span class="operator">-</span>p <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>dept_partition_level<span class="operator">/</span><span class="keyword">year</span><span class="operator">=</span><span class="number">2021</span><span class="operator">/</span><span class="keyword">month</span><span class="operator">=</span><span class="number">12</span><span class="operator">/</span><span class="keyword">day</span><span class="operator">=</span><span class="number">28</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+</span></span><br><span class="line"><span class="operator">|</span> DFS Output  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+</span></span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> selected (<span class="number">0.017</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>上传数据  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/emp/partitioned/dept_20210609.log&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> mock_data.dept_partition_level <span class="keyword">partition</span>(<span class="keyword">year</span><span class="operator">=</span><span class="string">&#x27;2021&#x27;</span>, <span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;12&#x27;</span>,<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;28&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.275</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>查询数据  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span></span><br><span class="line"> <span class="keyword">from</span> mock_data.dept_partition_level</span><br><span class="line"> <span class="keyword">where</span> <span class="keyword">year</span> <span class="operator">=</span> <span class="number">2021</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span></span><br><span class="line"><span class="operator">|</span> dept_partition_level.deptno  <span class="operator">|</span> dept_partition_level.dname  <span class="operator">|</span> dept_partition_level.loc  <span class="operator">|</span> dept_partition_level.year  <span class="operator">|</span> dept_partition_level.month  <span class="operator">|</span> dept_partition_level.day  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">60</span>                           <span class="operator">|</span> ACCOUNTING                  <span class="operator">|</span> <span class="number">1700</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">04</span>                          <span class="operator">|</span> <span class="number">01</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>                           <span class="operator">|</span> ACCOUNTING                  <span class="operator">|</span> <span class="number">1700</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">06</span>                          <span class="operator">|</span> <span class="number">09</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>                           <span class="operator">|</span> RESEARCH                    <span class="operator">|</span> <span class="number">1800</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">06</span>                          <span class="operator">|</span> <span class="number">09</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>                           <span class="operator">|</span> ACCOUNTING                  <span class="operator">|</span> <span class="number">1700</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">10</span>                          <span class="operator">|</span> <span class="number">26</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>                           <span class="operator">|</span> RESEARCH                    <span class="operator">|</span> <span class="number">1800</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">10</span>                          <span class="operator">|</span> <span class="number">26</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>                           <span class="operator">|</span> ACCOUNTING                  <span class="operator">|</span> <span class="number">1700</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">12</span>                          <span class="operator">|</span> <span class="number">28</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>                           <span class="operator">|</span> RESEARCH                    <span class="operator">|</span> <span class="number">1800</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">12</span>                          <span class="operator">|</span> <span class="number">28</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span></span><br><span class="line"><span class="number">7</span> <span class="keyword">rows</span> selected (<span class="number">0.107</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="7-1-3-动态分区"><a href="#7-1-3-动态分区" class="headerlink" title="7.1.3 动态分区"></a>7.1.3 动态分区</h3></li>
</ul>
</li>
</ul>
</li>
</ol>
<ul>
<li>关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置。<ol>
<li>开启动态分区参数设置<ol>
<li>开启动态分区功能（默认true，开启） <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive.exec.dynamic.partition=true</span><br></pre></td></tr></table></figure></li>
<li>设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。） <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive.exec.dynamic.partition.mode=nonstrict</span><br></pre></td></tr></table></figure></li>
<li>在所有执行MR的节点上，最大一共可以创建多少个动态分区。默认1000 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive.exec.max.dynamic.partitions=1000</span><br></pre></td></tr></table></figure></li>
<li>在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive.exec.max.dynamic.partitions.pernode=100</span><br></pre></td></tr></table></figure></li>
<li>整个MR Job中，最大可以创建多少个HDFS文件。默认100000 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive.exec.max.created.files=100000</span><br></pre></td></tr></table></figure></li>
<li>当有空分区生成时，是否抛出异常。一般不需要设置。默认false <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive.error.on.empty.partition=false</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>案例实操</li>
</ol>
<ul>
<li>需求：将dept表中的数据按照地区（loc字段），插入到目标表dept_partition的相应分区中。<ol>
<li>创建目标分区表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition_dynamic (</span><br><span class="line">    deptno <span class="type">int</span>,</span><br><span class="line">    dname  string</span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (loc string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>设置动态分区，插入数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">set</span> hive.exec.dynamic.partition.mode <span class="operator">=</span> nonstrict;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.005</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> mock_data.dept_partition_dynamic <span class="keyword">partition</span>(loc)</span><br><span class="line"> <span class="keyword">select</span> deptno,dname,loc</span><br><span class="line"> <span class="keyword">from</span> mock_data.dept_partition_level;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">30.709</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>查看目标分区表的分区情况 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> partitions mock_data.dept_partition_dynamic;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------------+</span></span><br><span class="line"><span class="operator">|</span>            <span class="keyword">partition</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------------+</span></span><br><span class="line"><span class="operator">|</span> loc<span class="operator">=</span><span class="number">1700</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> loc<span class="operator">=</span><span class="number">1800</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> loc<span class="operator">=</span><span class="number">1900</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> loc<span class="operator">=</span><span class="number">2000</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> loc<span class="operator">=</span>__HIVE_DEFAULT_PARTITION__  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> selected (<span class="number">0.096</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>思考：目标分区表是如何匹配到分区字段的？<ul>
<li>通过元数据获取分区表和分区字段</li>
<li>通过分区字段匹配查询结果中的字段</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ul>
<h2 id="7-2-分桶表"><a href="#7-2-分桶表" class="headerlink" title="7.2 分桶表"></a>7.2 分桶表</h2><ul>
<li>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。</li>
<li>分桶是将数据集分解成更容易管理的若干部分的另一个技术。</li>
<li>分区针对的是数据的存储路径；分桶针对的是数据文件。</li>
</ul>
<ol>
<li>先创建分桶表<ol>
<li>数据准备 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1001	ss1</span><br><span class="line">1002	ss2</span><br><span class="line">1003	ss3</span><br><span class="line">1004	ss4</span><br><span class="line">1005	ss5</span><br><span class="line">1006	ss6</span><br><span class="line">1007	ss7</span><br><span class="line">1008	ss8</span><br><span class="line">1009	ss9</span><br><span class="line">1010	ss10</span><br><span class="line">1011	ss11</span><br><span class="line">1012	ss12</span><br><span class="line">1013	ss13</span><br><span class="line">1014	ss14</span><br><span class="line">1015	ss15</span><br><span class="line">1016	ss16</span><br></pre></td></tr></table></figure></li>
<li>创建分桶表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_bucket(</span><br><span class="line">    id   <span class="type">int</span>,</span><br><span class="line">    name string</span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span> (id) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot;\t&quot;;</span><br></pre></td></tr></table></figure></li>
<li>查看表结构   <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">desc</span> formatted stu_bucket;</span><br><span class="line">Num Buckets:            <span class="number">4</span>   </span><br></pre></td></tr></table></figure></li>
<li>导入数据到分桶表中，load的方式 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/bucket/stu_bucket.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_bucket;</span><br></pre></td></tr></table></figure></li>
<li>查看创建的分桶表中是否分成4个桶 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>stu_bucket;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">4</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup         38 2021-10-26 18:44 /user/hive/warehouse/mock_data.db/stu_bucket/000000_0 |</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup         37 2021-10-26 18:44 /user/hive/warehouse/mock_data.db/stu_bucket/000001_0 |</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup         38 2021-10-26 18:44 /user/hive/warehouse/mock_data.db/stu_bucket/000002_0 |</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup         38 2021-10-26 18:44 /user/hive/warehouse/mock_data.db/stu_bucket/000003_0 |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> selected (<span class="number">0.01</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>分桶规则：<ul>
<li>根据结果可知：Hive的分桶采用对分桶字段的值进行哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中</li>
</ul>
</li>
</ol>
</li>
<li>分桶表操作需要注意的事项:<ol>
<li>reduce的个数设置为-1,让Job自行决定需要用多少个reduce或者将reduce的个数设置为大于等于分桶表的桶数</li>
<li>从hdfs中load数据到分桶表中，避免本地文件找不到问题</li>
<li>不要使用本地模式(考虑到资源问题，防止MR执行失败，yarn会分配任务到不同节点)</li>
</ol>
</li>
<li>insert方式将数据导入分桶表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> stu_bucket <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
<h2 id="7-3-抽样查询"><a href="#7-3-抽样查询" class="headerlink" title="7.3 抽样查询"></a>7.3 抽样查询</h2></li>
</ol>
<ul>
<li>对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。</li>
<li>语法: TABLESAMPLE(BUCKET x OUT OF y ON 分桶字段) </li>
<li>查询表stu_buck中的数据。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> stu_buck <span class="keyword">tablesample</span>(bucket <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">4</span> <span class="keyword">on</span> id);</span><br></pre></td></tr></table></figure></li>
<li>注意：x的值必须小于等于y的值，否则</li>
</ul>
<h1 id="八、函数"><a href="#八、函数" class="headerlink" title="八、函数"></a>八、函数</h1><h2 id="8-1-系统内置函数"><a href="#8-1-系统内置函数" class="headerlink" title="8.1 系统内置函数"></a>8.1 系统内置函数</h2><ol>
<li>查看系统自带的函数 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> functions <span class="keyword">like</span> count;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+</span></span><br><span class="line"><span class="operator">|</span> tab_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+</span></span><br><span class="line"><span class="operator">|</span> count     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.019</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>显示自带的函数的用法 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> <span class="keyword">function</span> upper;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                      tab_name                      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="built_in">upper</span>(str) <span class="operator">-</span> <span class="keyword">Returns</span> str <span class="keyword">with</span> <span class="keyword">all</span> characters changed <span class="keyword">to</span> uppercase <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.019</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>详细显示自带的函数的用法 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> <span class="keyword">function</span> extended upper;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                      tab_name                      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="built_in">upper</span>(str) <span class="operator">-</span> <span class="keyword">Returns</span> str <span class="keyword">with</span> <span class="keyword">all</span> characters changed <span class="keyword">to</span> uppercase <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Synonyms: ucase                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Example:                                           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   <span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="built_in">upper</span>(<span class="string">&#x27;Facebook&#x27;</span>) <span class="keyword">FROM</span> src LIMIT <span class="number">1</span>;     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   <span class="string">&#x27;FACEBOOK&#x27;</span>                                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">Function</span> class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">Function</span> type:BUILTIN                              <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">7</span> <span class="keyword">rows</span> selected (<span class="number">0.079</span> seconds)</span><br></pre></td></tr></table></figure>
<h2 id="8-2-常用内置函数"><a href="#8-2-常用内置函数" class="headerlink" title="8.2 常用内置函数"></a>8.2 常用内置函数</h2><h3 id="8-2-1-空字段赋值"><a href="#8-2-1-空字段赋值" class="headerlink" title="8.2.1 空字段赋值"></a>8.2.1 空字段赋值</h3></li>
<li>函数说明</li>
</ol>
<ul>
<li>NVL：给值为NULL的数据赋值，它的格式是NVL( value，default_value)。它的功能是如果value为NULL，则NVL函数返回default_value的值，否则返回value的值，如果两个参数都为NULL ，则返回NULL。<ul>
<li>示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+-----------------+-----------+-----------------+</span></span><br><span class="line"><span class="operator">|</span> dept.dept_no  <span class="operator">|</span> dept.dept_name  <span class="operator">|</span> dept.loc  <span class="operator">|</span> dept.dept_desc  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+-----------------+-----------+-----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>            <span class="operator">|</span> CEO             <span class="operator">|</span> <span class="number">1700</span>      <span class="operator">|</span> 这是老板            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>            <span class="operator">|</span> ACCOUNTING      <span class="operator">|</span> <span class="number">1700</span>      <span class="operator">|</span> <span class="keyword">NULL</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>            <span class="operator">|</span> RESEARCH        <span class="operator">|</span> <span class="number">1800</span>      <span class="operator">|</span> <span class="keyword">NULL</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">30</span>            <span class="operator">|</span> SALES           <span class="operator">|</span> <span class="number">1900</span>      <span class="operator">|</span> <span class="keyword">NULL</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">40</span>            <span class="operator">|</span> OPERATIONS      <span class="operator">|</span> <span class="number">1700</span>      <span class="operator">|</span> <span class="keyword">NULL</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+-----------------+-----------+-----------------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> selected (<span class="number">0.058</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> dept_no, dept_name, loc, nvl(dept_desc,&quot;无描述&quot;) <span class="keyword">from</span> dept;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-------+-------+</span></span><br><span class="line"><span class="operator">|</span> dept_no  <span class="operator">|</span>  dept_name  <span class="operator">|</span>  loc  <span class="operator">|</span>  _c3  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-------+-------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>       <span class="operator">|</span> CEO         <span class="operator">|</span> <span class="number">1700</span>  <span class="operator">|</span> 这是老板  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>       <span class="operator">|</span> ACCOUNTING  <span class="operator">|</span> <span class="number">1700</span>  <span class="operator">|</span> 无描述   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>       <span class="operator">|</span> RESEARCH    <span class="operator">|</span> <span class="number">1800</span>  <span class="operator">|</span> 无描述   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">30</span>       <span class="operator">|</span> SALES       <span class="operator">|</span> <span class="number">1900</span>  <span class="operator">|</span> 无描述   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">40</span>       <span class="operator">|</span> OPERATIONS  <span class="operator">|</span> <span class="number">1700</span>  <span class="operator">|</span> 无描述   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-------+-------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> selected (<span class="number">0.06</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="8-2-2-CASE-WHEN-THEN-ELSE-END"><a href="#8-2-2-CASE-WHEN-THEN-ELSE-END" class="headerlink" title="8.2.2 CASE WHEN THEN ELSE END"></a>8.2.2 CASE WHEN THEN ELSE END</h3><ul>
<li>示例1  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span></span><br><span class="line">        emp_name,</span><br><span class="line">         <span class="keyword">case</span> emp_name</span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;SMITH&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;史密斯&#x27;</span></span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;ALLEN&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;艾伦&#x27;</span></span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;WARD&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;沃德&#x27;</span></span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;JONES&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;琼斯&#x27;</span></span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;MARTIN&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;马丁&#x27;</span></span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;BLAKE&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;布莱克&#x27;</span></span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;CLARK&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;克拉克&#x27;</span></span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;SCOTT&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;斯科特&#x27;</span></span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;KING&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;肯&#x27;</span></span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;TURNER&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;特纳&#x27;</span></span><br><span class="line">             <span class="keyword">else</span> <span class="string">&#x27;爱啥啥&#x27;</span> <span class="keyword">end</span> chinese_name</span><br><span class="line"> <span class="keyword">from</span> emp;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+---------------+</span></span><br><span class="line"><span class="operator">|</span> emp_name  <span class="operator">|</span> chinese_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+---------------+</span></span><br><span class="line"><span class="operator">|</span> SMITH     <span class="operator">|</span> 史密斯           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> ALLEN     <span class="operator">|</span> 艾伦            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> WARD      <span class="operator">|</span> 沃德            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> JONES     <span class="operator">|</span> 琼斯            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> MARTIN    <span class="operator">|</span> 马丁            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> BLAKE     <span class="operator">|</span> 布莱克           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> CLARK     <span class="operator">|</span> 克拉克           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> SCOTT     <span class="operator">|</span> 斯科特           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> KING      <span class="operator">|</span> 肯             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> TURNER    <span class="operator">|</span> 特纳            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> ADAMS     <span class="operator">|</span> 爱啥啥           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> JAMES     <span class="operator">|</span> 爱啥啥           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> FORD      <span class="operator">|</span> 爱啥啥           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> MILLER    <span class="operator">|</span> 爱啥啥           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+---------------+</span></span><br><span class="line"><span class="number">14</span> <span class="keyword">rows</span> selected (<span class="number">0.09</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>示例2  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    dept_no,</span><br><span class="line">       <span class="built_in">sum</span>(<span class="keyword">case</span> <span class="keyword">when</span> sal <span class="keyword">between</span> <span class="number">3000</span> <span class="keyword">and</span> <span class="number">8000</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span> ) <span class="keyword">as</span> `<span class="number">3000</span>到<span class="number">8000</span>`,</span><br><span class="line">       <span class="built_in">sum</span>(if(sal <span class="operator">&lt;</span> <span class="number">3000</span>, <span class="number">1</span>, <span class="number">0</span>)) <span class="keyword">as</span> `小于<span class="number">3000</span>`</span><br><span class="line"><span class="keyword">from</span> emp</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> dept_no;</span><br></pre></td></tr></table></figure>
<h3 id="8-2-3-行转列"><a href="#8-2-3-行转列" class="headerlink" title="8.2.3 行转列"></a>8.2.3 行转列</h3></li>
</ul>
<ol>
<li>相关函数说明<ol>
<li>CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;</li>
<li>CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;<ul>
<li>注意: CONCAT_WS must be “string or array<string>”</li>
</ul>
</li>
<li>COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。 </li>
</ol>
</li>
<li>数据准备  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">name	constellation	blood_type</span><br><span class="line">孙悟空	白羊座	A</span><br><span class="line">大海	射手座	A</span><br><span class="line">宋宋	白羊座	B</span><br><span class="line">猪八戒	白羊座	A</span><br><span class="line">凤姐	射手座	A</span><br><span class="line">苍老师	白羊座	B</span><br></pre></td></tr></table></figure></li>
<li>需求：把星座和血型一样的人归类到一起。结果如下： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">射手座,A            大海|凤姐</span><br><span class="line">白羊座,A            孙悟空|猪八戒</span><br><span class="line">白羊座,B            宋宋|苍老师</span><br></pre></td></tr></table></figure></li>
<li>创建hive表并导入数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000&gt; create table person_constellation_blood</span><br><span class="line"> (</span><br><span class="line">     name          string,</span><br><span class="line">     constellation string,</span><br><span class="line">     blood_type    string</span><br><span class="line"> )</span><br><span class="line"> row format delimited fields terminated by <span class="string">&quot;\t&quot;</span>;</span><br><span class="line">No rows affected (5.123 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; load data <span class="built_in">local</span> inpath <span class="string">&quot;/home/atguigu/hive/person_constellation_blood/person_constellation_blood.txt&quot;</span> into table person_constellation_blood;</span><br><span class="line">No rows affected (0.111 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br></pre></td></tr></table></figure></li>
<li>按需求查询数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> t1.cb,</span><br><span class="line">        concat_ws(&quot;|&quot;, collect_set(t1.name))</span><br><span class="line"> <span class="keyword">from</span> (<span class="keyword">select</span> name,</span><br><span class="line">              concat_ws(&quot;,&quot;, constellation, blood_type) cb</span><br><span class="line">       <span class="keyword">from</span> person_constellation_blood) t1</span><br><span class="line"> <span class="keyword">group</span> <span class="keyword">by</span> t1.cb;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------+----------+</span></span><br><span class="line"><span class="operator">|</span> t1.cb  <span class="operator">|</span>   _c1    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------+----------+</span></span><br><span class="line"><span class="operator">|</span> 射手座,A  <span class="operator">|</span> 大海<span class="operator">|</span>凤姐    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 白羊座,A  <span class="operator">|</span> 孙悟空<span class="operator">|</span>猪八戒  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 白羊座,B  <span class="operator">|</span> 宋宋<span class="operator">|</span>苍老师   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------+----------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> selected (<span class="number">14.797</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="8-2-4-列转行"><a href="#8-2-4-列转行" class="headerlink" title="8.2.4 列转行"></a>8.2.4 列转行</h3><ol>
<li>函数说明<ul>
<li>EXPLODE(col)：炸裂函数，将hive一列中复杂的array或者map结构拆分成多行。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> explode(split(&quot;1,2,3,4,5&quot;,&quot;,&quot;));</span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> col  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">3</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">4</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">5</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> selected (<span class="number">0.057</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>LATERAL VIEW<ul>
<li>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias</li>
<li>解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</li>
</ul>
</li>
</ul>
</li>
<li>数据准备<table>
<thead>
<tr>
<th>movie</th>
<th>category</th>
</tr>
</thead>
<tbody><tr>
<td>《疑犯追踪》</td>
<td>悬疑,动作,科幻,剧情</td>
</tr>
<tr>
<td>《Lie to me》</td>
<td>悬疑,警匪,动作,心理,剧情</td>
</tr>
<tr>
<td>《战狼2》</td>
<td>战争,动作,灾难</td>
</tr>
</tbody></table>
</li>
<li>需求：将电影分类中的数组数据展开。结果如下： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">《疑犯追踪》	悬疑</span><br><span class="line">《疑犯追踪》	动作</span><br><span class="line">《疑犯追踪》	科幻</span><br><span class="line">《疑犯追踪》	剧情</span><br><span class="line">《Lie to me》	悬疑</span><br><span class="line">《Lie to me》	警匪</span><br><span class="line">《Lie to me》	动作</span><br><span class="line">《Lie to me》	心理</span><br><span class="line">《Lie to me》	剧情</span><br><span class="line">《战狼2》	战争</span><br><span class="line">《战狼2》	动作</span><br><span class="line">《战狼2》	灾难</span><br></pre></td></tr></table></figure></li>
<li>创建hive表并导入数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> movie_info(</span><br><span class="line">     movie string,</span><br><span class="line">     category string)</span><br><span class="line"> <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot;\t&quot;;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.065</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/movie_info/movie_info.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> mock_data.movie_info;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.113</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>按需求查询数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span></span><br><span class="line">        movie, category_name</span><br><span class="line"> <span class="keyword">from</span> movie_info</span><br><span class="line"> <span class="keyword">lateral</span> <span class="keyword">view</span></span><br><span class="line">     explode(split(category,<span class="string">&#x27;,&#x27;</span>)) movie_info_tmp <span class="keyword">as</span> category_name;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+----------------+</span></span><br><span class="line"><span class="operator">|</span>    movie     <span class="operator">|</span> category_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+----------------+</span></span><br><span class="line"><span class="operator">|</span> 《疑犯追踪》       <span class="operator">|</span> 悬疑             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《疑犯追踪》       <span class="operator">|</span> 动作             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《疑犯追踪》       <span class="operator">|</span> 科幻             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《疑犯追踪》       <span class="operator">|</span> 剧情             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《Lie <span class="keyword">to</span> me》  <span class="operator">|</span> 悬疑             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《Lie <span class="keyword">to</span> me》  <span class="operator">|</span> 警匪             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《Lie <span class="keyword">to</span> me》  <span class="operator">|</span> 动作             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《Lie <span class="keyword">to</span> me》  <span class="operator">|</span> 心理             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《Lie <span class="keyword">to</span> me》  <span class="operator">|</span> 剧情             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《战狼<span class="number">2</span>》        <span class="operator">|</span> 战争             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《战狼<span class="number">2</span>》        <span class="operator">|</span> 动作             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《战狼<span class="number">2</span>》        <span class="operator">|</span> 灾难             <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+----------------+</span></span><br><span class="line"><span class="number">12</span> <span class="keyword">rows</span> selected (<span class="number">0.105</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="8-2-5-窗口函数（开窗函数）"><a href="#8-2-5-窗口函数（开窗函数）" class="headerlink" title="8.2.5 窗口函数（开窗函数）"></a>8.2.5 窗口函数（开窗函数）</h3><h4 id="8-2-5-1-窗口函数基本使用"><a href="#8-2-5-1-窗口函数基本使用" class="headerlink" title="8.2.5.1 窗口函数基本使用"></a>8.2.5.1 窗口函数基本使用</h4><ol>
<li>介绍：<ul>
<li>普通聚合函数聚合的行集是组，开窗函数聚合的行集是窗口。因此，普通聚合函数每组（Group by）只有一个返回值，而<font color ='red' >开窗函数则可以为窗口中的每行都返回一个值</font>。</li>
</ul>
</li>
<li>基本结构 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">分析函数(如：sum(), max(), row_number()...) 窗口子句over(partition by 列名 order by 列名 rows between 开始位置 and 结束位置)</span><br></pre></td></tr></table></figure></li>
<li>over函数<ul>
<li>语法结构  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">over(partition by [column_n] order by [column_m])</span><br></pre></td></tr></table></figure>
<ul>
<li>先按照column_n分区，相同的column_n分为一区，每个分区根据column_m排序（默认升序）。</li>
</ul>
</li>
</ul>
</li>
<li>测试数据<ul>
<li>建表并插入数据  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student_scores <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> stack(<span class="number">20</span>,</span><br><span class="line">   <span class="number">1</span>, <span class="number">111</span>, <span class="number">68</span>, <span class="number">69</span>, <span class="number">90</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department1&#x27;</span>,</span><br><span class="line">   <span class="number">2</span>, <span class="number">112</span>, <span class="number">73</span>, <span class="number">80</span>, <span class="number">96</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department1&#x27;</span>,</span><br><span class="line">   <span class="number">3</span>, <span class="number">113</span>, <span class="number">90</span>, <span class="number">74</span>, <span class="number">75</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department1&#x27;</span>,</span><br><span class="line">   <span class="number">4</span>, <span class="number">114</span>, <span class="number">89</span>, <span class="number">94</span>, <span class="number">93</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department1&#x27;</span>,</span><br><span class="line">   <span class="number">5</span>, <span class="number">115</span>, <span class="number">99</span>, <span class="number">93</span>, <span class="number">89</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department1&#x27;</span>,</span><br><span class="line">   <span class="number">6</span>, <span class="number">121</span>, <span class="number">96</span>, <span class="number">74</span>, <span class="number">79</span>, <span class="string">&#x27;class2&#x27;</span>, <span class="string">&#x27;department1&#x27;</span>,</span><br><span class="line">   <span class="number">7</span>, <span class="number">122</span>, <span class="number">89</span>, <span class="number">86</span>, <span class="number">85</span>, <span class="string">&#x27;class2&#x27;</span>, <span class="string">&#x27;department1&#x27;</span>,</span><br><span class="line">   <span class="number">8</span>, <span class="number">123</span>, <span class="number">70</span>, <span class="number">78</span>, <span class="number">61</span>, <span class="string">&#x27;class2&#x27;</span>, <span class="string">&#x27;department1&#x27;</span>,</span><br><span class="line">   <span class="number">9</span>, <span class="number">124</span>, <span class="number">76</span>, <span class="number">70</span>, <span class="number">76</span>, <span class="string">&#x27;class2&#x27;</span>, <span class="string">&#x27;department1&#x27;</span>,</span><br><span class="line">   <span class="number">10</span>, <span class="number">211</span>, <span class="number">89</span>, <span class="number">93</span>, <span class="number">60</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">11</span>, <span class="number">212</span>, <span class="number">76</span>, <span class="number">83</span>, <span class="number">75</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">12</span>, <span class="number">213</span>, <span class="number">71</span>, <span class="number">94</span>, <span class="number">90</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">13</span>, <span class="number">214</span>, <span class="number">94</span>, <span class="number">94</span>, <span class="number">66</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">14</span>, <span class="number">215</span>, <span class="number">84</span>, <span class="number">82</span>, <span class="number">73</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">15</span>, <span class="number">216</span>, <span class="number">85</span>, <span class="number">74</span>, <span class="number">93</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">16</span>, <span class="number">221</span>, <span class="number">77</span>, <span class="number">99</span>, <span class="number">61</span>, <span class="string">&#x27;class2&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">17</span>, <span class="number">222</span>, <span class="number">80</span>, <span class="number">78</span>, <span class="number">96</span>, <span class="string">&#x27;class2&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">18</span>, <span class="number">223</span>, <span class="number">79</span>, <span class="number">74</span>, <span class="number">96</span>, <span class="string">&#x27;class2&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">19</span>, <span class="number">224</span>, <span class="number">75</span>, <span class="number">80</span>, <span class="number">78</span>, <span class="string">&#x27;class2&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">20</span>, <span class="number">225</span>, <span class="number">82</span>, <span class="number">85</span>, <span class="number">63</span>, <span class="string">&#x27;class2&#x27;</span>, <span class="string">&#x27;department2&#x27;</span></span><br><span class="line">) <span class="keyword">as</span> (id,studentId,<span class="keyword">language</span>,math,english,classId,departmentId);</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>窗口示例<ul>
<li>运行sql  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> studentId,</span><br><span class="line">       math,</span><br><span class="line">       departmentId,</span><br><span class="line">       classId,</span><br><span class="line">       <span class="comment">-- 符合所有条件的行作为窗口聚合的行集，这里符合department1的有9个</span></span><br><span class="line">       <span class="built_in">count</span>(math) <span class="keyword">over</span> ()                                                                                    <span class="keyword">as</span> count1,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">count</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId)                                                                <span class="keyword">as</span> count2,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据按照math升序排列后作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">count</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math)                                                  <span class="keyword">as</span> count3,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据按照math升序排列后，当前行向前1行，向后2行（n-1,n,n+1,n+2共四行数据）作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">count</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> preceding <span class="keyword">and</span> <span class="number">2</span> following)         <span class="keyword">as</span> count4,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据按照math升序排列后，当前行向后到结束行作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">count</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="type">row</span> <span class="keyword">and</span> unbounded following) <span class="keyword">as</span> count5,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据按照math升序排列后，从起点行到当前行作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">count</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math <span class="keyword">rows</span> <span class="keyword">between</span> unbounded preceding <span class="keyword">and</span> <span class="keyword">current</span> <span class="type">row</span> ) <span class="keyword">as</span> count6</span><br><span class="line"><span class="keyword">from</span> student_scores</span><br><span class="line"><span class="keyword">where</span> departmentId <span class="operator">=</span> <span class="string">&#x27;department1&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>结果  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+---------+---------+---------+---------+---------+---------+</span></span><br><span class="line"><span class="operator">|</span> studentid  <span class="operator">|</span> math  <span class="operator">|</span> departmentid  <span class="operator">|</span> classid  <span class="operator">|</span> count1  <span class="operator">|</span> count2  <span class="operator">|</span> count3  <span class="operator">|</span> count4  <span class="operator">|</span> count5  <span class="operator">|</span> count6  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+---------+---------+---------+---------+---------+---------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">9</span>       <span class="operator">|</span> <span class="number">5</span>       <span class="operator">|</span> <span class="number">1</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span> <span class="number">5</span>       <span class="operator">|</span> <span class="number">1</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">9</span>       <span class="operator">|</span> <span class="number">5</span>       <span class="operator">|</span> <span class="number">2</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">2</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">9</span>       <span class="operator">|</span> <span class="number">5</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">9</span>       <span class="operator">|</span> <span class="number">5</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span> <span class="number">2</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">9</span>       <span class="operator">|</span> <span class="number">5</span>       <span class="operator">|</span> <span class="number">5</span>       <span class="operator">|</span> <span class="number">2</span>       <span class="operator">|</span> <span class="number">1</span>       <span class="operator">|</span> <span class="number">5</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">9</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">1</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">1</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">9</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">2</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span> <span class="number">2</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">9</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span> <span class="number">2</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">9</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">2</span>       <span class="operator">|</span> <span class="number">1</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+---------+---------+---------+---------+---------+---------+</span></span><br></pre></td></tr></table></figure></li>
<li>结果解析：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">studentid=114 为例</span><br><span class="line">count1: 为departmentId=department1的行数为9，</span><br><span class="line">count2: 为分区class1中的行数5，</span><br><span class="line">count3: 为分区class1中math升序排列到当前行94 match&lt;=94行数5</span><br><span class="line">count4: 为分区class1中math升序排列后取，当前行-1=115，114，当前行+1=不存在，当前行+2=不存在，行数2</span><br><span class="line">count5: 为分区class1中math升序排列后取，当前行到结束行，114为当前分区最后一行，行数1</span><br><span class="line">count6: 为分区class1中math升序排列后取，开始行到当前行，行数5</span><br></pre></td></tr></table></figure>
<ul>
<li>如果不指定ROWS BETWEEN，默认统计窗口是从起点到当前行</li>
<li>ROWS BETWEEN，也叫做window子句。<ul>
<li>PRECEDING：往前<ul>
<li>n PRECEDING：往前n行数据</li>
</ul>
</li>
<li>FOLLOWING：往后<ul>
<li>n FOLLOWING：往后n行数据</li>
</ul>
</li>
<li>CURRENT ROW：当前行</li>
<li>UNBOUNDED：无边界（一般结合PRECEDING，FOLLOWING使用）</li>
<li>UNBOUNDED PRECEDING 表示该窗口最前面的行（起点）</li>
<li>UNBOUNDED FOLLOWING：表示该窗口最后面的行（终点）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="8-2-5-2-聚合开窗函数"><a href="#8-2-5-2-聚合开窗函数" class="headerlink" title="8.2.5.2 聚合开窗函数"></a>8.2.5.2 聚合开窗函数</h4><ol>
<li>sum()，min()，max()，avg()都与count()类似<ul>
<li>示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> studentId,</span><br><span class="line">       math,</span><br><span class="line">       departmentId,</span><br><span class="line">       classId,</span><br><span class="line">       <span class="comment">-- 符合所有条件的行作为窗口聚合的行集，这里符合department1的有9个</span></span><br><span class="line">       <span class="built_in">avg</span>(math) <span class="keyword">over</span> ()                                                                                    <span class="keyword">as</span> avg1,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">avg</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId)                                                                <span class="keyword">as</span> avg2,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据按照math升序排列后到当前行截止，作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">avg</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math)                                                  <span class="keyword">as</span> avg3,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据按照math升序排列后，当前行向前1行，向后2行（n-1,n,n+1,n+2共四行数据）作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">avg</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> preceding <span class="keyword">and</span> <span class="number">2</span> following)         <span class="keyword">as</span> avg4,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据按照math升序排列后，当前行向后到结束行作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">avg</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="type">row</span> <span class="keyword">and</span> unbounded following) <span class="keyword">as</span> avg5,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据按照math升序排列后，从起点行到当前行作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">avg</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math <span class="keyword">rows</span> <span class="keyword">between</span> unbounded preceding <span class="keyword">and</span> <span class="keyword">current</span> <span class="type">row</span> ) <span class="keyword">as</span> avg6</span><br><span class="line"><span class="keyword">from</span> student_scores</span><br><span class="line"><span class="keyword">where</span> departmentId <span class="operator">=</span> <span class="string">&#x27;department1&#x27;</span>;</span><br><span class="line">       </span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+--------------------+-------+--------------------+--------------------+--------------------+--------------------+</span></span><br><span class="line"><span class="operator">|</span> studentid  <span class="operator">|</span> math  <span class="operator">|</span> departmentid  <span class="operator">|</span> classid  <span class="operator">|</span>        avg1        <span class="operator">|</span> avg2  <span class="operator">|</span>        avg3        <span class="operator">|</span>        avg4        <span class="operator">|</span>        avg5        <span class="operator">|</span>        avg6        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+--------------------+-------+--------------------+--------------------+--------------------+--------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">79.77777777777777</span>  <span class="operator">|</span> <span class="number">82.0</span>  <span class="operator">|</span> <span class="number">69.0</span>               <span class="operator">|</span> <span class="number">74.33333333333333</span>  <span class="operator">|</span> <span class="number">82.0</span>               <span class="operator">|</span> <span class="number">69.0</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">79.77777777777777</span>  <span class="operator">|</span> <span class="number">82.0</span>  <span class="operator">|</span> <span class="number">71.5</span>               <span class="operator">|</span> <span class="number">79.0</span>               <span class="operator">|</span> <span class="number">85.25</span>              <span class="operator">|</span> <span class="number">71.5</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">79.77777777777777</span>  <span class="operator">|</span> <span class="number">82.0</span>  <span class="operator">|</span> <span class="number">74.33333333333333</span>  <span class="operator">|</span> <span class="number">85.25</span>              <span class="operator">|</span> <span class="number">89.0</span>               <span class="operator">|</span> <span class="number">74.33333333333333</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">79.77777777777777</span>  <span class="operator">|</span> <span class="number">82.0</span>  <span class="operator">|</span> <span class="number">79.0</span>               <span class="operator">|</span> <span class="number">89.0</span>               <span class="operator">|</span> <span class="number">93.5</span>               <span class="operator">|</span> <span class="number">79.0</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">79.77777777777777</span>  <span class="operator">|</span> <span class="number">82.0</span>  <span class="operator">|</span> <span class="number">82.0</span>               <span class="operator">|</span> <span class="number">93.5</span>               <span class="operator">|</span> <span class="number">94.0</span>               <span class="operator">|</span> <span class="number">82.0</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">79.77777777777777</span>  <span class="operator">|</span> <span class="number">77.0</span>  <span class="operator">|</span> <span class="number">70.0</span>               <span class="operator">|</span> <span class="number">74.0</span>               <span class="operator">|</span> <span class="number">77.0</span>               <span class="operator">|</span> <span class="number">70.0</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">79.77777777777777</span>  <span class="operator">|</span> <span class="number">77.0</span>  <span class="operator">|</span> <span class="number">72.0</span>               <span class="operator">|</span> <span class="number">77.0</span>               <span class="operator">|</span> <span class="number">79.33333333333333</span>  <span class="operator">|</span> <span class="number">72.0</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">79.77777777777777</span>  <span class="operator">|</span> <span class="number">77.0</span>  <span class="operator">|</span> <span class="number">74.0</span>               <span class="operator">|</span> <span class="number">79.33333333333333</span>  <span class="operator">|</span> <span class="number">82.0</span>               <span class="operator">|</span> <span class="number">74.0</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">79.77777777777777</span>  <span class="operator">|</span> <span class="number">77.0</span>  <span class="operator">|</span> <span class="number">77.0</span>               <span class="operator">|</span> <span class="number">82.0</span>               <span class="operator">|</span> <span class="number">86.0</span>               <span class="operator">|</span> <span class="number">77.0</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+--------------------+-------+--------------------+--------------------+--------------------+--------------------+</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>first_value <ul>
<li>作用：返回分区中的第一个值</li>
<li>sql示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> studentId,</span><br><span class="line">       math,</span><br><span class="line">       departmentId,</span><br><span class="line">       classId,</span><br><span class="line">       <span class="built_in">first_value</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math) first_value_1,</span><br><span class="line">       <span class="built_in">first_value</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> preceding <span class="keyword">and</span> <span class="number">2</span> following) first_value_2</span><br><span class="line"><span class="keyword">from</span> student_scores</span><br><span class="line"><span class="keyword">where</span> departmentId <span class="operator">=</span> <span class="string">&#x27;department1&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+----------------+----------------+</span></span><br><span class="line"><span class="operator">|</span> studentid  <span class="operator">|</span> math  <span class="operator">|</span> departmentid  <span class="operator">|</span> classid  <span class="operator">|</span> first_value_1  <span class="operator">|</span> first_value_2  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+----------------+----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">69</span>             <span class="operator">|</span> <span class="number">69</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">69</span>             <span class="operator">|</span> <span class="number">69</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">69</span>             <span class="operator">|</span> <span class="number">74</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">69</span>             <span class="operator">|</span> <span class="number">80</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">69</span>             <span class="operator">|</span> <span class="number">93</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">70</span>             <span class="operator">|</span> <span class="number">70</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">70</span>             <span class="operator">|</span> <span class="number">70</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">70</span>             <span class="operator">|</span> <span class="number">74</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">70</span>             <span class="operator">|</span> <span class="number">78</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+----------------+----------------+</span></span><br></pre></td></tr></table></figure></li>
<li>解析  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">studentId=115为例</span><br><span class="line">first_value1：为分区class1中按照math排序的第一个值69(rows between未指定为第一行到当前行)，</span><br><span class="line">first_value2：为分区class2中按照math排序后当前行向前1行向后2行区间的第一个值80。</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>last_vlaue <ul>
<li>作用：返回分区最后一个值</li>
<li>示例sql  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+---------------+---------------+</span></span><br><span class="line"><span class="operator">|</span> studentid  <span class="operator">|</span> math  <span class="operator">|</span> departmentid  <span class="operator">|</span> classid  <span class="operator">|</span> last_value_1  <span class="operator">|</span> last_value_2  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+---------------+---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">69</span>            <span class="operator">|</span> <span class="number">80</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">74</span>            <span class="operator">|</span> <span class="number">93</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">80</span>            <span class="operator">|</span> <span class="number">94</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">93</span>            <span class="operator">|</span> <span class="number">94</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">94</span>            <span class="operator">|</span> <span class="number">94</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">70</span>            <span class="operator">|</span> <span class="number">78</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">74</span>            <span class="operator">|</span> <span class="number">86</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">78</span>            <span class="operator">|</span> <span class="number">86</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">86</span>            <span class="operator">|</span> <span class="number">86</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+---------------+---------------+</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>lag <ul>
<li>作用：LAG(col, n, DEFAULT)用于统计窗口内向上第n行的值<ul>
<li>col：列名</li>
<li>n：向上n行，[可选，默认为1]</li>
<li>DEFAULT：当向上n行为NULL时，取默认值；如果不指定，则为NULL</li>
</ul>
</li>
<li>示例sql:  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> studentId,</span><br><span class="line">       math,</span><br><span class="line">       departmentId,</span><br><span class="line">       classId,</span><br><span class="line">       <span class="built_in">lag</span>(math,<span class="number">1</span>,<span class="number">60</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math) lag_1,</span><br><span class="line">       <span class="built_in">lag</span>(math,<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math) lag_1</span><br><span class="line"><span class="keyword">from</span> student_scores</span><br><span class="line"><span class="keyword">where</span> departmentId <span class="operator">=</span> <span class="string">&#x27;department1&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+--------+--------+</span></span><br><span class="line"><span class="operator">|</span> studentid  <span class="operator">|</span> math  <span class="operator">|</span> departmentid  <span class="operator">|</span> classid  <span class="operator">|</span> lag_1  <span class="operator">|</span> lag_1  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+--------+--------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">60</span>     <span class="operator">|</span> <span class="keyword">NULL</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">69</span>     <span class="operator">|</span> <span class="keyword">NULL</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">74</span>     <span class="operator">|</span> <span class="number">69</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">80</span>     <span class="operator">|</span> <span class="number">74</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">93</span>     <span class="operator">|</span> <span class="number">80</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">60</span>     <span class="operator">|</span> <span class="keyword">NULL</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">70</span>     <span class="operator">|</span> <span class="keyword">NULL</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">74</span>     <span class="operator">|</span> <span class="number">70</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">78</span>     <span class="operator">|</span> <span class="number">74</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+--------+--------+</span></span><br></pre></td></tr></table></figure></li>
<li>解析  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">studentId=113，</span><br><span class="line">lag1为分区class1按照math排序后当前行向上2行的值NULL，但是设置了DEFUALT，所以为60，</span><br><span class="line">lag2因为没有设置DEFAULT，所以为NULL。</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>lead<ul>
<li>作用：LEAD(col, n, DEFAULT)与LAG相反，用于统计窗口内向下n行的值<ul>
<li>col：列名</li>
<li>n：向下n行，[可选，默认为1]</li>
<li>DEFAULT：当向下n行为NULL时，取默认值；如果不指定，则为NULL</li>
</ul>
</li>
<li>sql示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> studentId,</span><br><span class="line">       math,</span><br><span class="line">       departmentId,</span><br><span class="line">       classId,</span><br><span class="line">       <span class="built_in">lead</span>(math,<span class="number">1</span>,<span class="number">60</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math) lead_1,</span><br><span class="line">       <span class="built_in">lead</span>(math,<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math) lead_1</span><br><span class="line"><span class="keyword">from</span> student_scores</span><br><span class="line"><span class="keyword">where</span> departmentId <span class="operator">=</span> <span class="string">&#x27;department1&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+--------+--------+</span></span><br><span class="line"><span class="operator">|</span> studentid  <span class="operator">|</span> math  <span class="operator">|</span> departmentid  <span class="operator">|</span> classid  <span class="operator">|</span> lead_1  <span class="operator">|</span> lead_1  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+--------+--------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">74</span>     <span class="operator">|</span> <span class="number">80</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">80</span>     <span class="operator">|</span> <span class="number">93</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">93</span>     <span class="operator">|</span> <span class="number">94</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">94</span>     <span class="operator">|</span> <span class="keyword">NULL</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">60</span>     <span class="operator">|</span> <span class="keyword">NULL</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">74</span>     <span class="operator">|</span> <span class="number">78</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">78</span>     <span class="operator">|</span> <span class="number">86</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">86</span>     <span class="operator">|</span> <span class="keyword">NULL</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">60</span>     <span class="operator">|</span> <span class="keyword">NULL</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+--------+--------+</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>cume_dist<ul>
<li>作用：计算某个窗口或分区中某个值的累积分布。假定升序排序，则使用以下公式确定累积分布：(小于等于当前值的行数) / (分区内总行数)</li>
<li>sql示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> studentId,</span><br><span class="line">       math,</span><br><span class="line">       departmentId,</span><br><span class="line">       classId,</span><br><span class="line">       <span class="built_in">cume_dist</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> math)                      <span class="keyword">as</span> cume_dist1,</span><br><span class="line">       <span class="built_in">cume_dist</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> math <span class="keyword">desc</span>)                 <span class="keyword">as</span> cume_dist2,</span><br><span class="line">       <span class="built_in">cume_dist</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math) <span class="keyword">as</span> cume_dist3</span><br><span class="line"><span class="keyword">from</span> student_scores</span><br><span class="line"><span class="keyword">where</span> departmentId <span class="operator">=</span> <span class="string">&#x27;department1&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+---------------------+---------------------+-------------+</span></span><br><span class="line"><span class="operator">|</span> studentid  <span class="operator">|</span> math  <span class="operator">|</span> departmentid  <span class="operator">|</span> classid  <span class="operator">|</span>     cume_dist1      <span class="operator">|</span>     cume_dist2      <span class="operator">|</span> cume_dist3  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+---------------------+---------------------+-------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">0.1111111111111111</span>  <span class="operator">|</span> <span class="number">1.0</span>                 <span class="operator">|</span> <span class="number">0.2</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">0.4444444444444444</span>  <span class="operator">|</span> <span class="number">0.7777777777777778</span>  <span class="operator">|</span> <span class="number">0.4</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">0.6666666666666666</span>  <span class="operator">|</span> <span class="number">0.4444444444444444</span>  <span class="operator">|</span> <span class="number">0.6</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">0.8888888888888888</span>  <span class="operator">|</span> <span class="number">0.2222222222222222</span>  <span class="operator">|</span> <span class="number">0.8</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">1.0</span>                 <span class="operator">|</span> <span class="number">0.1111111111111111</span>  <span class="operator">|</span> <span class="number">1.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">0.2222222222222222</span>  <span class="operator">|</span> <span class="number">0.8888888888888888</span>  <span class="operator">|</span> <span class="number">0.25</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">0.4444444444444444</span>  <span class="operator">|</span> <span class="number">0.7777777777777778</span>  <span class="operator">|</span> <span class="number">0.5</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">0.5555555555555556</span>  <span class="operator">|</span> <span class="number">0.5555555555555556</span>  <span class="operator">|</span> <span class="number">0.75</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">0.7777777777777778</span>  <span class="operator">|</span> <span class="number">0.3333333333333333</span>  <span class="operator">|</span> <span class="number">1.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+---------------------+---------------------+-------------+</span></span><br></pre></td></tr></table></figure></li>
<li>结果解析  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">studentId=115为例</span><br><span class="line">cume_dist1=小于等于93的行数8/总行数9=0.8888888888888888</span><br><span class="line">cume_dist2=大于等于93的行数2/总行数9=0.2222222222222222</span><br><span class="line">cume_dist3=class1分区内小于等于93的行数4/总行数5=0.8</span><br></pre></td></tr></table></figure>
<h4 id="8-2-5-3-排序开窗函数"><a href="#8-2-5-3-排序开窗函数" class="headerlink" title="8.2.5.3 排序开窗函数"></a>8.2.5.3 排序开窗函数</h4></li>
</ul>
</li>
<li>row_number<ul>
<li>作用：row_number() over([partition by col1] [order by col2])开窗函数是基于over子句中order by列的一个排名。在窗口或分区内从1开始排序，即使遇到col2相等时，名次依旧增加。例如：有两条记录相等，但一个是第一，一个是第二</li>
<li>sql示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> id,</span><br><span class="line">       studentId,</span><br><span class="line">       <span class="keyword">language</span>,</span><br><span class="line">       math,</span><br><span class="line">       english,</span><br><span class="line">       classId,</span><br><span class="line">       departmentId,</span><br><span class="line">       <span class="built_in">row_number</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> math) <span class="keyword">as</span> math_order,</span><br><span class="line">       <span class="built_in">row_number</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> english) <span class="keyword">as</span> english_order,</span><br><span class="line">       <span class="built_in">row_number</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">language</span>) <span class="keyword">as</span> class_language_order</span><br><span class="line"><span class="keyword">from</span> student_scores;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+------------+---------------+----------------------+</span></span><br><span class="line"><span class="operator">|</span> id  <span class="operator">|</span> studentid  <span class="operator">|</span> <span class="keyword">language</span>  <span class="operator">|</span> math  <span class="operator">|</span> english  <span class="operator">|</span> classid  <span class="operator">|</span> departmentid  <span class="operator">|</span> math_order  <span class="operator">|</span> english_order  <span class="operator">|</span> class_language_order  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+------------+---------------+----------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">68</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">1</span>          <span class="operator">|</span> <span class="number">14</span>            <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">12</span>  <span class="operator">|</span> <span class="number">213</span>        <span class="operator">|</span> <span class="number">71</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">17</span>         <span class="operator">|</span> <span class="number">15</span>            <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">73</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">10</span>         <span class="operator">|</span> <span class="number">20</span>            <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">11</span>  <span class="operator">|</span> <span class="number">212</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">83</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">12</span>         <span class="operator">|</span> <span class="number">8</span>             <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">14</span>  <span class="operator">|</span> <span class="number">215</span>        <span class="operator">|</span> <span class="number">84</span>        <span class="operator">|</span> <span class="number">82</span>    <span class="operator">|</span> <span class="number">73</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">11</span>         <span class="operator">|</span> <span class="number">6</span>             <span class="operator">|</span> <span class="number">5</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">15</span>  <span class="operator">|</span> <span class="number">216</span>        <span class="operator">|</span> <span class="number">85</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">4</span>          <span class="operator">|</span> <span class="number">16</span>            <span class="operator">|</span> <span class="number">6</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>  <span class="operator">|</span> <span class="number">211</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">60</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">16</span>         <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span> <span class="number">7</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">18</span>         <span class="operator">|</span> <span class="number">17</span>            <span class="operator">|</span> <span class="number">8</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">90</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">5</span>          <span class="operator">|</span> <span class="number">7</span>             <span class="operator">|</span> <span class="number">9</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">13</span>  <span class="operator">|</span> <span class="number">214</span>        <span class="operator">|</span> <span class="number">94</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">66</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">19</span>         <span class="operator">|</span> <span class="number">5</span>             <span class="operator">|</span> <span class="number">10</span>                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">5</span>   <span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">99</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">89</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">15</span>         <span class="operator">|</span> <span class="number">13</span>            <span class="operator">|</span> <span class="number">11</span>                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">8</span>   <span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">70</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">7</span>          <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">19</span>  <span class="operator">|</span> <span class="number">224</span>        <span class="operator">|</span> <span class="number">75</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">78</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">9</span>          <span class="operator">|</span> <span class="number">10</span>            <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">9</span>   <span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> <span class="number">76</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">2</span>          <span class="operator">|</span> <span class="number">9</span>             <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">16</span>  <span class="operator">|</span> <span class="number">221</span>        <span class="operator">|</span> <span class="number">77</span>        <span class="operator">|</span> <span class="number">99</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">20</span>         <span class="operator">|</span> <span class="number">3</span>             <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">18</span>  <span class="operator">|</span> <span class="number">223</span>        <span class="operator">|</span> <span class="number">79</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">3</span>          <span class="operator">|</span> <span class="number">19</span>            <span class="operator">|</span> <span class="number">5</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">17</span>  <span class="operator">|</span> <span class="number">222</span>        <span class="operator">|</span> <span class="number">80</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">8</span>          <span class="operator">|</span> <span class="number">18</span>            <span class="operator">|</span> <span class="number">6</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>  <span class="operator">|</span> <span class="number">225</span>        <span class="operator">|</span> <span class="number">82</span>        <span class="operator">|</span> <span class="number">85</span>    <span class="operator">|</span> <span class="number">63</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">13</span>         <span class="operator">|</span> <span class="number">4</span>             <span class="operator">|</span> <span class="number">7</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7</span>   <span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> <span class="number">85</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">14</span>         <span class="operator">|</span> <span class="number">12</span>            <span class="operator">|</span> <span class="number">8</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">6</span>   <span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">96</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">79</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">6</span>          <span class="operator">|</span> <span class="number">11</span>            <span class="operator">|</span> <span class="number">9</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+------------+---------------+----------------------+</span></span><br></pre></td></tr></table></figure></li>
<li>结果解析   <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">math_order: 以match排序</span><br><span class="line">english_order: 以english排序</span><br><span class="line">class_order: 以classId,departmentId分组后组内排序</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>rank<ul>
<li>作用：rank() over([partition by col1] [order by col2])，当遇到col2相等时，名次相同，但是下一个col2值的名次递增N（N是重复的次数）。例如：有两条记录是并列第一，下一个是第三，没有第二。</li>
<li>sql示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> id,</span><br><span class="line">       studentId,</span><br><span class="line">       <span class="keyword">language</span>,</span><br><span class="line">       math,</span><br><span class="line">       english,</span><br><span class="line">       classId,</span><br><span class="line">       departmentId,</span><br><span class="line">       <span class="built_in">rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> math) <span class="keyword">as</span> math_rank,</span><br><span class="line">       <span class="built_in">rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> english) <span class="keyword">as</span> english_rank,</span><br><span class="line">       <span class="built_in">rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId,departmentId <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">language</span> ) <span class="keyword">as</span> class_language_rank</span><br><span class="line"><span class="keyword">from</span> student_scores;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+------------+---------------+----------------------+</span></span><br><span class="line"><span class="operator">|</span> id  <span class="operator">|</span> studentid  <span class="operator">|</span> <span class="keyword">language</span>  <span class="operator">|</span> math  <span class="operator">|</span> english  <span class="operator">|</span> classid  <span class="operator">|</span> departmentid  <span class="operator">|</span> math_rank  <span class="operator">|</span> english_rank  <span class="operator">|</span> class_language_rank  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+------------+---------------+----------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">68</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">1</span>          <span class="operator">|</span> <span class="number">14</span>            <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">73</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">9</span>          <span class="operator">|</span> <span class="number">18</span>            <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">17</span>         <span class="operator">|</span> <span class="number">16</span>            <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">90</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">3</span>          <span class="operator">|</span> <span class="number">7</span>             <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">5</span>   <span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">99</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">89</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">15</span>         <span class="operator">|</span> <span class="number">13</span>            <span class="operator">|</span> <span class="number">5</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">12</span>  <span class="operator">|</span> <span class="number">213</span>        <span class="operator">|</span> <span class="number">71</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">17</span>         <span class="operator">|</span> <span class="number">14</span>            <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">11</span>  <span class="operator">|</span> <span class="number">212</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">83</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">12</span>         <span class="operator">|</span> <span class="number">7</span>             <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">14</span>  <span class="operator">|</span> <span class="number">215</span>        <span class="operator">|</span> <span class="number">84</span>        <span class="operator">|</span> <span class="number">82</span>    <span class="operator">|</span> <span class="number">73</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">11</span>         <span class="operator">|</span> <span class="number">6</span>             <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">15</span>  <span class="operator">|</span> <span class="number">216</span>        <span class="operator">|</span> <span class="number">85</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">3</span>          <span class="operator">|</span> <span class="number">16</span>            <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>  <span class="operator">|</span> <span class="number">211</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">60</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">15</span>         <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span> <span class="number">5</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">13</span>  <span class="operator">|</span> <span class="number">214</span>        <span class="operator">|</span> <span class="number">94</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">66</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">17</span>         <span class="operator">|</span> <span class="number">5</span>             <span class="operator">|</span> <span class="number">6</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">8</span>   <span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">70</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">7</span>          <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">9</span>   <span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> <span class="number">76</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">2</span>          <span class="operator">|</span> <span class="number">9</span>             <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7</span>   <span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> <span class="number">85</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">14</span>         <span class="operator">|</span> <span class="number">12</span>            <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">6</span>   <span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">96</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">79</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">3</span>          <span class="operator">|</span> <span class="number">11</span>            <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">19</span>  <span class="operator">|</span> <span class="number">224</span>        <span class="operator">|</span> <span class="number">75</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">78</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">9</span>          <span class="operator">|</span> <span class="number">10</span>            <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">16</span>  <span class="operator">|</span> <span class="number">221</span>        <span class="operator">|</span> <span class="number">77</span>        <span class="operator">|</span> <span class="number">99</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">20</span>         <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">18</span>  <span class="operator">|</span> <span class="number">223</span>        <span class="operator">|</span> <span class="number">79</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">3</span>          <span class="operator">|</span> <span class="number">18</span>            <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">17</span>  <span class="operator">|</span> <span class="number">222</span>        <span class="operator">|</span> <span class="number">80</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">7</span>          <span class="operator">|</span> <span class="number">18</span>            <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>  <span class="operator">|</span> <span class="number">225</span>        <span class="operator">|</span> <span class="number">82</span>        <span class="operator">|</span> <span class="number">85</span>    <span class="operator">|</span> <span class="number">63</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">13</span>         <span class="operator">|</span> <span class="number">4</span>             <span class="operator">|</span> <span class="number">5</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+------------+---------------+----------------------+</span></span><br></pre></td></tr></table></figure></li>
<li>结果解析  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">math_rank: 以match排序 ,student_id：113、216、121、223 math相同，排名都为3，123序号=7</span><br><span class="line">class_language_rank: 以classId,departmentId分组后组内排序</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>dense_rank<ul>
<li>作用：dense_rank() over([partition by col1] [order by col2])与rank类似，当遇到col2相等时，名次同样相等，不同的是，下一个col2值的名次+1，而不是+N。</li>
<li>sql示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> id,</span><br><span class="line">       studentId,</span><br><span class="line">       <span class="keyword">language</span>,</span><br><span class="line">       math,</span><br><span class="line">       english,</span><br><span class="line">       classId,</span><br><span class="line">       departmentId,</span><br><span class="line">       <span class="built_in">dense_rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> math) <span class="keyword">as</span> math_dense_rank,</span><br><span class="line">       <span class="built_in">dense_rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> english) <span class="keyword">as</span> english_dense_rank,</span><br><span class="line">       <span class="built_in">dense_rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId,departmentId <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">language</span> ) <span class="keyword">as</span> class_language_dense_rank</span><br><span class="line"><span class="keyword">from</span> student_scores;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+------------------+---------------------+----------------------------+</span></span><br><span class="line"><span class="operator">|</span> id  <span class="operator">|</span> studentid  <span class="operator">|</span> <span class="keyword">language</span>  <span class="operator">|</span> math  <span class="operator">|</span> english  <span class="operator">|</span> classid  <span class="operator">|</span> departmentid  <span class="operator">|</span> math_dense_rank  <span class="operator">|</span> english_dense_rank  <span class="operator">|</span> class_language_dense_rank  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+------------------+---------------------+----------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">68</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">1</span>                <span class="operator">|</span> <span class="number">12</span>                  <span class="operator">|</span> <span class="number">1</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">73</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">5</span>                <span class="operator">|</span> <span class="number">14</span>                  <span class="operator">|</span> <span class="number">2</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">11</span>               <span class="operator">|</span> <span class="number">13</span>                  <span class="operator">|</span> <span class="number">3</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">90</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">3</span>                <span class="operator">|</span> <span class="number">6</span>                   <span class="operator">|</span> <span class="number">4</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">5</span>   <span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">99</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">89</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">10</span>               <span class="operator">|</span> <span class="number">11</span>                  <span class="operator">|</span> <span class="number">5</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">12</span>  <span class="operator">|</span> <span class="number">213</span>        <span class="operator">|</span> <span class="number">71</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">11</span>               <span class="operator">|</span> <span class="number">12</span>                  <span class="operator">|</span> <span class="number">1</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">11</span>  <span class="operator">|</span> <span class="number">212</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">83</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">7</span>                <span class="operator">|</span> <span class="number">6</span>                   <span class="operator">|</span> <span class="number">2</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">14</span>  <span class="operator">|</span> <span class="number">215</span>        <span class="operator">|</span> <span class="number">84</span>        <span class="operator">|</span> <span class="number">82</span>    <span class="operator">|</span> <span class="number">73</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">6</span>                <span class="operator">|</span> <span class="number">5</span>                   <span class="operator">|</span> <span class="number">3</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">15</span>  <span class="operator">|</span> <span class="number">216</span>        <span class="operator">|</span> <span class="number">85</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">3</span>                <span class="operator">|</span> <span class="number">13</span>                  <span class="operator">|</span> <span class="number">4</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>  <span class="operator">|</span> <span class="number">211</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">60</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">10</span>               <span class="operator">|</span> <span class="number">1</span>                   <span class="operator">|</span> <span class="number">5</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">13</span>  <span class="operator">|</span> <span class="number">214</span>        <span class="operator">|</span> <span class="number">94</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">66</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">11</span>               <span class="operator">|</span> <span class="number">4</span>                   <span class="operator">|</span> <span class="number">6</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">8</span>   <span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">70</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">4</span>                <span class="operator">|</span> <span class="number">2</span>                   <span class="operator">|</span> <span class="number">1</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">9</span>   <span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> <span class="number">76</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">2</span>                <span class="operator">|</span> <span class="number">7</span>                   <span class="operator">|</span> <span class="number">2</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7</span>   <span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> <span class="number">85</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">9</span>                <span class="operator">|</span> <span class="number">10</span>                  <span class="operator">|</span> <span class="number">3</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">6</span>   <span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">96</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">79</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">3</span>                <span class="operator">|</span> <span class="number">9</span>                   <span class="operator">|</span> <span class="number">4</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">19</span>  <span class="operator">|</span> <span class="number">224</span>        <span class="operator">|</span> <span class="number">75</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">78</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">5</span>                <span class="operator">|</span> <span class="number">8</span>                   <span class="operator">|</span> <span class="number">1</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">16</span>  <span class="operator">|</span> <span class="number">221</span>        <span class="operator">|</span> <span class="number">77</span>        <span class="operator">|</span> <span class="number">99</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">12</span>               <span class="operator">|</span> <span class="number">2</span>                   <span class="operator">|</span> <span class="number">2</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">18</span>  <span class="operator">|</span> <span class="number">223</span>        <span class="operator">|</span> <span class="number">79</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">3</span>                <span class="operator">|</span> <span class="number">14</span>                  <span class="operator">|</span> <span class="number">3</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">17</span>  <span class="operator">|</span> <span class="number">222</span>        <span class="operator">|</span> <span class="number">80</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">4</span>                <span class="operator">|</span> <span class="number">14</span>                  <span class="operator">|</span> <span class="number">4</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>  <span class="operator">|</span> <span class="number">225</span>        <span class="operator">|</span> <span class="number">82</span>        <span class="operator">|</span> <span class="number">85</span>    <span class="operator">|</span> <span class="number">63</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">8</span>                <span class="operator">|</span> <span class="number">3</span>                   <span class="operator">|</span> <span class="number">5</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+------------------+---------------------+----------------------------+</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>ntile<ul>
<li>作用：ntile(N) over([partition by col1] [order by col2])，将分区中的数据按照顺序划分为N片，返回当前片的值。<ul>
<li>注1：如果切片分布不均匀，默认增加第一个切片的分布</li>
<li>注2：ntile不支持ROWS BETWEEN</li>
</ul>
</li>
<li>sql示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> id,</span><br><span class="line">       studentId,</span><br><span class="line">       <span class="keyword">language</span>,</span><br><span class="line">       math,</span><br><span class="line">       english,</span><br><span class="line">       classId,</span><br><span class="line">       departmentId,</span><br><span class="line">       <span class="built_in">ntile</span>(<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> math) <span class="keyword">as</span> math_ntile,</span><br><span class="line">       <span class="built_in">ntile</span>(<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> english) <span class="keyword">as</span> english_ntile,</span><br><span class="line">       <span class="built_in">ntile</span>(<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId,departmentId <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">language</span> ) <span class="keyword">as</span> class_language_ntile</span><br><span class="line"><span class="keyword">from</span> student_scores;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+-------------+----------------+-----------------------+</span></span><br><span class="line"><span class="operator">|</span> id  <span class="operator">|</span> studentid  <span class="operator">|</span> <span class="keyword">language</span>  <span class="operator">|</span> math  <span class="operator">|</span> english  <span class="operator">|</span> classid  <span class="operator">|</span> departmentid  <span class="operator">|</span> math_ntile  <span class="operator">|</span> english_ntile  <span class="operator">|</span> class_language_ntile  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+-------------+----------------+-----------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">68</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">73</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">90</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">5</span>   <span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">99</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">89</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">12</span>  <span class="operator">|</span> <span class="number">213</span>        <span class="operator">|</span> <span class="number">71</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">11</span>  <span class="operator">|</span> <span class="number">212</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">83</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">14</span>  <span class="operator">|</span> <span class="number">215</span>        <span class="operator">|</span> <span class="number">84</span>        <span class="operator">|</span> <span class="number">82</span>    <span class="operator">|</span> <span class="number">73</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">15</span>  <span class="operator">|</span> <span class="number">216</span>        <span class="operator">|</span> <span class="number">85</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>  <span class="operator">|</span> <span class="number">211</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">60</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">13</span>  <span class="operator">|</span> <span class="number">214</span>        <span class="operator">|</span> <span class="number">94</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">66</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">8</span>   <span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">70</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">9</span>   <span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> <span class="number">76</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7</span>   <span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> <span class="number">85</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">6</span>   <span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">96</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">79</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">19</span>  <span class="operator">|</span> <span class="number">224</span>        <span class="operator">|</span> <span class="number">75</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">78</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">16</span>  <span class="operator">|</span> <span class="number">221</span>        <span class="operator">|</span> <span class="number">77</span>        <span class="operator">|</span> <span class="number">99</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">18</span>  <span class="operator">|</span> <span class="number">223</span>        <span class="operator">|</span> <span class="number">79</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">17</span>  <span class="operator">|</span> <span class="number">222</span>        <span class="operator">|</span> <span class="number">80</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>  <span class="operator">|</span> <span class="number">225</span>        <span class="operator">|</span> <span class="number">82</span>        <span class="operator">|</span> <span class="number">85</span>    <span class="operator">|</span> <span class="number">63</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+-------------+----------------+-----------------------+</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>percent_rank<ul>
<li>作用：percent_rank() over([partition by col1] [order by col2])，计算给定行的百分比排名。分组内当前行的RANK值-1/分组内总行数-1，可以用来计算超过了百分之多少的人。</li>
<li>sql示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> id,</span><br><span class="line">       studentid,</span><br><span class="line">       <span class="keyword">language</span>,</span><br><span class="line">       math,</span><br><span class="line">       english,</span><br><span class="line">       classid,</span><br><span class="line">       departmentid,</span><br><span class="line">       <span class="built_in">percent_rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> math)                                        <span class="keyword">as</span> math_percent_rank,</span><br><span class="line">       <span class="built_in">percent_rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> english)                                     <span class="keyword">as</span> english_percent_rank,</span><br><span class="line">       <span class="built_in">percent_rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId,departmentId <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">language</span> ) <span class="keyword">as</span> class_language_percent_rank</span><br><span class="line"><span class="keyword">from</span> student_scores;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+----------------------+-----------------------+------------------------------+</span></span><br><span class="line"><span class="operator">|</span> id  <span class="operator">|</span> studentid  <span class="operator">|</span> <span class="keyword">language</span>  <span class="operator">|</span> math  <span class="operator">|</span> english  <span class="operator">|</span> classid  <span class="operator">|</span> departmentid  <span class="operator">|</span>  math_percent_rank   <span class="operator">|</span> english_percent_rank  <span class="operator">|</span> class_language_percent_rank  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+----------------------+-----------------------+------------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">68</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">0.0</span>                  <span class="operator">|</span> <span class="number">0.6842105263157895</span>    <span class="operator">|</span> <span class="number">0.0</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">73</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">0.42105263157894735</span>  <span class="operator">|</span> <span class="number">0.8947368421052632</span>    <span class="operator">|</span> <span class="number">0.25</span>                         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">0.8421052631578947</span>   <span class="operator">|</span> <span class="number">0.7894736842105263</span>    <span class="operator">|</span> <span class="number">0.5</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">90</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">0.10526315789473684</span>  <span class="operator">|</span> <span class="number">0.3157894736842105</span>    <span class="operator">|</span> <span class="number">0.75</span>                         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">5</span>   <span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">99</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">89</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">0.7368421052631579</span>   <span class="operator">|</span> <span class="number">0.631578947368421</span>     <span class="operator">|</span> <span class="number">1.0</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">12</span>  <span class="operator">|</span> <span class="number">213</span>        <span class="operator">|</span> <span class="number">71</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.8421052631578947</span>   <span class="operator">|</span> <span class="number">0.6842105263157895</span>    <span class="operator">|</span> <span class="number">0.0</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">11</span>  <span class="operator">|</span> <span class="number">212</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">83</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.5789473684210527</span>   <span class="operator">|</span> <span class="number">0.3157894736842105</span>    <span class="operator">|</span> <span class="number">0.2</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">14</span>  <span class="operator">|</span> <span class="number">215</span>        <span class="operator">|</span> <span class="number">84</span>        <span class="operator">|</span> <span class="number">82</span>    <span class="operator">|</span> <span class="number">73</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.5263157894736842</span>   <span class="operator">|</span> <span class="number">0.2631578947368421</span>    <span class="operator">|</span> <span class="number">0.4</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">15</span>  <span class="operator">|</span> <span class="number">216</span>        <span class="operator">|</span> <span class="number">85</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.10526315789473684</span>  <span class="operator">|</span> <span class="number">0.7894736842105263</span>    <span class="operator">|</span> <span class="number">0.6</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>  <span class="operator">|</span> <span class="number">211</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">60</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.7368421052631579</span>   <span class="operator">|</span> <span class="number">0.0</span>                   <span class="operator">|</span> <span class="number">0.8</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">13</span>  <span class="operator">|</span> <span class="number">214</span>        <span class="operator">|</span> <span class="number">94</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">66</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.8421052631578947</span>   <span class="operator">|</span> <span class="number">0.21052631578947367</span>   <span class="operator">|</span> <span class="number">1.0</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">8</span>   <span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">70</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">0.3157894736842105</span>   <span class="operator">|</span> <span class="number">0.05263157894736842</span>   <span class="operator">|</span> <span class="number">0.0</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">9</span>   <span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> <span class="number">76</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">0.05263157894736842</span>  <span class="operator">|</span> <span class="number">0.42105263157894735</span>   <span class="operator">|</span> <span class="number">0.3333333333333333</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7</span>   <span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> <span class="number">85</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">0.6842105263157895</span>   <span class="operator">|</span> <span class="number">0.5789473684210527</span>    <span class="operator">|</span> <span class="number">0.6666666666666666</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">6</span>   <span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">96</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">79</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">0.10526315789473684</span>  <span class="operator">|</span> <span class="number">0.5263157894736842</span>    <span class="operator">|</span> <span class="number">1.0</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">19</span>  <span class="operator">|</span> <span class="number">224</span>        <span class="operator">|</span> <span class="number">75</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">78</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.42105263157894735</span>  <span class="operator">|</span> <span class="number">0.47368421052631576</span>   <span class="operator">|</span> <span class="number">0.0</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">16</span>  <span class="operator">|</span> <span class="number">221</span>        <span class="operator">|</span> <span class="number">77</span>        <span class="operator">|</span> <span class="number">99</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">1.0</span>                  <span class="operator">|</span> <span class="number">0.05263157894736842</span>   <span class="operator">|</span> <span class="number">0.25</span>                         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">18</span>  <span class="operator">|</span> <span class="number">223</span>        <span class="operator">|</span> <span class="number">79</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.10526315789473684</span>  <span class="operator">|</span> <span class="number">0.8947368421052632</span>    <span class="operator">|</span> <span class="number">0.5</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">17</span>  <span class="operator">|</span> <span class="number">222</span>        <span class="operator">|</span> <span class="number">80</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.3157894736842105</span>   <span class="operator">|</span> <span class="number">0.8947368421052632</span>    <span class="operator">|</span> <span class="number">0.75</span>                         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>  <span class="operator">|</span> <span class="number">225</span>        <span class="operator">|</span> <span class="number">82</span>        <span class="operator">|</span> <span class="number">85</span>    <span class="operator">|</span> <span class="number">63</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.631578947368421</span>    <span class="operator">|</span> <span class="number">0.15789473684210525</span>   <span class="operator">|</span> <span class="number">1.0</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+----------------------+-----------------------+------------------------------+</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h4 id="8-2-5-4-多维度分析"><a href="#8-2-5-4-多维度分析" class="headerlink" title="8.2.5.4 多维度分析"></a>8.2.5.4 多维度分析</h4><ol>
<li>GROUPING SETS<ul>
<li>作用：在一个GROUP BY查询中，根据不同的维度组合进行聚合，等价于将不同维度的GROUP BY结果集进行UNION ALL</li>
<li>示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,</span><br><span class="line">       <span class="keyword">day</span>,</span><br><span class="line">       <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv</span><br><span class="line"><span class="keyword">FROM</span> visit_log</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>, <span class="keyword">day</span></span><br><span class="line">    <span class="keyword">GROUPING</span> SETS ( <span class="keyword">month</span>, <span class="keyword">day</span>);</span><br><span class="line">    </span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+</span></span><br><span class="line"><span class="operator">|</span>  <span class="keyword">month</span>   <span class="operator">|</span>     <span class="keyword">day</span>     <span class="operator">|</span> uv  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-15</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-16</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">5</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">6</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+</span></span><br></pre></td></tr></table></figure></li>
<li>分析  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 等价于</span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="keyword">NULL</span> <span class="keyword">as</span> <span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv <span class="keyword">FROM</span> test2 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span> </span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">NULL</span> <span class="keyword">as</span> <span class="keyword">month</span>,<span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv <span class="keyword">FROM</span> test2 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">day</span>;</span><br></pre></td></tr></table></figure></li>
<li>升级版 GROUPING__ID，表示结果属于哪一个分组集合。（<font color ='red' >GROUPING__ID</font>两个下划线__）  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="keyword">month</span>,</span><br><span class="line">    <span class="keyword">day</span>,</span><br><span class="line">    <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,</span><br><span class="line">    GROUPING__ID</span><br><span class="line"><span class="keyword">FROM</span> visit_log</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span></span><br><span class="line">    <span class="keyword">GROUPING</span> SETS (<span class="keyword">month</span>,<span class="keyword">day</span>,(<span class="keyword">month</span>,<span class="keyword">day</span>))</span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+---------------+</span></span><br><span class="line"><span class="operator">|</span>  <span class="keyword">month</span>   <span class="operator">|</span>     <span class="keyword">day</span>     <span class="operator">|</span> uv  <span class="operator">|</span> grouping__id  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-16</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-15</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">5</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">6</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-16</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-15</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+---------------+</span></span><br></pre></td></tr></table></figure></li>
<li>分析  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 等价</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="keyword">NULL</span> <span class="keyword">as</span> <span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">1</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test2 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span> </span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">NULL</span> <span class="keyword">as</span> <span class="keyword">month</span>,<span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">2</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test2 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">day</span></span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">3</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test2 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>CUBE<ul>
<li>作用：根据GROUP BY的维度的所有组合进行聚合。</li>
<li>示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="keyword">month</span>,</span><br><span class="line">    <span class="keyword">day</span>,</span><br><span class="line">    <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,</span><br><span class="line">    GROUPING__ID</span><br><span class="line"><span class="keyword">FROM</span> visit_log</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span></span><br><span class="line">    <span class="keyword">WITH</span> <span class="keyword">CUBE</span></span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+---------------+</span></span><br><span class="line"><span class="operator">|</span>  <span class="keyword">month</span>   <span class="operator">|</span>     <span class="keyword">day</span>     <span class="operator">|</span> uv  <span class="operator">|</span> grouping__id  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-16</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-15</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">5</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">6</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-16</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-15</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">7</span>   <span class="operator">|</span> <span class="number">3</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+---------------+</span></span><br></pre></td></tr></table></figure></li>
<li>解析  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">等价于</span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">NULL</span> <span class="keyword">as</span> <span class="keyword">month</span>,<span class="keyword">NULL</span> <span class="keyword">as</span> <span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">0</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test2</span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="keyword">NULL</span> <span class="keyword">as</span> <span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">1</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test2 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span> </span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">NULL</span> <span class="keyword">as</span> <span class="keyword">month</span>,<span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">2</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test2 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">day</span></span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">3</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test2 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>ROLLUP<ul>
<li>作用：是CUBE的子集，以最左侧的维度为主，从该维度进行层级聚合。</li>
<li>比如，以month维度进行层级聚合：实现的上钻过程：月天的UV-&gt;月的UV-&gt;总UV  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="keyword">month</span>,</span><br><span class="line">    <span class="keyword">day</span>,</span><br><span class="line">    <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,</span><br><span class="line">    GROUPING__ID</span><br><span class="line"><span class="keyword">FROM</span> visit_log</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span></span><br><span class="line">    <span class="keyword">WITH</span> <span class="keyword">ROLLUP</span></span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br><span class="line">    </span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+---------------+</span></span><br><span class="line"><span class="operator">|</span>  <span class="keyword">month</span>   <span class="operator">|</span>     <span class="keyword">day</span>     <span class="operator">|</span> uv  <span class="operator">|</span> grouping__id  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-16</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-15</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">6</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">5</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">7</span>   <span class="operator">|</span> <span class="number">3</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+---------------+</span></span><br></pre></td></tr></table></figure></li>
<li>如果把month和day调换顺序，则以day维度进行层级聚合：实现的上钻过程：天月的UV-&gt;天的UV-&gt;总UV  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="keyword">day</span>,</span><br><span class="line">    <span class="keyword">month</span>,</span><br><span class="line">    <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,</span><br><span class="line">    GROUPING__ID</span><br><span class="line"><span class="keyword">FROM</span> visit_log</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">day</span>,<span class="keyword">month</span></span><br><span class="line">    <span class="keyword">WITH</span> <span class="keyword">ROLLUP</span></span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br><span class="line">    </span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+----------+-----+---------------+</span></span><br><span class="line"><span class="operator">|</span>     <span class="keyword">day</span>     <span class="operator">|</span>  <span class="keyword">month</span>   <span class="operator">|</span> uv  <span class="operator">|</span> grouping__id  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+----------+-----+---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-16</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-15</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-12</span>  <span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-16</span>  <span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-10</span>  <span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-15</span>  <span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-12</span>  <span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">7</span>   <span class="operator">|</span> <span class="number">3</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+----------+-----+---------------+</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h4 id="8-2-5-5-开窗函数练习"><a href="#8-2-5-5-开窗函数练习" class="headerlink" title="8.2.5.5 开窗函数练习"></a>8.2.5.5 开窗函数练习</h4><h5 id="8-2-5-5-1-聚合开窗练习"><a href="#8-2-5-5-1-聚合开窗练习" class="headerlink" title="8.2.5.5.1 聚合开窗练习"></a>8.2.5.5.1 聚合开窗练习</h5><ol>
<li>建表导入数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> business <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> stack(<span class="number">14</span>,</span><br><span class="line">    <span class="string">&#x27;jack&#x27;</span>,<span class="string">&#x27;2017-01-01&#x27;</span>,<span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;tony&#x27;</span>,<span class="string">&#x27;2017-01-02&#x27;</span>,<span class="string">&#x27;15&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;jack&#x27;</span>,<span class="string">&#x27;2017-02-03&#x27;</span>,<span class="string">&#x27;23&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;tony&#x27;</span>,<span class="string">&#x27;2017-01-04&#x27;</span>,<span class="string">&#x27;29&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;jack&#x27;</span>,<span class="string">&#x27;2017-01-05&#x27;</span>,<span class="string">&#x27;46&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;jack&#x27;</span>,<span class="string">&#x27;2017-04-06&#x27;</span>,<span class="string">&#x27;42&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;tony&#x27;</span>,<span class="string">&#x27;2017-01-07&#x27;</span>,<span class="string">&#x27;50&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;jack&#x27;</span>,<span class="string">&#x27;2017-01-08&#x27;</span>,<span class="string">&#x27;55&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;mart&#x27;</span>,<span class="string">&#x27;2017-04-08&#x27;</span>,<span class="string">&#x27;62&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;mart&#x27;</span>,<span class="string">&#x27;2017-04-09&#x27;</span>,<span class="string">&#x27;68&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;neil&#x27;</span>,<span class="string">&#x27;2017-05-10&#x27;</span>,<span class="string">&#x27;12&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;mart&#x27;</span>,<span class="string">&#x27;2017-04-11&#x27;</span>,<span class="string">&#x27;75&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;neil&#x27;</span>,<span class="string">&#x27;2017-06-12&#x27;</span>,<span class="string">&#x27;80&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;mart&#x27;</span>,<span class="string">&#x27;2017-04-13&#x27;</span>,<span class="string">&#x27;94&#x27;</span>)</span><br><span class="line"><span class="keyword">as</span> (name, orderdate, cost)</span><br></pre></td></tr></table></figure></li>
<li>需求一： 查询在2017年4月份购买过的顾客及总人数 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">       name,</span><br><span class="line">       <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">over</span> ()</span><br><span class="line"><span class="keyword">from</span> business</span><br><span class="line"><span class="keyword">where</span> substr(orderdate, <span class="number">0</span>, <span class="number">7</span>) <span class="operator">=</span> <span class="string">&#x27;2017-04&#x27;</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> name;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-----------------+</span></span><br><span class="line"><span class="operator">|</span> name  <span class="operator">|</span> count_window_0  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-----------------+</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">2</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">2</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-----------------+</span></span><br></pre></td></tr></table></figure></li>
<li>需求二：查询顾客的购买明细及月购买总额 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">       name,</span><br><span class="line">       cost,</span><br><span class="line">       orderdate,</span><br><span class="line">       substr(orderdate, <span class="number">0</span>, <span class="number">7</span>),</span><br><span class="line">       <span class="built_in">sum</span>(cost) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> substr(orderdate, <span class="number">0</span>, <span class="number">7</span>))</span><br><span class="line"><span class="keyword">from</span> business;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+----------+---------------+</span></span><br><span class="line"><span class="operator">|</span> name  <span class="operator">|</span> cost  <span class="operator">|</span>  orderdate  <span class="operator">|</span>   _c3    <span class="operator">|</span> sum_window_0  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+----------+---------------+</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">10</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">205.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">55</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-08</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">205.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">50</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-07</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">205.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">46</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-05</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">205.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">29</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">205.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">15</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-02</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">205.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">23</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-02</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-02</span>  <span class="operator">|</span> <span class="number">23.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">341.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">42</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-06</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">341.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">75</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-11</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">341.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">68</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-09</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">341.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">62</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-08</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">341.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> neil  <span class="operator">|</span> <span class="number">12</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-05</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-05</span>  <span class="operator">|</span> <span class="number">12.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> neil  <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-06</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-06</span>  <span class="operator">|</span> <span class="number">80.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+----------+---------------+</span></span><br></pre></td></tr></table></figure></li>
<li>需求三：查询每个顾客的购买明细及月购买总额 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">       name,</span><br><span class="line">       cost,</span><br><span class="line">       orderdate,</span><br><span class="line">       substr(orderdate, <span class="number">0</span>, <span class="number">7</span>),</span><br><span class="line">       <span class="built_in">sum</span>(cost) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> substr(orderdate, <span class="number">0</span>, <span class="number">7</span>),name)</span><br><span class="line"><span class="keyword">from</span> business;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+----------+---------------+</span></span><br><span class="line"><span class="operator">|</span> name  <span class="operator">|</span> cost  <span class="operator">|</span>  orderdate  <span class="operator">|</span>   _c3    <span class="operator">|</span> sum_window_0  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+----------+---------------+</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">55</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-08</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">111.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">46</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-05</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">111.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">10</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">111.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">50</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-07</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">94.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">29</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">94.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">15</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-02</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">94.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">23</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-02</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-02</span>  <span class="operator">|</span> <span class="number">23.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">42</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-06</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">42.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">75</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-11</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">299.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">299.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">68</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-09</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">299.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">62</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-08</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">299.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> neil  <span class="operator">|</span> <span class="number">12</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-05</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-05</span>  <span class="operator">|</span> <span class="number">12.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> neil  <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-06</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-06</span>  <span class="operator">|</span> <span class="number">80.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+----------+---------------+</span></span><br></pre></td></tr></table></figure></li>
<li>需求四：查询每个顾客的购买明细及月购买总额, 将每个顾客的cost按照日期进行累加 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">       name,</span><br><span class="line">       cost,</span><br><span class="line">       orderdate,</span><br><span class="line">       substr(orderdate, <span class="number">0</span>, <span class="number">7</span>),</span><br><span class="line">       <span class="built_in">sum</span>(cost) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate),</span><br><span class="line">       <span class="built_in">sum</span>(cost) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> substr(orderdate, <span class="number">0</span>, <span class="number">7</span>),name)</span><br><span class="line"><span class="keyword">from</span> business</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> name,orderdate;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+----------+--------+---------------+</span></span><br><span class="line"><span class="operator">|</span> name  <span class="operator">|</span> cost  <span class="operator">|</span>  orderdate  <span class="operator">|</span>   _c3    <span class="operator">|</span>  _c4   <span class="operator">|</span> sum_window_1  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+----------+--------+---------------+</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">10</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">10.0</span>   <span class="operator">|</span> <span class="number">111.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">46</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-05</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">56.0</span>   <span class="operator">|</span> <span class="number">111.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">55</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-08</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">111.0</span>  <span class="operator">|</span> <span class="number">111.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">23</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-02</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-02</span>  <span class="operator">|</span> <span class="number">134.0</span>  <span class="operator">|</span> <span class="number">23.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">42</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-06</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">176.0</span>  <span class="operator">|</span> <span class="number">42.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">62</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-08</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">62.0</span>   <span class="operator">|</span> <span class="number">299.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">68</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-09</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">130.0</span>  <span class="operator">|</span> <span class="number">299.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">75</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-11</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">205.0</span>  <span class="operator">|</span> <span class="number">299.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">299.0</span>  <span class="operator">|</span> <span class="number">299.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> neil  <span class="operator">|</span> <span class="number">12</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-05</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-05</span>  <span class="operator">|</span> <span class="number">12.0</span>   <span class="operator">|</span> <span class="number">12.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> neil  <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-06</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-06</span>  <span class="operator">|</span> <span class="number">92.0</span>   <span class="operator">|</span> <span class="number">80.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">15</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-02</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">15.0</span>   <span class="operator">|</span> <span class="number">94.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">29</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">44.0</span>   <span class="operator">|</span> <span class="number">94.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">50</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-07</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">94.0</span>   <span class="operator">|</span> <span class="number">94.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+----------+--------+---------------+</span></span><br></pre></td></tr></table></figure></li>
<li>需求五：查询每个顾客上次的购买时间以及下一次的购买时间 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">       name,</span><br><span class="line">       cost,</span><br><span class="line">       orderdate,</span><br><span class="line">       <span class="built_in">lag</span>(orderdate, <span class="number">1</span>, &quot;first&quot;) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate),</span><br><span class="line">       <span class="built_in">lead</span>(orderdate, <span class="number">1</span>, &quot;last&quot;) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate)</span><br><span class="line"><span class="keyword">from</span> business</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> name, orderdate;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+---------------+----------------+</span></span><br><span class="line"><span class="operator">|</span> name  <span class="operator">|</span> cost  <span class="operator">|</span>  orderdate  <span class="operator">|</span> lag_window_0  <span class="operator">|</span> lead_window_1  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+---------------+----------------+</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">10</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-01</span>  <span class="operator">|</span> <span class="keyword">first</span>         <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-05</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">46</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-05</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-01</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-08</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">55</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-08</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-05</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-02</span><span class="number">-03</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">23</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-02</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-08</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-06</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">42</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-06</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-02</span><span class="number">-03</span>    <span class="operator">|</span> <span class="keyword">last</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">62</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-08</span>  <span class="operator">|</span> <span class="keyword">first</span>         <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-09</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">68</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-09</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-08</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-11</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">75</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-11</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-09</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-13</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-11</span>    <span class="operator">|</span> <span class="keyword">last</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> neil  <span class="operator">|</span> <span class="number">12</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-05</span><span class="number">-10</span>  <span class="operator">|</span> <span class="keyword">first</span>         <span class="operator">|</span> <span class="number">2017</span><span class="number">-06</span><span class="number">-12</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> neil  <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-06</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-05</span><span class="number">-10</span>    <span class="operator">|</span> <span class="keyword">last</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">15</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-02</span>  <span class="operator">|</span> <span class="keyword">first</span>         <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">29</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-02</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-07</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">50</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-07</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span>    <span class="operator">|</span> <span class="keyword">last</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+---------------+----------------+</span></span><br></pre></td></tr></table></figure></li>
<li>需求六：查询前20%时间的订单信息 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">select</span> name,</span><br><span class="line">                cost,</span><br><span class="line">                orderdate,</span><br><span class="line">                <span class="built_in">ntile</span>(<span class="number">5</span>) <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> orderdate) num</span><br><span class="line">         <span class="keyword">from</span> business</span><br><span class="line">     ) t1</span><br><span class="line"><span class="keyword">where</span> t1.num <span class="operator">=</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> name, orderdate;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+----------+---------------+---------+</span></span><br><span class="line"><span class="operator">|</span> t1.name  <span class="operator">|</span> t1.cost  <span class="operator">|</span> t1.orderdate  <span class="operator">|</span> t1.num  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+----------+---------------+---------+</span></span><br><span class="line"><span class="operator">|</span> jack     <span class="operator">|</span> <span class="number">10</span>       <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-01</span>    <span class="operator">|</span> <span class="number">1</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony     <span class="operator">|</span> <span class="number">15</span>       <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-02</span>    <span class="operator">|</span> <span class="number">1</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony     <span class="operator">|</span> <span class="number">29</span>       <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span>    <span class="operator">|</span> <span class="number">1</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+----------+---------------+---------+</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h5 id="8-2-5-5-1-排序开窗练习"><a href="#8-2-5-5-1-排序开窗练习" class="headerlink" title="8.2.5.5.1 排序开窗练习"></a>8.2.5.5.1 排序开窗练习</h5><ol>
<li>建表导数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> stack(<span class="number">12</span>,</span><br><span class="line"> &quot;孙悟空&quot;, &quot;语文&quot;, &quot;87&quot;,</span><br><span class="line"> &quot;孙悟空&quot;, &quot;数学&quot;, &quot;95&quot;,</span><br><span class="line"> &quot;孙悟空&quot;, &quot;英语&quot;, &quot;68&quot;,</span><br><span class="line"> &quot;大海&quot;, &quot;语文&quot;, &quot;94&quot;,</span><br><span class="line"> &quot;大海&quot;, &quot;数学&quot;, &quot;56&quot;,</span><br><span class="line"> &quot;大海&quot;, &quot;英语&quot;, &quot;84&quot;,</span><br><span class="line"> &quot;宋宋&quot;, &quot;语文&quot;, &quot;64&quot;,</span><br><span class="line"> &quot;宋宋&quot;, &quot;数学&quot;, &quot;86&quot;,</span><br><span class="line"> &quot;宋宋&quot;, &quot;英语&quot;, &quot;84&quot;,</span><br><span class="line"> &quot;婷婷&quot;, &quot;语文&quot;, &quot;65&quot;,</span><br><span class="line"> &quot;婷婷&quot;, &quot;数学&quot;, &quot;85&quot;,</span><br><span class="line"> &quot;婷婷&quot;, &quot;英语&quot;, &quot;78&quot;)</span><br><span class="line"><span class="keyword">as</span> (name, subject, score);</span><br></pre></td></tr></table></figure></li>
<li>根据学科进行分区操作 并且按照成绩字段进行倒序，最后利用排名函数 完成排名  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name,</span><br><span class="line">       subject,</span><br><span class="line">       score,</span><br><span class="line">       <span class="built_in">rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span> ),</span><br><span class="line">       <span class="built_in">dense_rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span> ),</span><br><span class="line">       <span class="built_in">row_number</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span> )</span><br><span class="line"><span class="keyword">from</span> score;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+----------+--------+----------------+----------------------+----------------------+</span></span><br><span class="line"><span class="operator">|</span> name  <span class="operator">|</span> subject  <span class="operator">|</span> score  <span class="operator">|</span> rank_window_0  <span class="operator">|</span> dense_rank_window_1  <span class="operator">|</span> row_number_window_2  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+----------+--------+----------------+----------------------+----------------------+</span></span><br><span class="line"><span class="operator">|</span> 孙悟空   <span class="operator">|</span> 数学       <span class="operator">|</span> <span class="number">95</span>     <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 宋宋    <span class="operator">|</span> 数学       <span class="operator">|</span> <span class="number">86</span>     <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 婷婷    <span class="operator">|</span> 数学       <span class="operator">|</span> <span class="number">85</span>     <span class="operator">|</span> <span class="number">3</span>              <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 大海    <span class="operator">|</span> 数学       <span class="operator">|</span> <span class="number">56</span>     <span class="operator">|</span> <span class="number">4</span>              <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 宋宋    <span class="operator">|</span> 英语       <span class="operator">|</span> <span class="number">84</span>     <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 大海    <span class="operator">|</span> 英语       <span class="operator">|</span> <span class="number">84</span>     <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 婷婷    <span class="operator">|</span> 英语       <span class="operator">|</span> <span class="number">78</span>     <span class="operator">|</span> <span class="number">3</span>              <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 孙悟空   <span class="operator">|</span> 英语       <span class="operator">|</span> <span class="number">68</span>     <span class="operator">|</span> <span class="number">4</span>              <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 大海    <span class="operator">|</span> 语文       <span class="operator">|</span> <span class="number">94</span>     <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 孙悟空   <span class="operator">|</span> 语文       <span class="operator">|</span> <span class="number">87</span>     <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 婷婷    <span class="operator">|</span> 语文       <span class="operator">|</span> <span class="number">65</span>     <span class="operator">|</span> <span class="number">3</span>              <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 宋宋    <span class="operator">|</span> 语文       <span class="operator">|</span> <span class="number">64</span>     <span class="operator">|</span> <span class="number">4</span>              <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+----------+--------+----------------+----------------------+----------------------+</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="8-2-6-其他常用函数"><a href="#8-2-6-其他常用函数" class="headerlink" title="8.2.6 其他常用函数"></a>8.2.6 其他常用函数</h3><table>
<thead>
<tr>
<th>类别</th>
<th>函数</th>
<th>描述</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td>常用日期函数</td>
<td>unix_timestamp</td>
<td>返回当前或指定时间的时间戳</td>
<td>select unix_timestamp();select unix_timestamp(“2020-10-28”,’yyyy-MM-dd’);</td>
</tr>
<tr>
<td></td>
<td>from_unixtime</td>
<td>将时间戳转为日期格式</td>
<td>select from_unixtime(1603843200);</td>
</tr>
<tr>
<td></td>
<td>current_date</td>
<td>当前日期</td>
<td>select current_date;</td>
</tr>
<tr>
<td></td>
<td>current_timestamp</td>
<td>当前的日期加时间</td>
<td>select current_timestamp;</td>
</tr>
<tr>
<td></td>
<td>to_date</td>
<td>抽取日期部分</td>
<td>select to_date(‘2020-10-28 12:12:12’);</td>
</tr>
<tr>
<td></td>
<td>year</td>
<td>获取年</td>
<td>select year(‘2020-10-28 12:12:12’);</td>
</tr>
<tr>
<td></td>
<td>month</td>
<td>获取月</td>
<td>select month(‘2020-10-28 12:12:12’);</td>
</tr>
<tr>
<td></td>
<td>day</td>
<td>获取日</td>
<td>select day(‘2020-10-28 12:12:12’);</td>
</tr>
<tr>
<td></td>
<td>hour</td>
<td>获取时</td>
<td>select hour(‘2020-10-28 12:12:12’);</td>
</tr>
<tr>
<td></td>
<td>minute</td>
<td>获取分</td>
<td>select minute(‘2020-10-28 12:12:12’);</td>
</tr>
<tr>
<td></td>
<td>second</td>
<td>获取秒</td>
<td>select second(‘2020-10-28 12:12:12’);</td>
</tr>
<tr>
<td></td>
<td>weekofyear</td>
<td>当前时间是一年中的第几周</td>
<td>select weekofyear(‘2020-10-28 12:12:12’);</td>
</tr>
<tr>
<td></td>
<td>dayofmonth</td>
<td>当前时间是一个月中的第几天</td>
<td>select dayofmonth(‘2020-10-28 12:12:12’);</td>
</tr>
<tr>
<td></td>
<td>months_between</td>
<td>两个日期间的月份</td>
<td>select months_between(‘2020-04-01’,’2020-10-28’);</td>
</tr>
<tr>
<td></td>
<td>add_months</td>
<td>日期加减月</td>
<td>select add_months(‘2020-10-28’,-3);</td>
</tr>
<tr>
<td></td>
<td>datediff</td>
<td>两个日期相差的天数</td>
<td>select datediff(‘2020-11-04’,’2020-10-28’);</td>
</tr>
<tr>
<td></td>
<td>date_add</td>
<td>日期加天数</td>
<td>select date_add(‘2020-10-28’,4);</td>
</tr>
<tr>
<td></td>
<td>date_sub</td>
<td>日期减天数</td>
<td>select date_sub(‘2020-10-28’,-4);</td>
</tr>
<tr>
<td></td>
<td>last_day</td>
<td>日期的当月的最后一天</td>
<td>select last_day(‘2020-02-30’);</td>
</tr>
<tr>
<td></td>
<td>date_format</td>
<td>格式化日期</td>
<td>select date_format(‘2020-10-28 12:12:12’,’yyyy/MM/dd HH:mm:ss’);</td>
</tr>
<tr>
<td>常用取整函数</td>
<td>round</td>
<td>四舍五入</td>
<td>select round(3.14);select round(3.54);</td>
</tr>
<tr>
<td></td>
<td>ceil</td>
<td>向上取整</td>
<td>select ceil(3.14);select ceil(3.54);</td>
</tr>
<tr>
<td></td>
<td>floor</td>
<td>向下取整</td>
<td>select floor(3.14);select floor(3.54);</td>
</tr>
<tr>
<td>常用字符串操作函数</td>
<td>upper</td>
<td>转大写</td>
<td>select upper(‘low’);</td>
</tr>
<tr>
<td></td>
<td>lower</td>
<td>转小写</td>
<td>select lower(‘low’);</td>
</tr>
<tr>
<td></td>
<td>length</td>
<td>长度</td>
<td>select length(“atguigu”);</td>
</tr>
<tr>
<td></td>
<td>trim</td>
<td>前后去空格</td>
<td>select trim(“ atguigu “);</td>
</tr>
<tr>
<td></td>
<td>lpad</td>
<td>向左补齐，到指定长度</td>
<td>select lpad(‘atguigu’,9,’g’);</td>
</tr>
<tr>
<td></td>
<td>rpad</td>
<td>向右补齐，到指定长度</td>
<td>select rpad(‘atguigu’,9,’g’);</td>
</tr>
<tr>
<td></td>
<td>regexp_replace</td>
<td>使用正则表达式匹配目标字符串，匹配成功后替换！</td>
<td>SELECT regexp_replace(‘2020/10/25’, ‘/‘, ‘-‘);</td>
</tr>
<tr>
<td>集合操作</td>
<td>size</td>
<td>集合中元素的个数</td>
<td>select size(friends) from person_info;</td>
</tr>
<tr>
<td></td>
<td>map_keys</td>
<td>返回map中的key</td>
<td>select map_keys(children) from person_info;</td>
</tr>
<tr>
<td></td>
<td>map_value</td>
<td>返回map中的value</td>
<td>select map_values(children) from person_info;</td>
</tr>
<tr>
<td></td>
<td>array_contains</td>
<td>判断array中是否包含某个元素</td>
<td>select array_contains(friends,’bingbing’) from person_info;</td>
</tr>
<tr>
<td></td>
<td>sort_array</td>
<td>将array中的元素排序</td>
<td>select sort_array(friends) from person_info;</td>
</tr>
<tr>
<td></td>
<td>grouping_set</td>
<td>多维分析</td>
<td></td>
</tr>
</tbody></table>
<h2 id="8-3-自定义函数"><a href="#8-3-自定义函数" class="headerlink" title="8.3 自定义函数"></a>8.3 自定义函数</h2><ol>
<li>Hive 自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。</li>
<li>当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。</li>
<li>根据用户自定义函数类别分为以下三种：<ul>
<li>UDF（User-Defined-Function）：一进一出</li>
<li>UDAF（User-Defined Aggregation Function）：聚集函数，多进一出，类似于：count/max/min</li>
<li>UDTF（User-Defined Table-Generating Functions）一进多出，如lateral view explode()</li>
</ul>
</li>
<li>官方文档地址<ul>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">https://cwiki.apache.org/confluence/display/Hive/HivePlugins</a></li>
</ul>
</li>
<li>编程步骤：<ol>
<li>继承Hive提供的类<ul>
<li>org.apache.hadoop.hive.ql.udf.generic.GenericUDF  </li>
<li>org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</li>
</ul>
</li>
<li>实现类中的抽象方法</li>
<li>在hive的命令行窗口创建函数 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 添加jar</span></span><br><span class="line"><span class="keyword">add</span> jar linux_jar_path</span><br><span class="line"><span class="comment">-- 创建function</span></span><br><span class="line"><span class="keyword">create</span> [temporary] <span class="keyword">function</span> [dbname.]function_name <span class="keyword">AS</span> class_name;</span><br></pre></td></tr></table></figure></li>
<li>在hive的命令行窗口删除函数 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> [temporary] <span class="keyword">function</span> [if <span class="keyword">exists</span>] [dbname.]function_name;</span><br></pre></td></tr></table></figure>
<h2 id="8-4-自定义UDF函数"><a href="#8-4-自定义UDF函数" class="headerlink" title="8.4 自定义UDF函数"></a>8.4 自定义UDF函数</h2></li>
</ol>
</li>
<li>创建函数类</li>
<li>继承org.apache.hadoop.hive.ql.udf.generic.GenericUDF</li>
<li>重写initialize，evaluate，getDisplayString</li>
<li>执行hive add jar</li>
<li>创建函数</li>
<li>使用函数</li>
<li>示例 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 一进一出</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UDFTest</span> <span class="keyword">extends</span> <span class="title">GenericUDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> objectInspectors</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> UDFArgumentException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ObjectInspector <span class="title">initialize</span><span class="params">(ObjectInspector[] objectInspectors)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (objectInspectors == <span class="keyword">null</span> || objectInspectors.length != <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> UDFArgumentLengthException(<span class="string">&quot;参数数量有误&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        ObjectInspector objectInspector = objectInspectors[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (objectInspector.getCategory() != ObjectInspector.Category.PRIMITIVE) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> UDFArgumentTypeException(<span class="number">0</span>, <span class="string">&quot;参数类型错误&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> PrimitiveObjectInspectorFactory.javaIntObjectInspector;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 函数体的核心逻辑</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> deferredObjects</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> HiveException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">evaluate</span><span class="params">(DeferredObject[] deferredObjects)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (deferredObjects == <span class="keyword">null</span> || deferredObjects.length != <span class="number">1</span> || deferredObjects[<span class="number">0</span>].get() == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> deferredObjects[<span class="number">0</span>].get().toString().length();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 针对当前函数的说明</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> strings</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getDisplayString</span><span class="params">(String[] strings)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;针对当前函数的说明&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">add</span> jar <span class="operator">/</span>home<span class="operator">/</span>atguigu<span class="operator">/</span>hive<span class="operator">/</span>custom_function<span class="operator">/</span>sgg<span class="operator">-</span>hive<span class="operator">-</span>custom<span class="operator">-</span><span class="keyword">function</span><span class="number">-0.0</span><span class="number">.1</span><span class="operator">-</span>SNAPSHOT.jar;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.037</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> temporary <span class="keyword">function</span> str_split <span class="keyword">as</span> <span class="string">&#x27;tech.anzhen.sgg.hive.custom.function.udf.UDTFTest&#x27;</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.028</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> str_split(<span class="string">&#x27;a,d,d,w&#x27;</span>,<span class="string">&#x27;,&#x27;</span>,<span class="string">&#x27;12&#x27;</span>);</span><br><span class="line">Error: Error while compiling statement: FAILED: UDFArgumentLengthException 参数数量有误 (state<span class="operator">=</span><span class="number">42000</span>,code<span class="operator">=</span><span class="number">40000</span>)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> str_split(<span class="string">&#x27;a,d,d,w&#x27;</span>,<span class="string">&#x27;,&#x27;</span>);</span><br><span class="line"><span class="operator">+</span><span class="comment">-------+</span></span><br><span class="line"><span class="operator">|</span> word  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+</span></span><br><span class="line"><span class="operator">|</span> a     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> d     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> d     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> w     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+</span></span><br></pre></td></tr></table></figure>
<h2 id="8-5-自定义UDTF函数"><a href="#8-5-自定义UDTF函数" class="headerlink" title="8.5 自定义UDTF函数"></a>8.5 自定义UDTF函数</h2></li>
<li>创建函数类</li>
<li>继承org.apache.hadoop.hive.ql.udf.generic.GenericUDTF</li>
<li>重写initialize，process，close</li>
<li>执行hive add jar</li>
<li>创建函数</li>
<li>使用函数</li>
<li>示例 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UDTFMultiColumnTest</span> <span class="keyword">extends</span> <span class="title">GenericUDTF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> StructObjectInspector <span class="title">initialize</span><span class="params">(StructObjectInspector argOIs)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">        <span class="comment">//参数数量</span></span><br><span class="line">        List&lt;? extends StructField&gt; allStructFieldRefs = argOIs.getAllStructFieldRefs();</span><br><span class="line">        <span class="keyword">if</span> (allStructFieldRefs == <span class="keyword">null</span> || allStructFieldRefs.size() != <span class="number">3</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> UDFArgumentLengthException(<span class="string">&quot;参数数量有误&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//参数数据类型</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; allStructFieldRefs.size(); i++) &#123;</span><br><span class="line">            StructField structField = allStructFieldRefs.get(i);</span><br><span class="line">            <span class="keyword">if</span> (structField.getFieldObjectInspector().getCategory() != ObjectInspector.Category.PRIMITIVE) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> UDFArgumentTypeException(i, <span class="string">&quot;参数类型有误&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        ArrayList&lt;String&gt; fieldNames = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        fieldNames.add(<span class="string">&quot;word1&quot;</span>);</span><br><span class="line">        fieldNames.add(<span class="string">&quot;word2&quot;</span>);</span><br><span class="line">        ArrayList&lt;ObjectInspector&gt; fieldOIs = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line">        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames,</span><br><span class="line">                fieldOIs);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Object[] args)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">        String str = args[<span class="number">0</span>].toString();</span><br><span class="line">        String split1 = args[<span class="number">1</span>].toString();</span><br><span class="line">        String split2 = args[<span class="number">2</span>].toString();</span><br><span class="line">        String[] split1Array = str.split(split1);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; split1Array.length; i++) &#123;</span><br><span class="line">            String split2Str = split1Array[i];</span><br><span class="line">            String[] split2Array = split2Str.split(split2);</span><br><span class="line">            forward(split2Array);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 收尾</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> HiveException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">add</span> jar <span class="operator">/</span>home<span class="operator">/</span>atguigu<span class="operator">/</span>hive<span class="operator">/</span>custom_function<span class="operator">/</span>sgg<span class="operator">-</span>hive<span class="operator">-</span>custom<span class="operator">-</span><span class="keyword">function</span><span class="number">-0.0</span><span class="number">.1</span><span class="operator">-</span>SNAPSHOT.jar;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.037</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> temporary <span class="keyword">function</span> str_splits <span class="keyword">as</span> <span class="string">&#x27;tech.anzhen.sgg.hive.custom.function.udf.UDTFMultiColumnTest&#x27;</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.037</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> str_splits(<span class="string">&#x27;a,d,d,w&#x27;</span>,<span class="string">&#x27;,&#x27;</span>);</span><br><span class="line">Error: Error while compiling statement: FAILED: UDFArgumentLengthException 参数数量有误 (state<span class="operator">=</span><span class="number">42000</span>,code<span class="operator">=</span><span class="number">40000</span>)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> str_splits(<span class="string">&#x27;a-1,b-2,c-3&#x27;</span>,<span class="string">&#x27;,&#x27;</span>,<span class="string">&#x27;-&#x27;</span>);</span><br><span class="line"><span class="operator">+</span><span class="comment">--------+--------+</span></span><br><span class="line"><span class="operator">|</span> word1  <span class="operator">|</span> word2  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------+--------+</span></span><br><span class="line"><span class="operator">|</span> a      <span class="operator">|</span> <span class="number">1</span>      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> b      <span class="operator">|</span> <span class="number">2</span>      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> c      <span class="operator">|</span> <span class="number">3</span>      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------+--------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> selected (<span class="number">0.062</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="九、压缩和存储"><a href="#九、压缩和存储" class="headerlink" title="九、压缩和存储"></a>九、压缩和存储</h1><h2 id="9-1-Hadoop压缩配置"><a href="#9-1-Hadoop压缩配置" class="headerlink" title="9.1 Hadoop压缩配置"></a>9.1 Hadoop压缩配置</h2><h3 id="9-1-1-MR支持的压缩编码"><a href="#9-1-1-MR支持的压缩编码" class="headerlink" title="9.1.1 MR支持的压缩编码"></a>9.1.1 MR支持的压缩编码</h3><ul>
<li><p>MR支持的压缩编码</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
</tr>
<tr>
<td>Gzip</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
</tr>
<tr>
<td>bzip2</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
</tr>
<tr>
<td>LZO</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
</tr>
<tr>
<td>Snappy</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
</tr>
</tbody></table>
</li>
<li><p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示：</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码/解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
</li>
<li><p>压缩性能的比较：</p>
<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB/s</td>
<td>74.6MB/s</td>
</tr>
</tbody></table>
<blockquote>
<p><a href="http://google.github.io/snappy/">http://google.github.io/snappy/</a><br>On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.</p>
</blockquote>
</li>
</ul>
<h3 id="9-1-2-压缩参数配置"><a href="#9-1-2-压缩参数配置" class="headerlink" title="9.1.2 压缩参数配置"></a>9.1.2 压缩参数配置</h3><ul>
<li>要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）：<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody><tr>
<td>io.compression.codecs（在core-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec,</td>
<td></td>
<td></td>
</tr>
<tr>
<td>org.apache.hadoop.io.compress.Lz4Codec</td>
<td>输入压缩</td>
<td>Hadoop使用文件扩展名判断是否支持某种编解码器</td>
<td></td>
</tr>
<tr>
<td>mapreduce.map.output.compress</td>
<td>false</td>
<td>mapper输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.map.output.compress.codec</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>mapper输出</td>
<td>使用LZO、LZ4或snappy编解码器在此阶段压缩数据</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress</td>
<td>false</td>
<td>reducer输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.codec</td>
<td>org.apache.hadoop.io.compress. DefaultCodec</td>
<td>reducer输出</td>
<td>使用标准工具或者编解码器，如gzip和bzip2</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.type</td>
<td>RECORD</td>
<td>reducer输出</td>
<td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td>
</tr>
</tbody></table>
</li>
</ul>
<h2 id="9-2-开启Map输出阶段压缩"><a href="#9-2-开启Map输出阶段压缩" class="headerlink" title="9.2 开启Map输出阶段压缩"></a>9.2 开启Map输出阶段压缩</h2><p>开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下：</p>
<ol>
<li>开启hive中间传输数据压缩功能 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">set</span> hive.exec.compress.intermediate<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>开启mapreduce中map输出压缩功能 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">set</span> mapreduce.map.output.compress<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>设置mapreduce中map输出数据的压缩方式 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">set</span> mapreduce.map.output.compress.codec <span class="operator">=</span> org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure></li>
<li>执行查询语句 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(ename) name <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="9-3-开启Reduce输出阶段压缩"><a href="#9-3-开启Reduce输出阶段压缩" class="headerlink" title="9.3 开启Reduce输出阶段压缩"></a>9.3 开启Reduce输出阶段压缩</h2><p>当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。</p>
<ol>
<li>开启hive最终输出数据压缩功能 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">set</span> hive.exec.compress.output<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>开启mapreduce最终输出数据压缩 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>设置mapreduce最终数据输出压缩方式 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.codec <span class="operator">=</span> org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure></li>
<li>设置mapreduce最终数据输出压缩为块压缩 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.type<span class="operator">=</span>BLOCK;</span><br></pre></td></tr></table></figure></li>
<li>测试一下输出结果是否是压缩文件 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/opt/module/hive-3.1.2/datas/distribute-result&#x27;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp distribute <span class="keyword">by</span> deptno sort <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="9-4-文件存储格式"><a href="#9-4-文件存储格式" class="headerlink" title="9.4 文件存储格式"></a>9.4 文件存储格式</h2><ul>
<li>建表语法中以下规则就是确定Hive的存储格式  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[STORED <span class="keyword">AS</span> file_format] </span><br></pre></td></tr></table></figure></li>
<li>Hive中的存储格式<ul>
<li>textfile   (默认格式) — 行存</li>
<li>SEQUENCEFILE  二进制的序列文件 — 行存</li>
<li>ORC (Hive中最常用的一种存储格式) — 列存</li>
<li>PARQUET (和ORC接近的一种存储格式，hive中支持比较早的一种存储格式)– 列存</li>
</ul>
</li>
</ul>
<h3 id="9-4-1-列式存储和行式存储"><a href="#9-4-1-列式存储和行式存储" class="headerlink" title="9.4.1 列式存储和行式存储"></a>9.4.1 列式存储和行式存储</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16355110552074.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br>如图所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。</p>
<ol>
<li>行存储的特点: 查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</li>
<li>列存储的特点: 因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</li>
</ol>
<ul>
<li>TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；</li>
<li>ORC和PARQUET是基于列式存储的。</li>
</ul>
<h3 id="9-4-2-TextFile格式"><a href="#9-4-2-TextFile格式" class="headerlink" title="9.4.2 TextFile格式"></a>9.4.2 TextFile格式</h3><p>默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作</p>
<h3 id="9-4-3-Orc格式"><a href="#9-4-3-Orc格式" class="headerlink" title="9.4.3 Orc格式"></a>9.4.3 Orc格式</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16355111657327.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<p>Orc (Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式。<br>如图所示可以看到每个Orc文件由1个或多个stripe组成，每个stripe一般为HDFS的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，对应到Parquet中的row group的概念。每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer：</p>
<ol>
<li>Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data中的offset。</li>
<li>Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。</li>
<li>Stripe Footer：存的是各个Stream的类型，长度等信息。</li>
<li>每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。</li>
<li>以json方式查看orc文件 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive --orcfiledump -j -p /user/hive/warehouse/dim.db/dim_province/000000_0</span><br></pre></td></tr></table></figure>
<ul>
<li>文件示例：<a href="media/16346959668071/log_orc.json">log_orc.json</a><br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16356457996704.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><ul>
<li>schema: 为每一个字段做了编号，从1开始，编号为0的columnId中描述了整个表的字段定义。<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16356459007816.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>stripeStatistics:是ORC文件中所有stripes的统计信息，其中有每个stripe中每个字段的min/max值，是否有空值等等<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16356459954433.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>fileStatistics:整个文件中每个字段的统计信息，该表只有一个文件，也只有一个stripe<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16356461542820.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>stripes:这里列出了所有stripes的元数据信息，包括index data, row data和stripe footer。<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16356462325881.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="9-4-4-Parquet格式"><a href="#9-4-4-Parquet格式" class="headerlink" title="9.4.4 Parquet格式"></a>9.4.4 Parquet格式</h3><p>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。</p>
<ol>
<li>行组(Row Group)：每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，类似于orc的stripe的概念。</li>
<li>列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。</li>
<li>页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。</li>
<li>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式。</li>
</ol>
<h3 id="9-4-5-主流文件存储格式对比实验"><a href="#9-4-5-主流文件存储格式对比实验" class="headerlink" title="9.4.5 主流文件存储格式对比实验"></a>9.4.5 主流文件存储格式对比实验</h3><p>数据文件 <a href="media/16346959668071/log.data">log.data</a></p>
<ol>
<li>textfile<ol>
<li>建表导数据 存储数据格式为TEXTFILE <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_text</span><br><span class="line">(</span><br><span class="line">    track_time  string,</span><br><span class="line">    url         string,</span><br><span class="line">    session_id  string,</span><br><span class="line">    referer     string,</span><br><span class="line">    ip          string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id     string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> textfile;</span><br><span class="line"></span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/hive-3.1.2/datas/log.data&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> log_text ;</span><br></pre></td></tr></table></figure></li>
<li>查看占用空间占用 18.1 M <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>log_text;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">1</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup     18.1 M 2021-10-30 10:34 /user/hive/warehouse/mock_data.db/log_text/log.data |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.008</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>orc<ol>
<li>建表导数据，指定orc格式 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc (</span><br><span class="line">    track_time  string,</span><br><span class="line">    url         string,</span><br><span class="line">    session_id  string,</span><br><span class="line">    referer     string,</span><br><span class="line">    ip          string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id     string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> orc</span><br><span class="line">tblproperties (&quot;orc.compress&quot; <span class="operator">=</span> &quot;NONE&quot;); <span class="comment">-- 设置orc存储不使用压缩</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> log_orc <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log_text ;</span><br></pre></td></tr></table></figure></li>
<li>查看文件大小 7.7 M <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>log_orc;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">1</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup      7.7 M 2021-10-30 10:37 /user/hive/warehouse/mock_data.db/log_orc/000000_0 |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.005</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>parquet<ol>
<li>建表导数据，指定parquet格式 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_parquet (</span><br><span class="line">    track_time  string,</span><br><span class="line">    url         string,</span><br><span class="line">    session_id  string,</span><br><span class="line">    referer     string,</span><br><span class="line">    ip          string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id     string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> parquet;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> log_parquet <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log_text ;</span><br></pre></td></tr></table></figure></li>
<li>查看文件大小 13.1 M <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>log_parquet;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">1</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup     13.1 M 2021-10-30 10:49 /user/hive/warehouse/mock_data.db/log_parquet/000000_0 |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.009</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>存储文件大小的对比总结：<br> ORC &gt;  Parquet &gt;  textFile</li>
<li>存储文件的查询速度测试<br> ORC &gt;  Parquet &gt;  textFile <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">from</span> person <span class="keyword">where</span> id<span class="operator">=</span><span class="number">4</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> _c0  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">21.249</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">set</span> hive.optimize.index.filter<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.005</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">from</span> person_orc <span class="keyword">where</span> id<span class="operator">=</span><span class="number">4</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> _c0  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">17.151</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">from</span> person_parquet <span class="keyword">where</span> id<span class="operator">=</span><span class="number">4</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> _c0  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">18.485</span> seconds)</span><br></pre></td></tr></table></figure>
<h2 id="9-5-存储和压缩结合"><a href="#9-5-存储和压缩结合" class="headerlink" title="9.5 存储和压缩结合"></a>9.5 存储和压缩结合</h2><h3 id="9-5-1-测试存储和压缩"><a href="#9-5-1-测试存储和压缩" class="headerlink" title="9.5.1 测试存储和压缩"></a>9.5.1 测试存储和压缩</h3></li>
</ol>
<ul>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC">官方文档</a></li>
<li>ORC存储方式的压缩：<table>
<thead>
<tr>
<th>Key</th>
<th>Default</th>
<th>Notes</th>
</tr>
</thead>
<tbody><tr>
<td>orc.compress</td>
<td>ZLIB</td>
<td>high level compression (one of NONE, ZLIB, SNAPPY)</td>
</tr>
<tr>
<td>orc.compress.size</td>
<td>262,144</td>
<td>number of bytes in each compression chunk</td>
</tr>
<tr>
<td>orc.stripe.size</td>
<td>268,435,456</td>
<td>number of bytes in each stripe</td>
</tr>
<tr>
<td>orc.row.index.stride</td>
<td>10,000</td>
<td>number of rows between index entries (must be &gt;= 1000)</td>
</tr>
<tr>
<td>orc.create.index</td>
<td>true</td>
<td>whether to create row indexes</td>
</tr>
<tr>
<td>orc.bloom.filter.columns</td>
<td>“”</td>
<td>comma separated list of column names for which bloom filter should be created</td>
</tr>
<tr>
<td>orc.bloom.filter.fpp</td>
<td>0.05</td>
<td>false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)</td>
</tr>
</tbody></table>
</li>
<li>注意：所有关于ORCFile的参数都是在HQL语句的TBLPROPERTIES字段里面出现</li>
<li>测试<ol>
<li>创建ZLIB压缩的ORC存储方式<ul>
<li>建表导数据  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_zlib (</span><br><span class="line">    track_time  string,</span><br><span class="line">    url         string,</span><br><span class="line">    session_id  string,</span><br><span class="line">    referer     string,</span><br><span class="line">    ip          string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id     string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> orc</span><br><span class="line">tblproperties (&quot;orc.compress&quot; <span class="operator">=</span> &quot;ZLIB&quot;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> log_orc_zlib <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log_text;</span><br></pre></td></tr></table></figure></li>
<li>查看文件大小 2.8 M  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>log_orc_zlib;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">1</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup      2.8 M 2021-10-30 10:50 /user/hive/warehouse/mock_data.db/log_orc_zlib/000000_0 |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.011</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>创建一个SNAPPY压缩的ORC存储方式<ul>
<li>建表导数据  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_snappy (</span><br><span class="line">    track_time  string,</span><br><span class="line">    url         string,</span><br><span class="line">    session_id  string,</span><br><span class="line">    referer     string,</span><br><span class="line">    ip          string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id     string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> orc</span><br><span class="line">tblproperties (&quot;orc.compress&quot; <span class="operator">=</span> &quot;SNAPPY&quot;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> log_orc_snappy <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log_text;</span><br></pre></td></tr></table></figure></li>
<li>查看文件大小 3.7 M  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>log_orc_snappy;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">1</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup      3.7 M 2021-10-30 10:51 /user/hive/warehouse/mock_data.db/log_orc_snappy/000000_0 |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.009</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>创建一个SNAPPY压缩的parquet存储方式<ul>
<li>建表导数据  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_parquet_snappy (</span><br><span class="line">    track_time  string,</span><br><span class="line">    url         string,</span><br><span class="line">    session_id  string,</span><br><span class="line">    referer     string,</span><br><span class="line">    ip          string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id     string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> parquet</span><br><span class="line">tblproperties (&quot;parquet.compression&quot; <span class="operator">=</span> &quot;SNAPPY&quot;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> log_parquet_snappy <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log_text;</span><br></pre></td></tr></table></figure></li>
<li>查看文件大小 6.4 M  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>log_parquet_snappy;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">1</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup      6.4 M 2021-10-31 12:29 /user/hive/warehouse/mock_data.db/log_parquet_snappy/000000_0 |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.078</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>存储方式和压缩总结<ul>
<li>在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo。<h1 id="十、性能优化"><a href="#十、性能优化" class="headerlink" title="十、性能优化"></a>十、性能优化</h1><h2 id="10-1-执行计划（Explain）"><a href="#10-1-执行计划（Explain）" class="headerlink" title="10.1 执行计划（Explain）"></a>10.1 执行计划（Explain）</h2></li>
</ul>
</li>
</ol>
</li>
</ul>
<ol>
<li>概念：获取SQL执行的规划，主要用于分析SQL需要的优化依据信息</li>
<li>基本语法 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">EXPLAIN [EXTENDED <span class="operator">|</span> DEPENDENCY <span class="operator">|</span> <span class="keyword">AUTHORIZATION</span>] query</span><br></pre></td></tr></table></figure></li>
<li>案例实操<ul>
<li>不生成MR任务(关注Fetch Operator)  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> explain <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                      Explain                       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> STAGE DEPENDENCIES:                                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage<span class="number">-0</span> <span class="keyword">is</span> a root stage                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> STAGE PLANS:                                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage: Stage<span class="number">-0</span>                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>     <span class="keyword">Fetch</span> Operator                                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       limit: <span class="number">-1</span>                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Processor Tree:                              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>         TableScan                                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           alias: emp                               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6570</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           <span class="keyword">Select</span> Operator                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             expressions: emp_no (type: <span class="type">int</span>), emp_name (type: string), job (type: string), mgr (type: <span class="type">int</span>), hire_date (type: string), sal (type: <span class="keyword">double</span>), comm (type: <span class="keyword">double</span>), dept_no (type: <span class="type">int</span>) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6570</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             ListSink                               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">17</span> <span class="keyword">rows</span> selected (<span class="number">0.076</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>生成MR任务的（关注Map Operator Tree）  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> explain <span class="keyword">select</span> dept_no, <span class="built_in">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> dept_no;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                      Explain                       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> STAGE DEPENDENCIES:                                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage<span class="number">-1</span> <span class="keyword">is</span> a root stage                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage<span class="number">-0</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-1</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> STAGE PLANS:                                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage: Stage<span class="number">-1</span>                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>     Map Reduce                                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Map Operator Tree:                           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           TableScan                                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             alias: emp                             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6570</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             <span class="keyword">Select</span> Operator                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               expressions: sal (type: <span class="keyword">double</span>), dept_no (type: <span class="type">int</span>) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               outputColumnNames: sal, dept_no      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6570</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               <span class="keyword">Group</span> <span class="keyword">By</span> Operator                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 aggregations: <span class="built_in">sum</span>(sal), <span class="built_in">count</span>(sal) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 keys: dept_no (type: <span class="type">int</span>)          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 mode: hash                         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 outputColumnNames: _col0, _col1, _col2 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6570</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 Reduce Output Operator             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   key expressions: _col0 (type: <span class="type">int</span>) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   sort <span class="keyword">order</span>: <span class="operator">+</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   Map<span class="operator">-</span>reduce <span class="keyword">partition</span> columns: _col0 (type: <span class="type">int</span>) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6570</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   <span class="keyword">value</span> expressions: _col1 (type: <span class="keyword">double</span>), _col2 (type: <span class="type">bigint</span>) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Execution mode: vectorized                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Reduce Operator Tree:                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>         <span class="keyword">Group</span> <span class="keyword">By</span> Operator                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           aggregations: <span class="built_in">sum</span>(VALUE._col0), <span class="built_in">count</span>(VALUE._col1) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           keys: KEY._col0 (type: <span class="type">int</span>)              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           mode: mergepartial                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           outputColumnNames: _col0, _col1, _col2   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6570</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           <span class="keyword">Select</span> Operator                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             expressions: _col0 (type: <span class="type">int</span>), (_col1 <span class="operator">/</span> _col2) (type: <span class="keyword">double</span>) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             outputColumnNames: _col0, _col1        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6570</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             File Output Operator                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               compressed: <span class="literal">false</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6570</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               <span class="keyword">table</span>:                               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   input format: org.apache.hadoop.mapred.SequenceFileInputFormat <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage: Stage<span class="number">-0</span>                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>     <span class="keyword">Fetch</span> Operator                                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       limit: <span class="number">-1</span>                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Processor Tree:                              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>         ListSink                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">53</span> <span class="keyword">rows</span> selected (<span class="number">0.063</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>查看详细执行计划  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain extended <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain extended <span class="keyword">select</span> deptno, <span class="built_in">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure>
<h2 id="10-2-Fetch抓取"><a href="#10-2-Fetch抓取" class="headerlink" title="10.2 Fetch抓取"></a>10.2 Fetch抓取</h2></li>
</ul>
</li>
</ol>
<ul>
<li>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。</li>
<li>例如：SELECT * FROM employees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。</li>
<li>在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.fetch.task.conversion<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>more<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      Expects one of [none, minimal, more].</span><br><span class="line">      Some select queries can be converted to single FETCH task minimizing latency.</span><br><span class="line">      Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incurs RS), lateral views and joins.</span><br><span class="line">      0. none : disable hive.fetch.task.conversion</span><br><span class="line">      1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only</span><br><span class="line">      2. more  : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<ol>
<li>案例实操：<ol>
<li>把hive.fetch.task.conversion设置成none，然后执行查询语句，都会执行mapreduce程序。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; <span class="built_in">set</span> hive.fetch.task.conversion=none;</span><br><span class="line">hive (default)&gt; select * from emp;</span><br><span class="line">hive (default)&gt; select ename from emp;</span><br><span class="line">hive (default)&gt; select ename from emp <span class="built_in">limit</span> 3;</span><br></pre></td></tr></table></figure></li>
<li>把hive.fetch.task.conversion设置成more，然后执行查询语句，如下查询方式都不会执行mapreduce程序。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; <span class="built_in">set</span> hive.fetch.task.conversion=more;</span><br><span class="line">hive (default)&gt; select * from emp;</span><br><span class="line">hive (default)&gt; select ename from emp;</span><br><span class="line">hive (default)&gt; select ename from emp <span class="built_in">limit</span> 3;</span><br></pre></td></tr></table></figure>
<h2 id="10-3-本地模式"><a href="#10-3-本地模式" class="headerlink" title="10.3 本地模式"></a>10.3 本地模式</h2></li>
</ol>
</li>
</ol>
<ul>
<li>大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。</li>
<li>用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- //开启本地mr</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 设置local mr的最大输入数据量，当输入数据量小于这个值时采用local  mr的方式，默认为134217728，即128M</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.inputbytes.max<span class="operator">=</span><span class="number">134217728</span>;</span><br><span class="line"><span class="comment">-- 设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.input.files.max<span class="operator">=</span><span class="number">4</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
<ol>
<li>开启本地模式，并执行查询语句 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> hive.exec.mode.local.auto<span class="operator">=</span><span class="literal">true</span>; </span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">1.328</span> seconds, Fetched: <span class="number">14</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure></li>
<li>关闭本地模式，并执行查询语句 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> hive.exec.mode.local.auto<span class="operator">=</span><span class="literal">false</span>; </span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">20.09</span> seconds, Fetched: <span class="number">14</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="10-4-表的优化"><a href="#10-4-表的优化" class="headerlink" title="10.4 表的优化"></a>10.4 表的优化</h2><h3 id="10-4-1-小表大表Join-MapJoin-内连接场景"><a href="#10-4-1-小表大表Join-MapJoin-内连接场景" class="headerlink" title="10.4.1 小表大表Join(MapJoin)内连接场景"></a>10.4.1 小表大表Join(MapJoin)内连接场景</h3><ul>
<li>将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join让小的维度表（1000条以下的记录条数）先进内存。在map端完成join。<blockquote>
<p>实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。</p>
</blockquote>
</li>
<li>案例实操<ol>
<li>需求测试大表JOIN小表和小表JOIN大表的效率</li>
<li>开启MapJoin参数设置<ol>
<li>设置自动选择Mapjoin <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join <span class="operator">=</span> <span class="literal">true</span>; 默认为<span class="literal">true</span></span><br></pre></td></tr></table></figure></li>
<li>大表小表的阈值设置（默认25M以下认为是小表）： <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize <span class="operator">=</span> <span class="number">25000000</span>;</span><br></pre></td></tr></table></figure></li>
<li>MapJoin工作机制<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16355531696295.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>建大表、小表和JOIN后表的语句 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"> <span class="comment">-- 创建大表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable (</span><br><span class="line">     id        <span class="type">bigint</span>,</span><br><span class="line">     t         <span class="type">bigint</span>,</span><br><span class="line">     uid       string,</span><br><span class="line">     keyword   string,</span><br><span class="line">     url_rank  <span class="type">int</span>,</span><br><span class="line">     click_num <span class="type">int</span>,</span><br><span class="line">     click_url string</span><br><span class="line"> ) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"> <span class="comment">-- 创建小表</span></span><br><span class="line"> <span class="keyword">create</span> <span class="keyword">table</span> smalltable (</span><br><span class="line">     id        <span class="type">bigint</span>,</span><br><span class="line">     t         <span class="type">bigint</span>,</span><br><span class="line">     uid       string,</span><br><span class="line">     keyword   string,</span><br><span class="line">     url_rank  <span class="type">int</span>,</span><br><span class="line">     click_num <span class="type">int</span>,</span><br><span class="line">     click_url string</span><br><span class="line"> ) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"> </span><br><span class="line"> <span class="comment">-- 创建join后表的语句</span></span><br><span class="line"> <span class="keyword">create</span> <span class="keyword">table</span> jointable (</span><br><span class="line">     id        <span class="type">bigint</span>,</span><br><span class="line">     t         <span class="type">bigint</span>,</span><br><span class="line">     uid       string,</span><br><span class="line">     keyword   string,</span><br><span class="line">     url_rank  <span class="type">int</span>,</span><br><span class="line">     click_num <span class="type">int</span>,</span><br><span class="line">     click_url string</span><br><span class="line"> ) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>分别向大表和小表中导入数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/hive-3.1.2/datas/bigtable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> bigtable;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span>load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/hive-3.1.2/datas/smalltable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> smalltable;</span><br></pre></td></tr></table></figure></li>
<li>小表JOIN大表语句 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">       b.id, </span><br><span class="line">       b.t, </span><br><span class="line">       b.uid, </span><br><span class="line">       b.keyword, </span><br><span class="line">       b.url_rank, </span><br><span class="line">       b.click_num, </span><br><span class="line">       b.click_url</span><br><span class="line"><span class="keyword">from</span> smalltable s</span><br><span class="line">         <span class="keyword">join</span> bigtable b <span class="keyword">on</span> b.id <span class="operator">=</span> s.id;</span><br></pre></td></tr></table></figure></li>
<li>执行大表JOIN小表语句 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">       b.id, </span><br><span class="line">       b.t, </span><br><span class="line">       b.uid, </span><br><span class="line">       b.keyword, </span><br><span class="line">       b.url_rank, </span><br><span class="line">       b.click_num, </span><br><span class="line">       b.click_url</span><br><span class="line"><span class="keyword">from</span> bigtable  b</span><br><span class="line">    <span class="keyword">join</span> smalltable  s <span class="keyword">on</span> s.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure>
<h3 id="10-4-2-大表Join大表"><a href="#10-4-2-大表Join大表" class="headerlink" title="10.4.2 大表Join大表"></a>10.4.2 大表Join大表</h3></li>
</ol>
</li>
</ol>
</li>
</ul>
<ol>
<li>大表和大表join时，MR一定执行ReduceJoin操作，需要注意优化的点就是ReduceJoin带来的问题！！！<h4 id="10-4-2-1-空Key过滤（针对数据倾斜的一种解决方案）"><a href="#10-4-2-1-空Key过滤（针对数据倾斜的一种解决方案）" class="headerlink" title="10.4.2.1 空Key过滤（针对数据倾斜的一种解决方案）"></a>10.4.2.1 空Key过滤（针对数据倾斜的一种解决方案）</h4></li>
<li>当大表和大表Join走MR的时候，相同Key对应的values会进入一个Reduce，如果出现大量相同搞得key，会导致数据倾斜</li>
<li>案例实操<ol>
<li>创建原始数据表、空id表、合并后数据表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建空id表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> nullidtable (</span><br><span class="line">    id        <span class="type">bigint</span>,</span><br><span class="line">    t         <span class="type">bigint</span>,</span><br><span class="line">    uid       string,</span><br><span class="line">    keyword   string,</span><br><span class="line">    url_rank  <span class="type">int</span>,</span><br><span class="line">    click_num <span class="type">int</span>,</span><br><span class="line">    click_url string</span><br><span class="line">) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>分别加载原始数据和空id数据到对应表中 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/hive-3.1.2/datas/nullid&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> nullidtable;</span><br></pre></td></tr></table></figure></li>
<li>测试不过滤空id <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable <span class="keyword">select</span> n.<span class="operator">*</span> <span class="keyword">from</span> nullidtable n <span class="keyword">left</span> <span class="keyword">join</span> bigtable o <span class="keyword">on</span> n.id <span class="operator">=</span> o.id;</span><br></pre></td></tr></table></figure></li>
<li>测试过滤空id <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable <span class="keyword">select</span> n.<span class="operator">*</span> <span class="keyword">from</span> (<span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> nullidtable <span class="keyword">where</span> id <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">null</span> ) n  <span class="keyword">left</span> <span class="keyword">join</span> bigtable o <span class="keyword">on</span> n.id <span class="operator">=</span> o.id;</span><br></pre></td></tr></table></figure>
<h4 id="10-4-2-1-SMB-Sort-Merge-Bucket-join"><a href="#10-4-2-1-SMB-Sort-Merge-Bucket-join" class="headerlink" title="10.4.2.1 SMB(Sort Merge Bucket join)"></a>10.4.2.1 SMB(Sort Merge Bucket join)</h4></li>
</ol>
</li>
<li>数据量超级大的时候可能导致Join时间过长，或者直接导致Job失败</li>
<li>原理：从建表的时候就将其创建为分桶表，两表的关联字段作为分桶字段，后续做Join的时候就对桶进行Join，避免了每一条数据进行Join</li>
<li>案例实操<ul>
<li>创建第二张大表  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable2 (</span><br><span class="line">    id        <span class="type">bigint</span>,</span><br><span class="line">    t         <span class="type">bigint</span>,</span><br><span class="line">    uid       string,</span><br><span class="line">    keyword   string,</span><br><span class="line">    url_rank  <span class="type">int</span>,</span><br><span class="line">    click_num <span class="type">int</span>,</span><br><span class="line">    click_url string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">    </span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/table_optimize/bigtable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> bigtable2;</span><br></pre></td></tr></table></figure></li>
<li>测试大表直接JOIN 耗时51.176  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id,</span><br><span class="line">       b.t,</span><br><span class="line">       b.uid,</span><br><span class="line">       b.keyword,</span><br><span class="line">       b.url_rank,</span><br><span class="line">       b.click_num,</span><br><span class="line">       b.click_url</span><br><span class="line"><span class="keyword">from</span> bigtable s</span><br><span class="line"><span class="keyword">join</span> bigtable2 b <span class="keyword">on</span> b.id <span class="operator">=</span> s.id;</span><br><span class="line"></span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">51.176</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>创建分桶表1, 创建分桶表2, 桶的个数不要超过可用CPU的核数  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable_buck1 (</span><br><span class="line">    id        <span class="type">bigint</span>,</span><br><span class="line">    t         <span class="type">bigint</span>,</span><br><span class="line">    uid       string,</span><br><span class="line">    keyword   string,</span><br><span class="line">    url_rank  <span class="type">int</span>,</span><br><span class="line">    click_num <span class="type">int</span>,</span><br><span class="line">    click_url string</span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span> (id)</span><br><span class="line">sorted <span class="keyword">by</span> (id)</span><br><span class="line"><span class="keyword">into</span> <span class="number">8</span> buckets</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable_buck2 (</span><br><span class="line">    id        <span class="type">bigint</span>,</span><br><span class="line">    t         <span class="type">bigint</span>,</span><br><span class="line">    uid       string,</span><br><span class="line">    keyword   string,</span><br><span class="line">    url_rank  <span class="type">int</span>,</span><br><span class="line">    click_num <span class="type">int</span>,</span><br><span class="line">    click_url string</span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span> (id)</span><br><span class="line">sorted <span class="keyword">by</span> (id)</span><br><span class="line"><span class="keyword">into</span> <span class="number">8</span> buckets</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"></span><br><span class="line">load data inpath <span class="string">&#x27;/bigtable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> bigtable_buck1;</span><br><span class="line">load data inpath <span class="string">&#x27;/bigtable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> bigtable_buck2;</span><br></pre></td></tr></table></figure></li>
<li>设置参数  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;</span><br></pre></td></tr></table></figure></li>
<li>测试 耗时23.272 seconds  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id,</span><br><span class="line">       b.t,</span><br><span class="line">       b.uid,</span><br><span class="line">       b.keyword,</span><br><span class="line">       b.url_rank,</span><br><span class="line">       b.click_num,</span><br><span class="line">       b.click_url</span><br><span class="line"><span class="keyword">from</span> bigtable_buck1 s</span><br><span class="line"><span class="keyword">join</span> bigtable_buck2 b <span class="keyword">on</span> b.id <span class="operator">=</span> s.id;</span><br><span class="line"></span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">23.272</span> seconds)</span><br></pre></td></tr></table></figure>
<h3 id="10-4-3-Group-By"><a href="#10-4-3-Group-By" class="headerlink" title="10.4.3 Group By"></a>10.4.3 Group By</h3></li>
</ul>
</li>
</ol>
<ul>
<li>分组聚合操作时，如果一个组内出现大量的数据，都会交给一个reduce处理，很可能会造成数据倾斜<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16355799864007.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>解决思路<ol>
<li>是否在Map端进行聚合，默认为True<br> <code>set hive.map.aggr = true</code></li>
<li>在Map端进行聚合操作的条目数目<br><code>set   = 100000</code> </li>
<li>有数据倾斜的时候进行负载均衡（默认是false）<br> <code>set hive.groupby.skewindata = true</code><br> <font color ='red' >当选项设定为 true，生成的查询计划会有两个MR Job</font>。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是<font color ='red' >相同的Group By Key有可能被分发到不同的Reduce中</font>，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。</li>
</ol>
<ul>
<li>思路：会多执行一个Job做数据的负载均衡 解决数据倾斜！</li>
</ul>
</li>
<li>测试<ul>
<li>直接执行  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">       substr(birth, <span class="number">0</span>, <span class="number">7</span>), </span><br><span class="line">       <span class="built_in">count</span>(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">from</span> person</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> substr(birth, <span class="number">0</span>, <span class="number">7</span>)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> <span class="number">2</span> <span class="keyword">desc</span>;</span><br><span class="line"></span><br><span class="line">INFO  : MapReduce Jobs Launched:</span><br><span class="line">INFO  : Stage<span class="operator">-</span>Stage<span class="number">-1</span>: Map: <span class="number">1</span>  Reduce: <span class="number">1</span>   Cumulative CPU: <span class="number">2.12</span> sec   HDFS Read: <span class="number">14687</span> HDFS Write: <span class="number">138</span> SUCCESS</span><br><span class="line">INFO  : Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">2</span> seconds <span class="number">120</span> msec</span><br></pre></td></tr></table></figure></li>
<li>修改参数执行   <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">       substr(birth, <span class="number">0</span>, <span class="number">7</span>), </span><br><span class="line">       <span class="built_in">count</span>(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">from</span> person</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> substr(birth, <span class="number">0</span>, <span class="number">7</span>)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> <span class="number">2</span> <span class="keyword">desc</span>;</span><br><span class="line"></span><br><span class="line">INFO  : MapReduce Jobs Launched:</span><br><span class="line">INFO  : Stage<span class="operator">-</span>Stage<span class="number">-1</span>: Map: <span class="number">1</span>  Reduce: <span class="number">1</span>   Cumulative CPU: <span class="number">2.47</span> sec   HDFS Read: <span class="number">14399</span> HDFS Write: <span class="number">153</span> SUCCESS</span><br><span class="line">INFO  : Stage<span class="operator">-</span>Stage<span class="number">-2</span>: Map: <span class="number">1</span>  Reduce: <span class="number">1</span>   Cumulative CPU: <span class="number">2.08</span> sec   HDFS Read: <span class="number">7574</span> HDFS Write: <span class="number">138</span> SUCCESS</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="10-4-4-Count-Distinct-去重统计"><a href="#10-4-4-Count-Distinct-去重统计" class="headerlink" title="10.4.4 Count(Distinct) 去重统计"></a>10.4.4 Count(Distinct) 去重统计</h3><p>数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换,但是需要注意group by造成的数据倾斜问题.</p>
<ol>
<li>设置5个reduce个数 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces <span class="operator">=</span> <span class="number">5</span>;</span><br></pre></td></tr></table></figure></li>
<li>执行去重id查询 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="keyword">distinct</span> id) <span class="keyword">from</span> bigtable;</span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-1</span>: Map: <span class="number">1</span>  Reduce: <span class="number">1</span>   Cumulative CPU: <span class="number">7.12</span> sec   HDFS Read: <span class="number">120741990</span> HDFS Write: <span class="number">7</span> SUCCESS</span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">7</span> seconds <span class="number">120</span> msec</span><br><span class="line">OK</span><br><span class="line">c0</span><br><span class="line"><span class="number">100001</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">23.607</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure></li>
<li>采用GROUP by去重id <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(id) <span class="keyword">from</span> (<span class="keyword">select</span> id <span class="keyword">from</span> bigtable <span class="keyword">group</span> <span class="keyword">by</span> id) a;</span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-1</span>: Map: <span class="number">1</span>  Reduce: <span class="number">5</span>   Cumulative CPU: <span class="number">17.53</span> sec   HDFS Read: <span class="number">120752703</span> HDFS Write: <span class="number">580</span> SUCCESS</span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-2</span>: Map: <span class="number">1</span>  Reduce: <span class="number">1</span>   Cumulative CPU: <span class="number">4.29</span> sec2   HDFS Read: <span class="number">9409</span> HDFS Write: <span class="number">7</span> SUCCESS</span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">21</span> seconds <span class="number">820</span> msec</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line"><span class="number">100001</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">50.795</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure></li>
<li>虽然会多用一个Job来完成，但在数据量大的情况下，可以避免Reduce因为数据量大导致失败，所以是值得的。</li>
</ol>
<h3 id="10-4-5-笛卡尔积"><a href="#10-4-5-笛卡尔积" class="headerlink" title="10.4.5 笛卡尔积"></a>10.4.5 笛卡尔积</h3><p>尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。</p>
<h3 id="10-4-6-行列过滤"><a href="#10-4-6-行列过滤" class="headerlink" title="10.4.6 行列过滤"></a>10.4.6 行列过滤</h3><ul>
<li>列处理：在SELECT中，只拿需要的列，如果有分区，尽量使用分区过滤，少用SELECT *。</li>
<li>行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤，比如：</li>
<li>案例实操：<ol>
<li>测试先关联两张表，再用where条件过滤 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">       o.id </span><br><span class="line"><span class="keyword">from</span> bigtable b</span><br><span class="line"><span class="keyword">join</span> bigtable o <span class="keyword">on</span>  o.id <span class="operator">=</span> b.id</span><br><span class="line"><span class="keyword">where</span> o.id <span class="operator">&lt;=</span> <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="number">1</span>,<span class="number">081</span> <span class="keyword">rows</span> selected (<span class="number">18.979</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>通过子查询后，再关联表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">       b.id</span><br><span class="line"><span class="keyword">from</span> bigtable b</span><br><span class="line"><span class="keyword">join</span> (<span class="keyword">select</span> </span><br><span class="line">             id</span><br><span class="line">    <span class="keyword">from</span> bigtable</span><br><span class="line">    <span class="keyword">where</span> id <span class="operator">&lt;=</span> <span class="number">10</span>) o </span><br><span class="line"><span class="keyword">on</span> b.id <span class="operator">=</span> o.id;</span><br><span class="line"></span><br><span class="line"><span class="number">1</span>,<span class="number">081</span> <span class="keyword">rows</span> selected (<span class="number">17.753</span> seconds)</span><br></pre></td></tr></table></figure>
<h3 id="10-4-7-分区"><a href="#10-4-7-分区" class="headerlink" title="10.4.7 分区"></a>10.4.7 分区</h3>详见7.1章。</li>
</ol>
</li>
</ul>
<h3 id="10-4-8-分桶"><a href="#10-4-8-分桶" class="headerlink" title="10.4.8 分桶"></a>10.4.8 分桶</h3><pre><code>详见7.2章。
</code></pre>
<h2 id="10-5-合理设置Map及Reduce数"><a href="#10-5-合理设置Map及Reduce数" class="headerlink" title="10.5 合理设置Map及Reduce数"></a>10.5 合理设置Map及Reduce数</h2><ol>
<li>通常情况下，作业会产生一个或者多个map任务。<ul>
<li>主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。</li>
</ul>
</li>
<li>是不是map数越多越好？<ul>
<li>答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个片，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。</li>
</ul>
</li>
<li>是不是保证每个map处理接近128m的文件块，就高枕无忧了？<ul>
<li>答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。</li>
</ul>
</li>
</ol>
<ul>
<li>针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数；</li>
</ul>
<h3 id="10-5-1-复杂文件增加Map数"><a href="#10-5-1-复杂文件增加Map数" class="headerlink" title="10.5.1 复杂文件增加Map数"></a>10.5.1 复杂文件增加Map数</h3><ul>
<li>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。</li>
<li>增加map的方法为：根据<br>computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。</li>
<li>案例实操：<ol>
<li>执行查询 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> bigtable;</span><br><span class="line"></span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage<span class="number">-1</span>: number <span class="keyword">of</span> mappers: <span class="number">1</span>; number <span class="keyword">of</span> reducers: <span class="number">1</span></span><br></pre></td></tr></table></figure></li>
<li>设置最大切片值为100个字节 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.input.fileinputformat.split.maxsize<span class="operator">=</span><span class="number">10240</span>;</span><br><span class="line"></span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage<span class="number">-1</span>: number <span class="keyword">of</span> mappers: <span class="number">12613</span>; number <span class="keyword">of</span> reducers: <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="10-5-2-小文件进行合并"><a href="#10-5-2-小文件进行合并" class="headerlink" title="10.5.2 小文件进行合并"></a>10.5.2 小文件进行合并</h3></li>
</ol>
</li>
</ul>
<ol>
<li>在map执行前合并小文件，减少map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span> org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure></li>
<li>在Map-Reduce的任务结束时合并小文件的设置：<ul>
<li>在map-only任务结束时合并小文件，默认true  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.merge.mapfiles <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>在map-reduce任务结束时合并小文件，默认false  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.merge.mapredfiles <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>合并文件的大小，默认256M  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.merge.size.per.task <span class="operator">=</span> <span class="number">268435456</span>;</span><br></pre></td></tr></table></figure></li>
<li>当输出文件的平均大小小于该值时，启动一个独立的map-reduce任务进行文件merge  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.merge.smallfiles.avgsize <span class="operator">=</span> <span class="number">16777216</span>;</span><br></pre></td></tr></table></figure>
<h3 id="10-5-3-合理设置Reduce数"><a href="#10-5-3-合理设置Reduce数" class="headerlink" title="10.5.3 合理设置Reduce数"></a>10.5.3 合理设置Reduce数</h3></li>
</ul>
</li>
<li>调整reduce个数方法一<ol>
<li>每个Reduce处理的数据量默认是256MB <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive.exec.reducers.bytes.per.reducer<span class="operator">=</span><span class="number">256000000</span></span><br></pre></td></tr></table></figure></li>
<li>每个任务最大的reduce数，默认为1009 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive.exec.reducers.max<span class="operator">=</span><span class="number">1009</span></span><br></pre></td></tr></table></figure></li>
<li>计算reducer数的公式 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">N<span class="operator">=</span><span class="built_in">min</span>(参数<span class="number">2</span>，总输入数据量<span class="operator">/</span>参数<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>调整reduce个数方法二<ol>
<li>在hadoop的mapred-default.xml文件中修改，设置每个job的Reduce个数 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces <span class="operator">=</span> <span class="number">15</span>;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>reduce个数并不是越多越好<ol>
<li>过多的启动和初始化reduce也会消耗时间和资源；</li>
<li>另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</li>
<li>在设置reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适；</li>
</ol>
</li>
</ol>
<h3 id="10-6-并行执行"><a href="#10-6-并行执行" class="headerlink" title="10.6 并行执行"></a>10.6 并行执行</h3><ul>
<li>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。</li>
<li>通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 打开任务并行执行</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 同一个sql允许最大并行度，默认为8。</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number<span class="operator">=</span><span class="number">8</span>; </span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="10-7-严格模式"><a href="#10-7-严格模式" class="headerlink" title="10.7 严格模式"></a>10.7 严格模式</h3><p>Hive可以通过设置防止一些危险操作：</p>
<ol>
<li>分区表不使用分区过滤<ul>
<li>将hive.strict.checks.no.partition.filter设置为true时，对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。</li>
</ul>
</li>
<li>使用order by没有limit过滤<ul>
<li>将hive.strict.checks.orderby.no.limit设置为true时，对于使用了order by语句的查询，要求必须使用limit语句。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。</li>
</ul>
</li>
<li>笛卡尔积<ul>
<li>将hive.strict.checks.cartesian.product设置为true时，会限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在 执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。</li>
</ul>
</li>
</ol>
<h2 id="10-8-JVM重用"><a href="#10-8-JVM重用" class="headerlink" title="10.8 JVM重用"></a>10.8 JVM重用</h2><pre><code>详见hadoop优化文档中jvm重用
</code></pre>
<h2 id="10-9-压缩"><a href="#10-9-压缩" class="headerlink" title="10.9 压缩"></a>10.9 压缩</h2><pre><code>详见第9章。
</code></pre>
<h1 id="十一、Hive实战"><a href="#十一、Hive实战" class="headerlink" title="十一、Hive实战"></a>十一、Hive实战</h1><h2 id="11-1-需求描述"><a href="#11-1-需求描述" class="headerlink" title="11.1 需求描述"></a>11.1 需求描述</h2><p>统计硅谷影音视频网站的常规指标，各种TopN指标：</p>
<ul>
<li>统计视频观看数Top10</li>
<li>统计视频类别热度Top10</li>
<li>统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数</li>
<li>统计视频观看数Top50所关联视频的所属类别Rank</li>
<li>统计每个类别中的视频热度Top10,以Music为例</li>
<li>统计每个类别视频观看数Top10</li>
<li>统计上传视频最多的用户Top10以及他们上传的视频观看次数在前20的视频 </li>
</ul>
<h2 id="11-2-数据结构"><a href="#11-2-数据结构" class="headerlink" title="11.2 数据结构"></a>11.2 数据结构</h2><ol>
<li>视频表<table>
<thead>
<tr>
<th>字段</th>
<th>备注</th>
<th>详细描述</th>
</tr>
</thead>
<tbody><tr>
<td>videoId</td>
<td>视频唯一id（String）</td>
<td>11位字符串</td>
</tr>
<tr>
<td>uploader</td>
<td>视频上传者（String）</td>
<td>上传视频的用户名String</td>
</tr>
<tr>
<td>age</td>
<td>视频年龄（int）</td>
<td>视频在平台上的整数天</td>
</tr>
<tr>
<td>category</td>
<td>视频类别（Array<String>）</td>
<td>上传视频指定的视频分类</td>
</tr>
<tr>
<td>length</td>
<td>视频长度（Int）</td>
<td>整形数字标识的视频长度</td>
</tr>
<tr>
<td>views</td>
<td>观看次数（Int）</td>
<td>视频被浏览的次数</td>
</tr>
<tr>
<td>rate</td>
<td>视频评分（Double）</td>
<td>满分5分</td>
</tr>
<tr>
<td>Ratings</td>
<td>流量（Int）</td>
<td>视频的流量，整型数字</td>
</tr>
<tr>
<td>conments</td>
<td>评论数（Int）</td>
<td>一个视频的整数评论数</td>
</tr>
<tr>
<td>relatedId</td>
<td>相关视频id（Array<String>）</td>
<td>相关视频的id，最多20个</td>
</tr>
</tbody></table>
</li>
<li>用户表<table>
<thead>
<tr>
<th>字段</th>
<th>备注</th>
<th>字段类型</th>
</tr>
</thead>
<tbody><tr>
<td>uploader</td>
<td>上传者用户名</td>
<td>string</td>
</tr>
<tr>
<td>videos</td>
<td>上传视频数</td>
<td>int</td>
</tr>
<tr>
<td>friends</td>
<td>朋友数量</td>
<td>int</td>
</tr>
</tbody></table>
</li>
</ol>
<h2 id="11-3-准备工作"><a href="#11-3-准备工作" class="headerlink" title="11.3 准备工作"></a>11.3 准备工作</h2><h3 id="11-3-1-ETL"><a href="#11-3-1-ETL" class="headerlink" title="11.3.1 ETL"></a>11.3.1 ETL</h3><h3 id="11-3-2-准备表"><a href="#11-3-2-准备表" class="headerlink" title="11.3.2 准备表"></a>11.3.2 准备表</h3><ol>
<li>需要准备的表<ul>
<li>创建原始数据表：gulivideo_ori，gulivideo_user_ori，</li>
<li>创建最终表：gulivideo_orc，gulivideo_user_orc</li>
</ul>
</li>
<li>创建原始数据表：<ul>
<li>创建视频原始数据表：gulivideo_ori  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_ori (</span><br><span class="line">    videoId   string,</span><br><span class="line">    uploader  string,</span><br><span class="line">    age       <span class="type">int</span>,</span><br><span class="line">    category  <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>,</span><br><span class="line">    length    <span class="type">int</span>,</span><br><span class="line">    `views`     <span class="type">int</span>,</span><br><span class="line">    rate      <span class="type">float</span>,</span><br><span class="line">    ratings   <span class="type">int</span>,</span><br><span class="line">    comments  <span class="type">int</span>,</span><br><span class="line">    relatedId <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot;\t&quot;</span><br><span class="line">collection items terminated <span class="keyword">by</span> &quot;&amp;&quot;</span><br><span class="line">stored <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure></li>
<li>创建用户原始数据表: gulivideo_user_ori  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_user_ori(</span><br><span class="line">    uploader string,</span><br><span class="line">    videos <span class="type">int</span>,</span><br><span class="line">    friends <span class="type">int</span>)</span><br><span class="line"><span class="type">row</span> format delimited </span><br><span class="line">fields terminated <span class="keyword">by</span> &quot;\t&quot; </span><br><span class="line">stored <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>创建orc存储格式带snappy压缩的表：<ul>
<li>gulivideo_orc  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_orc(</span><br><span class="line">    videoId string, </span><br><span class="line">    uploader string, </span><br><span class="line">    age <span class="type">int</span>, </span><br><span class="line">    category <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>, </span><br><span class="line">    length <span class="type">int</span>, </span><br><span class="line">    views <span class="type">int</span>, </span><br><span class="line">    rate <span class="type">float</span>, </span><br><span class="line">    ratings <span class="type">int</span>, </span><br><span class="line">    comments <span class="type">int</span>,</span><br><span class="line">    relatedId <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>)</span><br><span class="line">stored <span class="keyword">as</span> orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;<span class="operator">=</span>&quot;SNAPPY&quot;);</span><br></pre></td></tr></table></figure></li>
<li>gulivideo_user_orc  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_user_orc(</span><br><span class="line">    uploader string,</span><br><span class="line">    videos <span class="type">int</span>,</span><br><span class="line">    friends <span class="type">int</span>)</span><br><span class="line"><span class="type">row</span> format delimited </span><br><span class="line">fields terminated <span class="keyword">by</span> &quot;\t&quot; </span><br><span class="line">stored <span class="keyword">as</span> orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;<span class="operator">=</span>&quot;SNAPPY&quot;);</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>向ori表插入数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">load data inpath &quot;/gulivideo/video/output&quot; <span class="keyword">into</span> <span class="keyword">table</span> gulivideo_ori;</span><br><span class="line">load data inpath &quot;/gulivideo/user&quot; <span class="keyword">into</span> <span class="keyword">table</span> gulivideo_user_ori;</span><br></pre></td></tr></table></figure></li>
<li>向orc表插入数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> gulivideo_orc <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> gulivideo_ori;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> gulivideo_user_orc <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> gulivideo_user_ori;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="11-3-3-安装Tez引擎"><a href="#11-3-3-安装Tez引擎" class="headerlink" title="11.3.3 安装Tez引擎"></a>11.3.3 安装Tez引擎</h3><p>Tez是一个Hive的运行引擎，性能优于MR。为什么优于MR呢？看下<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16356724726440.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>用Hive直接编写MR程序，假设有四个有依赖关系的MR作业，上图中，绿色是Reduce Task，云状表示写屏蔽，需要将中间结果持久化写到HDFS。</li>
<li>Tez可以将多个有依赖的作业转换为一个作业，这样只需写一次HDFS，且中间节点较少，从而大大提升作业的计算性能。</li>
<li>安装步骤<ol>
<li>将tez安装包拷贝到集群，并解压tar包 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ mkdir /opt/module/tez</span><br><span class="line">[atguigu@hadoop102 software]$ tar -zxvf /opt/software/tez-0.10.1-SNAPSHOT-minimal.tar.gz -C /opt/module/tez</span><br></pre></td></tr></table></figure></li>
<li>上传tez依赖到HDFS <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ hadoop fs -mkdir /tez</span><br><span class="line">[atguigu@hadoop102 software]$ hadoop fs -put /opt/software/tez-0.10.1-SNAPSHOT.tar.gz /tez</span><br></pre></td></tr></table></figure></li>
<li>新建$HADOOP_HOME/etc/hadoop/tez-site.xml添加如下内容： <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.lib.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;fs.defaultFS&#125;/tez/tez-0.10.1-SNAPSHOT.tar.gz<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.use.cluster.hadoop-libs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.am.resource.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.am.resource.cpu.vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.container.max.java.heap.fraction<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.task.resource.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.task.resource.cpu.vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<ol start="4">
<li>修改Hadoop环境变量$HADOOP_HOME/etc/hadoop/shellprofile.d/tez.sh <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 添加Tez的Jar包相关信息</span></span><br><span class="line">hadoop_add_profile tez</span><br><span class="line"><span class="keyword">function</span> _tez_hadoop_classpath</span><br><span class="line">&#123;</span><br><span class="line">    hadoop_add_classpath <span class="string">&quot;<span class="variable">$HADOOP_HOME</span>/etc/hadoop&quot;</span> after</span><br><span class="line">    hadoop_add_classpath <span class="string">&quot;/opt/module/tez/*&quot;</span> after</span><br><span class="line">    hadoop_add_classpath <span class="string">&quot;/opt/module/tez/lib/*&quot;</span> after</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>修改Hive的计算引擎$HIVE_HOME/conf/hive-site.xml <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.execution.engine<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>tez<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.tez.container.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>解决日志Jar包冲突 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ rm /opt/module/tez/lib/slf4j-log4j12-1.7.10.jar</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="11-4-业务分析"><a href="#11-4-业务分析" class="headerlink" title="11.4 业务分析"></a>11.4 业务分析</h2><ul>
<li>统计视频观看数Top10  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> videoId,</span><br><span class="line">       uploader,</span><br><span class="line">       age,</span><br><span class="line">       category,</span><br><span class="line">       length,</span><br><span class="line">       `views`,</span><br><span class="line">       rate,</span><br><span class="line">       ratings,</span><br><span class="line">       comments,</span><br><span class="line">       relatedId</span><br><span class="line"><span class="keyword">from</span> gulivideo_orc</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> `views` <span class="keyword">desc</span></span><br><span class="line">limit <span class="number">10</span>;</span><br></pre></td></tr></table></figure></li>
<li>统计视频类别热度Top10  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">       category_name,</span><br><span class="line">       <span class="built_in">count</span>(videoId) video_count</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    videoId,category_name</span><br><span class="line"><span class="keyword">from</span> gulivideo_orc</span><br><span class="line"> <span class="keyword">lateral</span> <span class="keyword">view</span> explode(category) explode_category <span class="keyword">as</span> category_name</span><br><span class="line">) t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> category_name</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> video_count <span class="keyword">desc</span> limit <span class="number">10</span>;</span><br></pre></td></tr></table></figure></li>
<li>统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">       t2.category_name,</span><br><span class="line">       <span class="built_in">count</span>(videoId) video_count</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">select</span></span><br><span class="line">                t1.videoId,</span><br><span class="line">                category_name</span><br><span class="line">         <span class="keyword">from</span> (</span><br><span class="line">                  <span class="keyword">select</span></span><br><span class="line">                         videoId,</span><br><span class="line">                         category,</span><br><span class="line">                         `views`</span><br><span class="line">                  <span class="keyword">from</span> gulivideo_orc</span><br><span class="line">                  <span class="keyword">order</span> <span class="keyword">by</span> `views` <span class="keyword">desc</span></span><br><span class="line">                  limit <span class="number">20</span></span><br><span class="line">              ) t1</span><br><span class="line">                  <span class="keyword">lateral</span> <span class="keyword">view</span> explode(t1.category) explode_category <span class="keyword">as</span> category_name</span><br><span class="line">     ) t2</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> t2.category_name</span><br><span class="line">;</span><br></pre></td></tr></table></figure>
</li>
<li>统计视频观看数Top50所关联视频的所属类别排名  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> category_name,</span><br><span class="line">       video_count,</span><br><span class="line">       <span class="built_in">rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> video_count) video_count_rank,</span><br><span class="line">       video_views,</span><br><span class="line">       <span class="built_in">rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> video_views) video_views_rank</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">select</span> category_name,</span><br><span class="line">                <span class="built_in">count</span>(t5.single_relatedId)  video_count,</span><br><span class="line">                <span class="built_in">sum</span>(t5.related_video_views) video_views</span><br><span class="line">         <span class="keyword">from</span> (</span><br><span class="line">                  <span class="keyword">select</span> t4.orign_video_id,</span><br><span class="line">                         t4.orign_video_views,</span><br><span class="line">                         t4.single_relatedId,</span><br><span class="line">                         t4.related_video_views,</span><br><span class="line">                         category_name</span><br><span class="line">                  <span class="keyword">from</span> (</span><br><span class="line">                           <span class="keyword">select</span> t2.videoId orign_video_id,</span><br><span class="line">                                  t2.views   orign_video_views,</span><br><span class="line">                                  t2.single_relatedId,</span><br><span class="line">                                  t3.views   related_video_views,</span><br><span class="line">                                  t3.category</span><br><span class="line">                           <span class="keyword">from</span> (<span class="keyword">select</span> t1.videoId,</span><br><span class="line">                                        t1.views,</span><br><span class="line">                                        single_relatedId</span><br><span class="line">                                 <span class="keyword">from</span> (</span><br><span class="line">                                          <span class="keyword">select</span> videoId,</span><br><span class="line">                                                 `views`,</span><br><span class="line">                                                 relatedId</span><br><span class="line">                                          <span class="keyword">from</span> gulivideo_orc</span><br><span class="line">                                          <span class="keyword">order</span> <span class="keyword">by</span> `views` <span class="keyword">desc</span></span><br><span class="line">                                          limit <span class="number">50</span></span><br><span class="line">                                      ) t1</span><br><span class="line">                                          <span class="keyword">lateral</span> <span class="keyword">view</span> explode(t1.relatedId) explode_relatedId <span class="keyword">as</span> single_relatedId) t2</span><br><span class="line">                                    <span class="keyword">left</span> <span class="keyword">join</span> gulivideo_orc t3 <span class="keyword">on</span> t2.single_relatedId <span class="operator">=</span> t3.videoId</span><br><span class="line">                           <span class="keyword">where</span> t3.videoId <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">null</span></span><br><span class="line">                       ) t4</span><br><span class="line">                           <span class="keyword">lateral</span> <span class="keyword">view</span> explode(t4.category) explode_category <span class="keyword">as</span> category_name</span><br><span class="line">              ) t5</span><br><span class="line">         <span class="keyword">group</span> <span class="keyword">by</span> t5.category_name</span><br><span class="line">     ) t6</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>统计每个类别中的视频热度Top10,以Music为例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 统计每个类别中的视频热度Top10,以Music为例</span></span><br><span class="line"><span class="keyword">select</span> videoId,</span><br><span class="line">       `views`,</span><br><span class="line">       category_name</span><br><span class="line"><span class="keyword">from</span> gulivideo_orc</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">VIEW</span> explode(category) gulivideo_orc_tmp <span class="keyword">AS</span> category_name</span><br><span class="line"><span class="keyword">where</span> array_contains(category, &quot;Music&quot;)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> `views` <span class="keyword">desc</span></span><br><span class="line">limit <span class="number">10</span>;</span><br><span class="line"><span class="comment">-- 统计每个类别中的视频热度Top10,以Music为例2</span></span><br><span class="line"><span class="keyword">SELECT</span> t1.videoId,</span><br><span class="line">       t1.views,</span><br><span class="line">       t1.category_name</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">         <span class="keyword">SELECT</span> videoId,</span><br><span class="line">                `views`,</span><br><span class="line">                category_name</span><br><span class="line">         <span class="keyword">FROM</span> gulivideo_orc</span><br><span class="line">                  <span class="keyword">lateral</span> <span class="keyword">VIEW</span> explode(category) gulivideo_orc_tmp <span class="keyword">AS</span> category_name</span><br><span class="line">     ) t1</span><br><span class="line"><span class="keyword">WHERE</span> t1.category_name <span class="operator">=</span> &quot;Music&quot;</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> t1.views <span class="keyword">DESC</span></span><br><span class="line">LIMIT <span class="number">10</span>;</span><br></pre></td></tr></table></figure></li>
<li>统计每个类别视频观看数Top10  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">SELECT</span> t1.videoId,</span><br><span class="line">                t1.views,</span><br><span class="line">                t1.category_name,</span><br><span class="line">                <span class="built_in">rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> t1.category_name <span class="keyword">order</span> <span class="keyword">by</span> t1.views <span class="keyword">desc</span>) video_rank</span><br><span class="line">         <span class="keyword">FROM</span> (</span><br><span class="line">                  <span class="keyword">SELECT</span> videoId,</span><br><span class="line">                         `views`,</span><br><span class="line">                         category_name</span><br><span class="line">                  <span class="keyword">FROM</span> gulivideo_orc</span><br><span class="line">                           <span class="keyword">lateral</span> <span class="keyword">VIEW</span> explode(category) gulivideo_orc_tmp <span class="keyword">AS</span> category_name</span><br><span class="line">              ) t1</span><br><span class="line">     ) t2</span><br><span class="line"><span class="keyword">where</span> t2.video_rank <span class="operator">&lt;=</span> <span class="number">10</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> t2.category_name, t2.video_rank</span><br></pre></td></tr></table></figure></li>
<li>统计上传视频最多的用户Top10以及他们上传的视频观看次数在前20的视频  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">select</span> uploader,</span><br><span class="line">                videoId,</span><br><span class="line">                `views`,</span><br><span class="line">                <span class="built_in">percent_rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> `views` )                                       views_percent_rank,</span><br><span class="line">                <span class="built_in">count</span>(videoId) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> uploader) video_count,</span><br><span class="line">                <span class="built_in">dense_rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> <span class="built_in">count</span>(videoId) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> uploader) <span class="keyword">desc</span>) video_count_rank</span><br><span class="line">         <span class="keyword">from</span> gulivideo_orc</span><br><span class="line">     ) t1</span><br><span class="line"><span class="keyword">where</span> t1.views_percent_rank <span class="operator">&gt;=</span> <span class="number">0.8</span></span><br><span class="line">  <span class="keyword">and</span> t1.video_count_rank <span class="operator">&lt;</span> <span class="number">10</span>;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">SELECT</span> t2.videoId,</span><br><span class="line">       t2.views,</span><br><span class="line">       t2.uploader</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">         <span class="keyword">SELECT</span> uploader,</span><br><span class="line">                videos</span><br><span class="line">         <span class="keyword">FROM</span> gulivideo_user_orc</span><br><span class="line">         <span class="keyword">ORDER</span> <span class="keyword">BY</span> videos <span class="keyword">DESC</span></span><br><span class="line">         LIMIT <span class="number">10</span></span><br><span class="line">     ) t1</span><br><span class="line">         <span class="keyword">JOIN</span> gulivideo_orc t2</span><br><span class="line">              <span class="keyword">ON</span> t1.uploader <span class="operator">=</span> t2.uploader</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> t2.views</span><br><span class="line">        <span class="keyword">DESC</span></span><br><span class="line">LIMIT <span class="number">20</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">SELECT</span> t2.videoId,</span><br><span class="line">                t2.views,</span><br><span class="line">                t2.uploader,</span><br><span class="line">                <span class="built_in">rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> t2.uploader <span class="keyword">order</span> <span class="keyword">by</span> t2.views <span class="keyword">desc</span> ) views_rank</span><br><span class="line">         <span class="keyword">FROM</span> (</span><br><span class="line">                  <span class="keyword">SELECT</span> uploader,</span><br><span class="line">                         videos</span><br><span class="line">                  <span class="keyword">FROM</span> gulivideo_user_orc</span><br><span class="line">                  <span class="keyword">ORDER</span> <span class="keyword">BY</span> videos <span class="keyword">DESC</span></span><br><span class="line">                  LIMIT <span class="number">10</span></span><br><span class="line">              ) t1</span><br><span class="line">                  <span class="keyword">JOIN</span> gulivideo_orc t2</span><br><span class="line">                       <span class="keyword">ON</span> t1.uploader <span class="operator">=</span> t2.uploader</span><br><span class="line">     ) t3</span><br><span class="line"><span class="keyword">where</span> t3.views_rank <span class="operator">&lt;=</span> <span class="number">20</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> uploader, views_rank</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>数仓</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop</title>
    <url>/2021/11/07/Hadoop/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><h3 id="1-Hadoop概念"><a href="#1-Hadoop概念" class="headerlink" title="1.Hadoop概念"></a>1.Hadoop概念</h3><p>Hadoop是一个综合性的架构（软件）</p>
<h3 id="2-Hadoop组件"><a href="#2-Hadoop组件" class="headerlink" title="2.Hadoop组件"></a>2.Hadoop组件</h3><ol>
<li>HDFS: 解决海量数据的存储和管理</li>
<li>MapReduce: 解决海量数据的计算分析 </li>
<li>Yarn: 负责MR程序运行的平台，做资源分配的工作</li>
<li>Zookeeper: 分布式资源协调管理</li>
</ol>
<h3 id="3-Hadoop集群搭建"><a href="#3-Hadoop集群搭建" class="headerlink" title="3.Hadoop集群搭建"></a>3.Hadoop集群搭建</h3><ol>
<li>HDFS集群</li>
<li>Yarn集群</li>
</ol>
<h2 id="一、大数据概论"><a href="#一、大数据概论" class="headerlink" title="一、大数据概论"></a>一、大数据概论</h2><h3 id="1-大数据概念"><a href="#1-大数据概念" class="headerlink" title="1.大数据概念"></a>1.大数据概念</h3><p>大数据: 解决海量数据的存储和分析计算</p>
<h3 id="2-大数据特点"><a href="#2-大数据特点" class="headerlink" title="2.大数据特点"></a>2.大数据特点</h3><ol>
<li>Volume: 大量</li>
<li>Velocity: 高速</li>
<li>Variety: 多样</li>
<li>Value: 低价值</li>
</ol>
<h3 id="3-大数据应用场景"><a href="#3-大数据应用场景" class="headerlink" title="3.大数据应用场景"></a>3.大数据应用场景</h3><ol>
<li>物流仓储、零售、旅游···</li>
</ol>
<h3 id="4-大数据部门组织结构"><a href="#4-大数据部门组织结构" class="headerlink" title="4.大数据部门组织结构"></a>4.大数据部门组织结构</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16337824506474.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h2 id="二、从Hadoop框架了解大数据生态"><a href="#二、从Hadoop框架了解大数据生态" class="headerlink" title="二、从Hadoop框架了解大数据生态"></a>二、从Hadoop框架了解大数据生态</h2><h3 id="1-Hadoop是什么"><a href="#1-Hadoop是什么" class="headerlink" title="1. Hadoop是什么"></a>1. Hadoop是什么</h3><ul>
<li>Hadoop是Apache基金会开发的<font color ='red' >分布式系统基础架构</font></li>
<li>解决海量数据的<font color ='red' ><font color ='red' ></font></font></li>
<li>广义上指更广泛的Hadoop生态圈</li>
</ul>
<h3 id="2-Hadoop发展历史"><a href="#2-Hadoop发展历史" class="headerlink" title="2.Hadoop发展历史"></a>2.Hadoop发展历史</h3><ol>
<li>Lucene开发者Doug Cutting实现全文搜索</li>
<li>2001年Lucene成为Apache基金会子项目</li>
<li>Lucene处理海量数据暴露数据存储困难，检索速度慢的问题</li>
<li>微型版Nutch解决问题</li>
<li>Google <ul>
<li>GFS–&gt;HDFS </li>
<li>Map-Reduce–&gt;MR</li>
<li>BigTable–&gt;Hbase</li>
</ul>
</li>
<li>2003-2004年，Google公开了部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用了2年业余时间实现了DFS和MapReduce机制，使Nutch性能飙升。</li>
<li>2005 年Hadoop 作为 Lucene的子项目 Nutch的一部分正式引入Apache基金会。</li>
<li>2006 年 3 月份，Map-Reduce和Nutch Distributed File System （NDFS）分别被纳入到 Hadoop 项目中，Hadoop就此正式诞生，标志着大数据时代来临。</li>
<li>名字来源于Doug Cutting儿子的玩具大象</li>
</ol>
<h3 id="3-Hadoop发行版本"><a href="#3-Hadoop发行版本" class="headerlink" title="3.Hadoop发行版本"></a>3.Hadoop发行版本</h3><p>Hadoop 三大发行版本：Apache、Cloudera、Hortonworks。</p>
<ul>
<li>Apache 版本最原始（最基础）的版本，对于入门学习最好。</li>
<li>Cloudera 内部集成了很多大数据框架，对应产品 CDH。</li>
<li>Hortonworks 文档较好，对应产品 HDP。</li>
<li>Hortonworks 现在已经被 Cloudera 公司收购，推出新的品牌 CDP。</li>
</ul>
<h3 id="4-Hadoop优势"><a href="#4-Hadoop优势" class="headerlink" title="4.Hadoop优势"></a>4.Hadoop优势</h3><ol>
<li>高可靠性：Hadoop维护多个数据副本，单节点出现故障不会导致数据丢失</li>
<li>高拓展性：在集群间分配任务数据，可以方便的拓展数以千计的节点</li>
<li>高效性：在MapReduce的思想下，Hadoop是并行工作，加快任务处理速度</li>
<li>高容错性：能将失败的任务重新分配</li>
</ol>
<h3 id="5-Hadoop组成"><a href="#5-Hadoop组成" class="headerlink" title="5.Hadoop组成"></a>5.Hadoop组成</h3><p>1.x 2.x组成<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16337824731783.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h4 id="5-1-HDFS架构概述"><a href="#5-1-HDFS架构概述" class="headerlink" title="5.1 HDFS架构概述"></a>5.1 HDFS架构概述</h4><p>Hadoop Distributed File System，简称 HDFS，是一个分布式文件系统。负责海量数据的存储。</p>
<ol>
<li>NameNode（nn）：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的DataNode等。</li>
<li>DataNode(dn)：在本地文件系统存储文件块数据，以及块数据的校验和。</li>
<li>Secondary NameNode(2nn)：每隔一段时间对NameNode元数据备份。<h4 id="5-2-YARN"><a href="#5-2-YARN" class="headerlink" title="5.2 YARN"></a>5.2 YARN</h4>Yet Another Resource Negotiator 简称 YARN ，另一种资源协调者，是 Hadoop 的资源管理器。<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16337824915316.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ol>
<ol>
<li>ResourceManager（RM）：整个集群资源（内存、CPU等）的老大<ul>
<li>处理客户端请求</li>
<li>监控NodeManager</li>
<li>启动、监控ApplicationMaster</li>
<li>资源的分配与调度</li>
</ul>
</li>
<li>ApplicationMaster（AM）：单个任务运行的老大<ul>
<li>负责数据的切分</li>
<li>为用用程序申请资源并分配</li>
</ul>
</li>
<li>NodeManager（NM）：单个节点服务器资源老大</li>
<li>Container：容器，相当一台独立的服务器，里面封装了任务运行所需要的资源，如内存、CPU、磁盘、网络等。</li>
</ol>
<h4 id="5-3-MapReduce"><a href="#5-3-MapReduce" class="headerlink" title="5.3 MapReduce"></a>5.3 MapReduce</h4><p>MapReduce将计算过程分为两个阶段：Map和Reduce<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16337825183035.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>Map阶段并行处理输入数据</li>
<li>Reduce阶段对Map结果进行汇总<h3 id="6-大数据技术生态体系"><a href="#6-大数据技术生态体系" class="headerlink" title="6.大数据技术生态体系"></a>6.大数据技术生态体系</h3><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16337825330556.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ol>
<h3 id="7-推荐系统框架图"><a href="#7-推荐系统框架图" class="headerlink" title="7.推荐系统框架图"></a>7.推荐系统框架图</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16337825489826.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h2 id="三、-Hadoop运行环境搭建"><a href="#三、-Hadoop运行环境搭建" class="headerlink" title="三、 Hadoop运行环境搭建"></a>三、 Hadoop运行环境搭建</h2><h3 id="1-最小化安装Centos"><a href="#1-最小化安装Centos" class="headerlink" title="1. 最小化安装Centos"></a>1. 最小化安装Centos</h3><h3 id="2-配置虚拟机模板"><a href="#2-配置虚拟机模板" class="headerlink" title="2. 配置虚拟机模板"></a>2. 配置虚拟机模板</h3><ul>
<li>固定IP,修改主机名</li>
<li>Yum安装工具  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install -y epel-release psmisc nc net-tools rsync vim lrzsz ntp libzstd openssl-static tree iotop git</span><br></pre></td></tr></table></figure></li>
<li>关闭防火墙  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure></li>
<li>创建用户  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">adduser atguigu</span><br><span class="line">passwd atguigu</span><br></pre></td></tr></table></figure></li>
<li>添加sudo权限/etc/sudoers添加内容  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">atguigu  ALL=(ALL)   ALL</span><br></pre></td></tr></table></figure>
<h3 id="3-安装JDK"><a href="#3-安装JDK" class="headerlink" title="3. 安装JDK"></a>3. 安装JDK</h3></li>
</ul>
<ol>
<li>上传jdk-8u212-linux-x64.tar.gz到/opt/software下</li>
<li>解压JDK到/opt/module目录下<code>tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/</code></li>
<li>配置JDK环境变量新建/etc/profile.d/set_env.sh文件 添加如下内容 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ cat /etc/profile.d/my_env.sh</span><br><span class="line">#JAVA_HOME</span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="4-安装Hadoop"><a href="#4-安装Hadoop" class="headerlink" title="4. 安装Hadoop"></a>4. 安装Hadoop</h3><ol>
<li>上传 hadoop-3.1.3.tar.gz到/opt/software下</li>
<li>解压到/opt/module下面<code>tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/</code></li>
<li>配置环境变量/etc/profile.d/set_env.sh文件 添加如下内容 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#HADOOP_HOME</span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="四、Hadoop分布式部署"><a href="#四、Hadoop分布式部署" class="headerlink" title="四、Hadoop分布式部署"></a>四、Hadoop分布式部署</h2><h3 id="1-准备虚拟机"><a href="#1-准备虚拟机" class="headerlink" title="1. 准备虚拟机"></a>1. 准备虚拟机</h3><ol>
<li>静态IP<br> <code> BOOTPROTO=static  IPADDR=192.168.1.102  GATEWAY=192.168.1.2  DNS1=192.168.1.2 </code></li>
<li>关闭防火墙 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure></li>
<li>设置hostname <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hostnamectl set-hostname hadoop001</span><br></pre></td></tr></table></figure></li>
<li>修改/etc/hosts 添加解析内容 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">192.168.2.6 hadoop001</span><br><span class="line">192.168.2.7 hadoop002</span><br><span class="line">192.168.2.8 hadoop003</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-准备分发脚本xsync"><a href="#2-准备分发脚本xsync" class="headerlink" title="2. 准备分发脚本xsync"></a>2. 准备分发脚本xsync</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">1. 判断参数个数</span></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">  echo Not Enough Arguement!</span><br><span class="line">  exit;</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">2. 遍历集群所有机器</span></span><br><span class="line">for host in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">  echo ====================  $host  ====================</span><br><span class="line"><span class="meta">  #</span><span class="bash">3. 遍历所有目录，挨个发送</span></span><br><span class="line">  for file in $@</span><br><span class="line">  do</span><br><span class="line">    #4. 判断文件是否存在</span><br><span class="line">    if [ -e $file ]</span><br><span class="line">    then</span><br><span class="line">      #5. 获取父目录</span><br><span class="line">      pdir=$(cd -P $(dirname $file); pwd)</span><br><span class="line">      #6. 获取当前文件的名称</span><br><span class="line">      fname=$(basename $file)</span><br><span class="line">      ssh $host &quot;mkdir -p $pdir&quot;</span><br><span class="line">      rsync -av $pdir/$fname $host:$pdir</span><br><span class="line">    else</span><br><span class="line">      echo $file does not exists!</span><br><span class="line">    fi</span><br><span class="line">  done</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<h3 id="3-免密登录"><a href="#3-免密登录" class="headerlink" title="3. 免密登录"></a>3. 免密登录</h3><ol>
<li><p>生成公私钥<code>ssh-keygen</code></p>
</li>
<li><p>拷贝公钥到目标服务器</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh-copy-id hadoop102</span><br><span class="line">ssh-copy-id hadoop103</span><br><span class="line">ssh-copy-id hadoop104</span><br></pre></td></tr></table></figure></li>
<li><p>.ssh文件夹下（~/.ssh）的文件功能解释</p>
<table>
<thead>
<tr>
<th>文件</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>known_hosts</td>
<td>记录ssh访问过计算机的公钥(public key)</td>
</tr>
<tr>
<td>id_rsa</td>
<td>生成的私钥</td>
</tr>
<tr>
<td>id_rsa.pub</td>
<td>生成的公钥</td>
</tr>
<tr>
<td>authorized_keys</td>
<td>存放授权过的无密登录服务器公钥</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
</li>
<li><p>集群配置</p>
<ol>
<li><p>集群部署规划</p>
<ul>
<li><p>注意：NameNode和SecondaryNameNode不要安装在同一台服务器</p>
</li>
<li><p>注意：ResourceManager也很消耗内存，不要和NameNode、SecondaryNameNode配置在同一台机器上。</p>
<table>
<thead>
<tr>
<th align="left">服务器</th>
<th align="left">HDFS</th>
<th align="left">Yarn</th>
</tr>
</thead>
<tbody><tr>
<td align="left">hadoop102</td>
<td align="left">NameNode、DataNode</td>
<td align="left">NodeManager</td>
</tr>
<tr>
<td align="left">hadoop103</td>
<td align="left">DataNode</td>
<td align="left">ResourceManager、NodeManager</td>
</tr>
<tr>
<td align="left">hadoop104</td>
<td align="left">SecondaryNameNode、DataNode</td>
<td align="left">NodeManager</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>配置文件说明<br> Hadoop配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。</p>
<ul>
<li><p>默认配置文件：</p>
<table>
<thead>
<tr>
<th align="left">配置文件</th>
<th align="left">默认文件存放在Hadoop的jar包中的位置</th>
</tr>
</thead>
<tbody><tr>
<td align="left">core-default.xml</td>
<td align="left">hadoop-common-3.1.3.jar/ core-default.xml</td>
</tr>
<tr>
<td align="left">hdfs-default.xml</td>
<td align="left">hadoop-hdfs-3.1.3.jar/ hdfs-default.xml</td>
</tr>
<tr>
<td align="left">yarn-default.xml</td>
<td align="left">hadoop-yarn-common-3.1.3.jar/yarn-default.xml</td>
</tr>
<tr>
<td align="left">mapred-default.xml</td>
<td align="left">hadoop-mapreduce-client-core-3.1.3.jar/ mapred-default.xml</td>
</tr>
</tbody></table>
</li>
<li><p>自定义配置文件：<br>  core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径下，可以根据项目需求重新进行修改配置。</p>
</li>
<li><p>常用端口号说明</p>
<table>
<thead>
<tr>
<th align="left">Daemon</th>
<th align="left">App</th>
<th align="left">Hadoop2</th>
<th align="left">Hadoop3</th>
</tr>
</thead>
<tbody><tr>
<td align="left">NameNode Port</td>
<td align="left">Hadoop HDFS NameNode</td>
<td align="left">8020/9000</td>
<td align="left">9820</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Hadoop HDFS NameNode HTTP UI</td>
<td align="left">50070</td>
<td align="left">9870</td>
</tr>
<tr>
<td align="left">SecondaryNameNode Port</td>
<td align="left">Secondary NameNode</td>
<td align="left">50091</td>
<td align="left">9869</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Secondary NameNode HTTP UI</td>
<td align="left">50090</td>
<td align="left">9868</td>
</tr>
<tr>
<td align="left">DataNode Port</td>
<td align="left">Hadoop HDFS DataNode IPC</td>
<td align="left">50020</td>
<td align="left">9867</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Hadoop HDFS DataNod</td>
<td align="left">50010</td>
<td align="left">9866</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Hadoop HDFS DataNode HTTP UI</td>
<td align="left">50075</td>
<td align="left">9864</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p><strong>配置集群<code>$HADOOP_HOME/etc/hadoop</code></strong></p>
<ul>
<li><p>核心配置文件core-site.xml</p>
  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 指定NameNode的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9820<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定hadoop数据的存储目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置HDFS网页登录使用的静态用户为atguigu --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置该atguigu(superUser)允许通过代理访问的主机节点 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置该atguigu(superUser)允许通过代理用户所属组 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置该atguigu(superUser)允许通过代理的用户--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.users<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>HDFS配置文件hdfs-site.xml</p>
  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- nn web端访问地址--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 2nn web端访问地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:9868<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>YARN配置文件yarn-site.xml</p>
  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 指定MR走shuffle --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定ResourceManager的地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 环境变量的继承 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- yarn容器允许分配的最大最小内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>512<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- yarn容器允许管理的物理内存大小 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 关闭yarn对物理内存和虚拟内存的限制检查 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>MapReduce配置文件mapred-site.xml</p>
  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 指定MapReduce程序运行在Yarn上 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>集群配置workers</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure></li>
<li><p>同步配置</p>
  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">xsync /opt/module/hadoop-3.1.3/etc</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
</li>
</ol>
]]></content>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>HDFS</title>
    <url>/2021/11/07/HDFS/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h1><h1 id="一、HDFS概述"><a href="#一、HDFS概述" class="headerlink" title="一、HDFS概述"></a>一、HDFS概述</h1><h2 id="1-1-背景及定义"><a href="#1-1-背景及定义" class="headerlink" title="1.1 背景及定义"></a>1.1 背景及定义</h2><h3 id="1-1-1-背景"><a href="#1-1-1-背景" class="headerlink" title="1.1.1 背景"></a>1.1.1 背景</h3><ol>
<li>海量数据无法单台服务器存储</li>
<li>需要管理多台机器上的文件</li>
</ol>
<h3 id="1-1-2-定义"><a href="#1-1-2-定义" class="headerlink" title="1.1.2 定义"></a>1.1.2 定义</h3><ol>
<li>HDFS: Hadoop Distributed File System 分布式文件系统，多台服务器联合实现功能，集群中各自承担不同角色</li>
<li>适合一次写入，多次读出的场景，<font color ='red' >支持文件追加，不支持文件修改</font>。适合做数据分析，不适合做网盘应用</li>
</ol>
<hr>
<h2 id="1-2-优缺点"><a href="#1-2-优缺点" class="headerlink" title="1.2 优缺点"></a>1.2 优缺点</h2><ul>
<li><p>优点</p>
<ol>
<li><p>高容错性</p>
<ul>
<li><p>数据自动保存多个副本，通过增加副本提高容错性<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16338313699180.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>某个副本丢失以后，可以自动回复<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16338314377557.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
</ul>
</li>
<li><p>适合处理大数据</p>
<ul>
<li>数据规模：能处理GB,TB,PB级别的数据</li>
<li>文件规模：能处理百万千万规模以上的文件数量</li>
</ul>
</li>
<li><p>可构建在廉价机器上，通过多副本机制提高可靠性</p>
</li>
</ol>
</li>
<li><p>缺点</p>
<ol>
<li>不适合低延迟数据访问，比如毫秒级的数据存储 </li>
<li>无法高效的对大量小文件进行存储<ul>
<li>大量小文件会占用NameNode大量内存保存元数据信息（目录和块信息）</li>
<li>小文件存储的寻址时间超过读取时间，不符合HDFS设计目标</li>
</ul>
</li>
<li>不支持并发写入，文件随机修改<ul>
<li>同一个文件只能单线程写，不支持多线程并发写</li>
<li>仅支持数据append，不支持文件随机修改</li>
</ul>
</li>
</ol>
</li>
</ul>
<hr>
<h2 id="1-3-组成架构"><a href="#1-3-组成架构" class="headerlink" title="1.3 组成架构"></a>1.3 组成架构</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16338314790454.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="1-3-1-NameNode（NN）"><a href="#1-3-1-NameNode（NN）" class="headerlink" title="1.3.1 NameNode（NN）"></a>1.3.1 NameNode（NN）</h3><ol>
<li>Master，它是一个主管、管理者。负责：</li>
<li>管理HDFS的名称空间</li>
<li>配置副本策略</li>
<li>管理数据块（Block）映射信息</li>
<li>处理客户端读写请求</li>
</ol>
<h3 id="1-3-2-DataNode（DN）"><a href="#1-3-2-DataNode（DN）" class="headerlink" title="1.3.2 DataNode（DN）"></a>1.3.2 DataNode（DN）</h3><ol>
<li>Slave。NameNode下达命令，DataNode执行实际的操作</li>
<li>存储实际的数据块</li>
<li>执行数据块的读/写操作</li>
</ol>
<h3 id="1-3-3-Client客户端"><a href="#1-3-3-Client客户端" class="headerlink" title="1.3.3 Client客户端"></a>1.3.3 Client客户端</h3><ol>
<li>文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传</li>
<li>与NameNode交互，获取文件的位置信息</li>
<li>与DataNode交互，读取或者写入数据；</li>
<li>Client提供一些命令来管理HDFS，比如NameNode格式化</li>
<li>Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作</li>
</ol>
<h3 id="1-3-4-Secondary-NameNode-2NN"><a href="#1-3-4-Secondary-NameNode-2NN" class="headerlink" title="1.3.4 Secondary NameNode(2NN)"></a>1.3.4 Secondary NameNode(2NN)</h3><ol>
<li>并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。</li>
<li>辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode</li>
<li>在紧急情况下，可辅助恢复NameNode。</li>
</ol>
<hr>
<h2 id="1-4-HDFS文件块大小"><a href="#1-4-HDFS文件块大小" class="headerlink" title="1.4 HDFS文件块大小"></a>1.4 HDFS文件块大小</h2><p>HDFS中的文件在物理上是分块存储（Block），块的大小可以通过配置参数(dfs.blocksize）来规定，默认大小在Hadoop2.x/3.x版本中是128M，1.x版本中是64M。<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16338322175049.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br><em>思考：为什么块的大小不能设置太小，也不能设置太大？</em></p>
<blockquote>
<ol>
<li>HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置；</li>
<li>如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。</li>
</ol>
<p>总结：HDFS块的大小设置主要取决于磁盘传输速率。</p>
</blockquote>
<hr>
<h1 id="二、Shell操作HDFS"><a href="#二、Shell操作HDFS" class="headerlink" title="二、Shell操作HDFS"></a>二、Shell操作HDFS</h1><h2 id="2-1-基本语法"><a href="#2-1-基本语法" class="headerlink" title="2.1 基本语法"></a>2.1 基本语法</h2><p><code>hadoop fs [genericOptions] [commandOptions]</code></p>
<hr>
<h2 id="2-2-命令参考"><a href="#2-2-命令参考" class="headerlink" title="2.2 命令参考"></a>2.2 命令参考</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ hadoop fs</span><br><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">	[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-checksum &lt;src&gt; ...]</span><br><span class="line">	[-chgrp [-R] GROUP PATH...]</span><br><span class="line">	[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">	[-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">	[-copyFromLocal [-f] [-p] [-l] [-d] [-t &lt;thread count&gt;] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-count [-q] [-h] [-v] [-t [&lt;storage <span class="built_in">type</span>&gt;]] [-u] [-x] [-e] &lt;path&gt; ...]</span><br><span class="line">	[-cp [-f] [-p | -p[topax]] [-d] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">	[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">	[-df [-h] [&lt;path&gt; ...]]</span><br><span class="line">	[-du [-s] [-h] [-v] [-x] &lt;path&gt; ...]</span><br><span class="line">	[-expunge]</span><br><span class="line">	[-find &lt;path&gt; ... &lt;expression&gt; ...]</span><br><span class="line">	[-get [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-getfacl [-R] &lt;path&gt;]</span><br><span class="line">	[-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line">	[-getmerge [-nl] [-skip-empty-file] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-head &lt;file&gt;]</span><br><span class="line">	[-<span class="built_in">help</span> [cmd ...]]</span><br><span class="line">	[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [&lt;path&gt; ...]]</span><br><span class="line">	[-mkdir [-p] &lt;path&gt; ...]</span><br><span class="line">	[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-mv &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-put [-f] [-p] [-l] [-d] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line">	[-rm [-f] [-r|-R] [-skipTrash] [-safely] &lt;src&gt; ...]</span><br><span class="line">	[-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]</span><br><span class="line">	[-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--<span class="built_in">set</span> &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">	[-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]</span><br><span class="line">	[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">stat</span> [format] &lt;path&gt; ...]</span><br><span class="line">	[-tail [-f] [-s &lt;sleep interval&gt;] &lt;file&gt;]</span><br><span class="line">	[-<span class="built_in">test</span> -[defsz] &lt;path&gt;]</span><br><span class="line">	[-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-touch [-a] [-m] [-t TIMESTAMP ] [-c] &lt;path&gt; ...]</span><br><span class="line">	[-touchz &lt;path&gt; ...]</span><br><span class="line">	[-truncate [-w] &lt;length&gt; &lt;path&gt; ...]</span><br><span class="line">	[-usage [cmd ...]]</span><br><span class="line">    </span><br><span class="line">Generic options supported are:</span><br><span class="line">-conf &lt;configuration file&gt;        specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;               define a value <span class="keyword">for</span> a given property</span><br><span class="line">-fs &lt;file:///|hdfs://namenode:port&gt; specify default filesystem URL to use, overrides <span class="string">&#x27;fs.defaultFS&#x27;</span> property from configurations.</span><br><span class="line">-jt &lt;<span class="built_in">local</span>|resourcemanager:port&gt;  specify a ResourceManager</span><br><span class="line">-files &lt;file1,...&gt;                specify a comma-separated list of files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;jar1,...&gt;               specify a comma-separated list of jar files to be included <span class="keyword">in</span> the classpath</span><br><span class="line">-archives &lt;archive1,...&gt;          specify a comma-separated list of archives to be unarchived on the compute machines</span><br><span class="line">    </span><br><span class="line">The general <span class="built_in">command</span> line syntax is:</span><br><span class="line"><span class="built_in">command</span> [genericOptions] [commandOptions]</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="2-3-常用命令"><a href="#2-3-常用命令" class="headerlink" title="2.3 常用命令"></a>2.3 常用命令</h2><h3 id="2-3-1-上传"><a href="#2-3-1-上传" class="headerlink" title="2.3.1 上传"></a>2.3.1 上传</h3><ul>
<li>-moveFromLocal：从本地剪切粘贴到 HDFS  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 sanguo]$ ls</span><br><span class="line">guanyu.txt  liubei.txt  sanguo.txt  zhangfei.txt  zhaoyun.txt</span><br><span class="line">[atguigu@hadoop001 sanguo]$ hadoop fs -moveFromLocal zhangfei.txt /sanguo</span><br><span class="line">2021-10-10 11:01:49,884 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">[atguigu@hadoop001 sanguo]$ ls</span><br><span class="line">guanyu.txt  liubei.txt  sanguo.txt  zhaoyun.txt</span><br><span class="line">[atguigu@hadoop001 sanguo]$</span><br></pre></td></tr></table></figure></li>
<li>-copyFromLocal：从本地文件系统中拷贝文件到 HDFS 路径去  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ hadoop fs -copyFromLocal zhaoyun.txt /sanguo</span><br><span class="line">2021-10-10 10:21:55,357 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
<li>-put：等同于 copyFromLocal，生产环境更习惯用 put  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 sanguo]$ hadoop fs -put caocao.txt /sanguo</span><br><span class="line">2021-10-10 12:05:07,051 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">[atguigu@hadoop001 sanguo]$ hadoop fs -ls /sanguo</span><br><span class="line">Found 8 items</span><br><span class="line">-rw-r--r--   3 atguigu supergroup         34 2021-10-10 12:05 /sanguo/caocao.txt</span><br><span class="line">-rw-r--r--   3 atguigu supergroup         48 2021-10-10 10:26 /sanguo/guanyu.txt</span><br><span class="line">-rw-r--r--   3 atguigu supergroup  195013152 2021-10-10 10:21 /sanguo/jdk-8u212-linux-x64.tar.gz</span><br><span class="line">-rw-r--r--   3 atguigu supergroup         22 2021-10-10 10:25 /sanguo/liubei.txt</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-10-10 10:33 /sanguo/shu</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-10-10 10:33 /sanguo/wu</span><br><span class="line">-rw-r--r--   3 atguigu supergroup         22 2021-10-10 11:01 /sanguo/zhangfei.txt</span><br><span class="line">-rw-r--r--   6 atguigu supergroup         28 2021-10-10 10:21 /sanguo/zhaoyun.txt</span><br><span class="line">[atguigu@hadoop001 sanguo]$</span><br></pre></td></tr></table></figure></li>
<li>-appendToFile：追加一个文件到已经存在的文件末尾</li>
</ul>
<h3 id="2-3-2-查看"><a href="#2-3-2-查看" class="headerlink" title="2.3.2 查看"></a>2.3.2 查看</h3><h3 id="2-3-3-下载"><a href="#2-3-3-下载" class="headerlink" title="2.3.3 下载"></a>2.3.3 下载</h3><h3 id="2-3-4-删除"><a href="#2-3-4-删除" class="headerlink" title="2.3.4 删除"></a>2.3.4 删除</h3><h3 id="2-3-5-HDFS-直接操作"><a href="#2-3-5-HDFS-直接操作" class="headerlink" title="2.3.5 HDFS 直接操作"></a>2.3.5 HDFS 直接操作</h3><hr>
<h1 id="四、HDFS的数据流"><a href="#四、HDFS的数据流" class="headerlink" title="四、HDFS的数据流"></a>四、HDFS的数据流</h1><h2 id="4-1-HDFS写数据流程"><a href="#4-1-HDFS写数据流程" class="headerlink" title="4.1 HDFS写数据流程"></a>4.1 HDFS写数据流程</h2><h3 id="4-1-1-剖析文件写入"><a href="#4-1-1-剖析文件写入" class="headerlink" title="4.1.1 剖析文件写入"></a>4.1.1 剖析文件写入</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16338477292310.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>请求上传文件<ol>
<li>创建Hadoop客户端DistributionFileSystem</li>
</ol>
</li>
<li>请求NN上传文件<ol>
<li>NN校验：权限，路径，文件名</li>
<li>NN响应允许上传</li>
</ol>
</li>
<li>请求NN上传Block</li>
<li>NN根据副本数量按照<font color ='red' >机架感知</font>规则返回副本存储节点DN</li>
<li>客户端创建输出流FSDataOutputStream，根据<font color ='red' >网络拓扑计算节点距离</font>，请求<font color ='red' ><font color ='red' ></font>最近节点</font>建立Block传输通道<ol>
<li>客户端请求DN1建立通道</li>
<li>DN1请求DN2建立通道</li>
<li>DN2请求DN3建立通道</li>
</ol>
</li>
<li>通道建立应答成功<ol>
<li>DN3应答DN2成功</li>
<li>DN2应答DN1成功</li>
<li>DN1应答客户端成功</li>
</ol>
</li>
<li>传输数据Packet<ol>
<li>block 分解成多个 packet （每个64K）,packet分解成多个chunk(每个512B)，每个chunk都有个校验和chacksum</li>
<li>DFSOutputStream负责把数据写入dataQueue，写入单位为packet</li>
<li>DataStreamer从dataQueue中提取packet,发送到管道中的第一个datanode,同时将该packet写入 ackQueue中（block是有副本的，在写block之前就已经确定了，这些副本要写到哪些datanode上，这些datanode形成一个数据管道，DataStreamer只会把数据写入管道的第一个datanode,然后第一个dataNode向第二个datanode写数据，第二个再向第三个写，它们之间使用socket传输数据）</li>
<li>ResponseProcessor会从datanodes接收ack（此ack是Datanode接收packet成功后的确定），ResponseProcessor接收到所有Datanote的ack后，就从ackQueue中移除相应的packet。</li>
<li>如果遇到异常，所有的packets都从ackQueue移除，并排除异常的Datanode，再重新申请一个管道 。</li>
<li>然后 DataStreamer又重新从dataQueue中，获得packets并发送。</li>
<li><a href="https://www.jianshu.com/p/419a2068987c">参考</a></li>
</ol>
</li>
<li>当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。</li>
</ol>
<h3 id="4-1-2-网络拓扑-节点距离计算"><a href="#4-1-2-网络拓扑-节点距离计算" class="headerlink" title="4.1.2 网络拓扑-节点距离计算"></a>4.1.2 网络拓扑-节点距离计算</h3><ul>
<li>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。</li>
<li>节点距离：两个节点到达最近的共同祖先的距离总和。<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16339649640120.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
<h3 id="4-1-3-机架感知（副本存储节点选择）"><a href="#4-1-3-机架感知（副本存储节点选择）" class="headerlink" title="4.1.3 机架感知（副本存储节点选择）"></a>4.1.3 机架感知（副本存储节点选择）</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16339657833860.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。</li>
<li>第二个副本在另一个机架的随机一个节点</li>
<li>第三个副本在第二个副本所在机架的随机节点</li>
</ul>
<blockquote>
<p>思考</p>
<ol>
<li>HDFS根据请求返回DataNode的节点的策略？– 机架感知</li>
</ol>
<ul>
<li>如果当前Client所在机器有DataNode节点，那就返回当前机器DN1,否则从集群中随机一台。</li>
<li>根据第一台机器的位置，然后再其他机架上随机一台，在第二台机器所在机架上再随机一台。</li>
<li>以上策略的缘由：为了提高数据的可靠性，同时一定程度也保证数据传输的效率！</li>
</ul>
<ol start="2">
<li>客户端建立传输通道的时候如何确定和哪一台DataNode先建立连接？– 网络拓扑</li>
</ol>
<ul>
<li>找离client最近的一台机器先建立通道。</li>
</ul>
<ol start="3">
<li>Client为什么是以串行的方式建立通道？</li>
</ol>
<ul>
<li>本质上就是为了降低client的IO开销</li>
</ul>
<ol start="4">
<li>数据传输的时候如何保证数据成功？（了解）</li>
</ol>
<ul>
<li>采用了ack回执的策略保证了数据完整成功上传。</li>
</ul>
</blockquote>
<hr>
<h2 id="4-2-HDFS读数据流程"><a href="#4-2-HDFS读数据流程" class="headerlink" title="4.2 HDFS读数据流程"></a>4.2 HDFS读数据流程</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16339658069723.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>客户端通过DistributedFileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。</li>
<li>挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。</li>
<li>DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。</li>
<li>客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</li>
</ol>
<hr>
<h1 id="五、NameNode和SecondaryNameNode"><a href="#五、NameNode和SecondaryNameNode" class="headerlink" title="五、NameNode和SecondaryNameNode"></a>五、NameNode和SecondaryNameNode</h1><h2 id="5-1-NN和2NN工作机制"><a href="#5-1-NN和2NN工作机制" class="headerlink" title="5.1 NN和2NN工作机制"></a>5.1 NN和2NN工作机制</h2><h3 id="5-1-1-元数据信息要保存在哪？"><a href="#5-1-1-元数据信息要保存在哪？" class="headerlink" title="5.1.1 元数据信息要保存在哪？"></a>5.1.1 元数据信息要保存在哪？</h3><ol>
<li><p>保存到磁盘</p>
<ul>
<li>优点：数据安全</li>
<li>不足：读写速度慢 效率低！</li>
</ul>
</li>
<li><p>保存内存</p>
<ul>
<li>优点： 读写效率高！</li>
<li>不足：数据不安全</li>
</ul>
</li>
<li><p>最终的解决方案： 磁盘 + 内存<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16339951016390.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>第一阶段：NameNode启动<ol>
<li>第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</li>
<li>客户端对元数据进行增删改的请求。</li>
<li>NameNode记录操作日志，更新滚动日志。</li>
<li>NameNode在内存中对元数据进行增删改。 </li>
</ol>
</li>
<li>第二阶段：Secondary NameNode工作<ol>
<li>Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。</li>
<li>Secondary NameNode请求执行CheckPoint。</li>
<li>NameNode滚动正在写的Edits日志。</li>
<li>将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。</li>
<li>Secondary NameNode加载编辑日志和镜像文件到内存，并合并。</li>
<li>生成新的镜像文件fsimage.chkpoint。</li>
<li>拷贝fsimage.chkpoint到NameNode。</li>
<li>NameNode将fsimage.chkpoint重新命名成fsimage。</li>
</ol>
</li>
</ul>
</li>
</ol>
<hr>
<h2 id="5-2-Fsimage和Edits解析"><a href="#5-2-Fsimage和Edits解析" class="headerlink" title="5.2 Fsimage和Edits解析"></a>5.2 Fsimage和Edits解析</h2><p>NameNode被格式化之后，将在/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current目录中产生如下文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 subdir0]$ <span class="built_in">cd</span> /opt/module/hadoop-3.1.3/data/dfs/name/current/</span><br><span class="line">[atguigu@hadoop001 current]$ ll</span><br><span class="line">总用量 6320</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 1048576 10月 12 07:26 edits_0000000000000000666-0000000000000000666</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu      42 10月 12 09:45 edits_0000000000000000667-0000000000000000668</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 1048576 10月 12 09:45 edits_inprogress_0000000000000000669</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu    4452 10月 12 09:05 fsimage_0000000000000000666</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu      62 10月 12 09:05 fsimage_0000000000000000666.md5</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu    4452 10月 12 09:45 fsimage_0000000000000000668</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu      62 10月 12 09:45 fsimage_0000000000000000668.md5</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu       4 10月 12 09:45 seen_txid</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu     215 10月 12 09:05 VERSION</span><br><span class="line">[atguigu@hadoop001 current]$</span><br></pre></td></tr></table></figure>
<ol>
<li>Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件inode的序列化信息。 </li>
<li>Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中。 </li>
<li>seen_txid文件保存的是一个数字，就是最后一个edits_的数字 </li>
<li>每次NameNode启动的时候都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并</li>
<li>解析Fsimage文件<ol>
<li>oiv查看Fsimage文件<ol>
<li>查看oiv和oev命令 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 current]$ hdfs --<span class="built_in">help</span> | grep -E <span class="string">&#x27;oev|oiv&#x27;</span></span><br><span class="line">oev                  apply the offline edits viewer to an edits file</span><br><span class="line">oiv                  apply the offline fsimage viewer to an fsimage</span><br><span class="line">[atguigu@hadoop001 current]$</span><br></pre></td></tr></table></figure></li>
<li>基本语法<br> <code>hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径</code></li>
<li>操作 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 current]$ hdfs oiv -p xml -i fsimage_0000000000000000666 -o ~/fsimage/fsimage.xml</span><br><span class="line">2021-10-12 10:02:40,652 INFO offlineImageViewer.FSImageHandler: Loading 3 strings</span><br><span class="line">[atguigu@hadoop001 current]$</span><br></pre></td></tr></table></figure></li>
<li>部分内容 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">INodeSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">lastInodeId</span>&gt;</span>16507<span class="tag">&lt;/<span class="name">lastInodeId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">numInodes</span>&gt;</span>54<span class="tag">&lt;/<span class="name">numInodes</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>16385<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">type</span>&gt;</span>DIRECTORY<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span><span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mtime</span>&gt;</span>1633959143943<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">permission</span>&gt;</span>atguigu:supergroup:0755<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">nsquota</span>&gt;</span>9223372036854775807<span class="tag">&lt;/<span class="name">nsquota</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">dsquota</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">inode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>16470<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">type</span>&gt;</span>DIRECTORY<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>sanguo<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mtime</span>&gt;</span>1633838707139<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">permission</span>&gt;</span>atguigu:supergroup:0755<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">nsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">nsquota</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">dsquota</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">inode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>16471<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">type</span>&gt;</span>FILE<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>jdk-8u212-linux-x64.tar.gz<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">replication</span>&gt;</span>3<span class="tag">&lt;/<span class="name">replication</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mtime</span>&gt;</span>1633832463721<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">atime</span>&gt;</span>1633832462757<span class="tag">&lt;/<span class="name">atime</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">preferredBlockSize</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">preferredBlockSize</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">permission</span>&gt;</span>atguigu:supergroup:0644<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">blocks</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">block</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">id</span>&gt;</span>1073741861<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">genstamp</span>&gt;</span>1037<span class="tag">&lt;/<span class="name">genstamp</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">numBytes</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">numBytes</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">block</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">block</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">id</span>&gt;</span>1073741862<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">genstamp</span>&gt;</span>1038<span class="tag">&lt;/<span class="name">genstamp</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">numBytes</span>&gt;</span>60795424<span class="tag">&lt;/<span class="name">numBytes</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">block</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">blocks</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">storagePolicyId</span>&gt;</span>0<span class="tag">&lt;/<span class="name">storagePolicyId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">inode</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">INodeSection</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">INodeDirectorySection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">directory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">parent</span>&gt;</span>16385<span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16470<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16475<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16386<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16507<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16392<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16403<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16430<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16453<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">directory</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">directory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">parent</span>&gt;</span>16470<span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16484<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16473<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16471<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16474<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16477<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16478<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16480<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16472<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">directory</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">INodeDirectorySection</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<blockquote>
<p>思考：可以看出，Fsimage中没有记录块所对应DataNode，为什么？<br>  在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报。</p>
</blockquote>
</li>
<li>oev查看Edits文件<ol>
<li>基本语法<br> <code>hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径</code></li>
<li>操作 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 current]$ hdfs oev -p xml -i edits_0000000000000000666-0000000000000000666 -o ~/fsimage/edits.xml</span><br><span class="line">[atguigu@hadoop001 current]$ ll ~/fsimage/edits.xml</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 221 10月 12 12:05 /home/atguigu/fsimage/edits.xml</span><br><span class="line">[atguigu@hadoop001 current]$</span><br></pre></td></tr></table></figure></li>
<li>内容 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot;?&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">EDITS</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">EDITS_VERSION</span>&gt;</span>-64<span class="tag">&lt;/<span class="name">EDITS_VERSION</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_START_LOG_SEGMENT<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>328<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_MKDIR<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>329<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">INODEID</span>&gt;</span>16470<span class="tag">&lt;/<span class="name">INODEID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/sanguo<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TIMESTAMP</span>&gt;</span>1633832427513<span class="tag">&lt;/<span class="name">TIMESTAMP</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">USERNAME</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">USERNAME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">GROUPNAME</span>&gt;</span>supergroup<span class="tag">&lt;/<span class="name">GROUPNAME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">MODE</span>&gt;</span>493<span class="tag">&lt;/<span class="name">MODE</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ADD<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>330<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">INODEID</span>&gt;</span>16471<span class="tag">&lt;/<span class="name">INODEID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/sanguo/jdk-8u212-linux-x64.tar.gz._COPYING_<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">REPLICATION</span>&gt;</span>3<span class="tag">&lt;/<span class="name">REPLICATION</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">MTIME</span>&gt;</span>1633832462757<span class="tag">&lt;/<span class="name">MTIME</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">ATIME</span>&gt;</span>1633832462757<span class="tag">&lt;/<span class="name">ATIME</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">BLOCKSIZE</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">BLOCKSIZE</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">CLIENT_NAME</span>&gt;</span>DFSClient_NONMAPREDUCE_-1241738550_1<span class="tag">&lt;/<span class="name">CLIENT_NAME</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">CLIENT_MACHINE</span>&gt;</span>192.168.2.6<span class="tag">&lt;/<span class="name">CLIENT_MACHINE</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">OVERWRITE</span>&gt;</span>true<span class="tag">&lt;/<span class="name">OVERWRITE</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">USERNAME</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">USERNAME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">GROUPNAME</span>&gt;</span>supergroup<span class="tag">&lt;/<span class="name">GROUPNAME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">MODE</span>&gt;</span>420<span class="tag">&lt;/<span class="name">MODE</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">ERASURE_CODING_POLICY_ID</span>&gt;</span>0<span class="tag">&lt;/<span class="name">ERASURE_CODING_POLICY_ID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">RPC_CLIENTID</span>&gt;</span>a0a774c6-277a-43ec-9f31-a8c2cbcaa322<span class="tag">&lt;/<span class="name">RPC_CLIENTID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">RPC_CALLID</span>&gt;</span>3<span class="tag">&lt;/<span class="name">RPC_CALLID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ALLOCATE_BLOCK_ID<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>331<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741861<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_SET_GENSTAMP_V2<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>332<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">GENSTAMPV2</span>&gt;</span>1037<span class="tag">&lt;/<span class="name">GENSTAMPV2</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ADD_BLOCK<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>333<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/sanguo/jdk-8u212-linux-x64.tar.gz._COPYING_<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741861<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">NUM_BYTES</span>&gt;</span>0<span class="tag">&lt;/<span class="name">NUM_BYTES</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">GENSTAMP</span>&gt;</span>1037<span class="tag">&lt;/<span class="name">GENSTAMP</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">RPC_CLIENTID</span>/&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">RPC_CALLID</span>&gt;</span>-2<span class="tag">&lt;/<span class="name">RPC_CALLID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ALLOCATE_BLOCK_ID<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>334<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741862<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_SET_GENSTAMP_V2<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>335<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">GENSTAMPV2</span>&gt;</span>1038<span class="tag">&lt;/<span class="name">GENSTAMPV2</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ADD_BLOCK<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>336<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/sanguo/jdk-8u212-linux-x64.tar.gz._COPYING_<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741861<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">NUM_BYTES</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">NUM_BYTES</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">GENSTAMP</span>&gt;</span>1037<span class="tag">&lt;/<span class="name">GENSTAMP</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741862<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">NUM_BYTES</span>&gt;</span>0<span class="tag">&lt;/<span class="name">NUM_BYTES</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">GENSTAMP</span>&gt;</span>1038<span class="tag">&lt;/<span class="name">GENSTAMP</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">RPC_CLIENTID</span>/&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">RPC_CALLID</span>&gt;</span>-2<span class="tag">&lt;/<span class="name">RPC_CALLID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_CLOSE<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>337<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">INODEID</span>&gt;</span>0<span class="tag">&lt;/<span class="name">INODEID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/sanguo/jdk-8u212-linux-x64.tar.gz._COPYING_<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">REPLICATION</span>&gt;</span>3<span class="tag">&lt;/<span class="name">REPLICATION</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">MTIME</span>&gt;</span>1633832463721<span class="tag">&lt;/<span class="name">MTIME</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">ATIME</span>&gt;</span>1633832462757<span class="tag">&lt;/<span class="name">ATIME</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">BLOCKSIZE</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">BLOCKSIZE</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">CLIENT_NAME</span>/&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">CLIENT_MACHINE</span>/&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">OVERWRITE</span>&gt;</span>false<span class="tag">&lt;/<span class="name">OVERWRITE</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741861<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">NUM_BYTES</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">NUM_BYTES</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">GENSTAMP</span>&gt;</span>1037<span class="tag">&lt;/<span class="name">GENSTAMP</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741862<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">NUM_BYTES</span>&gt;</span>60795424<span class="tag">&lt;/<span class="name">NUM_BYTES</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">GENSTAMP</span>&gt;</span>1038<span class="tag">&lt;/<span class="name">GENSTAMP</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">USERNAME</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">USERNAME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">GROUPNAME</span>&gt;</span>supergroup<span class="tag">&lt;/<span class="name">GROUPNAME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">MODE</span>&gt;</span>420<span class="tag">&lt;/<span class="name">MODE</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_RENAME_OLD<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>338<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">SRC</span>&gt;</span>/sanguo/jdk-8u212-linux-x64.tar.gz._COPYING_<span class="tag">&lt;/<span class="name">SRC</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">DST</span>&gt;</span>/sanguo/jdk-8u212-linux-x64.tar.gz<span class="tag">&lt;/<span class="name">DST</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TIMESTAMP</span>&gt;</span>1633832463726<span class="tag">&lt;/<span class="name">TIMESTAMP</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">RPC_CLIENTID</span>&gt;</span>a0a774c6-277a-43ec-9f31-a8c2cbcaa322<span class="tag">&lt;/<span class="name">RPC_CLIENTID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">RPC_CALLID</span>&gt;</span>9<span class="tag">&lt;/<span class="name">RPC_CALLID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_END_LOG_SEGMENT<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>339<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">EDITS</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考：NameNode如何确定下次开机启动的时候合并哪些Edits？<br>通过最新合并的fsimage_的序号（例如fsimage_0000000000000000584）和seen_txid存放的序号（如585）取它们中间的edits序号合并即可<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340122811449.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</blockquote>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<hr>
<h2 id="5-3-CheckPoint时间设置"><a href="#5-3-CheckPoint时间设置" class="headerlink" title="5.3 CheckPoint时间设置"></a>5.3 CheckPoint时间设置</h2><p>配置文件：hdfs-default.xml</p>
<ol>
<li>默认配置SecondaryNameNode一小时同步一次 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>一分钟检查一次操作次数，达到100W次触发一次同步 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">&lt;/property &gt;</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h2 id="5-4-NameNode故障处理"><a href="#5-4-NameNode故障处理" class="headerlink" title="5.4 NameNode故障处理"></a>5.4 NameNode故障处理</h2><h3 id="5-4-1-使用SecondaryNameNode中的数据恢复"><a href="#5-4-1-使用SecondaryNameNode中的数据恢复" class="headerlink" title="5.4.1 使用SecondaryNameNode中的数据恢复"></a>5.4.1 使用SecondaryNameNode中的数据恢复</h3><p>将SecondaryNameNode中的数据拷贝到NameNode存储元数据的位置</p>
<ol>
<li>杀掉NameNode进程 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 name]$ jps</span><br><span class="line">1921 NodeManager</span><br><span class="line">4834 NameNode</span><br><span class="line">1638 DataNode</span><br><span class="line">4909 Jps</span><br><span class="line">2095 JobHistoryServer</span><br><span class="line">[atguigu@hadoop001 name]$ <span class="built_in">kill</span> -9 1921</span><br><span class="line">[atguigu@hadoop001 name]$ jps</span><br><span class="line">4834 NameNode</span><br><span class="line">1638 DataNode</span><br><span class="line">4922 Jps</span><br><span class="line">2095 JobHistoryServer</span><br><span class="line">[atguigu@hadoop001 name]$</span><br></pre></td></tr></table></figure></li>
<li>删除NameNode数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 name]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-3.1.3/data/dfs/name</span><br><span class="line">[atguigu@hadoop001 name]$ ls</span><br><span class="line">current  in_use.lock</span><br><span class="line">[atguigu@hadoop001 name]$ rm -rf *</span><br><span class="line">[atguigu@hadoop001 name]$ ll</span><br><span class="line">总用量 0</span><br><span class="line">[atguigu@hadoop001 name]$</span><br></pre></td></tr></table></figure></li>
<li>拷贝SecondaryNameNode数据到NameNode<br> SecondaryNameNode: <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop003 namesecondary]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-3.1.3/data/dfs/namesecondary</span><br><span class="line">[atguigu@hadoop003 namesecondary]$ ls</span><br><span class="line">current  in_use.lock</span><br><span class="line">[atguigu@hadoop003 namesecondary]$</span><br></pre></td></tr></table></figure>
 NameNode: <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 name]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-3.1.3/data/dfs/name</span><br><span class="line">[atguigu@hadoop001 name]$ scp -r hadoop003:/opt/module/hadoop-3.1.3/data/dfs/namesecondary/* ./</span><br><span class="line">edits_0000000000000000415-0000000000000000416          100%   42    71.3KB/s   00:00</span><br><span class="line">edits_0000000000000000618-0000000000000000623          100%  311   726.8KB/s   00:00</span><br><span class="line">edits_0000000000000000671-0000000000000000672          100%   42    96.1KB/s   00:00</span><br><span class="line">edits_0000000000000000002-0000000000000000217          100%   27KB  28.4MB/s   00:00</span><br><span class="line">edits_0000000000000000664-0000000000000000665          100%   42    96.8KB/s   00:00</span><br><span class="line">edits_0000000000000000667-0000000000000000668          100%   42   101.8KB/s   00:00</span><br><span class="line">VERSION                                                100%  215   519.5KB/s   00:00</span><br><span class="line">edits_0000000000000000218-0000000000000000219          100%   42   105.8KB/s   00:00</span><br><span class="line">edits_0000000000000000410-0000000000000000411          100%   42   102.3KB/s   00:00</span><br><span class="line">edits_0000000000000000645-0000000000000000649          100%  291   761.2KB/s   00:00</span><br><span class="line">edits_0000000000000000662-0000000000000000663          100%   42   113.7KB/s   00:00</span><br><span class="line">edits_0000000000000000220-0000000000000000221          100%   42   111.9KB/s   00:00</span><br><span class="line">edits_0000000000000000412-0000000000000000413          100%   42   102.2KB/s   00:00</span><br><span class="line">edits_0000000000000000654-0000000000000000655          100%   42   105.8KB/s   00:00</span><br><span class="line">edits_0000000000000000222-0000000000000000223          100%   42   102.6KB/s   00:00</span><br><span class="line">edits_0000000000000000408-0000000000000000409          100%   42   106.7KB/s   00:00</span><br><span class="line">edits_0000000000000000656-0000000000000000657          100%   42   109.0KB/s   00:00</span><br><span class="line">edits_0000000000000000225-0000000000000000325          100% 1024KB  88.8MB/s   00:00</span><br><span class="line">edits_0000000000000000326-0000000000000000327          100%   42   130.9KB/s   00:00</span><br><span class="line">edits_0000000000000000639-0000000000000000644          100%  310     1.0MB/s   00:00</span><br><span class="line">edits_0000000000000000650-0000000000000000651          100%   42   164.8KB/s   00:00</span><br><span class="line">edits_0000000000000000669-0000000000000000670          100%   42   170.4KB/s   00:00</span><br><span class="line">edits_0000000000000000328-0000000000000000339          100%  902     3.5MB/s   00:00</span><br><span class="line">fsimage_0000000000000000670                            100% 4452    13.4MB/s   00:00</span><br><span class="line">fsimage_0000000000000000672.md5                        100%   62   255.0KB/s   00:00</span><br><span class="line">edits_0000000000000000340-0000000000000000382          100% 3512    11.5MB/s   00:00</span><br><span class="line">edits_0000000000000000417-0000000000000000525          100% 6663    19.3MB/s   00:00</span><br><span class="line">edits_0000000000000000658-0000000000000000659          100%   42   166.4KB/s   00:00</span><br><span class="line">fsimage_0000000000000000670.md5                        100%   62   251.4KB/s   00:00</span><br><span class="line">edits_0000000000000000383-0000000000000000405          100% 1708     5.9MB/s   00:00</span><br><span class="line">edits_0000000000000000526-0000000000000000617          100% 5206    14.9MB/s   00:00</span><br><span class="line">edits_0000000000000000660-0000000000000000661          100%   42   168.7KB/s   00:00</span><br><span class="line">fsimage_0000000000000000672                            100% 4452    13.6MB/s   00:00</span><br><span class="line">edits_0000000000000000406-0000000000000000407          100%   42   167.7KB/s   00:00</span><br><span class="line">edits_0000000000000000624-0000000000000000638          100%  934     3.6MB/s   00:00</span><br><span class="line">edits_0000000000000000652-0000000000000000653          100%   42   162.7KB/s   00:00</span><br><span class="line">in_use.lock                                            100%   14    53.2KB/s   00:00</span><br><span class="line">[atguigu@hadoop001 name]$ ls</span><br><span class="line">current  in_use.lock</span><br><span class="line">[atguigu@hadoop001 name]$</span><br></pre></td></tr></table></figure></li>
<li>重新启动NameNode <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 name]$ jps</span><br><span class="line">4977 Jps</span><br><span class="line">1638 DataNode</span><br><span class="line">2095 JobHistoryServer</span><br><span class="line">[atguigu@hadoop001 name]$ hdfs --daemon start namenode</span><br><span class="line">[atguigu@hadoop001 name]$ jps</span><br><span class="line">1638 DataNode</span><br><span class="line">5030 NameNode</span><br><span class="line">5101 Jps</span><br><span class="line">2095 JobHistoryServer</span><br><span class="line">[atguigu@hadoop001 name]$</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="5-4-2-使用importCheckpoint恢复"><a href="#5-4-2-使用importCheckpoint恢复" class="headerlink" title="5.4.2 使用importCheckpoint恢复"></a>5.4.2 使用importCheckpoint恢复</h3><p>使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。</p>
<ol>
<li>修改hdfs-site.xml <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>120<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>杀掉NameNode进程</li>
<li>删除NameNode存储的数据（/opt/module/hadoop-3.1.3/data/dfs/name） <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ rm -rf /opt/module/hadoop-3.1.3/data/dfs/name/*</span><br></pre></td></tr></table></figure></li>
<li>如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 dfs]$ scp -r atguigu@hadoop104:/opt/module/hadoop-3.1.3/data/dfs/namesecondary ./</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 namesecondary]$ rm -rf in_use.lock</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 dfs]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-3.1.3/data/dfs</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 dfs]$ ls</span><br><span class="line">data  name  namesecondary</span><br></pre></td></tr></table></figure></li>
<li>导入检查点数据（等待一会ctrl+c结束掉） <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ bin/hdfs namenode -importCheckpoint</span><br></pre></td></tr></table></figure></li>
<li>启动NameNode <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hdfs --daemon start namenode</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h2 id="5-5-集群安全模式"><a href="#5-5-集群安全模式" class="headerlink" title="5.5 集群安全模式"></a>5.5 集群安全模式</h2><ol>
<li>NameNode启动：NameNode启动时，首先降镜像文件FsImage载入内存，并执行编辑日志Edits中的各项操作。在内存中成功建立完整的文件系统元数据之后，创建一个空的编辑日志，生成新的Fsimage文件。此时，NameNode开始监听DateNode请求，这个过程期间，NameNode一直运行安全模式，即对于客户端来说是只读的</li>
<li>DateNode启动：系统中的数据块位置不由NameNode维护，而是以块列表的形式存储在DateNode中。在系统的正常操作期间，NameNode会在内存中保留所有块位置的信息。在安全模式下，各个DateNode会向NameNode发送最新的块列表信息，NomeNode获取到足够多的块信息之后，即可高效运行文件系统</li>
<li>安全模式退出判断：如果满足<font color ='red' >最小副本条件</font>（在整个文件系统中99.9%的块满足最小副本级别（默认值：dfs.replication.min=1））。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以NameNode不会进入安全模式。</li>
<li>手动进入安全模式 <ol>
<li>基本语法<br> 集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看安全模式状态</span></span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode get</span><br><span class="line">Safe mode is OFF</span><br><span class="line"><span class="comment"># 进入安全模式</span></span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode enter</span><br><span class="line">Safe mode is ON</span><br><span class="line"><span class="comment"># 查看安全模式状态</span></span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode get</span><br><span class="line">Safe mode is ON</span><br><span class="line"><span class="comment"># 离开安全模式</span></span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode leave</span><br><span class="line">Safe mode is OFF</span><br><span class="line"><span class="comment"># 查看安全模式状态</span></span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode get</span><br><span class="line">Safe mode is OFF</span><br><span class="line"><span class="comment"># 进入安全模式</span></span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode enter</span><br><span class="line">Safe mode is ON</span><br><span class="line"><span class="comment"># 等待安全模式退出，阻塞当前进程</span></span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode <span class="built_in">wait</span></span><br><span class="line"></span><br><span class="line">^C</span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode leave</span><br><span class="line">Safe mode is OFF</span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode get</span><br><span class="line">Safe mode is OFF</span><br><span class="line"><span class="comment"># 安全模式退出，wait进程继续</span></span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode <span class="built_in">wait</span></span><br><span class="line">Safe mode is OFF</span><br><span class="line">[atguigu@hadoop001 name]$</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>示例<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340146742555.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ol>
</li>
</ol>
<hr>
<h2 id="5-6-NameNode多目录配置"><a href="#5-6-NameNode多目录配置" class="headerlink" title="5.6 NameNode多目录配置"></a>5.6 NameNode多目录配置</h2><ol>
<li>NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性（不同目录最好分布在不同的磁盘）</li>
<li>具体配置如下<ol>
<li>在hdfs-site.xml文件中添加如下内容 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/dfs/name1,file://$&#123;hadoop.tmp.dir&#125;/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>停止集群，删除三台节点的data和logs中所有数据。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ rm -rf data/ logs/</span><br><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ rm -rf data/ logs/</span><br><span class="line">[atguigu@hadoop104 hadoop-3.1.3]$ rm -rf data/ logs/</span><br></pre></td></tr></table></figure></li>
<li>格式化集群并启动。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ bin/hdfs namenode -format</span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure></li>
<li>查看结果 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 dfs]$ ll</span><br><span class="line">总用量 12</span><br><span class="line">drwx------. 3 atguigu atguigu 4096 12月 11 08:03 data</span><br><span class="line">drwxrwxr-x. 3 atguigu atguigu 4096 12月 11 08:03 name1</span><br><span class="line">drwxrwxr-x. 3 atguigu atguigu 4096 12月 11 08:03 name2</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<hr>
<h1 id="六、DataNode"><a href="#六、DataNode" class="headerlink" title="六、DataNode"></a>六、DataNode</h1><h2 id="6-1-DateNode工作机制"><a href="#6-1-DateNode工作机制" class="headerlink" title="6.1 DateNode工作机制"></a>6.1 DateNode工作机制</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340068224418.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。</li>
<li>DataNode启动后向NameNode注册，通过后，周期性（6小时）的向NameNode上报所有的块信息。 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">hdfs-default.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blockreport.intervalMsec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>21600000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Determines block reporting interval in milliseconds.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">hdfs-default.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    Determines datanode heartbeat interval in seconds.</span><br><span class="line">    Can use the following suffix (case insensitive):</span><br><span class="line">    ms(millis), s(sec), m(min), h(hour), d(day)</span><br><span class="line">    to specify the time (such as 2s, 2m, 1h, etc.).</span><br><span class="line">    Or provide complete number in seconds (such as 30 for 30 seconds).</span><br><span class="line">    If no time unit is specified then seconds is assumed.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>如果超过10分钟30秒没有收到某个DataNode的心跳，则认为该节点不可用。</li>
<li>集群运行中可以安全加入和退出一些机器。</li>
</ol>
<hr>
<h2 id="6-2-数据完整性"><a href="#6-2-数据完整性" class="headerlink" title="6.2 数据完整性"></a>6.2 数据完整性</h2><p>思考：如果电脑磁盘里面存储的数据是控制高铁信号灯的红灯信号（1）和绿灯信号（0），但是存储该数据的磁盘坏了，一直显示是绿灯，是否很危险？同理DataNode节点上的数据损坏了，却没有发现，是否也很危险，那么如何解决呢？如下是DataNode节点保证数据完整性的方法。</p>
<ol>
<li>当DataNode读取Block的时候，它会计算CheckSum。</li>
<li>如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。</li>
<li>Client读取其他DataNode上的Block。</li>
<li>常见的校验算法 crc（32），md5（128），sha1（160）</li>
<li>DataNode在其文件创建后周期验证CheckSum。</li>
</ol>
<hr>
<h2 id="6-3-掉线时限参数设置"><a href="#6-3-掉线时限参数设置" class="headerlink" title="6.3 掉线时限参数设置"></a>6.3 掉线时限参数设置</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340157434025.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>效果如图<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340162105064.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ol>
<hr>
<h2 id="6-4-服役新节点"><a href="#6-4-服役新节点" class="headerlink" title="6.4 服役新节点"></a>6.4 服役新节点</h2><ol>
<li>需求：<br> 随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点</li>
<li>环境准备<ol>
<li>在hadoop104主机上再克隆一台hadoop105主机</li>
<li>修改IP地址和主机名称</li>
<li>删除原来HDFS文件系统留存的文件（/opt/module/hadoop-3.1.3/data和logs）</li>
<li>source一下配置文件 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop105 hadoop-3.1.3]$ <span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>具体步骤<ol>
<li>直接启动DataNode，即可关联到集群 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop105 hadoop-3.1.3]$ hdfs --daemon start datanode</span><br><span class="line">[atguigu@hadoop105 hadoop-3.1.3]$ yarn --daemon start nodemanager</span><br></pre></td></tr></table></figure></li>
<li>在hadoop105上上传文件 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop105 hadoop-3.1.3]$ hadoop fs -put /opt/module/hadoop-3.1.3/LICENSE.txt /</span><br></pre></td></tr></table></figure></li>
<li>如果数据不均衡，可以用命令实现集群的再平衡 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 sbin]$ ./start-balancer.sh</span><br><span class="line">starting balancer, logging to /opt/module/hadoop-3.1.3/logs/hadoop-atguigu-balancer-hadoop102.out</span><br><span class="line">Time Stamp               Iteration<span class="comment">#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<hr>
<h2 id="6-5-退役旧数据节点"><a href="#6-5-退役旧数据节点" class="headerlink" title="6.5 退役旧数据节点"></a>6.5 退役旧数据节点</h2><h3 id="6-5-1-添加白名单和黑名单"><a href="#6-5-1-添加白名单和黑名单" class="headerlink" title="6.5.1 添加白名单和黑名单"></a>6.5.1 添加白名单和黑名单</h3><ul>
<li>白名单和黑名单是hadoop管理集群主机的一种机制。</li>
<li>添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出。</li>
<li>添加到黑名单的主机节点，不允许访问NameNode，会在数据迁移后退出。</li>
<li>实际情况下，白名单用于确定允许访问NameNode的DataNode节点，内容配置一般与workers文件内容一致。 黑名单用于在集群运行过程中退役DataNode节点。<br>配置白名单和黑名单的具体步骤如下：<ol>
<li>在NameNode节点的/opt/module/hadoop-3.1.3/etc/hadoop目录下分别创建whitelist 和blacklist文件，白名单whitelist添加节点内容，blacklist黑名单暂时为空。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-3.1.3/etc/hadoop</span><br><span class="line">[atguigu@hadoop001 hadoop]$ touch blacklist</span><br><span class="line">[atguigu@hadoop001 hadoop]$ touch whitelist</span><br><span class="line">[atguigu@hadoop001 hadoop]$ ll *list</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 0 10月 12 18:50 blacklist</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 0 10月 12 18:50 whitelist</span><br><span class="line">[atguigu@hadoop001 hadoop]$ vim whitelist</span><br><span class="line">[atguigu@hadoop001 hadoop]$ cat whitelist</span><br><span class="line">hadoop001</span><br><span class="line">hadoop002</span><br><span class="line">hadoop003</span><br><span class="line">hadoop004</span><br><span class="line">[atguigu@hadoop001 hadoop]$</span><br></pre></td></tr></table></figure></li>
<li>在hdfs-site.xml配置文件中增加dfs.hosts和 dfs.hosts.exclude配置参数 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 白名单 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/etc/hadoop/whitelist<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 黑名单 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/etc/hadoop/blacklist<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>分发配置文件whitelist，blacklist，hdfs-site.xml (注意：004节点也要发一份) <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop]$ xsync /opt/module/hadoop-3.1.3/etc/hadoop/</span><br><span class="line">[atguigu@hadoop001 hadoop]$ scp -r * hadoop004:/opt/module/hadoop-3.1.3/etc/hadoop/</span><br></pre></td></tr></table></figure></li>
<li>重新启动集群(注意：105节点没有添加到workers，因此要单独起停) <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ stop-dfs.sh</span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ start-dfs.sh</span><br><span class="line">[atguigu@hadoop105 hadoop-3.1.3]$ hdfs –daemon start datanode</span><br></pre></td></tr></table></figure></li>
<li>在web浏览器上查看目前正常工作的DN节点<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340373303979.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><h3 id="6-5-2-黑名单退役"><a href="#6-5-2-黑名单退役" class="headerlink" title="6.5.2 黑名单退役"></a>6.5.2 黑名单退役</h3></li>
</ol>
</li>
</ul>
<ol>
<li>编辑/opt/module/hadoop-3.1.3/etc/hadoop目录下的blacklist文件 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop]$ vim blacklist</span><br><span class="line">[atguigu@hadoop001 hadoop]$ cat blacklist</span><br><span class="line">hadoop004</span><br><span class="line">[atguigu@hadoop001 hadoop]$</span><br></pre></td></tr></table></figure></li>
<li>分发blacklist到所有节点 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop]$ xsync /opt/module/hadoop-3.1.3/etc/hadoop/</span><br><span class="line">[atguigu@hadoop001 hadoop]$ scp -r * hadoop004:/opt/module/hadoop-3.1.3/etc/hadoop/</span><br></pre></td></tr></table></figure></li>
<li>刷新NameNode、刷新ResourceManager <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop]$ hdfs dfsadmin -refreshNodes</span><br><span class="line">Refresh nodes successful</span><br><span class="line">[atguigu@hadoop001 hadoop]$ yarn rmadmin -refreshNodes</span><br><span class="line">2021-10-12 19:20:20,896 INFO client.RMProxy: Connecting to ResourceManager at hadoop002/192.168.2.7:8033</span><br><span class="line">[atguigu@hadoop001 hadoop]$</span><br></pre></td></tr></table></figure></li>
<li>检查Web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点<img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340378419483.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役<img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340378878057.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>如果数据不均衡，可以用命令实现集群的再平衡 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop-3.1.3]$ sbin/start-balancer.sh</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：不允许白名单和黑名单中同时出现同一个主机名称，既然使用了黑名单blacklist成功退役了hadoop105节点，因此要将白名单whitelist里面的hadoop105去掉。</p>
</blockquote>
</li>
</ol>
<hr>
<h2 id="6-6-DataNode多目录配置"><a href="#6-6-DataNode多目录配置" class="headerlink" title="6.6 DataNode多目录配置"></a>6.6 DataNode多目录配置</h2><ol>
<li>DataNode可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本</li>
<li>具体配置如下<ol>
<li>在hdfs-site.xml文件中添加如下内容 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/dfs/data1,file://$&#123;hadoop.tmp.dir&#125;/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>停止集群，删除三台节点的data和logs中所有数据。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ rm -rf data/ logs/</span><br><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ rm -rf data/ logs/</span><br><span class="line">[atguigu@hadoop104 hadoop-3.1.3]$ rm -rf data/ logs/</span><br></pre></td></tr></table></figure></li>
<li>格式化集群并启动。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ bin/hdfs namenode –format</span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure></li>
<li>查看结果 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 dfs]$ ll</span><br><span class="line">总用量 12</span><br><span class="line">drwx------. 3 atguigu atguigu 4096 4月   4 14:22 data1</span><br><span class="line">drwx------. 3 atguigu atguigu 4096 4月   4 14:22 data2</span><br><span class="line">drwxrwxr-x. 3 atguigu atguigu 4096 12月 11 08:03 name1</span><br><span class="line">drwxrwxr-x. 3 atguigu atguigu 4096 12月 11 08:03 name2</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<hr>
<h2 id="问答"><a href="#问答" class="headerlink" title="问答"></a>问答</h2><h3 id="一、描述一下HDFS的数据写入流程"><a href="#一、描述一下HDFS的数据写入流程" class="headerlink" title="一、描述一下HDFS的数据写入流程"></a>一、描述一下HDFS的数据写入流程</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340443194587.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>客户端调用DistributedFileSystem#create方法通过DfsClient请求NameNode上传文件</li>
<li>NameNode校验：权限，路径，文件名通过后在对应路径创建空文件，写入Edits操作记录，返回文件信息</li>
<li>DistributedFileSystem#create 返回值为FSDataOutputStream输出流，FSDataOutputStream#create方法会初始化DFSOutputStream和DataStreamer；</li>
<li>调用DFSOutputStream#addBlock请求NN申请新的Block</li>
<li>NN根据机架感知-网络拓扑计算节点距离返回合适的DN列表</li>
<li>客户端调用FSDataOutputStream.PositionCache#write -&gt; DFSOutputStream#writeChunk写入数据</li>
<li>读取满一个packet(128个chunk)放入DFSOutputStream#enqueueCurrentPacketFull</li>
<li>DataStreamer#setPipeline初始化pipeline 首先和最近的DN1建立连接，然后DN1传输给DN2,DN2传输给DN3</li>
<li>DataStreamer把packet从dataqueue移动到ackqueue</li>
<li>DN3校验后返回ack给DN2,DN2校验后返回ack给DN1,DN1校验后返回ack给客户端</li>
<li>ResponseProcessor.ResponseProcessor接收响应并把packet从ackqueue移除</li>
<li>出现异常后ackqueue会重新移动到dataqueue再次创建连接重新发送</li>
<li>全部packet发送完成后当前block上传结束</li>
<li>读取下一个block重复4-13步骤</li>
<li>全部读取完成通知NN上传结束</li>
</ol>
<ul>
<li>block(文件块): 类似Linux中常见文件系统的最小操作单位, HDFS也是如此, block就是HDFS操作文件的最小单元, 默认128MB/个 (逻辑上设置的大小)</li>
<li>chunk(校验块): 文件块(block)实际存储/校验的最小单位, 严谨说chunk分为数据域和校验域两个部分组成, 但一般情况大家指的是数据域<ul>
<li>数据域: 默认512字节, 比如你写513个字节数据, 它占据一个block (而此block内部是两个chunk组成) , 又称为chunk data</li>
<li>校验域: 固定4字节, 存放校验码, 它与数据域一一对应, 又称chunk checksum</li>
</ul>
</li>
<li>packet(数据包): HDFS各组件间数据传输的基本单位, 默认64KB, 类似网络传输中数据包的概念, 主要是 header + body/data 两个部分组成<ul>
<li>packet header : 存储传输数据过程中的一些基本信息(数据包长度/版本/标志位等), 变长</li>
<li>packet body: 存储实际传输的数据(chunk), 同样packet里实际分布的最小单位也是chunk</li>
</ul>
</li>
</ul>
<h3 id="二、描述一下HDFS的数据读取流程"><a href="#二、描述一下HDFS的数据读取流程" class="headerlink" title="二、描述一下HDFS的数据读取流程"></a>二、描述一下HDFS的数据读取流程</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340854804776.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>客户端向namenode发RPC请求, 读取文件对应blocks的DN信息</li>
<li>namenode检查文件是否存在，如果存在则获取文件的元信息（按顺序返回BlockId以及对应所在的datanode列表）</li>
<li>客户端收到block对应的DN信息后选取一个网络访问最近的DN, 依次读取每个数据块(客户端这里有校验逻辑, 如果发现异常会汇报坏块)</li>
<li>客户端与DN建立socket连接，通过流以packet为单位传输对应的数据块, 客户端收到数据写入本地磁盘</li>
<li>依次传输剩下的数据块，直到整个文件读取完成, 最后关闭输入流</li>
</ol>
<h3 id="三、简述HDFS的架构，其中每个服务的作用"><a href="#三、简述HDFS的架构，其中每个服务的作用" class="headerlink" title="三、简述HDFS的架构，其中每个服务的作用"></a>三、简述HDFS的架构，其中每个服务的作用</h3><ol>
<li>NameNode: HDFS集群的管理者<ol>
<li>管理HDFS的命名空间</li>
<li>配置副本策略</li>
<li>维护元数据信息：文件基本属性，目录层级，文件-块的映射</li>
<li>处理客户端的读写请求</li>
</ol>
</li>
<li>DataNode：HDFS集群的数据存储节点<ol>
<li>存储Block数据</li>
<li>保证数据完整性</li>
<li>接受NameNode指令，执行实际操作</li>
<li>接受客户端请求读写Block</li>
</ol>
</li>
<li>SecondaryNameNode: NameNode的备份<ol>
<li>并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。</li>
<li>辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode</li>
<li>在紧急情况下，可辅助恢复NameNode。</li>
</ol>
</li>
<li>Client<ol>
<li>文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传</li>
<li>与NameNode交互，获取文件的位置信息</li>
<li>与DataNode交互，读取或者写入数据；</li>
<li>Client提供一些命令来管理HDFS，比如NameNode格式化</li>
<li>Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作</li>
</ol>
</li>
</ol>
<h3 id="四、HDFS中如何实现元数据的维护？"><a href="#四、HDFS中如何实现元数据的维护？" class="headerlink" title="四、HDFS中如何实现元数据的维护？"></a>四、HDFS中如何实现元数据的维护？</h3><ol>
<li>存储<ul>
<li>内存：保存全量元数据信息</li>
<li>磁盘：FsImage + Edits = 完整的元数据信息<ol>
<li>FsImage：NameNode内存中的元数据镜像</li>
<li>Edits：NameNode已经执行的的操作记录</li>
</ol>
</li>
</ul>
</li>
<li>NameNode启动操作<ol>
<li>加载FsImage到内存</li>
<li>在内存中根据seen_txid执行Edits记录生成完整元数据</li>
<li>内存中完整的元数据信息写入新的FsImage</li>
<li>生成新的Edits</li>
</ol>
</li>
<li>SecondaryNameNode定期合并<ol>
<li>触发合并时机<ol>
<li>默认配置一小时合并一次</li>
<li>每分钟检查操作次数，到达100W条，立即触发一次</li>
</ol>
</li>
<li>合并流程 <ol>
<li>SecondaryNameNode请求NameNode进行合并</li>
<li>NameNode停止使用当前的Edits文件，生成新的Edits文件继续执行操作</li>
<li>SecondaryNameNode拉取FsImage和Edits，加载到内存总合并，生成新的FsImage.checkpoint</li>
<li>SecondaryNameNode推送FsImage.checkpoint到NameNode</li>
<li>NameNode重命名FsImage.checkpoint为FsImage，合并流程结束</li>
</ol>
</li>
</ol>
</li>
<li>正常运行<ol>
<li>client对数据进行操作时，将这些操作记录到Edits</li>
</ol>
</li>
</ol>
<h3 id="五、2NN如何对NN的元数据进行合并？"><a href="#五、2NN如何对NN的元数据进行合并？" class="headerlink" title="五、2NN如何对NN的元数据进行合并？"></a>五、2NN如何对NN的元数据进行合并？</h3><ol>
<li>触发合并时机<ol>
<li>默认配置一小时合并一次</li>
<li>每分钟检查操作次数，到达100W条，立即触发一次</li>
</ol>
</li>
<li>合并流程 <ol>
<li>SecondaryNameNode请求NameNode进行合并</li>
<li>NameNode停止使用当前的Edits文件，生成新的Edits文件继续执行操作</li>
<li>SecondaryNameNode拉取FsImage和Edits，加载到内存总合并，生成新的FsImage.checkpoint</li>
<li>SecondaryNameNode推送FsImage.checkpoint到NameNode</li>
<li>NameNode重命名FsImage.checkpoint为FsImage，合并流程结束</li>
</ol>
</li>
</ol>
<h3 id="六、假如NN的数据发生的丢失，依靠2NN恢复完全靠谱？"><a href="#六、假如NN的数据发生的丢失，依靠2NN恢复完全靠谱？" class="headerlink" title="六、假如NN的数据发生的丢失，依靠2NN恢复完全靠谱？"></a>六、假如NN的数据发生的丢失，依靠2NN恢复完全靠谱？</h3><ol>
<li>可能会造成部分数据丢失原因如下：<ol>
<li>NN数据发生丢之前，完整元数据保存在内存中，在磁盘中Fsimage+edits=完整元数据</li>
<li>如果edits包含操作记录，且未合并到Fsimage，2NN中的Fsimage和NN的Fsimage相同</li>
<li>如果丢失edits，使用2NN的Fsimage恢复NN的Fsimage，会丢失NN的Edits部分的数据</li>
</ol>
</li>
</ol>
<h3 id="七、HDFS集群的安全模式有了解吗？"><a href="#七、HDFS集群的安全模式有了解吗？" class="headerlink" title="七、HDFS集群的安全模式有了解吗？"></a>七、HDFS集群的安全模式有了解吗？</h3><ol>
<li>HDFS集群的安全模式是对数据完整性的一种保护措施</li>
<li>NameNode启动进入安全模式，对客户端保持只读<ol>
<li>加载磁盘Fsimage到内存</li>
<li>执行edits操作记录，生成完整的元数据信息</li>
<li>生成新的edits</li>
<li>内存中的元数据信息，生成新的Fsimage</li>
<li>开始监听DateNode请求</li>
</ol>
</li>
<li>DataNode启动<ol>
<li>向NameNode注册，加入集群</li>
<li>注册成功向NameNode发送Block数据块信息</li>
</ol>
</li>
<li>NameNode接收到足够多的Block数据块位置信息退出安全模式<ol>
<li>满足“最小副本条件”， NameNode会在30秒钟之后就退出安全模式<ol>
<li>最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别（默认值：dfs.replication.min=1）</li>
</ol>
</li>
</ol>
</li>
<li>clien正常读写</li>
</ol>
<h3 id="八、如何在合理利用HDFS安全模式的操作命令完成一些工作？"><a href="#八、如何在合理利用HDFS安全模式的操作命令完成一些工作？" class="headerlink" title="八、如何在合理利用HDFS安全模式的操作命令完成一些工作？"></a>八、如何在合理利用HDFS安全模式的操作命令完成一些工作？</h3><ol>
<li>利用安全模式的wait命令实现等待集群恢复后自动执行某些操作</li>
<li><code>hdfs dfsadmin -safemode wait</code></li>
</ol>
<h3 id="九、描述一下NN和DN的关系，以及DN的工作流程"><a href="#九、描述一下NN和DN的关系，以及DN的工作流程" class="headerlink" title="九、描述一下NN和DN的关系，以及DN的工作流程"></a>九、描述一下NN和DN的关系，以及DN的工作流程</h3><ol>
<li>NN负责管理集群，维护元数据信息，接收客户端请求</li>
<li>DN负责保存Block数据块信息，保证数据完整性，接收客户端读写block请求</li>
<li>DataNode启动后会向NameNode注册，注册成功后，发送Block数据块的信息</li>
<li>DataNode运行过程中会周期性（6小时，hdfs-default.xml&gt;dfs.blockreport.intervalMsecp配置）的向NameNode上报Block数据块的信息</li>
<li>DataNode运行过程中每3秒（dfs.heartbeat.interval）会向NameNode发送心跳，并带回NameNode的命令</li>
<li>NameNode超过配置时间（2<em>心跳重新检测间隔默认五分钟+10</em>心跳间隔默认三秒=10分钟30秒）未收到心跳会下线DataNode</li>
<li>服役新节点通过配置白名单和works文件</li>
<li>退役节点通过配置黑名单，NameNode执行刷新集群，退役节点变成退役中（移动数据块），移动完成，变成退役状态</li>
</ol>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase</title>
    <url>/2021/11/07/HBase/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h1><h1 id="第1章-HBase简介"><a href="#第1章-HBase简介" class="headerlink" title="第1章 HBase简介"></a>第1章 HBase简介</h1><h2 id="1-1-HBase定义"><a href="#1-1-HBase定义" class="headerlink" title="1.1 HBase定义"></a>1.1 HBase定义</h2><p>HBase是一种分布式、可扩展、支持海量数据存储的NoSQL数据库。</p>
<ul>
<li>HBase是一种面向列簇存储的非关系型数据库。</li>
<li>用于存储结构化和非结构化的数据，适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。</li>
<li>基于HDFS，数据持久化存储的体现形式是HFile，存放于DataNode中，被ResionServer以region的形式进行管理。</li>
<li>延迟较低，接入在线业务使用，面对大量的企业数据，HBase可以实现单表大量数据的存储，同时提供了高效的数据访问速度。<h2 id="1-2-HBase数据模型"><a href="#1-2-HBase数据模型" class="headerlink" title="1.2 HBase数据模型"></a>1.2 HBase数据模型</h2>逻辑上，HBase的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。但从HBase的底层物理存储结构（K-V）来看，HBase更像是一个multi-dimensional map。</li>
</ul>
<h3 id="1-2-1-HBase逻辑结构"><a href="#1-2-1-HBase逻辑结构" class="headerlink" title="1.2.1 HBase逻辑结构"></a>1.2.1 HBase逻辑结构</h3><p><img src="https://i.loli.net/2021/11/07/MIeQBJnp3RSNElH.jpg"></p>
<h3 id="1-2-2HBase物理存储结构"><a href="#1-2-2HBase物理存储结构" class="headerlink" title="1.2.2HBase物理存储结构"></a>1.2.2HBase物理存储结构</h3><p><img src="https://i.loli.net/2021/11/07/BbljIXvo31RaptL.jpg"></p>
<h3 id="1-2-3-数据模型"><a href="#1-2-3-数据模型" class="headerlink" title="1.2.3 数据模型"></a>1.2.3 数据模型</h3><ol>
<li>Name Space<ul>
<li>命名空间，类似于关系型数据库的DatabBase概念，每个命名空间下有多个表。HBase有两个自带的命名空间，分别是hbase和default，hbase中存放的是HBase内置的表，default表是用户默认使用的命名空间。</li>
</ul>
</li>
<li>Region<ul>
<li>类似于关系型数据库的表概念。不同的是，HBase定义表时只需要声明列族即可，不需要声明具体的列。这意味着，往HBase写入数据时，字段可以动态、按需指定。因此，和关系型数据库相比，HBase能够轻松应对字段变更的场景。</li>
</ul>
</li>
<li>Row<ul>
<li>HBase表中的每行数据都由一个RowKey和多个Column（列）组成，数据是按照RowKey的字典顺序存储的，并且查询数据时只能根据RowKey进行检索，所以RowKey的设计十分重要。</li>
</ul>
</li>
<li>Row Key<ul>
<li>Rowkey 的概念和 mysql 中的主键类似，Hbase 使用 Rowkey 来唯一的区分某一行的数据。Hbase只支持3种查询方式： 1、基于Rowkey的单行查询，2、基于Rowkey的范围扫描 ，3、全表扫描</li>
<li>因此，Rowkey对Hbase的性能影响非常大。设计的时候要兼顾基于Rowkey的单行查询也要键入Rowkey的范围扫描。</li>
<li>Rowkey 行键可以是任意字符串(最大长度是64KB，实际应用中长度一般为 10-100bytes)，最好是16。在HBase 内部，Rowkey 保存为字节数组。HBase会对表中的数据按照 Rowkey 字典序排序</li>
</ul>
</li>
<li>Column Family（列簇）<ul>
<li>Hbase 通过列簇划分数据的存储，列簇下面可以包含任意多的列，实现灵活的数据存取。列簇是由一个一个的列组成（任意多），在列数据为空的情况下，不会占用存储空间。</li>
<li>Hbase 创建表的时候必须指定列簇。就像关系型数据库创建的时候必须指定具体的列是一样的。</li>
<li>Hbase的列簇不是越多越好，官方推荐的是列簇最好小于或者等于3。一般是1个列簇。</li>
<li>新的列簇成员（列）可以随后动态加入，Family下面可以有多个Qualifier，所以可以简单的理解为，HBase中的列是二级列，也就是说Family是第一级列，Qualifier是第二级列。</li>
<li>权限控制、存储以及调优都是在列簇层面进行的；</li>
<li>HBase把同一列簇里面的数据存储在同一目录下，由几个文件保存。</li>
</ul>
</li>
<li>Column<ul>
<li>HBase中的每个列都由Column Family(列族)和Column Qualifier（列限定符）进行限定，例如info：name，info：age。建表时，只需指明列族，而列限定符无需预先定义。</li>
</ul>
</li>
<li>Time Stamp<ul>
<li>用于标识数据的不同版本（version），每条数据写入时，如果不指定时间戳，系统会自动为其加上该字段，其值为写入HBase的时间。</li>
</ul>
</li>
<li>Cell<ul>
<li>由{rowkey, column Family：column Qualifier, time Stamp} 唯一确定的单元。cell中的数据是没有类型的，全部是字节数组形式存贮。</li>
</ul>
</li>
</ol>
<h2 id="1-3-HBase基本架构"><a href="#1-3-HBase基本架构" class="headerlink" title="1.3 HBase基本架构"></a>1.3 HBase基本架构</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16359030360669.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>Region Server<ul>
<li>Region Server为 Region的管理者，其实现类为HRegionServer，主要作用如下:<ul>
<li>对于数据的操作：get, put, delete；</li>
<li>对于Region的操作：splitRegion、compactRegion。</li>
</ul>
</li>
</ul>
</li>
<li>Master<ul>
<li>Master是所有Region Server的管理者，其实现类为HMaster，主要作用如下：<ul>
<li>对于表的操作：create, delete, alter</li>
<li>对于RegionServer的操作：分配regions到每个RegionServer，监控每个RegionServer的状态，负载均衡和故障转移。</li>
</ul>
</li>
</ul>
</li>
<li>Zookeeper<ul>
<li>HBase通过Zookeeper来做Master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作。</li>
</ul>
</li>
<li>HDFS<ul>
<li>HDFS为HBase提供最终的底层数据存储服务，同时为HBase提供高可用的支持。</li>
</ul>
</li>
</ol>
<h1 id="第2章-HBase快速入门"><a href="#第2章-HBase快速入门" class="headerlink" title="第2章 HBase快速入门"></a>第2章 HBase快速入门</h1><h2 id="2-1-HBase安装部署"><a href="#2-1-HBase安装部署" class="headerlink" title="2.1 HBase安装部署"></a>2.1 HBase安装部署</h2><h3 id="2-1-1-Zookeeper正常部署"><a href="#2-1-1-Zookeeper正常部署" class="headerlink" title="2.1.1 Zookeeper正常部署"></a>2.1.1 Zookeeper正常部署</h3><ul>
<li>首先保证Zookeeper集群的正常部署，并启动之：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 zookeeper-3.5.7]$ bin/zkServer.sh start</span><br><span class="line">[atguigu@hadoop103 zookeeper-3.5.7]$ bin/zkServer.sh start</span><br><span class="line">[atguigu@hadoop104 zookeeper-3.5.7]$ bin/zkServer.sh start</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2-1-2-Hadoop正常部署"><a href="#2-1-2-Hadoop正常部署" class="headerlink" title="2.1.2 Hadoop正常部署"></a>2.1.2 Hadoop正常部署</h3><ul>
<li>Hadoop集群的正常部署并启动：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh</span><br><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></li>
<li>集群启动异常<ol>
<li>时间同步 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ntpdate ntp.aliyun.com</span><br></pre></td></tr></table></figure></li>
<li>hdfs坏块 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">        </span><br></pre></td></tr></table></figure>
<h3 id="2-1-3-HBase的解压"><a href="#2-1-3-HBase的解压" class="headerlink" title="2.1.3 HBase的解压"></a>2.1.3 HBase的解压</h3></li>
</ol>
</li>
</ul>
<ol>
<li>解压Hbase到指定目录： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf hbase-2.2.4-bin.tar.gz -C /opt/module</span><br><span class="line">[atguigu@hadoop102 software]$ mv /opt/module/hbase-2.2.4 /opt/module/hbase</span><br></pre></td></tr></table></figure>
</li>
<li>配置环境变量 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment"># 添加</span></span><br><span class="line"><span class="comment">#HBASE_HOME</span></span><br><span class="line"><span class="built_in">export</span> HBASE_HOME=/opt/module/hbase</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HBASE_HOME</span>/bin</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="2-1-4-HBase的配置文件"><a href="#2-1-4-HBase的配置文件" class="headerlink" title="2.1.4 HBase的配置文件"></a>2.1.4 HBase的配置文件</h3>修改HBase对应的配置文件。</li>
<li>hbase-env.sh修改内容： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> HBASE_MANAGES_ZK=<span class="literal">false</span></span><br></pre></td></tr></table></figure></li>
<li>hbase-site.xml修改内容： <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:8020/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102,hadoop103,hadoop104<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.unsafe.stream.capability.enforce<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.wal.provider<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>filesystem<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>regionservers： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-1-5-HBase远程发送到其他集群"><a href="#2-1-5-HBase远程发送到其他集群" class="headerlink" title="2.1.5 HBase远程发送到其他集群"></a>2.1.5 HBase远程发送到其他集群</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ xsync hbase/</span><br></pre></td></tr></table></figure>

<h3 id="2-1-6-HBase服务的启动"><a href="#2-1-6-HBase服务的启动" class="headerlink" title="2.1.6 HBase服务的启动"></a>2.1.6 HBase服务的启动</h3><ol>
<li>启动方式1 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ bin/hbase-daemon.sh start master</span><br><span class="line">[atguigu@hadoop102 hbase]$ bin/hbase-daemon.sh start regionserver</span><br></pre></td></tr></table></figure>
<blockquote>
<p>提示：如果集群之间的节点时间不同步，会导致regionserver无法启动，抛出ClockOutOfSyncException异常。<br> 修复提示：</p>
</blockquote>
<ul>
<li>a、同步时间服务<ul>
<li>请参看帮助文档：《尚硅谷大数据技术之Hadoop入门》</li>
</ul>
</li>
<li>b、属性：hbase.master.maxclockskew设置更大的值  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.maxclockskew<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>180000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Time difference of regionserver from master<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>启动方式2 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">启动服务：</span><br><span class="line">[atguigu@hadoop102 hbase]$ bin/start-hbase.sh</span><br><span class="line">停止服务：</span><br><span class="line">[atguigu@hadoop102 hbase]$ bin/stop-hbase.sh</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-1-7-查看HBase页面"><a href="#2-1-7-查看HBase页面" class="headerlink" title="2.1.7 查看HBase页面"></a>2.1.7 查看HBase页面</h3><p>启动成功后，可以通过“host:port”的方式来访问HBase管理页面，例如：<br><a href="http://hadoop001:16010/">http://hadoop001:16010</a> </p>
<h3 id="2-1-8高可用-可选"><a href="#2-1-8高可用-可选" class="headerlink" title="2.1.8高可用(可选)"></a>2.1.8高可用(可选)</h3><p>在HBase中HMaster负责监控HRegionServer的生命周期，均衡RegionServer的负载，如果HMaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对HMaster的高可用配置。</p>
<ol>
<li>关闭HBase集群（如果没有开启则跳过此步） <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ bin/stop-hbase.sh</span><br></pre></td></tr></table></figure></li>
<li>在conf目录下创建backup-masters文件 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ touch conf/backup-masters</span><br></pre></td></tr></table></figure></li>
<li>在backup-masters文件中配置高可用HMaster节点 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ <span class="built_in">echo</span> hadoop103 &gt; conf/backup-masters</span><br></pre></td></tr></table></figure></li>
<li>将整个conf目录scp到其他节点 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ scp -r conf/ hadoop103:/opt/module/hbase/</span><br><span class="line">[atguigu@hadoop102 hbase]$ scp -r conf/ hadoop104:/opt/module/hbase/</span><br></pre></td></tr></table></figure></li>
<li>打开页面测试查看<br> <a href="http://hadooo102:16010/">http://hadooo102:16010</a> </li>
</ol>
<h2 id="2-2-HBase-Shell操作"><a href="#2-2-HBase-Shell操作" class="headerlink" title="2.2 HBase Shell操作"></a>2.2 HBase Shell操作</h2><h3 id="2-2-1-基本操作"><a href="#2-2-1-基本操作" class="headerlink" title="2.2.1 基本操作"></a>2.2.1 基本操作</h3><ol>
<li>进入HBase客户端命令行 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ bin/hbase shell</span><br></pre></td></tr></table></figure></li>
<li>查看帮助命令 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):001:0&gt; <span class="built_in">help</span></span><br></pre></td></tr></table></figure></li>
<li>查看当前数据库中有哪些表 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):002:0&gt; list</span><br></pre></td></tr></table></figure></li>
<li>查看命名空间 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):004:0&gt; list_namespace</span><br><span class="line">NAMESPACE</span><br><span class="line">api_test</span><br><span class="line">default</span><br><span class="line">hbase</span><br><span class="line">3 row(s)</span><br><span class="line">Took 0.0494 seconds</span><br><span class="line">hbase(main):005:0&gt;</span><br></pre></td></tr></table></figure></li>
<li>创建命名空间 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):005:0&gt; create_namespace <span class="string">&#x27;test&#x27;</span></span><br><span class="line">Took 0.2427 seconds</span><br><span class="line">hbase(main):006:0&gt; list_namespace</span><br><span class="line">NAMESPACE</span><br><span class="line">api_test</span><br><span class="line">default</span><br><span class="line">hbase</span><br><span class="line"><span class="built_in">test</span></span><br><span class="line">4 row(s)</span><br><span class="line">Took 0.0081 seconds</span><br><span class="line">hbase(main):007:0&gt;</span><br></pre></td></tr></table></figure></li>
<li>删除命名空间: 必须是空的 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):007:0&gt; drop_namespace <span class="string">&#x27;test&#x27;</span></span><br><span class="line">Took 0.2408 seconds</span><br><span class="line">hbase(main):008:0&gt; list_namespace</span><br><span class="line">NAMESPACE</span><br><span class="line">api_test</span><br><span class="line">default</span><br><span class="line">hbase</span><br><span class="line">3 row(s)</span><br><span class="line">Took 0.0102 seconds</span><br><span class="line">hbase(main):009:0&gt;</span><br></pre></td></tr></table></figure></li>
<li>查看命名空间中的表格 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):011:0&gt; list_namespace_tables <span class="string">&#x27;hbase&#x27;</span></span><br><span class="line">TABLE</span><br><span class="line">meta</span><br><span class="line">namespace</span><br><span class="line">2 row(s)</span><br><span class="line">Took 0.0051 seconds</span><br><span class="line">=&gt; [<span class="string">&quot;meta&quot;</span>, <span class="string">&quot;namespace&quot;</span>]</span><br><span class="line">hbase(main):012:0&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-2-2-表的操作"><a href="#2-2-2-表的操作" class="headerlink" title="2.2.2 表的操作"></a>2.2.2 表的操作</h3><ol>
<li>创建表 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):002:0&gt; create <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;info&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li>插入数据到表 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):003:0&gt; put <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1001&#x27;</span>,<span class="string">&#x27;info:sex&#x27;</span>,<span class="string">&#x27;male&#x27;</span></span><br><span class="line">hbase(main):004:0&gt; put <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1001&#x27;</span>,<span class="string">&#x27;info:age&#x27;</span>,<span class="string">&#x27;18&#x27;</span></span><br><span class="line">hbase(main):005:0&gt; put <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1002&#x27;</span>,<span class="string">&#x27;info:name&#x27;</span>,<span class="string">&#x27;Janna&#x27;</span></span><br><span class="line">hbase(main):006:0&gt; put <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1002&#x27;</span>,<span class="string">&#x27;info:sex&#x27;</span>,<span class="string">&#x27;female&#x27;</span></span><br><span class="line">hbase(main):007:0&gt; put <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1002&#x27;</span>,<span class="string">&#x27;info:age&#x27;</span>,<span class="string">&#x27;20&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>扫描查看表数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):008:0&gt; scan <span class="string">&#x27;student&#x27;</span></span><br><span class="line">hbase(main):009:0&gt; scan <span class="string">&#x27;student&#x27;</span>,&#123;STARTROW =&gt; <span class="string">&#x27;1001&#x27;</span>, STOPROW  =&gt; <span class="string">&#x27;1001&#x27;</span>&#125;</span><br><span class="line">hbase(main):010:0&gt; scan <span class="string">&#x27;student&#x27;</span>,&#123;STARTROW =&gt; <span class="string">&#x27;1001&#x27;</span>&#125;</span><br></pre></td></tr></table></figure></li>
<li>查看表结构 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):011:0&gt; describe <span class="string">&#x27;student&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li>更新指定字段的数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):012:0&gt; put <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1001&#x27;</span>,<span class="string">&#x27;info:name&#x27;</span>,<span class="string">&#x27;Nick&#x27;</span></span><br><span class="line">hbase(main):013:0&gt; put <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1001&#x27;</span>,<span class="string">&#x27;info:age&#x27;</span>,<span class="string">&#x27;100&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li>查看“指定行”或“指定列族:列”的数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):014:0&gt; get <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1001&#x27;</span></span><br><span class="line">hbase(main):015:0&gt; get <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1001&#x27;</span>,<span class="string">&#x27;info:name&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li>统计表数据行数 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):021:0&gt; count <span class="string">&#x27;student&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li>删除数据<ul>
<li>删除某rowkey的全部数据：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):016:0&gt; deleteall <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1001&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li>删除某rowkey的某一列数据：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):017:0&gt; delete <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1002&#x27;</span>,<span class="string">&#x27;info:sex&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>清空表数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):018:0&gt; truncate <span class="string">&#x27;student&#x27;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>提示：清空表的操作顺序为先disable，然后再truncate。</p>
</blockquote>
</li>
<li>删除表<ul>
<li>首先需要先让该表为disable状态：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):019:0&gt; <span class="built_in">disable</span> <span class="string">&#x27;student&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li>然后才能drop这个表：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):020:0&gt; drop <span class="string">&#x27;student&#x27;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>提示：如果直接drop表，会报错：ERROR: Table student is enabled. Disable it first.</p>
</blockquote>
</li>
</ul>
</li>
<li>变更表信息<ul>
<li>将info列族中的数据存放3个版本：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):022:0&gt; alter <span class="string">&#x27;student&#x27;</span>,&#123;NAME=&gt;<span class="string">&#x27;info&#x27;</span>,VERSIONS=&gt;3&#125;</span><br><span class="line">hbase(main):022:0&gt; get <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1001&#x27;</span>,&#123;COLUMN=&gt;<span class="string">&#x27;info:name&#x27;</span>,VERSIONS=&gt;3&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h1 id="第3章-HBase-API"><a href="#第3章-HBase-API" class="headerlink" title="第3章 HBase API"></a>第3章 HBase API</h1><h2 id="3-1-DDL"><a href="#3-1-DDL" class="headerlink" title="3.1 DDL"></a>3.1 DDL</h2><h3 id="3-1-1-创建命名空间"><a href="#3-1-1-创建命名空间" class="headerlink" title="3.1.1 创建命名空间"></a>3.1.1 创建命名空间</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create_namespace</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Admin admin = connection.getAdmin();</span><br><span class="line"></span><br><span class="line">    ) &#123;</span><br><span class="line">        <span class="keyword">final</span> String namespace = <span class="string">&quot;api_test&quot;</span>;</span><br><span class="line">        NamespaceDescriptor.Builder builder = NamespaceDescriptor.create(namespace);</span><br><span class="line">        admin.createNamespace(builder.build());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-1-2-查看命名空间"><a href="#3-1-2-查看命名空间" class="headerlink" title="3.1.2 查看命名空间"></a>3.1.2 查看命名空间</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">list_namespace</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Admin admin = connection.getAdmin();</span><br><span class="line"></span><br><span class="line">    ) &#123;</span><br><span class="line">        NamespaceDescriptor[] namespaceDescriptors = admin.listNamespaceDescriptors();</span><br><span class="line">        <span class="keyword">for</span> (NamespaceDescriptor namespaceDescriptor : namespaceDescriptors) &#123;</span><br><span class="line">            System.out.println(namespaceDescriptor.getName());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-1-3-删除命名空间"><a href="#3-1-3-删除命名空间" class="headerlink" title="3.1.3 删除命名空间"></a>3.1.3 删除命名空间</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">drop_namespace</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Admin admin = connection.getAdmin();</span><br><span class="line"></span><br><span class="line">    ) &#123;</span><br><span class="line">        admin.deleteNamespace(<span class="string">&quot;api_test&quot;</span>);</span><br><span class="line">        NamespaceDescriptor[] namespaceDescriptors = admin.listNamespaceDescriptors();</span><br><span class="line">        <span class="keyword">for</span> (NamespaceDescriptor namespaceDescriptor : namespaceDescriptors) &#123;</span><br><span class="line">            System.out.println(namespaceDescriptor);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h3 id="3-1-4-判断表是否存在"><a href="#3-1-4-判断表是否存在" class="headerlink" title="3.1.4 判断表是否存在"></a>3.1.4 判断表是否存在</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">tableExists</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Admin admin = connection.getAdmin();</span><br><span class="line"></span><br><span class="line">    ) &#123;</span><br><span class="line">        TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">boolean</span> b = admin.tableExists(tableName);</span><br><span class="line">        System.out.println(b);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-1-5-创建表"><a href="#3-1-5-创建表" class="headerlink" title="3.1.5 创建表"></a>3.1.5 创建表</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create_table</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Admin admin = connection.getAdmin();</span><br><span class="line"></span><br><span class="line">    ) &#123;</span><br><span class="line">        TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line">        TableDescriptorBuilder tableDescriptorBuilder = TableDescriptorBuilder.newBuilder(tableName);</span><br><span class="line">        ColumnFamilyDescriptorBuilder columnFamilyDescriptorBuilder = ColumnFamilyDescriptorBuilder.newBuilder(<span class="string">&quot;info&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        columnFamilyDescriptorBuilder.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">        tableDescriptorBuilder.setColumnFamily(columnFamilyDescriptorBuilder.build());</span><br><span class="line"></span><br><span class="line">        admin.createTable(tableDescriptorBuilder.build());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-1-6-删除表"><a href="#3-1-6-删除表" class="headerlink" title="3.1.6 删除表"></a>3.1.6 删除表</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">drop_table</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Admin admin = connection.getAdmin();</span><br><span class="line"></span><br><span class="line">    ) &#123;</span><br><span class="line">        TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line">        admin.disableTable(tableName);</span><br><span class="line">        admin.deleteTable(tableName);</span><br><span class="line">        TableName[] tableNames = admin.listTableNames();</span><br><span class="line">        <span class="keyword">for</span> (TableName tableName1 : tableNames) &#123;</span><br><span class="line">            System.out.println(tableName1);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-1-7-修改表"><a href="#3-1-7-修改表" class="headerlink" title="3.1.7 修改表"></a>3.1.7 修改表</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">alter_table</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Admin admin = connection.getAdmin();</span><br><span class="line"></span><br><span class="line">    ) &#123;</span><br><span class="line">        TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line"></span><br><span class="line">        ColumnFamilyDescriptorBuilder columnFamilyDescriptorBuilder</span><br><span class="line">                = ColumnFamilyDescriptorBuilder.newBuilder(<span class="string">&quot;job&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        columnFamilyDescriptorBuilder.setMaxVersions(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        admin.addColumnFamily(tableName, columnFamilyDescriptorBuilder.build());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ColumnFamilyDescriptorBuilder infoFamilyDescriptorBuilder</span><br><span class="line">                = ColumnFamilyDescriptorBuilder.newBuilder(<span class="string">&quot;info&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        infoFamilyDescriptorBuilder.setMaxVersions(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        admin.modifyColumnFamily(tableName, infoFamilyDescriptorBuilder.build());</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-1-8-查看表"><a href="#3-1-8-查看表" class="headerlink" title="3.1.8 查看表"></a>3.1.8 查看表</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">list_table</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Admin admin = connection.getAdmin();</span><br><span class="line"></span><br><span class="line">    ) &#123;</span><br><span class="line">        TableName[] tableNames = admin.listTableNames();</span><br><span class="line">        <span class="keyword">for</span> (TableName tableName : tableNames) &#123;</span><br><span class="line">            System.out.println(tableName);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="3-2-DML"><a href="#3-2-DML" class="headerlink" title="3.2 DML"></a>3.2 DML</h2><h3 id="3-2-1-插入数据"><a href="#3-2-1-插入数据" class="headerlink" title="3.2.1 插入数据"></a>3.2.1 插入数据</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Table table = connection.getTable(tableName);</span><br><span class="line">    ) &#123;</span><br><span class="line">        <span class="keyword">final</span> Put put_1001 = <span class="keyword">new</span> Put(<span class="string">&quot;1001&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        put_1001.addColumn(<span class="string">&quot;info&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;name&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;张三&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        table.put(put_1001);</span><br><span class="line"></span><br><span class="line">        put_1001.addColumn(<span class="string">&quot;info&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;age&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;18&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        table.put(put_1001);</span><br><span class="line"></span><br><span class="line">        put_1001.addColumn(<span class="string">&quot;info&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;gender&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;male&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        table.put(put_1001);</span><br><span class="line">        </span><br><span class="line">        put_1001.addColumn(<span class="string">&quot;info&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;telephone&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;18889898899&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line"></span><br><span class="line">        table.put(put_1001);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-2-查询单条数据"><a href="#3-2-2-查询单条数据" class="headerlink" title="3.2.2 查询单条数据"></a>3.2.2 查询单条数据</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">get</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Table table = connection.getTable(tableName);</span><br><span class="line">    ) &#123;</span><br><span class="line">        <span class="keyword">final</span> Get get = <span class="keyword">new</span> Get(<span class="string">&quot;1001&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> Result result = table.get(get);</span><br><span class="line">        <span class="keyword">final</span> Cell[] cells = result.rawCells();</span><br><span class="line">        <span class="keyword">for</span> (Cell cell : cells) &#123;</span><br><span class="line">            printCell(cell);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">printCell</span><span class="params">(Cell cell)</span> </span>&#123;</span><br><span class="line">    String row = <span class="keyword">new</span> String(CellUtil.cloneRow(cell), StandardCharsets.UTF_8);</span><br><span class="line">    String family = <span class="keyword">new</span> String(CellUtil.cloneFamily(cell), StandardCharsets.UTF_8);</span><br><span class="line">    String qualifier = <span class="keyword">new</span> String(CellUtil.cloneQualifier(cell), StandardCharsets.UTF_8);</span><br><span class="line">    String value = <span class="keyword">new</span> String(CellUtil.cloneValue(cell), StandardCharsets.UTF_8);</span><br><span class="line"></span><br><span class="line">    System.out.println(</span><br><span class="line">            <span class="string">&quot;row:&quot;</span> + row + <span class="string">&quot; &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;family:&quot;</span> + family + <span class="string">&quot; &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;qualifier:&quot;</span> + qualifier + <span class="string">&quot; &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;value:&quot;</span> + value + <span class="string">&quot; &quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-3-范围查询"><a href="#3-2-3-范围查询" class="headerlink" title="3.2.3 范围查询"></a>3.2.3 范围查询</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">scan</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Table table = connection.getTable(tableName);</span><br><span class="line">    ) &#123;</span><br><span class="line">        <span class="keyword">final</span> Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">        scan.withStartRow(<span class="string">&quot;1001&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        scan.withStopRow(<span class="string">&quot;1005&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> ResultScanner scanner = table.getScanner(scan);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Result result : scanner) &#123;</span><br><span class="line">            <span class="keyword">final</span> Cell[] cells = result.rawCells();</span><br><span class="line">            <span class="keyword">for</span> (Cell cell : cells) &#123;</span><br><span class="line">                printCell(cell);</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-4-追加数据"><a href="#3-2-4-追加数据" class="headerlink" title="3.2.4 追加数据"></a>3.2.4 追加数据</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">append</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Table table = connection.getTable(tableName);</span><br><span class="line">    ) &#123;</span><br><span class="line">        <span class="keyword">final</span> Append append = <span class="keyword">new</span> Append(<span class="string">&quot;1001&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        append.addColumn(<span class="string">&quot;info&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;salary&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;400000&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        table.append(append);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-5-删除数据"><a href="#3-2-5-删除数据" class="headerlink" title="3.2.5 删除数据"></a>3.2.5 删除数据</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Table table = connection.getTable(tableName);</span><br><span class="line">    ) &#123;</span><br><span class="line">        <span class="keyword">final</span> Delete delete = <span class="keyword">new</span> Delete(<span class="string">&quot;1001&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        delete.addColumn(<span class="string">&quot;info&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;name&quot;</span>.getBytes(StandardCharsets.UTF_8)</span><br><span class="line">        );</span><br><span class="line">        table.delete(delete);</span><br><span class="line">        get();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-6-清空数据"><a href="#3-2-6-清空数据" class="headerlink" title="3.2.6 清空数据"></a>3.2.6 清空数据</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">truncate</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            <span class="keyword">final</span> Admin admin = connection.getAdmin()</span><br><span class="line">    ) &#123;</span><br><span class="line">        admin.disableTable(tableName);</span><br><span class="line">        admin.truncateTable(tableName, <span class="keyword">false</span>);</span><br><span class="line">        scan();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="第4章-HBase进阶"><a href="#第4章-HBase进阶" class="headerlink" title="第4章 HBase进阶"></a>第4章 HBase进阶</h1><h2 id="4-1-架构原理"><a href="#4-1-架构原理" class="headerlink" title="4.1 架构原理"></a>4.1 架构原理</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16360144290588.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="4-1-1-Client"><a href="#4-1-1-Client" class="headerlink" title="4.1.1 Client"></a>4.1.1 Client</h3><ol>
<li>HBase META表：记录了用户所有表拆分出来的的 Region 映射信息，META可以有多个 Regoin</li>
<li>Client 访问用户数据前需要首先访问 ZooKeeper，找到META表所在RegionServer，最后才能找到用户数据的位置去访问，中间需要多次网络操作，不过 client 端会做 cache 缓存。</li>
</ol>
<h3 id="4-1-2-ZooKeeper"><a href="#4-1-2-ZooKeeper" class="headerlink" title="4.1.2 ZooKeeper"></a>4.1.2 ZooKeeper</h3><ol>
<li>ZooKeeper 为 HBase 提供 Failover 机制，选举 Master，避免单点 Master 单点故障问题</li>
<li>存储所有 Region 的寻址入口：<del>-ROOT-表在哪台服务器上</del> META表的位置信息(zookeeper路径：<code>/hbase/meta-region-server</code>)</li>
<li>实时监控 RegionServer 的状态，将 RegionServer 的上线和下线信息实时通知给 Master</li>
</ol>
<h3 id="4-1-3-Meta表结构"><a href="#4-1-3-Meta表结构" class="headerlink" title="4.1.3 Meta表结构"></a>4.1.3 Meta表结构</h3><ol>
<li>hbase:metab: 表存放着整个集群的所有Region信息，客户端数据的读写需要定位到具体需要操作的Region，说白了就是一张字典表。meta表只会有一个Region，这是为了确保meta表多次操作的原子性。</li>
<li>Meta表结构与内容 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):002:0&gt; scan <span class="string">&#x27;hbase:meta&#x27;</span></span><br><span class="line">ROW                                                                                         COLUMN+CELL</span><br><span class="line"> api_test_table                                                                             column=table:state, timestamp=1635927178467, value=\x08\x00</span><br><span class="line"> api_test_table,,1635927178047.d0d5f57b5659e0911c7aabbaa3a1bc04.                            column=info:regioninfo, timestamp=1635927185877, value=&#123;ENCODED =&gt; d0d5f57b5659e0911c7aabbaa3a1bc04, NAME =&gt; <span class="string">&#x27;api_test_table,,1635927178047.d0d5f57b5659e0911c7aabbaa3a1bc04.&#x27;</span>, STARTKEY =&gt; <span class="string">&#x27;&#x27;</span>, ENDKEY =&gt; <span class="string">&#x27;&#x27;</span>&#125;</span><br><span class="line"> api_test_table,,1635927178047.d0d5f57b5659e0911c7aabbaa3a1bc04.                            column=info:seqnumDuringOpen, timestamp=1635927185877, value=\x00\x00\x00\x00\x00\x00\x00\x05</span><br><span class="line"> api_test_table,,1635927178047.d0d5f57b5659e0911c7aabbaa3a1bc04.                            column=info:server, timestamp=1635927185877, value=hadoop001:16020</span><br><span class="line"> api_test_table,,1635927178047.d0d5f57b5659e0911c7aabbaa3a1bc04.                            column=info:serverstartcode, timestamp=1635927185877, value=1635912473426</span><br><span class="line"> api_test_table,,1635927178047.d0d5f57b5659e0911c7aabbaa3a1bc04.                            column=info:sn, timestamp=1635927185668, value=hadoop001,16020,1635912473426</span><br><span class="line"> api_test_table,,1635927178047.d0d5f57b5659e0911c7aabbaa3a1bc04.                            column=info:state, timestamp=1635927185877, value=OPEN</span><br><span class="line"> hbase:namespace                                                                            column=table:state, timestamp=1635908733224, value=\x08\x00</span><br><span class="line"> hbase:namespace,,1635908732447.5584783366fc148c901aaffe044a32ec.                           column=info:regioninfo, timestamp=1635922081791, value=&#123;ENCODED =&gt; 5584783366fc148c901aaffe044a32ec, NAME =&gt; <span class="string">&#x27;hbase:namespace,,1635908732447.5584783366fc148c901aaffe044a32ec.&#x27;</span>, STARTKEY =&gt; <span class="string">&#x27;&#x27;</span>, ENDKEY =&gt; <span class="string">&#x27;&#x27;</span>&#125;</span><br><span class="line"> hbase:namespace,,1635908732447.5584783366fc148c901aaffe044a32ec.                           column=info:seqnumDuringOpen, timestamp=1635922081791, value=\x00\x00\x00\x00\x00\x00\x00\x15</span><br><span class="line"> hbase:namespace,,1635908732447.5584783366fc148c901aaffe044a32ec.                           column=info:server, timestamp=1635922081791, value=hadoop004:16020</span><br><span class="line"> hbase:namespace,,1635908732447.5584783366fc148c901aaffe044a32ec.                           column=info:serverstartcode, timestamp=1635922081791, value=1635912472425</span><br><span class="line"> hbase:namespace,,1635908732447.5584783366fc148c901aaffe044a32ec.                           column=info:sn, timestamp=1635922081349, value=hadoop004,16020,1635912472425</span><br><span class="line"> hbase:namespace,,1635908732447.5584783366fc148c901aaffe044a32ec.                           column=info:state, timestamp=1635922081791, value=OPEN</span><br><span class="line"> student                                                                                    column=table:state, timestamp=1635910393993, value=\x08\x00</span><br><span class="line"> student,,1635910393240.55ab1df9560f226f1e7b4bc063e429ac.                                   column=info:regioninfo, timestamp=1635912481491, value=&#123;ENCODED =&gt; 55ab1df9560f226f1e7b4bc063e429ac, NAME =&gt; <span class="string">&#x27;student,,1635910393240.55ab1df9560f226f1e7b4bc063e429ac.&#x27;</span>, STARTKEY =&gt; <span class="string">&#x27;&#x27;</span>, ENDKEY =&gt; <span class="string">&#x27;&#x27;</span>&#125;</span><br><span class="line"> student,,1635910393240.55ab1df9560f226f1e7b4bc063e429ac.                                   column=info:seqnumDuringOpen, timestamp=1635912481491, value=\x00\x00\x00\x00\x00\x00\x00\x0F</span><br><span class="line"> student,,1635910393240.55ab1df9560f226f1e7b4bc063e429ac.                                   column=info:server, timestamp=1635912481491, value=hadoop003:16020</span><br><span class="line"> student,,1635910393240.55ab1df9560f226f1e7b4bc063e429ac.                                   column=info:serverstartcode, timestamp=1635912481491, value=1635912471638</span><br><span class="line"> student,,1635910393240.55ab1df9560f226f1e7b4bc063e429ac.                                   column=info:sn, timestamp=1635912480984, value=hadoop003,16020,1635912471638</span><br><span class="line"> student,,1635910393240.55ab1df9560f226f1e7b4bc063e429ac.                                   column=info:state, timestamp=1635912481491, value=OPEN</span><br><span class="line">6 row(s)</span><br><span class="line">Took 0.0450 seconds</span><br></pre></td></tr></table></figure></li>
<li>Meta表组成说明<ol>
<li>Rowkey组成: meta表中的一个Rowkey就代表了一个region。<ul>
<li>格式： <code>[table],[region start key],[region id]</code><ul>
<li>region id 由该 region 生成的时间戳（精确到毫秒）与 region encoded 组成</li>
<li>region encoded 由 region 所在的 表名, StartKey, 时间戳这三者的MD5值产生， HBase 在 HDFS 上存储 region 的路径就是 region encoded。</li>
</ul>
</li>
<li>[region start key]为空的，说明这是该table的第一个region。若对应region中startkey和endkey都为空的话，表明这个table只有一个region</li>
<li>示例<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Rowkey: student,,1635910393240.55ab1df9560f226f1e7b4bc063e429ac.</span><br><span class="line">table: student</span><br><span class="line">region start key: 空</span><br><span class="line">region id timestamp: 1635910393240</span><br><span class="line">region id encoded: 55ab1df9560f226f1e7b4bc063e429ac</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>Column组成<table>
<thead>
<tr>
<th>列名</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>info:state</td>
<td>Region状态</td>
</tr>
<tr>
<td>info:sn</td>
<td>Region Server Node，由server和serverstartcode 组成</td>
</tr>
<tr>
<td>info:serverstartcode</td>
<td>Region Server启动Code，实质上就是Region Server启动的时间戳</td>
</tr>
<tr>
<td>info:server</td>
<td>Region Server 地址和端口</td>
</tr>
<tr>
<td>info:seqnumDuringOpen</td>
<td>表示Region在线时长的一个二进制串</td>
</tr>
<tr>
<td>info:regioninfo</td>
<td>Region 的详细信息，和 .regioninfo 内容相同</td>
</tr>
</tbody></table>
</li>
<li>info:regioninfo 是重要信息<ul>
<li>ENCODED：基于${表名},${起始键},${region时间戳}生成的32位md5字符串，region数据存储在hdfs上时使用的唯一编号，可以从meta表中根据该值定位到hdfs中的具体路径。 rowkey中最后的${encode编码}就是 ENCODED 的值，其是rowkey组成的一部分。</li>
<li>NAME：与 ROWKEY 值相同</li>
<li>STARTKEY：该 region 的起始键</li>
<li>ENDKEY：该 region 的结束键</li>
</ul>
</li>
</ol>
</li>
</ol>
<h3 id="4-1-4-Master"><a href="#4-1-4-Master" class="headerlink" title="4.1.4 Master"></a>4.1.4 Master</h3><ol>
<li>为 RegionServer 分配 Region</li>
<li>负责 RegionServer 的负载均衡</li>
<li>发现失效的 RegionServer 并重新分配其上的 Region</li>
<li>HDFS 上的垃圾文件（HBase）回收</li>
<li>处理 Schema 更新请求（表的创建，删除，修改，列簇的增加等等）</li>
</ol>
<h3 id="4-1-5-RegionServer"><a href="#4-1-5-RegionServer" class="headerlink" title="4.1.5 RegionServer"></a>4.1.5 RegionServer</h3><ol>
<li>RegionServer 维护 Master 分配给它的 Region，处理对这些 Region 的 IO 请求</li>
<li>RegionServer 负责 Split 在运行过程中变得过大的 Region，负责 Compact 操作</li>
</ol>
<ul>
<li>可以看到，client 访问 HBase 上数据的过程并不需要 master 参与（寻址访问 zookeeper 和 RegioneServer，数据读写访问 RegioneServer），Master 仅仅维护者 Table 和 Region 的元数据信息，负载很低。</li>
<li>META 存的是所有的 Region 的位置信息，那么 RegioneServer 当中 Region 在进行分裂之后 的新产生的 Region，是由 Master 来决定发到哪个 RegioneServer，这就意味着，只有 Master 知道 new Region 的位置信息，所以，由 Master 来管理META表当中的数据的 CRUD</li>
</ul>
<ol start="5">
<li>所以结合以上两点表明，在没有 Region 分裂的情况，Master 宕机一段时间是可以忍受的。</li>
</ol>
<h3 id="4-1-6-HRegion"><a href="#4-1-6-HRegion" class="headerlink" title="4.1.6 HRegion"></a>4.1.6 HRegion</h3><ol>
<li>table在行的方向上分隔为多个Region。Region是HBase中分布式存储和负载均衡的最小单元，即不同的region可以分别在不同的Region Server上，但同一个Region是不会拆分到多个server上。</li>
<li>Region按大小分裂（split），每个表一般是只有一个region。随着数据不断插入表，region不断增大，当region的某个列族达到一个阈值时就会分裂两个新的region。</li>
<li>每个region由以下信息标识：&lt; 表名,startRowkey,创建时间&gt;</li>
<li>由元数据表META记录该region的endRowkey</li>
</ol>
<h3 id="4-1-7-Store"><a href="#4-1-7-Store" class="headerlink" title="4.1.7 Store"></a>4.1.7 Store</h3><ol>
<li>每一个region由一个或多个store组成，至少是一个store，hbase会把一起访问的数据放在一个store里面，即为每个 ColumnFamily建一个store，如果有几个ColumnFamily，也就有几个Store。</li>
<li>一个Store由一个memStore和0或者 多个StoreFile组成。</li>
<li>HBase以store的大小来判断是否需要切分region</li>
</ol>
<h3 id="4-1-8-MemStore"><a href="#4-1-8-MemStore" class="headerlink" title="4.1.8 MemStore"></a>4.1.8 MemStore</h3><ol>
<li>写缓存，由于HFile中的数据要求是有序的，所以数据是先存储在MemStore中，排好序后，等到达刷写时机才会刷写到HFile，每次刷写都会形成一个新的HFile。</li>
<li>memStore 是放在内存里的。保存修改的数据即keyValues。当memStore的大小达到一个阀值（默认128MB）时，memStore会被flush到文 件，即生成一个快照。目前hbase 会有一个线程来负责memStore的flush操作。</li>
<li>进入MemStore的数据对rowkey进行字典序排序</li>
</ol>
<h3 id="4-1-9-StoreFile"><a href="#4-1-9-StoreFile" class="headerlink" title="4.1.9 StoreFile"></a>4.1.9 StoreFile</h3><ol>
<li>保存实际数据的物理文件，StoreFile以HFile的形式存储在HDFS上。每个Store会有一个或多个StoreFile（HFile），数据在每个StoreFile中都是有序的。</li>
<li>memStore内存中的数据写到文件后就是StoreFile，StoreFile底层是以HFile的格式保存。当storefile文件的数量增长到一定阈值后，系统会进行合并（minor、major compaction），在合并过程中会进行版本合并和删除工作（majar），形成更大的storefile。</li>
</ol>
<h3 id="4-1-10-HFile"><a href="#4-1-10-HFile" class="headerlink" title="4.1.10 HFile"></a>4.1.10 HFile</h3><ol>
<li>HBase中KeyValue数据的存储格式，HFile是Hadoop的 二进制格式文件，实际上StoreFile就是对Hfile做了轻量级包装，即StoreFile底层就是HFile。</li>
</ol>
<h3 id="4-1-11-HLog"><a href="#4-1-11-HLog" class="headerlink" title="4.1.11 HLog"></a>4.1.11 HLog</h3><ol>
<li>HLog(WAL log)：WAL意为write ahead log，用来做灾难恢复使用，HLog记录数据的所有变更，一旦region server 宕机，就可以从log中进行恢复。</li>
<li>由于数据要经MemStore排序后才能刷写到HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入MemStore中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。</li>
<li>HLog文件就是一个普通的Hadoop Sequence File， Sequence File的value是key时HLogKey对象，其中记录了写入数据的归属信息，除了table和region名字外，还同时包括sequence number和timestamp，timestamp是写入时间，sequence number的起始值为0，或者是最近一次存入文件系统中的sequence number。 Sequence File的value是HBase的KeyValue对象，即对应HFile中的KeyValue。</li>
</ol>
<h2 id="4-2-写流程"><a href="#4-2-写流程" class="headerlink" title="4.2 写流程"></a>4.2 写流程</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16360175069448.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。</li>
<li>访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。</li>
<li>与目标Region Server进行通讯；</li>
<li>将数据顺序写入（追加）到WAL；</li>
<li>将数据写入对应的MemStore，数据会在MemStore进行排序（rowKey 字典序）；</li>
<li>向客户端发送ack；</li>
<li>等达到MemStore的刷写时机后，将数据刷写到HFile。</li>
</ol>
<h2 id="4-3-MemStore-Flush"><a href="#4-3-MemStore-Flush" class="headerlink" title="4.3 MemStore Flush"></a>4.3 MemStore Flush</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16360739438242.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br>MemStore刷写时机：</p>
<ul>
<li><p>MemStore 级别限制：</p>
<ul>
<li><code>hbase.hregion.memstore.flush.size</code></li>
<li>默认值：128M</li>
<li>当某个memstore的大小达到128M(默认情况下)时，<font color ='red' >其所在region的所有memstore都会刷写</font>，这个时候是不阻塞写操作的</li>
</ul>
</li>
<li><p>Region 级别限制：</p>
<ul>
<li><code>hbase.hregion.memstore.flush.size</code> * <code>hbase.hregion.memstore.block.multiplier</code> <ul>
<li>默认值: 128M * 4 = 512M</li>
<li>当一个Region的MemStore总量达到512M（默认情况下）时，会阻塞这个region的写操作，并强制刷写到HFile。触发这个刷新只会发生在MemStore即将写满128M时put了一个巨大的记录的情况，这时会阻塞写操作，强制刷写成功才能继续写入</li>
</ul>
</li>
</ul>
</li>
<li><p>RegionServer 级别限制：</p>
<ul>
<li><p><code>java_heapsize</code> * <code>hbase.regionserver.global.memstore.size</code> </p>
<ul>
<li>默认值：java_heapsize * 0.4</li>
<li>当RegionServer上所有的MemStore占用到达heap的40%时，强制阻塞所有的写操作，将所有的MemStore刷写到HFile</li>
</ul>
</li>
<li><p><code>java_heapsize</code> * <code>hbase.regionserver.global.memstore.size</code> * <code>hbase.regionserver.global.memstore.size.upper.limit</code></p>
<ul>
<li>默认值：java_heapsize * 0.4 * 0.95</li>
<li>当region server中memstore的总大小达到上述内存限制时，region server会把它的所有region按照其所有memstore的大小顺序（由大到小）依次进行刷写。直到region server中所有memstore的总大小减小到<code>hbase.regionserver.global.memstore.size.lower.limit</code>以下。</li>
</ul>
</li>
<li><p><code>hbase.regionserver.optionalcacheflushinterval</code></p>
<ul>
<li>默认值：3600000，1小时</li>
<li>到达自动刷写的时间，也会触发memstore flush。</li>
</ul>
</li>
<li><p><code>hbase.regionserver.max.logs</code></p>
<ul>
<li>默认值：32</li>
<li>当WAL文件的数量超过32，region会按照时间顺序依次进行刷写，直到WAL文件数量减小到hbase.regionserver.max.log以下（该属性名已经废弃，现无需手动设置，最大值为32）。</li>
</ul>
</li>
</ul>
</li>
<li><p>手动执行:</p>
<ul>
<li>可以通过 shell 命令 flush ‘tableName’ 或者 flush ‘regionName’ 分别对一个表或者一个 Region 进行 flush。 </li>
</ul>
</li>
</ul>
<h2 id="4-4-读流程"><a href="#4-4-读流程" class="headerlink" title="4.4 读流程"></a>4.4 读流程</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16360830898136.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="4-4-1-读流程"><a href="#4-4-1-读流程" class="headerlink" title="4.4.1 读流程"></a>4.4.1 读流程</h3><ol>
<li>Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。</li>
<li>访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。</li>
<li>与目标Region Server进行通讯；</li>
<li>分别在Block Cache（读缓存），MemStore和Store File（HFile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put/Delete）。<ul>
<li>如果在 Memstore 中找不到，则在 Storefile 中扫描 为了能快速的判断要查询的数据在不在这个 StoreFile 中，应用了 BloomFilter</li>
<li>BloomFilter，布隆过滤器：迅速判断一个元素是不是在一个庞大的集合内，但是他有一个 弱点：它有一定的误判率</li>
<li>误判率：原本不存在与该集合的元素，布隆过滤器有可能会判断说它存在，但是，如果布隆过滤器，判断说某一个元素不存在该集合，那么该元素就一定不在该集合内）</li>
</ul>
</li>
<li>将从文件中查询到的数据块（Block，HFile数据存储单元，默认大小为64KB）缓存到Block Cache。</li>
<li>将合并后的最终结果返回给客户端。</li>
</ol>
<h3 id="4-4-1-Block-Cache读缓存"><a href="#4-4-1-Block-Cache读缓存" class="headerlink" title="4.4.1 Block Cache读缓存"></a>4.4.1 Block Cache读缓存</h3><ol>
<li>读缓存：RegionServer级别，占堆内存的40%(默认)</li>
<li>当写操作导致缓存数据过期时，RegionServer会将过期的块释放  </li>
<li>如果没有命中BlockCache，会去memstore和所有的storefile里面查找数据</li>
</ol>
<h3 id="4-4-1-1-LruBlockCache"><a href="#4-4-1-1-LruBlockCache" class="headerlink" title="4.4.1.1 LruBlockCache"></a>4.4.1.1 LruBlockCache</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16360911357506.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li><p>LruBlockCache内部较为简单，主要就是一个map，如上图所示，由hfilename+offset来唯一标识一个block；</p>
</li>
<li><p>LruBlockCache所能够使用的内存为堆的一定比例，通过hfile.block.cache.size设置，默认是0.4；</p>
</li>
<li><p>maxSize = heapSize * hfile.block.cache.size，以下参数都根据maxSize计算；<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16360926195851.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>acceptSize<ul>
<li>使用量达到一定比例时会触发驱逐，该阈值通过hbase.lru.blockcache.acceptable.factor设置，默认是0.99；</li>
</ul>
</li>
<li>minSize<ul>
<li>驱逐后最少剩余比例，该阈值通过hbase.lru.blockcache.min.factor设置，默认是0.95；</li>
</ul>
</li>
<li>hardLimit<ul>
<li>使用量达到一定比例时则拒绝写入，该阈值通过hbase.lru.blockcache.hard.capacity.limit.factor设置，默认是1.2，这意味允许一定的超出；</li>
</ul>
</li>
</ul>
</li>
<li><p>关于驱逐：</p>
<ol>
<li>block分为3种类型，由BlockPriority字段区分，取值为single、mutli、inMem，空间分配默认为0.25：0.5：0.25；</li>
<li>系统表以及其它指定了InMem的表所含block会标记为inMem，其它block初次存入时标记为single，再次访问时会修改为multi；</li>
<li>存放时只要还有空间即可放入，空间分配比例只是在驱逐发生时进行计算使用；</li>
<li>驱逐时，会用minSize乘以各类型的比例，得到各类型最少要保留的minSize；</li>
<li>根据目前的算法，驱逐后的size，应该是略大于minSize的一个值，伪代码如下； <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">expectFreeSize = usedSize - minSize;//预期释放总大小</span><br><span class="line">freedSize = 0;//当前已释放总大小</span><br><span class="line">n=3;//类型数量</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">type</span> <span class="keyword">in</span> (<span class="string">&#x27;single&#x27;</span>,<span class="string">&#x27;multi&#x27;</span>,<span class="string">&#x27;inMem&#x27;</span>):</span><br><span class="line">    overFlow = type.usedSize - type.minSize</span><br><span class="line">    toBeFree = min(overFlow,(expectFreeSize - freedSize)/n)</span><br><span class="line">    free(toBeFree)</span><br><span class="line">    freedSize += toBeFree</span><br><span class="line">    n--;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<h3 id="4-4-1-2-CombinedBlockCache-LruBlockCache-BucketCache"><a href="#4-4-1-2-CombinedBlockCache-LruBlockCache-BucketCache" class="headerlink" title="4.4.1.2 CombinedBlockCache = LruBlockCache + BucketCache"></a>4.4.1.2 CombinedBlockCache = LruBlockCache + BucketCache</h3><ol>
<li>BucketCache<ol>
<li>LruBlockCache的优点是实现简单，缺点是block的存入和释放伴随着内存的申请和释放，会带来内存碎片和gc过多的问题；</li>
<li>BucketCache采用了类似池的思路，预先申请内存并划分为一个个的bucket，这些bucket会一直存在并重复使用；</li>
</ol>
</li>
</ol>
<p>总体的读写流程如下图所示：<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16360926651279.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br>Block缓存写入流程：</p>
<ol>
<li>将block写入RAMCache，然后系统会根据blockkey进行hash，根据hash结果将block分配到一组blockingQueue中；</li>
<li>HBase会同时启动多个WriteThead，分别关联一个blockingQueue，并发的执行异步写入；</li>
<li>每个WriteThead读取到block数据后，调用bucketAllocator为这些block分配内存空间；</li>
<li>BucketAllocator会选择与block大小对应的bucket进行存放，并且返回对应的物理地址偏移量offset；</li>
<li>WriteThead将block以及分配好的物理地址偏移量传给IOEngine模块，执行具体的内存写入操作；</li>
<li>写入成功后，将类似这样的映射关系写入BackingMap中，方便后续查找时根据blockkey可以直接定位；</li>
</ol>
<p>Block缓存读取流程：</p>
<ol>
<li>首先从RAMCache中查找，对于还没有来得及写入到bucket的缓存block，一定存储在RAMCache中；</li>
<li>如果在RAMCache中没有找到，再在BackingMap中根据blockKey找到对应entry；</li>
<li>根据entry中的offset可以直接从内存中查找对应的block数据；</li>
</ol>
<p>其中最核心的组件是BucketAllocator和IoEngine，前者负责block的逻辑地址分配，后者负责block的实际物理存放，内部结构如下：<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16361213582414.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>hbase中blocksize是可以灵活设置的，bucketCache预设了一组支持的大小，从4K~512k不等；</li>
<li>一个Bucket只能存放一种size的block，一种size对应一个BucketSizeInfo进行管理；</li>
<li>初始化时，每种size先分配1个bucket，剩余的都分配给最大的那个size，如黑色箭头所示；</li>
<li>分配过程中当前size如果空间不够，会挪用其它size的空闲bucket，如棕色箭头所示，这意味着有可能某个Bucket一开始存放了32k的block</li>
<li>释放后空闲，被挪用后变成存放64k的block；</li>
<li>ioEngine有多种实现，可支持onheap、offheap、disk等；</li>
</ol>
<p>关于驱逐：</p>
<ol>
<li>2种情况下会触发<ol>
<li>1是已使用超过95%（acceptableFactor），</li>
<li>2是某个size的block分配不了(总量虽然没达到阈值，但不存在完全空闲的bucket供挪用)；</li>
<li>驱逐后的最少剩余比例为85%（minFactor），遍历各个bucketSizeInfo，把超过85%的部分加起来，再乘以一个系数0.1（extraFreeFactor），就是要释放的大小；</li>
<li>具体计算方法复用了LruBlockCache的代码，也是按照single、multi、inMem及其比例进行计算和释放；</li>
<li>实际清理动作是修改一些状态数据，比如Bucket对象的freeList、freeCount，以及backMapping的键值对等，并不需要对底层的byteBuffer做什么操作；</li>
<li>对于refCount大于0的block，会先将其markedForEvict置为true，待各个使用方读取完成后调用returnBlock进行释放；</li>
</ol>
</li>
</ol>
<h2 id="4-5-StoreFile-Compaction"><a href="#4-5-StoreFile-Compaction" class="headerlink" title="4.5 StoreFile Compaction"></a>4.5 StoreFile Compaction</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16360927692130.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>由于memstore每次刷写都会生成一个新的HFile，且同一个字段的不同版本（timestamp）和不同类型（Put/Delete）有可能会分布在不同的HFile中，因此查询时需要遍历所有的HFile。为了减少HFile的个数，以及清理掉过期和删除的数据，会进行StoreFile Compaction。</li>
<li>Compaction分为两种，分别是Minor Compaction和Major Compaction。<ul>
<li>Minor Compaction会将临近的若干个较小的HFile合并成一个较大的HFile，但不会清理过期和删除的数据；不会对cell合并；性能要求低</li>
<li>Major Compaction会将一个Store下的所有的HFile合并成一个大HFile，并且会清理掉过期和删除的数据，对所有cell进行排序，性能要求高</li>
</ul>
</li>
<li>触发时机：<ul>
<li>Minor Compaction自动执行，超过3个以上的storeFile，根据ExploringCompactionPolicy算法判断触发</li>
<li>Major Compaction：默认七天一次，推荐手动执行</li>
</ul>
</li>
</ul>
<h2 id="4-6-Region-Split"><a href="#4-6-Region-Split" class="headerlink" title="4.6 Region Split"></a>4.6 Region Split</h2><ul>
<li>默认情况下，每个Table起初只有一个Region，随着数据的不断写入，Region会自动进行拆分。刚拆分时，两个子Region都位于当前的Region Server，但处于负载均衡的考虑，HMaster有可能会将某个Region转移给其他的Region Server。</li>
<li>Region Split时机：<ol>
<li>当1个region中的某个Store下所有StoreFile的总大小超过<code>hbase.hregion.max.filesize（默认值：10G）</code>，该Region就会进行拆分（0.94版本之前）。</li>
<li>当1个region中的某个Store下所有StoreFile的总大小超过<code>Min(R^3 * 2 * &quot;hbase.hregion.memstore.flush.size&quot;,hbase.hregion.max.filesize&quot;)</code>，该Region就会进行拆分，其中R为当前Region Server中属于该Table的Region个数（0.94版本之后）。快速分裂：使表能尽快分布到所有Region</li>
<li>Hbase 2.0引入了新的split策略：如果当前RegionServer上改表只有一个Region，按照<code>2 * hbase.hregion.memstore.flush.size 默认128M</code>分裂，否则按照<code>hbase.hregion.max.filesize 默认值10G</code>分裂。防止Region过多，导致小文件过多</li>
</ol>
</li>
<li>Region Split过程<br>  在子region文件夹下生成两个子文件夹daughterA、daughterB，并在两个文件夹内生成reference文件，分别指向父region中对应的文件随着Compaction的进行，RefenceFile会逐渐被删除，此时父Region数据没用了， 会被删除，Split结束<h2 id="4-7-Region-Merge"><a href="#4-7-Region-Merge" class="headerlink" title="4.7 Region Merge"></a>4.7 Region Merge</h2>如果Region数量过多，可以手动合并Region</li>
</ul>
<h1 id="第5章-HBase优化"><a href="#第5章-HBase优化" class="headerlink" title="第5章 HBase优化"></a>第5章 HBase优化</h1><h2 id="5-1-预分区"><a href="#5-1-预分区" class="headerlink" title="5.1 预分区"></a>5.1 预分区</h2><p>每一个region维护着StartRow与EndRow，如果加入的数据符合某个Region维护的RowKey范围，则该数据交给这个Region维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高HBase性能。</p>
<ol>
<li>手动设定预分区 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase&gt; create <span class="string">&#x27;staff1&#x27;</span>,<span class="string">&#x27;info&#x27;</span>,<span class="string">&#x27;partition1&#x27;</span>,SPLITS =&gt; [<span class="string">&#x27;1000&#x27;</span>,<span class="string">&#x27;2000&#x27;</span>,<span class="string">&#x27;3000&#x27;</span>,<span class="string">&#x27;4000&#x27;</span>]</span><br></pre></td></tr></table></figure></li>
<li>生成16进制序列预分区 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase&gt; create <span class="string">&#x27;staff2&#x27;</span>,<span class="string">&#x27;info&#x27;</span>,<span class="string">&#x27;partition2&#x27;</span>,&#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; <span class="string">&#x27;HexStringSplit&#x27;</span>&#125;</span><br></pre></td></tr></table></figure></li>
<li>按照文件中设置的规则预分区<ul>
<li>创建splits.txt文件内容如下：  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">aaaa</span><br><span class="line">bbbb</span><br><span class="line">cccc</span><br><span class="line">dddd</span><br></pre></td></tr></table></figure></li>
<li>然后执行：  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="string">&#x27;staff3&#x27;</span>,<span class="string">&#x27;partition3&#x27;</span>,SPLITS_FILE <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;/home/atguigu/hbase/splits.txt&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>使用JavaAPI创建预分区 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//自定义算法，产生一系列hash散列值存储在二维数组中</span></span><br><span class="line"><span class="keyword">byte</span>[][] splitKeys = 某个散列值函数</span><br><span class="line"><span class="comment">//创建HbaseAdmin实例</span></span><br><span class="line">HBaseAdmin hAdmin = <span class="keyword">new</span> HBaseAdmin(HbaseConfiguration.create());</span><br><span class="line"><span class="comment">//创建HTableDescriptor实例</span></span><br><span class="line">HTableDescriptor tableDesc = <span class="keyword">new</span> HTableDescriptor(tableName);</span><br><span class="line"><span class="comment">//通过HTableDescriptor实例和散列值二维数组创建带有预分区的Hbase表</span></span><br><span class="line">hAdmin.createTable(tableDesc, splitKeys);</span><br></pre></td></tr></table></figure>
<h2 id="5-2-RowKey设计"><a href="#5-2-RowKey设计" class="headerlink" title="5.2 RowKey设计"></a>5.2 RowKey设计</h2>一条数据的唯一标识就是RowKey，那么这条数据存储于哪个分区，取决于RowKey处于哪个一个预分区的区间内，设计RowKey的主要目的 ，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜。接下来我们就谈一谈RowKey常用的设计方案。</li>
<li>生成随机数、hash、散列值, 比如： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">原本rowKey为1001的，SHA1后变成：dd01903921ea24941c26a48f2cec24e0bb0e8cc7</span><br><span class="line">原本rowKey为3001的，SHA1后变成：49042c54de64a1e9bf0b33e00245660ef92dc7bd</span><br><span class="line">原本rowKey为5001的，SHA1后变成：7b61dec07e02c188790670af43e717f0f46e8913</span><br></pre></td></tr></table></figure>
<ul>
<li>在做此操作之前，一般我们会选择从数据集中抽取样本，来决定什么样的rowKey来Hash后作为每个分区的临界值。</li>
</ul>
</li>
<li>字符串反转 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">20170524000001转成10000042507102</span><br><span class="line">20170524000002转成20000042507102</span><br></pre></td></tr></table></figure>
 这样也可以在一定程度上散列逐步put进来的数据。</li>
<li>字符串拼接 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">20170524000001_a12e</span><br><span class="line">20170524000001_93i7</span><br></pre></td></tr></table></figure>
<h2 id="5-3-内存优化"><a href="#5-3-内存优化" class="headerlink" title="5.3 内存优化"></a>5.3 内存优化</h2>HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。</li>
</ol>
<h2 id="5-4-基础优化"><a href="#5-4-基础优化" class="headerlink" title="5.4 基础优化"></a>5.4 基础优化</h2><ol>
<li>允许在HDFS的文件中追加内容<ul>
<li>hdfs-site.xml、hbase-site.xml</li>
<li>属性：dfs.support.append</li>
<li>解释：开启HDFS追加同步，可以优秀的配合HBase的数据同步和持久化。默认值为true。</li>
</ul>
</li>
<li>优化DataNode允许的最大文件打开数<ul>
<li>hdfs-site.xml</li>
<li>属性：dfs.datanode.max.transfer.threads</li>
<li>解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096</li>
</ul>
</li>
<li>优化延迟高的数据操作的等待时间<ul>
<li>hdfs-site.xml</li>
<li>属性：dfs.image.transfer.timeout</li>
<li>解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值（默认60000毫秒），以确保socket不会被timeout掉。</li>
</ul>
</li>
<li>优化数据的写入效率<ul>
<li>mapred-site.xml</li>
<li>属性：<ul>
<li>mapreduce.map.output.compress</li>
<li>mapreduce.map.output.compress.codec</li>
</ul>
</li>
<li>解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec或者其他压缩方式。</li>
</ul>
</li>
<li>设置RPC监听数量<ul>
<li>hbase-site.xml</li>
<li>属性：Hbase.regionserver.handler.count</li>
<li>解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。</li>
</ul>
</li>
<li>优化HStore文件大小<ul>
<li>hbase-site.xml</li>
<li>属性：hbase.hregion.max.filesize</li>
<li>解释：默认值10737418240（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile。</li>
</ul>
</li>
<li>优化HBase客户端缓存<ul>
<li>hbase-site.xml</li>
<li>属性：hbase.client.write.buffer</li>
<li>解释：用于指定Hbase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。</li>
</ul>
</li>
<li>指定scan.next扫描HBase所获取的行数<ul>
<li>hbase-site.xml</li>
<li>属性：hbase.client.scanner.caching</li>
<li>解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大。</li>
</ul>
</li>
<li>flush、compact、split机制<br> 当MemStore达到阈值，将Memstore中的数据Flush进Storefile；compact机制则是把flush出来的小文件合并成大的Storefile文件。split则是当Region达到阈值，会把过大的Region一分为二。<ul>
<li>属性：<ul>
<li>hbase.hregion.memstore.flush.size：134217728<ul>
<li>128M就是Memstore的默认阈值</li>
<li>这个参数的作用是当单个HRegion内所有的Memstore大小总和超过指定值时，flush该HRegion的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM。</li>
</ul>
</li>
<li>hbase.regionserver.global.memstore.upperLimit：0.8</li>
<li>hbase.regionserver.global.memstore.lowerLimit：0.6<ul>
<li>当MemStore使用内存总量达到hbase.regionserver.global.memstore.upperLimit指定值时，将会有多个MemStores flush到文件中，MemStore flush 顺序是按照大小降序执行的，直到刷新到MemStore使用内存略小于lowerLimit</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h1 id="第6章-整合Phoenix"><a href="#第6章-整合Phoenix" class="headerlink" title="第6章 整合Phoenix"></a>第6章 整合Phoenix</h1><h2 id="6-1-Phoenix简介"><a href="#6-1-Phoenix简介" class="headerlink" title="6.1 Phoenix简介"></a>6.1 Phoenix简介</h2><h3 id="6-1-1-Phoenix定义"><a href="#6-1-1-Phoenix定义" class="headerlink" title="6.1.1 Phoenix定义"></a>6.1.1 Phoenix定义</h3><p>Phoenix是HBase的开源SQL皮肤。可以使用标准JDBC API代替HBase客户端API来创建表，插入数据和查询HBase数据。</p>
<h3 id="6-1-2-Phoenix特点"><a href="#6-1-2-Phoenix特点" class="headerlink" title="6.1.2 Phoenix特点"></a>6.1.2 Phoenix特点</h3><ol>
<li>容易集成：如Spark，Hive，Pig，Flume和Map Reduce；</li>
<li>操作简单：DML命令以及通过DDL命令创建和操作表和版本化增量更改；</li>
<li>支持HBase二级索引创建。</li>
</ol>
<h3 id="6-1-3-Phoenix架构"><a href="#6-1-3-Phoenix架构" class="headerlink" title="6.1.3 Phoenix架构"></a>6.1.3 Phoenix架构</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16361629213598.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h2 id="6-2-Phoenix快速入门"><a href="#6-2-Phoenix快速入门" class="headerlink" title="6.2 Phoenix快速入门"></a>6.2 Phoenix快速入门</h2><h3 id="6-2-1-安装"><a href="#6-2-1-安装" class="headerlink" title="6.2.1 安装"></a>6.2.1 安装</h3><ol>
<li>官网地址:<a href="http://phoenix.apache.org/">http://phoenix.apache.org/</a></li>
<li>Phoenix部署<ol>
<li>上传并解压tar包 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 module]$ tar -zxvf apache-phoenix-5.0.0-HBase-2.0-bin.tar.gz -C /opt/module/</span><br><span class="line">[atguigu@hadoop001 module]$ mv apache-phoenix-5.0.0-HBase-2.0-bin phoenix-5.0.0</span><br></pre></td></tr></table></figure></li>
<li>复制server包并拷贝到各个节点的hbase/lib <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 phoenix-5.0.0]$ cp /opt/module/phoenix-5.0.0/phoenix-5.0.0-HBase-2.0-server.jar /opt/module/hbase-2.0.5/lib/</span><br><span class="line">[atguigu@hadoop001 phoenix-5.0.0]$ xsync /opt/module/hbase-2.0.5/lib/phoenix-5.0.0-HBase-2.0-server.jar</span><br></pre></td></tr></table></figure></li>
<li>配置环境变量 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#phoenix</span></span><br><span class="line"><span class="built_in">export</span> PHOENIX_HOME=/opt/module/phoenix-5.0.0</span><br><span class="line"><span class="built_in">export</span> PHOENIX_CLASSPATH=<span class="variable">$PHOENIX_HOME</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$PHOENIX_HOME</span>/bin</span><br></pre></td></tr></table></figure></li>
<li>连接Phoenix <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 software]$ sqlline.py hadoop001,hadoop002,hadoop003,hadoop004,hadoop005:2181</span><br><span class="line">Setting property: [incremental, <span class="literal">false</span>]</span><br><span class="line">Setting property: [isolation, TRANSACTION_READ_COMMITTED]</span><br><span class="line">issuing: !connect jdbc:phoenix:hadoop001,hadoop002,hadoop003,hadoop004,hadoop005:2181 none none org.apache.phoenix.jdbc.PhoenixDriver</span><br><span class="line">Connecting to jdbc:phoenix:hadoop001,hadoop002,hadoop003,hadoop004,hadoop005:2181</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/opt/module/phoenix-5.0.0/phoenix-5.0.0-HBase-2.0-client.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/opt/module/ha-hadoop-3.1.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html<span class="comment">#multiple_bindings for an explanation.</span></span><br><span class="line">21/11/06 10:06:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Connected to: Phoenix (version 5.0)</span><br><span class="line">Driver: PhoenixEmbeddedDriver (version 5.0)</span><br><span class="line">Autocommit status: <span class="literal">true</span></span><br><span class="line">Transaction isolation: TRANSACTION_READ_COMMITTED</span><br><span class="line">Building list of tables and columns <span class="keyword">for</span> tab-completion (<span class="built_in">set</span> fastconnect to <span class="literal">true</span> to skip)...</span><br><span class="line">133/133 (100%) Done</span><br><span class="line">Done</span><br><span class="line">sqlline version 1.2.0</span><br><span class="line">0: jdbc:phoenix:hadoop001,hadoop002,hadoop003&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h3 id="6-2-2-Phoenix-Shell操作"><a href="#6-2-2-Phoenix-Shell操作" class="headerlink" title="6.2.2 Phoenix Shell操作"></a>6.2.2 Phoenix Shell操作</h3><ol>
<li>表的操作<ul>
<li>显示所有表  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:phoenix:hadoop001,hadoop002,hadoop003<span class="operator">&gt;</span> <span class="operator">!</span><span class="keyword">table</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+--------------+-------------+---------------+----------+------------+----------------------------+-----------------+--------------+-----------------+---------------+</span></span><br><span class="line"><span class="operator">|</span> TABLE_CAT  <span class="operator">|</span> TABLE_SCHEM  <span class="operator">|</span> TABLE_NAME  <span class="operator">|</span>  TABLE_TYPE   <span class="operator">|</span> REMARKS  <span class="operator">|</span> TYPE_NAME  <span class="operator">|</span> SELF_REFERENCING_COL_NAME  <span class="operator">|</span> REF_GENERATION  <span class="operator">|</span> INDEX_STATE  <span class="operator">|</span> IMMUTABLE_ROWS  <span class="operator">|</span> SALT_BUCKETS  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+--------------+-------------+---------------+----------+------------+----------------------------+-----------------+--------------+-----------------+---------------+</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> CATALOG     <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">false</span>           <span class="operator">|</span> <span class="keyword">null</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> <span class="keyword">FUNCTION</span>    <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">false</span>           <span class="operator">|</span> <span class="keyword">null</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> LOG         <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">true</span>            <span class="operator">|</span> <span class="number">32</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> SEQUENCE    <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">false</span>           <span class="operator">|</span> <span class="keyword">null</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> STATS       <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">false</span>           <span class="operator">|</span> <span class="keyword">null</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+--------------+-------------+---------------+----------+------------+----------------------------+-----------------+--------------+-----------------+---------------+</span></span><br><span class="line"><span class="number">0</span>: jdbc:phoenix:hadoop001,hadoop002,hadoop003<span class="operator">&gt;</span> <span class="operator">!</span>tables</span><br><span class="line"><span class="operator">+</span><span class="comment">------------+--------------+-------------+---------------+----------+------------+----------------------------+-----------------+--------------+-----------------+---------------+</span></span><br><span class="line"><span class="operator">|</span> TABLE_CAT  <span class="operator">|</span> TABLE_SCHEM  <span class="operator">|</span> TABLE_NAME  <span class="operator">|</span>  TABLE_TYPE   <span class="operator">|</span> REMARKS  <span class="operator">|</span> TYPE_NAME  <span class="operator">|</span> SELF_REFERENCING_COL_NAME  <span class="operator">|</span> REF_GENERATION  <span class="operator">|</span> INDEX_STATE  <span class="operator">|</span> IMMUTABLE_ROWS  <span class="operator">|</span> SALT_BUCKETS  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+--------------+-------------+---------------+----------+------------+----------------------------+-----------------+--------------+-----------------+---------------+</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> CATALOG     <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">false</span>           <span class="operator">|</span> <span class="keyword">null</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> <span class="keyword">FUNCTION</span>    <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">false</span>           <span class="operator">|</span> <span class="keyword">null</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> LOG         <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">true</span>            <span class="operator">|</span> <span class="number">32</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> SEQUENCE    <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">false</span>           <span class="operator">|</span> <span class="keyword">null</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> STATS       <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">false</span>           <span class="operator">|</span> <span class="keyword">null</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+--------------+-------------+---------------+----------+------------+----------------------------+-----------------+--------------+-----------------+---------------+</span></span><br><span class="line"><span class="number">0</span>: jdbc:phoenix:hadoop001,hadoop002,hadoop003<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>创建表  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 直接指定单个列作为RowKey</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> STUDENT (</span><br><span class="line">    id <span class="type">VARCHAR</span> <span class="keyword">primary</span> key,</span><br><span class="line">    name <span class="type">VARCHAR</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 指定多个列的联合作为RowKey</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> US_POPULATION (</span><br><span class="line">    State <span class="type">CHAR</span>(<span class="number">2</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">    City <span class="type">VARCHAR</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">    Population <span class="type">BIGINT</span></span><br><span class="line"><span class="keyword">CONSTRAINT</span> my_pk <span class="keyword">PRIMARY</span> KEY (state, city));</span><br></pre></td></tr></table></figure>
<blockquote>
<p>在phoenix中，表名等会自动转换为大写，若要小写，使用双引号如”us_population”。</p>
</blockquote>
</li>
<li>插入数据</li>
<li>查询记录  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">where</span> id<span class="operator">=</span><span class="string">&#x27;1001&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>删除记录  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> student <span class="keyword">where</span> id<span class="operator">=</span><span class="string">&#x27;1001&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>删除表  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> student;</span><br></pre></td></tr></table></figure></li>
<li>退出命令行  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">!</span>quit</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>表的映射<br> 默认情况下，直接在HBase中创建的表，通过Phoenix是查看不到的。如果要在Phoenix中操作直接在HBase中创建的表，则需要在Phoenix中进行表的映射。映射方式有两种：视图映射和表映射。<ul>
<li>视图映射<ol>
<li>Phoenix创建的视图是只读的，所以只能用来做查询，无法通过视图对源数据进行修改等操作</li>
<li>HBase创建表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hbase(main):<span class="number">031</span>:<span class="number">0</span><span class="operator">&gt;</span> <span class="keyword">create</span> <span class="string">&#x27;test&#x27;</span>,<span class="string">&#x27;info1&#x27;</span>,<span class="string">&#x27;info2&#x27;</span></span><br><span class="line">Created <span class="keyword">table</span> test</span><br><span class="line">Took <span class="number">0.7934</span> seconds</span><br><span class="line"><span class="operator">=</span><span class="operator">&gt;</span> Hbase::<span class="keyword">Table</span> <span class="operator">-</span> test</span><br><span class="line">hbase(main):<span class="number">032</span>:<span class="number">0</span><span class="operator">&gt;</span> put <span class="string">&#x27;test&#x27;</span>,<span class="string">&#x27;10001&#x27;</span>,<span class="string">&#x27;info1:name&#x27;</span>,<span class="string">&#x27;zhangsan&#x27;</span></span><br><span class="line">Took <span class="number">0.0326</span> seconds</span><br><span class="line">hbase(main):<span class="number">033</span>:<span class="number">0</span><span class="operator">&gt;</span> put <span class="string">&#x27;test&#x27;</span>,<span class="string">&#x27;10002&#x27;</span>,<span class="string">&#x27;info1:name&#x27;</span>,<span class="string">&#x27;zhangsan&#x27;</span></span><br><span class="line">Took <span class="number">0.0040</span> seconds</span><br><span class="line">hbase(main):<span class="number">034</span>:<span class="number">0</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>Phoenix创建视图，查询 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:phoenix:thin:url<span class="operator">=</span>http:<span class="operator">/</span><span class="operator">/</span>localhost:<span class="number">876</span><span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">view</span> &quot;test&quot;(id <span class="type">varchar</span> <span class="keyword">primary</span> key,&quot;info1&quot;.&quot;name&quot; <span class="type">varchar</span>, &quot;info2&quot;.&quot;address&quot; <span class="type">varchar</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">5.836</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:phoenix:thin:url<span class="operator">=</span>http:<span class="operator">/</span><span class="operator">/</span>localhost:<span class="number">876</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> &quot;test&quot;;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------+-----------+----------+</span></span><br><span class="line"><span class="operator">|</span>   ID   <span class="operator">|</span>   name    <span class="operator">|</span> address  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------+-----------+----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10001</span>  <span class="operator">|</span> zhangsan  <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10002</span>  <span class="operator">|</span> zhangsan  <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------+-----------+----------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.044</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>删除视图<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:phoenix:thin:url<span class="operator">=</span>http:<span class="operator">/</span><span class="operator">/</span>localhost:<span class="number">876</span><span class="operator">&gt;</span> <span class="keyword">drop</span> <span class="keyword">view</span> &quot;test&quot;;</span><br><span class="line"> <span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.015</span> seconds)</span><br><span class="line"> <span class="number">0</span>: jdbc:phoenix:thin:url<span class="operator">=</span>http:<span class="operator">/</span><span class="operator">/</span>localhost:<span class="number">876</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>表映射<ol>
<li>HBase中不存在表时，可以直接使用create table指令创建需要的表,系统将会自动在Phoenix和HBase中创建person_infomation的表，并会根据指令内的参数对表结构进行初始化。</li>
<li>当HBase中已经存在表时，可以以类似创建视图的方式创建关联表，只需要将create view改为create table即可。<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:phoenix:hadoop101,hadoop102,hadoop103<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> &quot;test&quot;(id <span class="type">varchar</span> <span class="keyword">primary</span> key,&quot;info1&quot;.&quot;name&quot; <span class="type">varchar</span>, &quot;info2&quot;.&quot;address&quot; <span class="type">varchar</span>) column_encoded_bytes<span class="operator">=</span><span class="number">0</span>;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
</li>
</ol>
<h3 id="6-2-3-Phoenix-JDBC操作"><a href="#6-2-3-Phoenix-JDBC操作" class="headerlink" title="6.2.3 Phoenix JDBC操作"></a>6.2.3 Phoenix JDBC操作</h3><ol>
<li>创建项目并导入依赖 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.phoenix/phoenix-queryserver-client --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-queryserver-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.0.0-HBase-2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>编写代码 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PhoenixJDBC</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> String connectionUrl = ThinClientUtil.getConnectionUrl(<span class="string">&quot;hadoop001&quot;</span>, <span class="number">8765</span>);</span><br><span class="line">        System.out.println(connectionUrl);</span><br><span class="line">        <span class="keyword">try</span> (</span><br><span class="line"></span><br><span class="line">                <span class="keyword">final</span> Connection connection = DriverManager.getConnection(connectionUrl);</span><br><span class="line">                <span class="keyword">final</span> PreparedStatement preparedStatement = connection.prepareStatement(<span class="string">&quot;select * from student&quot;</span>);</span><br><span class="line"></span><br><span class="line">        )&#123;</span><br><span class="line">            <span class="keyword">final</span> ResultSet resultSet = preparedStatement.executeQuery();</span><br><span class="line">            <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">                <span class="keyword">final</span> String id = resultSet.getString(<span class="number">1</span>);</span><br><span class="line">                <span class="keyword">final</span> String name = resultSet.getString(<span class="number">2</span>);</span><br><span class="line">                System.out.println(<span class="string">&quot;id: &quot;</span> + id + <span class="string">&quot;,&quot;</span> + <span class="string">&quot;name: &quot;</span> + name);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="6-3-Phoenix二级索引"><a href="#6-3-Phoenix二级索引" class="headerlink" title="6.3 Phoenix二级索引"></a>6.3 Phoenix二级索引</h2><p>已经有索引的情况下，</p>
<h3 id="6-3-1-HBase协处理器（扩展）"><a href="#6-3-1-HBase协处理器（扩展）" class="headerlink" title="6.3.1 HBase协处理器（扩展）"></a>6.3.1 HBase协处理器（扩展）</h3><ol>
<li>案例需求<br> 编写协处理器，实现在往A表插入数据的同时让HBase自身（协处理器）向B表中插入一条数据。</li>
<li>实现步骤<ol>
<li>创建一个maven项目，并引入以下依赖。 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">&lt;</span>dependencies<span class="operator">&gt;</span></span><br><span class="line">    <span class="operator">&lt;</span>dependency<span class="operator">&gt;</span></span><br><span class="line">        <span class="operator">&lt;</span>groupId<span class="operator">&gt;</span>org.apache.hbase<span class="operator">&lt;</span><span class="operator">/</span>groupId<span class="operator">&gt;</span></span><br><span class="line">        <span class="operator">&lt;</span>artifactId<span class="operator">&gt;</span>hbase<span class="operator">-</span>client<span class="operator">&lt;</span><span class="operator">/</span>artifactId<span class="operator">&gt;</span></span><br><span class="line">        <span class="operator">&lt;</span>version<span class="operator">&gt;</span><span class="number">2.2</span><span class="number">.4</span><span class="operator">&lt;</span><span class="operator">/</span>version<span class="operator">&gt;</span></span><br><span class="line">    <span class="operator">&lt;</span><span class="operator">/</span>dependency<span class="operator">&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="operator">&lt;</span>dependency<span class="operator">&gt;</span></span><br><span class="line">        <span class="operator">&lt;</span>groupId<span class="operator">&gt;</span>org.apache.hbase<span class="operator">&lt;</span><span class="operator">/</span>groupId<span class="operator">&gt;</span></span><br><span class="line">        <span class="operator">&lt;</span>artifactId<span class="operator">&gt;</span>hbase<span class="operator">-</span>server<span class="operator">&lt;</span><span class="operator">/</span>artifactId<span class="operator">&gt;</span></span><br><span class="line">        <span class="operator">&lt;</span>version<span class="operator">&gt;</span><span class="number">2.2</span><span class="number">.4</span><span class="operator">&lt;</span><span class="operator">/</span>version<span class="operator">&gt;</span></span><br><span class="line">    <span class="operator">&lt;</span><span class="operator">/</span>dependency<span class="operator">&gt;</span></span><br><span class="line"><span class="operator">&lt;</span><span class="operator">/</span>dependencies<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>定义FruitTableCoprocessor类并继承BaseRegionObserver类 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FruitTableCoprocessor</span> <span class="keyword">extends</span> <span class="title">BaseRegionObserver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">postPut</span><span class="params">(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取连接</span></span><br><span class="line">        Connection connection = ConnectionFactory.createConnection(HBaseConfiguration.create());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取表对象</span></span><br><span class="line">        Table table = connection.getTable(TableName.valueOf(<span class="string">&quot;fruit&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//插入数据</span></span><br><span class="line">        table.put(put);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//关闭资源</span></span><br><span class="line">        table.close();</span><br><span class="line">        connection.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h3 id="6-3-2-二级索引配置文件"><a href="#6-3-2-二级索引配置文件" class="headerlink" title="6.3.2 二级索引配置文件"></a>6.3.2 二级索引配置文件</h3><ul>
<li>添加如下配置到HBase的HRegionserver节点的hbase-site.xml  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- phoenix regionserver 配置参数--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.wal.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.region.server.rpc.scheduler.factory.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rpc.controllerfactory.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="6-3-3-全局二级索引"><a href="#6-3-3-全局二级索引" class="headerlink" title="6.3.3 全局二级索引"></a>6.3.3 全局二级索引</h3><ul>
<li><p>Global Index是默认的索引格式，创建全局索引时，会在HBase中建立一张新表。也就是说索引数据和数据表是存放在不同的表中的，因此<font color ='red' >全局索引适用于多读少写的业务场景</font>。</p>
</li>
<li><p>写数据的时候会消耗大量开销，因为索引表也要更新，而索引表是分布在不同的数据节点上的，跨节点的数据传输带来了较大的性能消耗。</p>
</li>
<li><p>在读数据的时候Phoenix会选择索引表来降低查询消耗的时间。</p>
</li>
<li><p>创建单个字段的全局索引</p>
  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> INDEX my_index <span class="keyword">ON</span> my_table (my_col);</span><br></pre></td></tr></table></figure>
<p>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16361817666391.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16361818090972.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<blockquote>
<p><font color ='red' >如果想查询的字段不是索引字段的话索引表不会被使用，也就是说不会带来查询速度的提升。</font></p>
</blockquote>
<ol>
<li>联合索引 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:phoenix:thin:url<span class="operator">=</span>http:<span class="operator">/</span><span class="operator">/</span>localhost:<span class="number">876</span><span class="operator">&gt;</span> <span class="keyword">create</span> index IDX_STU_NAME_AGE_IN <span class="keyword">on</span> STUDENT(name) INCLUDE(age);</span><br><span class="line"><span class="number">6</span> <span class="keyword">rows</span> affected (<span class="number">5.78</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>INCLOUD <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:phoenix:thin:url<span class="operator">=</span>http:<span class="operator">/</span><span class="operator">/</span>localhost:<span class="number">876</span><span class="operator">&gt;</span> <span class="keyword">create</span> index IDX_STU_NAME_AGE_IN <span class="keyword">on</span> STUDENT(name) INCLUDE(age);</span><br><span class="line"><span class="number">6</span> <span class="keyword">rows</span> affected (<span class="number">5.78</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<h3 id="6-3-4-本地二级索引"><a href="#6-3-4-本地二级索引" class="headerlink" title="6.3.4 本地二级索引"></a>6.3.4 本地二级索引</h3><p>Local Index适用于写操作频繁的场景。<br>索引数据和数据表的数据是存放在同一张表中（且是同一个Region），避免了在写操作的时候往不同服务器的索引表中写索引带来的额外开销。查询的字段不是索引字段索引表也会被使用，这会带来查询速度的提升。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">LOCAL</span> INDEX my_index <span class="keyword">ON</span> my_table (my_column);</span><br></pre></td></tr></table></figure>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16361840708409.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h1 id="第7章-与Hive的集成"><a href="#第7章-与Hive的集成" class="headerlink" title="第7章 与Hive的集成"></a>第7章 与Hive的集成</h1><h2 id="7-1-HBase与Hive的对比"><a href="#7-1-HBase与Hive的对比" class="headerlink" title="7.1 HBase与Hive的对比"></a>7.1 HBase与Hive的对比</h2><ol>
<li>Hive<ul>
<li>数仓工具<br>  Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。</li>
<li>用于数据分析、清洗<br>  Hive适用于离线的数据分析和清洗，延迟较高。</li>
<li>基于HDFS、MapReduce<br>  Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。</li>
</ul>
</li>
<li>HBase<ol>
<li>数据库<br> 是一种面向列族存储的非关系型数据库。</li>
<li>用于存储结构化和非结构化的数据<br> 适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。</li>
<li>基于HDFS<br> 数据持久化存储的体现形式是HFile，存放于DataNode中，被ResionServer以region的形式进行管理。</li>
<li>延迟较低，接入在线业务使用<br> 面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。</li>
</ol>
</li>
</ol>
<h2 id="7-2-HBase与Hive集成使用"><a href="#7-2-HBase与Hive集成使用" class="headerlink" title="7.2 HBase与Hive集成使用"></a>7.2 HBase与Hive集成使用</h2><ol>
<li><p>HBase没有计算分析能力，用Hive辅助分析</p>
</li>
<li><p>HBase扮演HDFS的角色，提供数据存储<br>在hive-site.xml中修改zookeeper的属性，如下：</p>
 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102,hadoop103,hadoop104<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.client.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>建立Hive表，关联HBase表，插入数据到Hive表的同时能够影响HBase表。</p>
<ol>
<li>在Hive中创建表同时关联HBase <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hive_hbase_emp_table(</span><br><span class="line">    empno    <span class="type">int</span>,</span><br><span class="line">    ename    string,</span><br><span class="line">    job      string,</span><br><span class="line">    mgr      <span class="type">int</span>,</span><br><span class="line">    hiredate string,</span><br><span class="line">    sal      <span class="keyword">double</span>,</span><br><span class="line">    comm     <span class="keyword">double</span>,</span><br><span class="line">    deptno   <span class="type">int</span></span><br><span class="line">)</span><br><span class="line">STORED <span class="keyword">BY</span> <span class="string">&#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;</span></span><br><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; <span class="operator">=</span> &quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;)</span><br><span class="line">TBLPROPERTIES (&quot;hbase.table.name&quot; <span class="operator">=</span> &quot;hbase_emp_table&quot;);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>提示：完成之后，可以分别进入Hive和HBase查看，都生成了对应的表</p>
</blockquote>
</li>
<li>在Hive中导入数据到hive_hbase_emp_table <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> hive_hbase_emp_table</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal<span class="operator">&gt;</span><span class="number">3000</span>;</span><br></pre></td></tr></table></figure></li>
<li>查看Hive以及关联的HBase表中是否已经成功的同步插入了数据<ul>
<li>Hive：  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> hive_hbase_emp_table;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------------------+-----------------------------+---------------------------+---------------------------+--------------------------------+---------------------------+----------------------------+------------------------------+</span></span><br><span class="line"><span class="operator">|</span> hive_hbase_emp_table.empno  <span class="operator">|</span> hive_hbase_emp_table.ename  <span class="operator">|</span> hive_hbase_emp_table.job  <span class="operator">|</span> hive_hbase_emp_table.mgr  <span class="operator">|</span> hive_hbase_emp_table.hiredate  <span class="operator">|</span> hive_hbase_emp_table.sal  <span class="operator">|</span> hive_hbase_emp_table.comm  <span class="operator">|</span> hive_hbase_emp_table.deptno  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------------------+-----------------------------+---------------------------+---------------------------+--------------------------------+---------------------------+----------------------------+------------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7839</span>                        <span class="operator">|</span> KING                        <span class="operator">|</span> PRESIDENT                 <span class="operator">|</span> <span class="keyword">NULL</span>                      <span class="operator">|</span> <span class="number">1981</span><span class="number">-11</span><span class="number">-17</span>                     <span class="operator">|</span> <span class="number">5000.0</span>                    <span class="operator">|</span> <span class="keyword">NULL</span>                       <span class="operator">|</span> <span class="number">10</span>                           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------------------+-----------------------------+---------------------------+---------------------------+--------------------------------+---------------------------+----------------------------+------------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.237</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>HBase：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):010:0&gt; scan <span class="string">&#x27;hbase_emp_table&#x27;</span></span><br><span class="line">ROW                                           COLUMN+CELL</span><br><span class="line"> 7839                                         column=info:deptno, timestamp=1636185055717, value=10</span><br><span class="line"> 7839                                         column=info:ename, timestamp=1636185055717, value=KING</span><br><span class="line"> 7839                                         column=info:hiredate, timestamp=1636185055717, value=1981-11-17</span><br><span class="line"> 7839                                         column=info:job, timestamp=1636185055717, value=PRESIDENT</span><br><span class="line"> 7839                                         column=info:sal, timestamp=1636185055717, value=5000.0</span><br><span class="line">1 row(s)</span><br><span class="line">Took 0.0418 seconds</span><br><span class="line">hbase(main):011:0&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>在HBase中已经存储了某一张表hbase_emp_table，然后在Hive中创建一个外部表来关联HBase中的hbase_emp_table这张表，使之可以借助Hive来分析HBase这张表中的数据。<ol>
<li>在Hive中创建外部表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> relevance_hbase_emp (</span><br><span class="line">    empno    <span class="type">int</span>,</span><br><span class="line">    ename    string,</span><br><span class="line">    job      string,</span><br><span class="line">    mgr      <span class="type">int</span>,</span><br><span class="line">    hiredate string,</span><br><span class="line">    sal      <span class="keyword">double</span>,</span><br><span class="line">    comm     <span class="keyword">double</span>,</span><br><span class="line">    deptno   <span class="type">int</span></span><br><span class="line">)</span><br><span class="line">STORED <span class="keyword">BY</span> <span class="string">&#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;</span></span><br><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; <span class="operator">=</span> &quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;)</span><br><span class="line">TBLPROPERTIES (&quot;hbase.table.name&quot; <span class="operator">=</span> &quot;hbase_emp_table&quot;);</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>关联后就可以使用Hive函数进行一些分析操作了 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> relevance_hbase_emp;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------+----------------------------+--------------------------+--------------------------+-------------------------------+--------------------------+---------------------------+-----------------------------+</span></span><br><span class="line"><span class="operator">|</span> relevance_hbase_emp.empno  <span class="operator">|</span> relevance_hbase_emp.ename  <span class="operator">|</span> relevance_hbase_emp.job  <span class="operator">|</span> relevance_hbase_emp.mgr  <span class="operator">|</span> relevance_hbase_emp.hiredate  <span class="operator">|</span> relevance_hbase_emp.sal  <span class="operator">|</span> relevance_hbase_emp.comm  <span class="operator">|</span> relevance_hbase_emp.deptno  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------+----------------------------+--------------------------+--------------------------+-------------------------------+--------------------------+---------------------------+-----------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7839</span>                       <span class="operator">|</span> KING                       <span class="operator">|</span> PRESIDENT                <span class="operator">|</span> <span class="keyword">NULL</span>                     <span class="operator">|</span> <span class="number">1981</span><span class="number">-11</span><span class="number">-17</span>                    <span class="operator">|</span> <span class="number">5000.0</span>                   <span class="operator">|</span> <span class="keyword">NULL</span>                      <span class="operator">|</span> <span class="number">10</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------+----------------------------+--------------------------+--------------------------+-------------------------------+--------------------------+---------------------------+-----------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.248</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>HBase</tag>
        <tag>Phoenix</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume</title>
    <url>/2021/11/07/Flume/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h1><h1 id="一、Flume-概述"><a href="#一、Flume-概述" class="headerlink" title="一、Flume 概述"></a>一、Flume 概述</h1><h2 id="1-1-Flume-定义"><a href="#1-1-Flume-定义" class="headerlink" title="1.1 Flume 定义"></a>1.1 Flume 定义</h2><ul>
<li>Flume 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传<br>输的系统。Flume 基于流式架构，灵活简单。<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16357485584074.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
<h2 id="1-2-Flume-基础架构"><a href="#1-2-Flume-基础架构" class="headerlink" title="1.2 Flume 基础架构"></a>1.2 Flume 基础架构</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16357485788013.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="1-2-1-Agent"><a href="#1-2-1-Agent" class="headerlink" title="1.2.1 Agent"></a>1.2.1 Agent</h3><ul>
<li>Agent 是一个 JVM 进程，它以事件的形式将数据从源头送至目的。</li>
<li>Agent 主要有 3 个部分组成，Source、Channel、Sink。<h3 id="1-2-2-Source"><a href="#1-2-2-Source" class="headerlink" title="1.2.2 Source"></a>1.2.2 Source</h3></li>
<li>Source 是负责接收数据到 Flume Agent 的组件。Source 组件可以处理各种类型、各种格式的日志数据，包括 avro、thrift、exec、jms、spooling directory、netcat、taildir、sequence generator、syslog、http、legacy。<h3 id="1-2-3-Sink"><a href="#1-2-3-Sink" class="headerlink" title="1.2.3 Sink"></a>1.2.3 Sink</h3></li>
<li>Sink 不断地轮询 Channel 中的事件且批量地移除它们，并将这些事件批量写入到存储<br>或索引系统、或者被发送到另一个 Flume Agent。</li>
<li>Sink 组件目的地包括 hdfs、logger、avro、thrift、ipc、file、HBase、solr、自定义。<h3 id="1-2-4-Channel"><a href="#1-2-4-Channel" class="headerlink" title="1.2.4 Channel"></a>1.2.4 Channel</h3></li>
<li>Channel 是位于 Source 和 Sink 之间的缓冲区。因此，Channel 允许 Source 和 Sink 运作在不同的速率上。Channel 是线程安全的，可以同时处理几个 Source 的写入操作和几个</li>
<li>Sink 的读取操作。<ul>
<li>Flume 自带两种 Channel：Memory Channel 和 File Channel。<ul>
<li>Memory Channel 是内存中的队列。Memory Channel 在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么 Memory Channel 就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。</li>
<li>File Channel 将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。<h3 id="1-2-5-Event"><a href="#1-2-5-Event" class="headerlink" title="1.2.5 Event"></a>1.2.5 Event</h3></li>
</ul>
</li>
</ul>
</li>
<li>传输单元，Flume 数据传输的基本单元，以 Event 的形式将数据从源头送至目的地。Event 由 Header 和 Body 两部分组成，Header 用来存放该 event 的一些属性，为 K-V 结构，</li>
<li>Body 用来存放该条数据，形式为字节数组。<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16357486063296.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
<h1 id="二、Flume入门"><a href="#二、Flume入门" class="headerlink" title="二、Flume入门"></a>二、Flume入门</h1><h2 id="2-1-Flume安装部署"><a href="#2-1-Flume安装部署" class="headerlink" title="2.1 Flume安装部署"></a>2.1 Flume安装部署</h2><h3 id="2-1-1-安装地址"><a href="#2-1-1-安装地址" class="headerlink" title="2.1.1 安装地址"></a>2.1.1 安装地址</h3><ol>
<li>Flume官网地址：<a href="http://flume.apache.org/">http://flume.apache.org/</a></li>
<li>文档查看地址：<a href="http://flume.apache.org/FlumeUserGuide.html">http://flume.apache.org/FlumeUserGuide.html</a></li>
<li>下载地址：<a href="http://archive.apache.org/dist/flume/">http://archive.apache.org/dist/flume/</a><h3 id="2-1-2-安装部署"><a href="#2-1-2-安装部署" class="headerlink" title="2.1.2 安装部署"></a>2.1.2 安装部署</h3></li>
<li>将apache-flume-1.9.0-bin.tar.gz上传到linux的/opt/software目录下</li>
<li>解压apache-flume-1.9.0-bin.tar.gz到/opt/module/目录下 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxf /opt/software/apache-flume-1.9.0-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li>
<li>修改apache-flume-1.9.0-bin的名称为flume <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ mv /opt/module/apache-flume-1.9.0-bin /opt/module/flume</span><br></pre></td></tr></table></figure></li>
<li>将lib文件夹下的guava-11.0.2.jar删除以兼容Hadoop 3.1.3 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 lib]$  rm /opt/module/flume/lib/guava-11.0.2.jar</span><br></pre></td></tr></table></figure></li>
<li>环境变量 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#FLUME_HOME</span></span><br><span class="line"><span class="built_in">export</span> FLUME_HOME=/opt/module/flume-1.9.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$FLUME_HOME</span>/bin</span><br></pre></td></tr></table></figure>
<h3 id="2-1-3-连接HDFS集群"><a href="#2-1-3-连接HDFS集群" class="headerlink" title="2.1.3 连接HDFS集群"></a>2.1.3 连接HDFS集群</h3></li>
<li>使用场景：应用服务器部署FLume Agent采集应用运行日志，上送到Flume Server，由FlumeServer存储到HDFS集群<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16359837253952.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>配置步骤<ol>
<li>复制Hadoop集群的hdfs-site.xml、core-site.xml两个配置到$FLUME_HOME/conf目录 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp <span class="variable">$HADOOP_HOME</span>/etc/hadoop/core-site.xml atguigu@datax:/opt/module/flume-1.9.0/conf</span><br><span class="line"></span><br><span class="line">scp <span class="variable">$HADOOP_HOME</span>/etc/hadoop/hdfs-site.xml atguigu@datax:/opt/module/flume-1.9.0/conf</span><br></pre></td></tr></table></figure></li>
<li>复制hadoop-common-3.1.3.jar到$FLUME_HOME/lib目录 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp <span class="variable">$HADOOP_HOME</span>/share/hadoop/common/hadoop-common-3.1.3.jar atguigu@datax:/opt/module/flume-1.9.0/lib</span><br></pre></td></tr></table></figure></li>
<li>复制Hadoop common依赖包到$FLUME_HOME/lib目录 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp <span class="variable">$HADOOP_HOME</span>/share/hadoop/common/lib/*.jar atguigu@datax:/opt/module/flume-1.9.0/lib</span><br></pre></td></tr></table></figure></li>
<li>复制<code>$HADOOP_HOME/share/hadoop/hdfs/*.jar</code>到$FLUME_HOME/lib目录 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp <span class="variable">$HADOOP_HOME</span>/share/hadoop/hdfs/*.jar atguigu@datax:/opt/module/flume-1.9.0/lib</span><br></pre></td></tr></table></figure></li>
<li>准备配置 a1.conf <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line"><span class="comment"># a1:表示agent的名称</span></span><br><span class="line"><span class="comment"># r1:表示a1的Source的名称</span></span><br><span class="line"><span class="attr">a1.sources</span> = r1</span><br><span class="line"><span class="comment"># k1:表示a1的Sink的名称</span></span><br><span class="line"><span class="attr">a1.sinks</span> = k1</span><br><span class="line"><span class="comment"># c1:表示a1的Channel的名称</span></span><br><span class="line"><span class="attr">a1.channels</span> = c1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="comment"># 表示a1的输入源类型为netcat端口类型</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = netcat</span><br><span class="line"><span class="comment"># 表示a1的监听的主机</span></span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line"><span class="comment"># 表示a1的监听的端口号</span></span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="number">44444</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = hdfs</span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.path</span> = hdfs://mycluster/flume/%Y%m%d/%H</span><br><span class="line"><span class="comment">#上传文件的前缀</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.filePrefix</span> = logs-</span><br><span class="line"><span class="comment">#是否按照时间滚动文件夹</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.round</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">#多少时间单位创建一个新的文件夹</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.roundValue</span> = <span class="number">1</span></span><br><span class="line"><span class="comment">#重新定义时间单位</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.roundUnit</span> = hour</span><br><span class="line"><span class="comment">#是否使用本地时间戳</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.useLocalTimeStamp</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">#积攒多少个Event才flush到HDFS一次</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.batchSize</span> = <span class="number">100</span></span><br><span class="line"><span class="comment">#设置文件类型，可支持压缩</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.fileType</span> = DataStream</span><br><span class="line"><span class="comment">#多久生成一个新的文件</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollInterval</span> = <span class="number">60</span></span><br><span class="line"><span class="comment">#设置每个文件的滚动大小</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollSize</span> = <span class="number">134217700</span></span><br><span class="line"><span class="comment">#文件的滚动与Event数量无关</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollCount</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="number">1000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = c1</span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>启动Flume <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/conf/<span class="built_in">jobs</span>/a1.conf -n a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li>
<li>打开nc测试，检查HDFS文件</li>
</ol>
</li>
<li>异常记录<ol>
<li><code>java.lang.NoClassDefFoundError</code>类型的异常为缺少jar包导致<ul>
<li>解决方案：复制hadoop-common-3.1.3.jar到$FLUME_HOME/lib目录</li>
</ul>
</li>
<li><code>java.lang.NoSuchMethodError</code>类型异常为jar包冲突导致<ul>
<li>解决方案：相同jar包保留一个版本，具体保留那个需要测试是否存在兼容性问题，优先保留Flume原生依赖jar</li>
</ul>
</li>
<li><code>No FileSystem for scheme &quot;hdfs&quot;</code>异常 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2021-11-04 10:32:45,923 (SinkRunner-PollingRunner-DefaultSinkProcessor) [WARN - org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:454)] HDFS IO error</span><br><span class="line">org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme &quot;hdfs&quot;</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)</span><br><span class="line">	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)</span><br><span class="line">	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)</span><br><span class="line">	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)</span><br><span class="line">	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)</span><br><span class="line">	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)</span><br><span class="line">	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)</span><br><span class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure>
<ul>
<li>解决方案：复制<code>$HADOOP_HOME/share/hadoop/hdfs/*.jar</code>到$FLUME_HOME/lib目录  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp <span class="variable">$HADOOP_HOME</span>/share/hadoop/hdfs/*.jar atguigu@datax:/opt/module/flume-1.9.0/lib</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><code>java.net.UnknownHostException: mycluster</code> 找不到mycluster<ul>
<li>解决方案：复制Hadoop集群的hdfs-site.xml、core-site.xml两个配置到$FLUME_HOME/conf目录  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ha-hadoop-3.1.3]$ scp <span class="variable">$HADOOP_HOME</span>/etc/hadoop/core-site.xml atguigu@datax:/opt/module/flume-1.9.0/conf</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop001 ha-hadoop-3.1.3]$ scp <span class="variable">$HADOOP_HOME</span>/etc/hadoop/hdfs-site.xml atguigu@datax:/opt/module/flume-1.9.0/conf</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><code>org.apache.hadoop.security.AccessControlException: Permission denied:</code> HDFS 权限不足，换用户 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">chown -R atguigu:atguigu flume-1.9.0/</span><br><span class="line">use atguigu</span><br></pre></td></tr></table></figure>
<h2 id="2-2-Flume入门案例"><a href="#2-2-Flume入门案例" class="headerlink" title="2.2 Flume入门案例"></a>2.2 Flume入门案例</h2><h3 id="2-2-1-监控端口数据官方案例"><a href="#2-2-1-监控端口数据官方案例" class="headerlink" title="2.2.1 监控端口数据官方案例"></a>2.2.1 监控端口数据官方案例</h3></li>
</ol>
</li>
<li>案例需求：<ul>
<li>使用Flume监听一个端口，收集该端口数据，并打印到控制台。 </li>
</ul>
</li>
<li>需求分析：<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16357500185897.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>实现步骤：<ol>
<li>安装netcat工具 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 module]$ sudo yum -y install nc</span><br></pre></td></tr></table></figure>
<ul>
<li>测试  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 开启端口</span></span><br><span class="line">[atguigu@hadoop001 ~]$ nc -l localhost 6666</span><br><span class="line">asdf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试连通</span></span><br><span class="line">[atguigu@hadoop001 ~]$ nc localhost 6666</span><br><span class="line">asdf</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>判断44444端口是否被占用 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 flume-telnet]$ sudo netstat -nlp | grep 44444</span><br></pre></td></tr></table></figure></li>
<li>在flume目录下创建job文件夹并进入job文件夹。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ mkdir job</span><br><span class="line">[atguigu@hadoop102 flume]$ <span class="built_in">cd</span> job/</span><br></pre></td></tr></table></figure></li>
<li>在job文件夹下创建Flume Agent配置文件flume-netcat-logger.conf。添加内容如下： <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line"><span class="comment"># a1:表示agent的名称</span></span><br><span class="line"><span class="comment"># r1:表示a1的Source的名称</span></span><br><span class="line"><span class="attr">a1.sources</span> = r1</span><br><span class="line"><span class="comment"># k1:表示a1的Sink的名称</span></span><br><span class="line"><span class="attr">a1.sinks</span> = k1</span><br><span class="line"><span class="comment"># c1:表示a1的Channel的名称</span></span><br><span class="line"><span class="attr">a1.channels</span> = c1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="comment"># 表示a1的输入源类型为netcat端口类型</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = netcat</span><br><span class="line"><span class="comment"># 表示a1的监听的主机</span></span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="comment"># 表示a1的监听的端口号</span></span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="number">44444</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="comment"># 表示a1的输出目的地是控制台logger类型</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = logger</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line"><span class="comment"># 表示a1的channel类型是memory内存型</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = memory</span><br><span class="line"><span class="comment"># 表示a1的channel总容量1000个event</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="number">1000</span></span><br><span class="line"><span class="comment"># 表示a1的channel传输时收集到了100条event以后再去提交事务</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="comment"># 表示将r1和c1连接起来</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = c1</span><br><span class="line"><span class="comment"># 表示将k1和c1连接起来</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>先开启flume监听端口<ul>
<li>第一种写法：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li>
<li>第二种写法：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent -c conf/ -n a1 -f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>
<ul>
<li>参数说明：<ul>
<li>–conf/-c：表示配置文件存储在conf/目录</li>
<li>–name/-n：表示给agent起名为a1</li>
<li>–conf-file/-f：flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。</li>
<li>-Dflume.root.logger=INFO,console ：-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
</ol>
<pre><code>7. 使用netcat工具向本机的44444端口发送内容
    <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ nc localhost 44444</span><br><span class="line">hello</span><br><span class="line">OK</span><br><span class="line">atguigu</span><br><span class="line">OK</span><br><span class="line">123</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>
8. 在Flume监听页面观察接收数据情况
    <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">2021-11-01 16:15:15,418 INFO  [lifecycleSupervisor-1-1] source.NetcatSource (NetcatSource.java:start(166)) - Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444]</span><br><span class="line">2021-11-01 16:15:47,687 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] sink.LoggerSink (LoggerSink.java:process(95)) - Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F                                  hello &#125;</span><br><span class="line">2021-11-01 16:15:51,920 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] sink.LoggerSink (LoggerSink.java:process(95)) - Event: &#123; headers:&#123;&#125; body: 61 74 67 75 69 67 75                            atguigu &#125;</span><br><span class="line">2021-11-01 16:15:53,944 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] sink.LoggerSink (LoggerSink.java:process(95)) - Event: &#123; headers:&#123;&#125; body: 31 32 33                                        123 &#125;</span><br></pre></td></tr></table></figure>
</code></pre>
<h3 id="2-2-2-实时监控单个追加文件"><a href="#2-2-2-实时监控单个追加文件" class="headerlink" title="2.2.2 实时监控单个追加文件"></a>2.2.2 实时监控单个追加文件</h3><ol>
<li>案例需求：实时监控Hive日志，并上传到HDFS中</li>
<li>需求分析：<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16357546764719.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>实现步骤：<ol>
<li>Flume要想将数据输出到HDFS，依赖Hadoop相关jar包。确认Hadoop和Java环境变量配置正确 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#JAVA_HOME</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="comment">#HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/ha-hadoop-3.1.3</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure></li>
<li>创建flume-file-hdfs.conf文件<ul>
<li>注：要想读取Linux系统中的文件，就得按照Linux命令的规则执行命令。由于Hive日志在Linux系统中所以读取文件的类型选择：exec即execute执行的意思。表示执行Linux命令来读取文件。  <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line"><span class="attr">a2.sources</span> = r2</span><br><span class="line"><span class="attr">a2.sinks</span> = k2</span><br><span class="line"><span class="attr">a2.channels</span> = c2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="attr">a2.sources.r2.type</span> = exec</span><br><span class="line"><span class="attr">a2.sources.r2.command</span> = tail -f /opt/module/hive/logs/hive.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="attr">a2.sinks.k2.type</span> = hdfs</span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.path</span> = hdfs://hadoop001:<span class="number">8020</span>/flume/%Y%m%d/%H</span><br><span class="line"><span class="comment">#上传文件的前缀</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.filePrefix</span> = logs-</span><br><span class="line"><span class="comment">#是否按照时间滚动文件夹</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.round</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">#多少时间单位创建一个新的文件夹</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.roundValue</span> = <span class="number">1</span></span><br><span class="line"><span class="comment">#重新定义时间单位</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.roundUnit</span> = hour</span><br><span class="line"><span class="comment">#是否使用本地时间戳</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.useLocalTimeStamp</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">#积攒多少个Event才flush到HDFS一次</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.batchSize</span> = <span class="number">100</span></span><br><span class="line"><span class="comment">#设置文件类型，可支持压缩</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.fileType</span> = DataStream</span><br><span class="line"><span class="comment">#多久生成一个新的文件</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.rollInterval</span> = <span class="number">60</span></span><br><span class="line"><span class="comment">#设置每个文件的滚动大小</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.rollSize</span> = <span class="number">134217700</span></span><br><span class="line"><span class="comment">#文件的滚动与Event数量无关</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.rollCount</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line"><span class="attr">a2.channels.c2.type</span> = memory</span><br><span class="line"><span class="attr">a2.channels.c2.capacity</span> = <span class="number">1000</span></span><br><span class="line"><span class="attr">a2.channels.c2.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="attr">a2.sources.r2.channels</span> = c2</span><br><span class="line"><span class="attr">a2.sinks.k2.channel</span> = c2</span><br></pre></td></tr></table></figure></li>
<li>注意: 对于所有与时间相关的转义序列，Event Header中必须存在以 “timestamp”的key（除非hdfs.useLocalTimeStamp设置为true，此方法会使用TimestampInterceptor自动添加timestamp）。a3.sinks.k3.hdfs.useLocalTimeStamp = true</li>
</ul>
</li>
<li>运行Flume <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">flume-ng agent --conf conf/ --name a2 --conf-file flume-file-hdfs.conf</span><br></pre></td></tr></table></figure></li>
<li>在HDFS上查看文件。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 job]$ hadoop fs -ls /flume/20211101/16</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 atguigu supergroup      79991 2021-11-01 16:48 /flume/20211101/16/logs-.1635756437252</span><br><span class="line">-rw-r--r--   3 atguigu supergroup       5037 2021-11-01 16:49 /flume/20211101/16/logs-.1635756498528</span><br><span class="line">[atguigu@hadoop001 job]$</span><br></pre></td></tr></table></figure>
<h3 id="2-2-3-实时监控目录下多个新文件"><a href="#2-2-3-实时监控目录下多个新文件" class="headerlink" title="2.2.3 实时监控目录下多个新文件"></a>2.2.3 实时监控目录下多个新文件</h3>1）案例需求：使用Flume监听整个目录的文件，并上传至HDFS<br>2）需求分析：</li>
</ol>
</li>
</ol>
<h3 id="2-2-4-实时监控目录下的多个追加文件"><a href="#2-2-4-实时监控目录下的多个追加文件" class="headerlink" title="2.2.4 实时监控目录下的多个追加文件"></a>2.2.4 实时监控目录下的多个追加文件</h3><ul>
<li>Exec source适用于监控一个实时追加的文件，不能实现断点续传；</li>
<li>Spooldir Source适合用于同步新文件，但不适合对实时追加日志的文件进行监听并同步；</li>
<li>Taildir Source适合用于监听多个实时追加的文件，并且能够实现断点续传。</li>
</ul>
<ol>
<li>案例需求:使用Flume监听整个目录的实时追加文件，并上传至HDFS</li>
<li>需求分析:<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16357594199633.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>实现步骤：<ol>
<li>创建配置文件flume-taildir-hdfs.conf, 添加如下内容 <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">a3.sources</span> = r3</span><br><span class="line"><span class="attr">a3.sinks</span> = k3</span><br><span class="line"><span class="attr">a3.channels</span> = c3</span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="attr">a3.sources.r3.type</span> = TAILDIR</span><br><span class="line"><span class="attr">a3.sources.r3.positionFile</span> = /opt/module/flume/tail_dir.json</span><br><span class="line"><span class="attr">a3.sources.r3.filegroups</span> = f1 f2</span><br><span class="line"><span class="attr">a3.sources.r3.filegroups.f1</span> = /opt/module/flume/files/.*file.*</span><br><span class="line"><span class="attr">a3.sources.r3.filegroups.f2</span> = /opt/module/flume/files2/.*log.*</span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="attr">a3.sinks.k3.type</span> = hdfs</span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.path</span> = hdfs://hadoop001:<span class="number">8020</span>/flume/TAILDIR/%Y%m%d/%H</span><br><span class="line"><span class="comment">#上传文件的前缀</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.filePrefix</span> = upload-</span><br><span class="line"><span class="comment">#是否按照时间滚动文件夹</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.round</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">#多少时间单位创建一个新的文件夹</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.roundValue</span> = <span class="number">1</span></span><br><span class="line"><span class="comment">#重新定义时间单位</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.roundUnit</span> = hour</span><br><span class="line"><span class="comment">#是否使用本地时间戳</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.useLocalTimeStamp</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">#积攒多少个Event才flush到HDFS一次</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.batchSize</span> = <span class="number">100</span></span><br><span class="line"><span class="comment">#设置文件类型，可支持压缩</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.fileType</span> = DataStream</span><br><span class="line"><span class="comment">#多久生成一个新的文件</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.rollInterval</span> = <span class="number">60</span></span><br><span class="line"><span class="comment">#设置每个文件的滚动大小大概是128M</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.rollSize</span> = <span class="number">134217700</span></span><br><span class="line"><span class="comment">#文件的滚动与Event数量无关</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.rollCount</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line"><span class="attr">a3.channels.c3.type</span> = memory</span><br><span class="line"><span class="attr">a3.channels.c3.capacity</span> = <span class="number">1000</span></span><br><span class="line"><span class="attr">a3.channels.c3.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="attr">a3.sources.r3.channels</span> = c3</span><br><span class="line"><span class="attr">a3.sinks.k3.channel</span> = c3</span><br></pre></td></tr></table></figure></li>
<li>运行Flume, 监控文件夹 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">flume-ng agent --conf conf/ --name a3 --conf-file job/flume-taildir-hdfs.conf</span><br></pre></td></tr></table></figure></li>
<li>查看HDFS上的数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 logs]$ hadoop fs -ls /flume/TAILDIR/20211101/18</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 atguigu supergroup   77439185 2021-11-01 18:05 /flume/TAILDIR/20211101/18/upload-.1635761078949</span><br><span class="line">[atguigu@hadoop001 logs]$</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>Taildir说明：<ul>
<li>Taildir Source维护了一个json格式的position File，其会定期的往position File中更新每个文件读取到的最新的位置，因此能够实现断点续传。Position File的格式如下：  <figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;inode&quot;</span>:<span class="number">2496272</span>,<span class="attr">&quot;pos&quot;</span>:<span class="number">12</span>,<span class="attr">&quot;file&quot;</span>:<span class="string">&quot;/opt/module/flume/files/file1.txt&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;inode&quot;</span>:<span class="number">2496275</span>,<span class="attr">&quot;pos&quot;</span>:<span class="number">12</span>,<span class="attr">&quot;file&quot;</span>:<span class="string">&quot;/opt/module/flume/files/file2.txt&quot;</span>&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>注：Linux中储存文件元数据的区域就叫做inode，每个inode都有一个号码，操作系统用inode号码来识别不同的文件，Unix/Linux系统内部不使用文件名，而使用inode号码来识别文件。</li>
</ul>
</li>
</ol>
<h1 id="三、Flume进阶"><a href="#三、Flume进阶" class="headerlink" title="三、Flume进阶"></a>三、Flume进阶</h1><h2 id="3-1-Flume事务"><a href="#3-1-Flume事务" class="headerlink" title="3.1 Flume事务"></a>3.1 Flume事务</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16358198414212.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>Put事务流程<ul>
<li>doPut:将批数据先写入临时缓冲区putList</li>
<li>doCommit:检查channel内存队列是否足够合并。</li>
<li>doRollback:channel内存队列空间不足，回滚数据</li>
</ul>
</li>
<li>Take事务<ul>
<li>doTake:将数据取到临时缓冲区takeList，并将数据发送到HDFS</li>
<li>doCommit:如果数据全部发送成功，则清除临时缓冲区takeList</li>
<li>doRollback:数据发送过程中如果出现异常，rollback将临时缓冲区takeList中的数据归还给channel内存队列。</li>
</ul>
</li>
</ol>
<h2 id="3-2-Flume-Agent内部原理"><a href="#3-2-Flume-Agent内部原理" class="headerlink" title="3.2 Flume Agent内部原理"></a>3.2 Flume Agent内部原理</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16358207311935.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>ChannelSelector<ul>
<li>ChannelSelector的作用就是选出Event将要被发往哪个Channel。其共有两种类型，分别是Replicating（复制）和Multiplexing（多路复用）。</li>
<li>ReplicatingSelector会将同一个Event发往所有的Channel，Multiplexing会根据相应的原则，将不同的Event发往不同的Channel。</li>
</ul>
</li>
<li>SinkProcessor<ul>
<li>SinkProcessor共有三种类型，分别是DefaultSinkProcessor、LoadBalancingSinkProcessor和FailoverSinkProcessor</li>
<li>DefaultSinkProcessor对应的是单个的Sink，LoadBalancingSinkProcessor和FailoverSinkProcessor对应的是Sink Group，LoadBalancingSinkProcessor可以实现负载均衡的功能，FailoverSinkProcessor可以错误恢复的功能。</li>
</ul>
</li>
</ol>
<h2 id="3-3-Flume拓扑结构"><a href="#3-3-Flume拓扑结构" class="headerlink" title="3.3 Flume拓扑结构"></a>3.3 Flume拓扑结构</h2><h3 id="3-3-1-简单串联"><a href="#3-3-1-简单串联" class="headerlink" title="3.3.1 简单串联"></a>3.3.1 简单串联</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16359417802480.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br>这种模式是将多个flume顺序连接起来了，从最初的source开始到最终sink传送的目的存储系统。此模式不建议桥接过多的flume数量， flume数量过多不仅会影响传输速率，而且一旦传输过程中某个节点flume宕机，会影响整个传输系统。</p>
<h3 id="3-3-2-复制和多路复用"><a href="#3-3-2-复制和多路复用" class="headerlink" title="3.3.2 复制和多路复用"></a>3.3.2 复制和多路复用</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16359418050492.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br>Flume支持将事件流向一个或者多个目的地。这种模式可以将相同数据复制到多个channel中，或者将不同数据分发到不同的channel中，sink可以选择传送到不同的目的地。</p>
<h3 id="3-3-3-负载均衡和故障转移"><a href="#3-3-3-负载均衡和故障转移" class="headerlink" title="3.3.3 负载均衡和故障转移"></a>3.3.3 负载均衡和故障转移</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16359418259918.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br>Flume支持使用将多个sink逻辑上分到一个sink组，sink组配合不同的SinkProcessor可以实现负载均衡和错误恢复的功能。</p>
<h3 id="3-3-4-聚合"><a href="#3-3-4-聚合" class="headerlink" title="3.3.4 聚合"></a>3.3.4 聚合</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16359418605656.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br>这种模式是我们最常见的，也非常实用，日常web应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用flume的这种组合方式能很好的解决这一问题，每台服务器部署一个flume采集日志，传送到一个集中收集日志的flume，再由此flume上传到hdfs、hive、hbase等，进行日志分析。</p>
<h2 id="3-4-Flume企业开发案例"><a href="#3-4-Flume企业开发案例" class="headerlink" title="3.4 Flume企业开发案例"></a>3.4 Flume企业开发案例</h2><h3 id="3-4-1-复制案例"><a href="#3-4-1-复制案例" class="headerlink" title="3.4.1 复制案例"></a>3.4.1 复制案例</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16359420042208.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>需求描述：<ul>
<li>使用Flume1监控文件变动，Flume1将变动内容传递给Flume2，</li>
<li>Flume2负责存储到HDFS。同时Flume1将变动内容传递给Flume3，</li>
<li>Flume3负责输出到控制台</li>
</ul>
</li>
<li>功能实现<ol>
<li>创建Flume1 的核心配置文件 a1.conf（<font color ='red' >channel selector指定replicating</font>） 具体配置信息如下： <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a1.sources</span> = r1</span><br><span class="line"><span class="attr">a1.channels</span> = c1 c2</span><br><span class="line"><span class="attr">a1.sinks</span> = k1 k2</span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = TAILDIR</span><br><span class="line"><span class="attr">a1.sources.r1.filegroups</span> = f1 f2</span><br><span class="line"><span class="attr">a1.sources.r1.filegroups.f1</span> = /opt/module/flume-<span class="number">1.9</span>.<span class="number">0</span>/jobs/taildir/.*file.*</span><br><span class="line"><span class="attr">a1.sources.r1.filegroups.f2</span> = /opt/module/flume-<span class="number">1.9</span>.<span class="number">0</span>/jobs/taildir/.*log.*</span><br><span class="line"><span class="attr">a1.sources.r1.positionFile</span> = /opt/module/flume-<span class="number">1.9</span>.<span class="number">0</span>/jobs/taildir/position/position.json</span><br><span class="line"></span><br><span class="line"><span class="comment">#channel selector</span></span><br><span class="line"><span class="attr">a1.sources.r1.selector.type</span> = replicating</span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c2.type</span> = memory</span><br><span class="line"><span class="attr">a1.channels.c2.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a1.channels.c2.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = avro</span><br><span class="line"><span class="attr">a1.sinks.k1.hostname</span> = localhost</span><br><span class="line"><span class="attr">a1.sinks.k1.port</span> = <span class="number">7777</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k2.type</span> = avro</span><br><span class="line"><span class="attr">a1.sinks.k2.hostname</span> = localhost</span><br><span class="line"><span class="attr">a1.sinks.k2.port</span> = <span class="number">8888</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = c1 c2</span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = c1</span><br><span class="line"><span class="attr">a1.sinks.k2.channel</span> = c2</span><br></pre></td></tr></table></figure></li>
<li>创建Flume2 的核心配置文件 a2.conf 具体配置信息如下： <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a2.sources</span> = r1</span><br><span class="line"><span class="attr">a2.channels</span> = c1 </span><br><span class="line"><span class="attr">a2.sinks</span> = k1 </span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a2.sources.r1.type</span> = avro</span><br><span class="line"><span class="attr">a2.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="attr">a2.sources.r1.port</span> = <span class="number">7777</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a2.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a2.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a2.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a2.sinks.k1.type</span> = hdfs</span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.path</span> = hdfs://mycluster/flume/%Y%m%d/%H</span><br><span class="line"><span class="comment">#上传文件的前缀</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.filePrefix</span> = logs-</span><br><span class="line"><span class="comment">#是否按照时间滚动文件夹</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.round</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">#多少时间单位创建一个新的文件夹</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.roundValue</span> = <span class="number">1</span></span><br><span class="line"><span class="comment">#重新定义时间单位</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.roundUnit</span> = hour</span><br><span class="line"><span class="comment">#是否使用本地时间戳</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.useLocalTimeStamp</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">#积攒多少个Event才flush到HDFS一次</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.batchSize</span> = <span class="number">100</span></span><br><span class="line"><span class="comment">#设置文件类型，可支持压缩</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.fileType</span> = DataStream</span><br><span class="line"><span class="comment">#多久生成一个新的文件</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.rollInterval</span> = <span class="number">60</span></span><br><span class="line"><span class="comment">#设置每个文件的滚动大小</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.rollSize</span> = <span class="number">134217700</span></span><br><span class="line"><span class="comment">#文件的滚动与Event数量无关</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.rollCount</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a2.sources.r1.channels</span> = c1 </span><br><span class="line"><span class="attr">a2.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>创建Flume3 的核心配置文件 a3.conf 具体配置信息如下： <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a3.sources</span> = r1</span><br><span class="line"><span class="attr">a3.channels</span> = c1 </span><br><span class="line"><span class="attr">a3.sinks</span> = k1 </span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a3.sources.r1.type</span> = avro</span><br><span class="line"><span class="attr">a3.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="attr">a3.sources.r1.port</span> = <span class="number">8888</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a3.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a3.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a3.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a3.sinks.k1.type</span> = logger</span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a3.sources.r1.channels</span> = c1 </span><br><span class="line"><span class="attr">a3.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>分别启动 Flume2和Flume3 然后在启动Flume1 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 启动Flume2</span></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/replicating/a2.conf -n a2 -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="comment"># 启动Flume3</span></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/replicating/a3.conf -n a3 -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="comment"># 启动Flume1</span></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/replicating/a1.conf -n a1 -Dflume.root.logger=INFO,console </span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>测试效果：往待监控的文件中追加内容，观察hdfs和控制台的变化！ <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#flume1日志</span></span><br><span class="line">2021-11-03 21:05:48,539 (PollableSourceRunner-TaildirSource-r1) [INFO - org.apache.flume.source.taildir.TaildirSource.closeTailFiles(TaildirSource.java:307)] Closed file: /opt/module/flume-1.9.0/<span class="built_in">jobs</span>/taildir/replicating.log, inode: 104723556, pos: 43</span><br><span class="line"></span><br><span class="line"><span class="comment">#flume2日志</span></span><br><span class="line">2021-11-03 21:11:56,526 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.hdfs.HDFSDataStream.configure(HDFSDataStream.java:57)] Serializer = TEXT, UseRawLocalFileSystem = <span class="literal">false</span></span><br><span class="line">2021-11-03 21:11:56,538 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:246)] Creating hdfs://mycluster/flume/20211103/21/logs-.1635945116527.tmp</span><br><span class="line"></span><br><span class="line"><span class="comment">#flume3日志</span></span><br><span class="line">2021-11-03 21:12:10,471 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 32 30 32 31 E5 B9 B4 20 31 31 E6 9C 88 20 30 33 2021... 11... 03 &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="3-4-2-负载均衡案例"><a href="#3-4-2-负载均衡案例" class="headerlink" title="3.4.2 负载均衡案例"></a>3.4.2 负载均衡案例</h3><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/02qi-ye-an-lifu-zai-jun-hengan-li-shi-li-tu.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="02_企业案例-负载均衡-案例实例图"></li>
</ol>
</li>
<li>需求描述：<ul>
<li>使用Flume1监控端口数据，将监控的数据按照指定规则（轮训、随机） </li>
<li>传递给Flume2和Flume3,最后Flume2和Flume3将数据打印到控制台</li>
</ul>
</li>
<li>功能实现：<ol>
<li>创建Flume1 的核心配置文件 a1.conf（<font color ='red' >processor.type = load_balance processorprocessor.selector = round_robin</font>） 具体配置信息如下： <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a1.sources</span> = r1</span><br><span class="line"><span class="attr">a1.channels</span> = c1</span><br><span class="line"><span class="attr">a1.sinks</span> = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = netcat</span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="number">6666</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = avro</span><br><span class="line"><span class="attr">a1.sinks.k1.hostname</span> = localhost</span><br><span class="line"><span class="attr">a1.sinks.k1.port</span> = <span class="number">7777</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k2.type</span> = avro</span><br><span class="line"><span class="attr">a1.sinks.k2.hostname</span> = localhost</span><br><span class="line"><span class="attr">a1.sinks.k2.port</span> = <span class="number">8888</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink Processor</span></span><br><span class="line"><span class="attr">a1.sinkgroups</span> = g1</span><br><span class="line"><span class="attr">a1.sinkgroups.g1.sinks</span> = k1 k2</span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.type</span> = load_balance</span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.selector</span> = round_robin</span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = c1</span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = c1</span><br><span class="line"><span class="attr">a1.sinks.k2.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>创建Flume2 的核心配置文件 a2.conf 具体配置信息如下： <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a2.sources</span> = r1</span><br><span class="line"><span class="attr">a2.channels</span> = c1 </span><br><span class="line"><span class="attr">a2.sinks</span> = k1 </span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a2.sources.r1.type</span> = avro</span><br><span class="line"><span class="attr">a2.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="attr">a2.sources.r1.port</span> = <span class="number">7777</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a2.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a2.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a2.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a2.sinks.k1.type</span> = logger</span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a2.sources.r1.channels</span> = c1 </span><br><span class="line"><span class="attr">a2.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>创建Flume3 的核心配置文件 a3.conf 具体配置信息如下： <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a3.sources</span> = r1</span><br><span class="line"><span class="attr">a3.channels</span> = c1 </span><br><span class="line"><span class="attr">a3.sinks</span> = k1 </span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a3.sources.r1.type</span> = avro</span><br><span class="line"><span class="attr">a3.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="attr">a3.sources.r1.port</span> = <span class="number">8888</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a3.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a3.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a3.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a3.sinks.k1.type</span> = logger</span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a3.sources.r1.channels</span> = c1 </span><br><span class="line"><span class="attr">a3.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>分别启动 Flume2和Flume3 然后在启动Flume1 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 启动Flume2</span></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/loadbalnace/a2.conf -n a2 -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="comment"># 启动Flume3</span></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/loadbalnace/a3.conf -n a3 -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="comment"># 启动Flume1</span></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/loadbalnace/a1.conf -n a1 -Dflume.root.logger=INFO,console </span><br></pre></td></tr></table></figure></li>
<li>测试效果：启动nc客户端 发消息，Flume1负责监听nc发来消息，然后以负载均衡的方式发送给Flume2和Flume3 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#发送TCP消息到flume1</span></span><br><span class="line">[atguigu@hadoop001 loadbalnace]$ nc localhost 6666</span><br><span class="line">a</span><br><span class="line">OK</span><br><span class="line">b</span><br><span class="line">OK</span><br><span class="line">c</span><br><span class="line">OK</span><br><span class="line">d</span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line"><span class="comment"># flume2日志    </span></span><br><span class="line">2021-11-03 21:23:39,584 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 61                                              a &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># flume3日志    </span></span><br><span class="line">2021-11-03 21:23:42,237 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 62                                              b &#125;</span><br><span class="line">2021-11-03 21:23:42,237 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 63                                              c &#125;</span><br><span class="line">2021-11-03 21:23:42,237 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 64                                              d &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<h3 id="3-4-3-故障转移案例"><a href="#3-4-3-故障转移案例" class="headerlink" title="3.4.3 故障转移案例"></a>3.4.3 故障转移案例</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/03qi-ye-an-ligu-zhang-zhuan-yian-li-shi-li-tu.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="03企业案例-故障转移-案例实例图"></p>
<ul>
<li><p>需求描述：</p>
<ul>
<li>使用Flume1监控端口数据，将监控到的数据按照故障转移的方式</li>
<li>传递给Flume2或者Flume3, Flume2或者Flume3将数据打印控制台</li>
</ul>
</li>
<li><p>功能实现：</p>
<ol>
<li>创建Flume1 的核心配置文件 a1.conf 具体配置信息如下：  <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a1.sources</span> = r1</span><br><span class="line"><span class="attr">a1.channels</span> = c1</span><br><span class="line"><span class="attr">a1.sinks</span> = k1 k2 </span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = netcat</span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="number">6666</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = avro</span><br><span class="line"><span class="attr">a1.sinks.k1.hostname</span> = localhost</span><br><span class="line"><span class="attr">a1.sinks.k1.port</span> = <span class="number">7777</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k2.type</span> = avro</span><br><span class="line"><span class="attr">a1.sinks.k2.hostname</span> = localhost</span><br><span class="line"><span class="attr">a1.sinks.k2.port</span> = <span class="number">8888</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink Processor</span></span><br><span class="line"><span class="attr">a1.sinkgroups</span> = g1</span><br><span class="line"><span class="attr">a1.sinkgroups.g1.sinks</span> = k1 k2</span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.type</span> = failover</span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.priority.k1</span> = <span class="number">5</span></span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.priority.k2</span> = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = c1</span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = c1</span><br><span class="line"><span class="attr">a1.sinks.k2.channel</span> = c1   </span><br></pre></td></tr></table></figure></li>
<li>创建Flume2 的核心配置文件 a2.conf 具体配置信息如下： <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a2.sources</span> = r1</span><br><span class="line"><span class="attr">a2.channels</span> = c1 </span><br><span class="line"><span class="attr">a2.sinks</span> = k1 </span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a2.sources.r1.type</span> = avro</span><br><span class="line"><span class="attr">a2.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="attr">a2.sources.r1.port</span> = <span class="number">7777</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a2.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a2.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a2.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a2.sinks.k1.type</span> = logger</span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a2.sources.r1.channels</span> = c1 </span><br><span class="line"><span class="attr">a2.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>创建Flume3 的核心配置文件 a3.conf 具体配置信息如下： <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a3.sources</span> = r1</span><br><span class="line"><span class="attr">a3.channels</span> = c1 </span><br><span class="line"><span class="attr">a3.sinks</span> = k1 </span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a3.sources.r1.type</span> = avro</span><br><span class="line"><span class="attr">a3.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="attr">a3.sources.r1.port</span> = <span class="number">8888</span></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a3.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a3.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a3.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a3.sinks.k1.type</span> = logger</span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a3.sources.r1.channels</span> = c1 </span><br><span class="line"><span class="attr">a3.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>分别启动 Flume2和Flume3 然后在启动Flume1 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 启动Flume2</span></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/failover/a2.conf -n a2 -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="comment"># 启动Flume3</span></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/failover/a3.conf -n a3 -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="comment"># 启动Flume1</span></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/failover/a1.conf -n a1 -Dflume.root.logger=INFO,console </span><br></pre></td></tr></table></figure></li>
<li>启动nc客户端，发送消息，Flume1或者Flume2监听到消息打印到控制台，然后在模拟退出一台Flume 再发消息，查看结果。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 failover]$ nc localhost 6666</span><br><span class="line">a</span><br><span class="line">OK</span><br><span class="line">b</span><br><span class="line">OK</span><br><span class="line">c</span><br><span class="line">OK</span><br><span class="line">d</span><br><span class="line">OK</span><br><span class="line"><span class="comment"># 查看flume2收到消息</span></span><br><span class="line"><span class="comment"># 停掉flume2继续发送消息</span></span><br><span class="line">1</span><br><span class="line">OK</span><br><span class="line">2</span><br><span class="line">OK</span><br><span class="line">3</span><br><span class="line">OK</span><br><span class="line">4</span><br><span class="line">OK</span><br><span class="line"><span class="comment"># 查看flume3收到消息，故障转移完成</span></span><br><span class="line"><span class="comment"># 重新启动flume2继续发送消息</span></span><br><span class="line">q</span><br><span class="line">OK</span><br><span class="line">w</span><br><span class="line">OK</span><br><span class="line">e</span><br><span class="line">OK</span><br><span class="line">r</span><br><span class="line">OK</span><br><span class="line"><span class="comment"># 查看flume2收到消息</span></span><br></pre></td></tr></table></figure></li>
<li>注意事项：模拟a3 故障后，a1会将数据发送给a2,但是如果把a3重新启动后，不会在第一时间将a3作为Active的sink,而是<br>遵循一个默认的规避原则，从2秒开始，a3故障一次重新启动后就对其规避使用累加上一次的两倍时间，默认30秒是上限。</li>
</ol>
</li>
</ul>
<h3 id="3-4-4-聚合案例"><a href="#3-4-4-聚合案例" class="headerlink" title="3.4.4 聚合案例"></a>3.4.4 聚合案例</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/04qi-ye-an-liju-hean-li-shi-li-tu.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="04_企业案例-聚合-案例实例图"></p>
<ul>
<li>需求描述：<ul>
<li>使用Flume1（hadoop102）监听端口数据，Flume2（hadoop103）监控追加文件的数据，</li>
<li>然后Flume1和Flume2将结果传递给Flume3,最后由Flume3将结果打印到控制台</li>
</ul>
</li>
<li>实现功能：<ol>
<li>准备工作<ul>
<li>将Flume软件发送给hadoop002和hadoop003</li>
<li>同步环境变量</li>
</ul>
</li>
<li>在hadoop102创建Flume1 的核心配置文件 a1.conf 具体配置信息如下（监控端口数据） <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a1.sources</span> = r1</span><br><span class="line"><span class="attr">a1.channels</span> = c1</span><br><span class="line"><span class="attr">a1.sinks</span> = k1</span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = netcat</span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="number">6666</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = avro</span><br><span class="line"><span class="attr">a1.sinks.k1.hostname</span> = hadoop003</span><br><span class="line"><span class="attr">a1.sinks.k1.port</span> = <span class="number">8888</span></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = c1</span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>在hadoop103创建Flume2 的核心配置文件 a2.conf 具体配置信息如下（监控文件追加的数据）  <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a2.sources</span> = r1</span><br><span class="line"><span class="attr">a2.channels</span> = c1</span><br><span class="line"><span class="attr">a2.sinks</span> = k1</span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a2.sources.r1.type</span> = TAILDIR</span><br><span class="line"><span class="attr">a2.sources.r1.filegroups</span> = f1 f2</span><br><span class="line"><span class="attr">a2.sources.r1.filegroups.f1</span> = /opt/module/flume-<span class="number">1.9</span>.<span class="number">0</span>/jobs/taildir/.*file.*</span><br><span class="line"><span class="attr">a2.sources.r1.filegroups.f2</span> = /opt/module/flume-<span class="number">1.9</span>.<span class="number">0</span>/jobs/taildir/.*log.*</span><br><span class="line"><span class="attr">a2.sources.r1.positionFile</span> = /opt/module/flume-<span class="number">1.9</span>.<span class="number">0</span>/jobs/taildir/position/position.json</span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a2.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a2.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a2.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a2.sinks.k1.type</span> = avro</span><br><span class="line"><span class="attr">a2.sinks.k1.hostname</span> = hadoop003</span><br><span class="line"><span class="attr">a2.sinks.k1.port</span> = <span class="number">8888</span></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a2.sources.r1.channels</span> = c1</span><br><span class="line"><span class="attr">a2.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>在hadoop104创建Flume3 的核心配置文件 a2.conf 具体配置信息如下(监控 Flume1和Flume2发送的数据) <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a3.sources</span> = r1</span><br><span class="line"><span class="attr">a3.channels</span> = c1</span><br><span class="line"><span class="attr">a3.sinks</span> = k1</span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a3.sources.r1.type</span> = avro</span><br><span class="line"><span class="attr">a3.sources.r1.bind</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line"><span class="attr">a3.sources.r1.port</span> = <span class="number">8888</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a3.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a3.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a3.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a3.sinks.k1.type</span> = logger</span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a3.sources.r1.channels</span> = c1</span><br><span class="line"><span class="attr">a3.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>分别启动 Flume3 然后再启动Flume1和Flume2 <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 启动Flume3（hadoop104）</span></span><br><span class="line">flume-ng agent -c $FLUME_HOME/conf -f $FLUME_HOME/jobs/aggre/a3.conf -n a3 <span class="attr">-Dflume.root.logger</span>=INFO,console</span><br><span class="line"><span class="comment"># 启动Flume2（hadoop103）</span></span><br><span class="line">flume-ng agent -c $FLUME_HOME/conf -f $FLUME_HOME/jobs/aggre/a2.conf -n a2 <span class="attr">-Dflume.root.logger</span>=INFO,console</span><br><span class="line"><span class="comment"># 启动Flume1 (hadoop102)</span></span><br><span class="line">flume-ng agent -c $FLUME_HOME/conf -f $FLUME_HOME/jobs/aggre/a1.conf -n a1 <span class="attr">-Dflume.root.logger</span>=INFO,console </span><br></pre></td></tr></table></figure></li>
<li>启动nc客户端，发送消息到Flume1，在Flume2监听监听的文件追加内容，查看flume3结果。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 发送数据到Flume1</span></span><br><span class="line">[atguigu@hadoop002 ~]$ nc hadoop001 6666</span><br><span class="line">1</span><br><span class="line">OK</span><br><span class="line">2</span><br><span class="line">OK</span><br><span class="line">3</span><br><span class="line">OK</span><br><span class="line">4</span><br><span class="line">OK</span><br><span class="line"><span class="comment"># 追加内容到Flume2监控的文件       </span></span><br><span class="line">[atguigu@hadoop002 ~]$ <span class="built_in">echo</span> `date`</span><br><span class="line">2021年 11月 03日 星期三 22:00:52 CST</span><br><span class="line">[atguigu@hadoop002 ~]$ <span class="built_in">echo</span> `date` &gt;&gt; /opt/module/flume-1.9.0/<span class="built_in">jobs</span>/taildir/aggre.log</span><br><span class="line">[atguigu@hadoop002 ~]$ <span class="built_in">echo</span> `date` &gt;&gt; /opt/module/flume-1.9.0/<span class="built_in">jobs</span>/taildir/aggre.log</span><br><span class="line">[atguigu@hadoop002 ~]$ <span class="built_in">echo</span> `date` &gt;&gt; /opt/module/flume-1.9.0/<span class="built_in">jobs</span>/taildir/aggre.log</span><br><span class="line"></span><br><span class="line"><span class="comment">#Flume3日志</span></span><br><span class="line">2021-11-03 22:00:23,272 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 31                                              1 &#125;</span><br><span class="line">2021-11-03 22:00:23,272 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 32                                              2 &#125;</span><br><span class="line">2021-11-03 22:00:23,272 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 33                                              3 &#125;</span><br><span class="line">2021-11-03 22:00:23,272 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 34                                              4 &#125;</span><br><span class="line">2021-11-03 22:01:33,278 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 32 30 32 31 E5 B9 B4 20 31 31 E6 9C 88 20 30 33 2021... 11... 03 &#125;</span><br><span class="line">2021-11-03 22:01:37,280 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 32 30 32 31 E5 B9 B4 20 31 31 E6 9C 88 20 30 33 2021... 11... 03 &#125;</span><br><span class="line">2021-11-03 22:01:37,280 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 32 30 32 31 E5 B9 B4 20 31 31 E6 9C 88 20 30 33 2021... 11... 03 &#125;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<h3 id="3-4-4-多路-拦截器-案例"><a href="#3-4-4-多路-拦截器-案例" class="headerlink" title="3.4.4 多路+拦截器 案例"></a>3.4.4 多路+拦截器 案例</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16359488070644.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"> </p>
<ul>
<li>需求描述：<ul>
<li>使用Flume1监控端口数据 </li>
<li>添加拦截器功能 对数据进行分类拦截，然后通过多路组件将不同规则的数据分别放入不同的channel中 </li>
<li>每一个channel对应一个 avro sink  </li>
<li>flume2 flume3 flume4 分别根据规则接受来自不同 avro sink 的数据，最后打印控制台</li>
</ul>
</li>
<li>完成功能：<ol>
<li>a1.conf (Flume1) <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line">    <span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line">    <span class="attr">a1.sources</span> = r1</span><br><span class="line">    <span class="attr">a1.channels</span> = c1 c2 c3 </span><br><span class="line">    <span class="attr">a1.sinks</span> = k1 k2 k3</span><br><span class="line">    <span class="comment">#Source</span></span><br><span class="line">    <span class="attr">a1.sources.r1.type</span> = netcat</span><br><span class="line">    <span class="attr">a1.sources.r1.bind</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line">    <span class="attr">a1.sources.r1.port</span> = <span class="number">6666</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#channel selector</span></span><br><span class="line">    <span class="attr">a1.sources.r1.selector.type</span> = multiplexing</span><br><span class="line">    <span class="attr">a1.sources.r1.selector.header</span> = title</span><br><span class="line">    <span class="attr">a1.sources.r1.selector.mapping.at</span> = c1</span><br><span class="line">    <span class="attr">a1.sources.r1.selector.mapping.sg</span> = c2</span><br><span class="line">    <span class="attr">a1.sources.r1.selector.mapping.ot</span> = c3</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#interceptor</span></span><br><span class="line">    <span class="attr">a1.sources.r1.interceptors</span> = i1</span><br><span class="line">    <span class="attr">a1.sources.r1.interceptors.i1.type</span> = com.atguigu.flume.interecptor.MyInterceptor<span class="variable">$MyBuilder</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Channel</span></span><br><span class="line">    <span class="attr">a1.channels.c1.type</span> = memory</span><br><span class="line">    <span class="attr">a1.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line">    <span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">a1.channels.c2.type</span> = memory</span><br><span class="line">    <span class="attr">a1.channels.c2.capacity</span> = <span class="number">10000</span></span><br><span class="line">    <span class="attr">a1.channels.c2.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">a1.channels.c3.type</span> = memory</span><br><span class="line">    <span class="attr">a1.channels.c3.capacity</span> = <span class="number">10000</span></span><br><span class="line">    <span class="attr">a1.channels.c3.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Sink </span></span><br><span class="line">    <span class="attr">a1.sinks.k1.type</span> = avro</span><br><span class="line">    <span class="attr">a1.sinks.k1.hostname</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line">    <span class="attr">a1.sinks.k1.port</span> = <span class="number">7777</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">a1.sinks.k2.type</span> = avro</span><br><span class="line">    <span class="attr">a1.sinks.k2.hostname</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line">    <span class="attr">a1.sinks.k2.port</span> = <span class="number">8888</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">a1.sinks.k3.type</span> = avro</span><br><span class="line">    <span class="attr">a1.sinks.k3.hostname</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line">    <span class="attr">a1.sinks.k3.port</span> = <span class="number">9999</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Bind</span></span><br><span class="line">    <span class="attr">a1.sources.r1.channels</span> = c1 c2 c3 </span><br><span class="line">    <span class="attr">a1.sinks.k1.channel</span> = c1</span><br><span class="line">    <span class="attr">a1.sinks.k2.channel</span> = c2</span><br><span class="line">    <span class="attr">a1.sinks.k3.channel</span> = c3</span><br><span class="line">    ```   </span><br><span class="line">1. a2.conf (Flume2)</span><br><span class="line">    ```ini</span><br><span class="line">    <span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line">    <span class="attr">a2.sources</span> = r1</span><br><span class="line">    <span class="attr">a2.channels</span> = c1 </span><br><span class="line">    <span class="attr">a2.sinks</span> = k1 </span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Source</span></span><br><span class="line">    <span class="attr">a2.sources.r1.type</span> = avro</span><br><span class="line">    <span class="attr">a2.sources.r1.bind</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line">    <span class="attr">a2.sources.r1.port</span> = <span class="number">7777</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Channel</span></span><br><span class="line">    <span class="attr">a2.channels.c1.type</span> = memory</span><br><span class="line">    <span class="attr">a2.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line">    <span class="attr">a2.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Sink </span></span><br><span class="line">    <span class="attr">a2.sinks.k1.type</span> = logger</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Bind</span></span><br><span class="line">    <span class="attr">a2.sources.r1.channels</span> = c1 </span><br><span class="line">    <span class="attr">a2.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>a3.conf (Flume3) <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a3.sources</span> = r1</span><br><span class="line"><span class="attr">a3.channels</span> = c1 </span><br><span class="line"><span class="attr">a3.sinks</span> = k1 </span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a3.sources.r1.type</span> = avro</span><br><span class="line"><span class="attr">a3.sources.r1.bind</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line"><span class="attr">a3.sources.r1.port</span> = <span class="number">8888</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a3.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a3.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a3.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a3.sinks.k1.type</span> = logger</span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a3.sources.r1.channels</span> = c1 </span><br><span class="line"><span class="attr">a3.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>a4.conf  (Flume4) <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a4.sources</span> = r1</span><br><span class="line"><span class="attr">a4.channels</span> = c1 </span><br><span class="line"><span class="attr">a4.sinks</span> = k1 </span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a4.sources.r1.type</span> = avro</span><br><span class="line"><span class="attr">a4.sources.r1.bind</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line"><span class="attr">a4.sources.r1.port</span> = <span class="number">9999</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a4.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a4.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a4.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a4.sinks.k1.type</span> = logger</span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a4.sources.r1.channels</span> = c1 </span><br><span class="line"><span class="attr">a4.sinks.k1.channel</span> = c1</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<ol start="5">
<li>测试：<ol>
<li>将自定义拦截器打包上传至Linux 中 <ul>
<li>注意：修改Flume1的 a1.conf 文件中的  <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">a1.sources.r1.interceptors.i1.type</span> = com.atguigu.flume.interceptor.DataTypeInterceptor<span class="variable">$MyBuilder</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>执行以下运行命令<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/multiplexing/a1.conf -n a1 -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/multiplexing/a2.conf -n a2 -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/multiplexing/a3.conf -n a3 -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/multiplexing/a4.conf -n a4 -Dflume.root.logger=INFO,console</span><br><span class="line"> </span><br></pre></td></tr></table></figure></li>
<li>nc模拟发送消息<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ nc hadoop001 6666</span><br><span class="line"><span class="comment"># 发送atguiguaaa路由到flueme2</span></span><br><span class="line">atguiguaaa</span><br><span class="line">OK</span><br><span class="line"><span class="comment"># 发送shangguiguasdf路由到flueme3</span></span><br><span class="line">shangguiguasdf</span><br><span class="line">OK</span><br><span class="line"><span class="comment"># 发送asdasdf路由到flueme4</span></span><br><span class="line">asdasdf</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>自定义拦截器 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyInterceptor</span> <span class="keyword">implements</span> <span class="title">Interceptor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 初始方法</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 拦截器的核心逻辑方法</span></span><br><span class="line"><span class="comment">     * 需求：</span></span><br><span class="line"><span class="comment">     *    区别采集数据的内容，包含 atguigu 或者 包含 sangguigu 再或者包含 其他</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> event</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 获取event中的header</span></span><br><span class="line">        Map&lt;String, String&gt; headers = event.getHeaders();</span><br><span class="line">        <span class="comment">// 获取event 中 body</span></span><br><span class="line">        <span class="keyword">byte</span>[] body = event.getBody();</span><br><span class="line">        String data = <span class="keyword">new</span> String(body);</span><br><span class="line">        <span class="keyword">if</span>(data.contains(<span class="string">&quot;atguigu&quot;</span>))&#123;</span><br><span class="line">            headers.put(<span class="string">&quot;title&quot;</span>,<span class="string">&quot;at&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(data.contains(<span class="string">&quot;shangguigu&quot;</span>))&#123;</span><br><span class="line">            headers.put(<span class="string">&quot;title&quot;</span>,<span class="string">&quot;sg&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            headers.put(<span class="string">&quot;title&quot;</span>,<span class="string">&quot;ot&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 拦截器的核心逻辑方法，当来的多个event会循环调用 上面的 intercept</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> list</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Event&gt; <span class="title">intercept</span><span class="params">(List&lt;Event&gt; list)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Event event : list) &#123;</span><br><span class="line">            intercept(event);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> list;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 收尾工作</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 声明一个内部类</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyBuilder</span> <span class="keyword">implements</span> <span class="title">Builder</span></span>&#123;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 实例化当前拦截器对象</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Interceptor <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> MyInterceptor();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 读取flume的配置信息</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="第4章-企业真实面试题"><a href="#第4章-企业真实面试题" class="headerlink" title="第4章 企业真实面试题"></a>第4章 企业真实面试题</h1><h2 id="4-1-你是如何实现Flume数据传输的监控的"><a href="#4-1-你是如何实现Flume数据传输的监控的" class="headerlink" title="4.1 你是如何实现Flume数据传输的监控的"></a>4.1 你是如何实现Flume数据传输的监控的</h2><ul>
<li>第三方框架Prometheus+Grafana 或者Ganglia<h2 id="4-2-Flume的Source，Sink，Channel的作用？你们Source是什么类型？"><a href="#4-2-Flume的Source，Sink，Channel的作用？你们Source是什么类型？" class="headerlink" title="4.2 Flume的Source，Sink，Channel的作用？你们Source是什么类型？"></a>4.2 Flume的Source，Sink，Channel的作用？你们Source是什么类型？</h2></li>
<li>作用<ol>
<li>Source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy</li>
<li>Channel组件对采集到的数据进行缓存，可以存放在Memory或File中。</li>
<li>Sink组件是用于把数据发送到目的地的组件，目的地包括Hdfs、Logger、avro、thrift、ipc、file、Hbase、solr、自定义。</li>
</ol>
</li>
<li>采用的Source类型为：<ul>
<li>监控后台日志：exec</li>
<li>监控后台产生日志的端口：netcat</li>
</ul>
</li>
</ul>
<h2 id="4-3-Flume的Channel-Selectors"><a href="#4-3-Flume的Channel-Selectors" class="headerlink" title="4.3 Flume的Channel Selectors"></a>4.3 Flume的Channel Selectors</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16360037790920.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h2 id="4-4-Flume参数调优"><a href="#4-4-Flume参数调优" class="headerlink" title="4.4 Flume参数调优"></a>4.4 Flume参数调优</h2><ol>
<li>Source<ul>
<li>增加Source个数（使用Tair Dir Source时可增加FileGroups个数）可以增大Source的读取数据的能力。例如：当某一个目录产生的文件过多时需要将这个文件目录拆分成多个文件目录，同时配置好多个Source 以保证Source有足够的能力获取到新产生的数据。</li>
<li>batchSize参数决定Source一次批量运输到Channel的event条数，适当调大这个参数可以提高Source搬运Event到Channel时的性能。</li>
</ul>
</li>
<li>Channel <ul>
<li>type：选择memory时Channel的性能最好，但是如果Flume进程意外挂掉可能会丢失数据。type选择file时Channel的容错性更好，但是性能上会比memory channel差。使用file Channel时dataDirs配置多个不同盘下的目录可以提高性能。</li>
<li>Capacity：参数决定Channel可容纳最大的event条数。transactionCapacity 参数决定每次Source往channel里面写的最大event条数和每次Sink从channel里面读的最大event条数。transactionCapacity需要大于Source和Sink的batchSize参数。</li>
</ul>
</li>
<li>Sink <ul>
<li>增加Sink的个数可以增加Sink消费event的能力。Sink也不是越多越好够用就行，过多的Sink会占用系统资源，造成系统资源不必要的浪费。</li>
<li>batchSize参数决定Sink一次批量从Channel读取的event条数，适当调大这个参数可以提高Sink从Channel搬出event的性能。</li>
</ul>
</li>
</ol>
<h2 id="4-5-Flume的事务机制"><a href="#4-5-Flume的事务机制" class="headerlink" title="4.5 Flume的事务机制"></a>4.5 Flume的事务机制</h2><p>Flume的事务机制（类似数据库的事务机制）：Flume使用两个独立的事务分别负责从Soucrce到Channel，以及从Channel到Sink的事件传递。比如spooling directory source 为文件的每一行创建一个事件，一旦事务中所有的事件全部传递到Channel且提交成功，那么Soucrce就将该文件标记为完成。同理，事务以类似的方式处理从Channel到Sink的传递过程，如果因为某种原因使得事件无法记录，那么事务将会回滚。且所有的事件都会保持到Channel中，等待重新传递。</p>
<h2 id="4-6-Flume采集数据会丢失吗"><a href="#4-6-Flume采集数据会丢失吗" class="headerlink" title="4.6 Flume采集数据会丢失吗?"></a>4.6 Flume采集数据会丢失吗?</h2><p>根据Flume的架构原理，Flume是不可能丢失数据的，其内部有完善的事务机制，Source到Channel是事务性的，Channel到Sink是事务性的，因此这两个环节不会出现数据的丢失，唯一可能丢失数据的情况是Channel采用memoryChannel，agent宕机导致数据丢失，或者Channel存储数据已满，导致Source不再写入，未写入的数据丢失。<br>Flume不会丢失数据，但是有可能造成数据的重复，例如数据已经成功由Sink发出，但是没有接收到响应，Sink会再次发送数据，此时可能会导致数据的重复。次发送数据，此时可能会导致数据的重复。</p>
]]></content>
      <categories>
        <category>数据采集</category>
      </categories>
      <tags>
        <tag>数据采集</tag>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2021/11/06/hello-world/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>about_me</title>
    <url>/2019/11/07/about-me/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Hello world</p>
]]></content>
  </entry>
</search>

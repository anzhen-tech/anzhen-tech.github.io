<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title> anzhen.tech</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
      />
      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
      <meta name="baidu-site-verification" content="code-BgeZtJHZzY" />
      <meta name="google-site-verification" content="wNOxVwDPcgD6IwrCt_pD_Xtq-E86p8USRXPN73jLu0A" />
    <link rel="alternate" href="/atom.xml" title="anzhen.tech" type="application/atom+xml">
</head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      
<section class="cover">
    
      
      <a class="forkMe" href="https://github.com/anzhen-tech"
        target="_blank"><img width="149" height="149" src="/images/forkme.png"
          class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="/images/cover1.jpg" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">anzhen.tech</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
        <img
          src="/images/ayer.svg"
          class="cover-logo"
          alt="anzhen.tech"
        />
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script>


<!-- Subtitle -->

  <script>
    try {
      var typed = new Typed("#subtitle", {
        strings: ['越努力，越美好', '愿你一生努力，一生被爱', '想要的都拥有，得不到的都释怀'],
        startDelay: 0,
        typeSpeed: 200,
        loop: true,
        backSpeed: 100,
        showCursor: true
      });
    } catch (err) {
      console.log(err)
    }
  </script>
  
<div id="main">
  <section class="outer">
  
  <ul class="ads">
    
        <li>
            <a target="_blank" rel="noopener" href="https://www.aliyun.com/minisite/goods?userCode=e6rdw2zn&amp;share_source=copy_link">
                <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/WX20211107-102324@2x.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" width="300" alt="阿里云服务器">
            </a>
        </li>
    
</ul>
  
  
  

<div class="notice" style="margin-top:50px">
    <i class="ri-heart-fill"></i>
    <div class="notice-content" id="broad"></div>
</div>
<script type="text/javascript">
    fetch('https://v1.hitokoto.cn')
        .then(response => response.json())
        .then(data => {
            document.getElementById("broad").innerHTML = data.hitokoto;
        })
        .catch(console.error)
</script>

<style>
    .notice {
        padding: 20px;
        border: 1px dashed #e6e6e6;
        color: #969696;
        position: relative;
        display: inline-block;
        width: 100%;
        background: #fbfbfb50;
        border-radius: 10px;
    }

    .notice i {
        float: left;
        color: #999;
        font-size: 16px;
        padding-right: 10px;
        vertical-align: middle;
        margin-top: -2px;
    }

    .notice-content {
        display: initial;
        vertical-align: middle;
    }
</style>
  
  <article class="articles">
    
    
    
    
    <article
  id="post-Dubbo面试题"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/11/07/Dubbo%E9%9D%A2%E8%AF%95%E9%A2%98/"
    >Dubbo面试题</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/11/07/Dubbo%E9%9D%A2%E8%AF%95%E9%A2%98/" class="article-date">
  <time datetime="2021-11-07T02:41:33.000Z" itemprop="datePublished">2021-11-07</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Dubbo/">Dubbo</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Dubbo"><a href="#Dubbo" class="headerlink" title="Dubbo"></a>Dubbo</h1><ol>
<li><p>Dubbo 有几种配置方式？</p>
<ul>
<li>XML 配置</li>
<li>注解配置</li>
<li>Java API 配置</li>
<li>属性配置</li>
</ul>
</li>
<li><p>Dubbo 如何和 Spring Boot 进行集成？</p>
<ul>
<li>官方提供提供了集成库 dubbo-spring-boot</li>
</ul>
</li>
<li><p>Dubbo 框架的分层设计</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850280657994.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>总体分成 Business、RPC、Remoting 三大层<ul>
<li>Service 业务层：业务代码的接口与实现。我们实际使用 Dubbo 的业务层级。接口层，给服务提供者和消费者来实现的。</li>
<li>RPC层：<ul>
<li>config 配置层：主要是对 Dubbo 进行各种配置的。</li>
<li>proxy 服务代理层：服务代理层，无论是 consumer 还是 provider，Dubbo 都会给你生成代理，代理之间进行网络通信。（ 对比Spring Cloud 体系，可以类比成 Feign 对于 consumer ，Spring MVC 对于 provider 。）</li>
<li>registry 注册中心层：服务注册层，负责服务的注册与发现。（对比 Spring Cloud 体系，可以类比成 Eureka Client ）</li>
<li>cluster 路由层：封装多个服务提供者的路由以及负载均衡，将多个实例组合成一个服务。（对比Spring Cloud 体系，可以类比成 Ribbon ）</li>
<li>monitor 监控层：对 rpc 接口的调用次数和调用时间进行监控。</li>
</ul>
</li>
<li>Remoting：<ul>
<li>protocol 远程调用层：远程调用层，封装 rpc 调用。</li>
<li>exchange 信息交换层：信息交换层，封装请求响应模式，同步转异步。</li>
<li>transport 网络传输层：抽象 mina 和 netty 为统一接口。</li>
<li>serialize 数据序列化层：可复用的一些工具，扩展接口为 Serialization, ObjectInput, ObjectOutput</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 调用流程</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850284072280.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850285070885.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>Provider<ul>
<li>第 0 步，start 启动服务。</li>
<li>第 1 步，register 注册服务到注册中心。</li>
</ul>
</li>
<li>Consumer<ul>
<li>第 2 步，subscribe 向注册中心订阅服务。<ul>
<li>注意，只订阅使用到的服务。</li>
<li>再注意，首次会拉取订阅的服务列表，缓存在本地。</li>
<li>【异步】第 3 步，notify 当服务发生变化时，获取最新的服务列表，更新本地缓存。</li>
</ul>
</li>
</ul>
</li>
<li>invoke 调用<ul>
<li>Consumer 直接发起对 Provider 的调用，无需经过注册中心。而对多个 Provider 的负载均衡，Consumer 通过 cluster 组件实现。</li>
</ul>
</li>
<li>count 监控<ul>
<li>【异步】Consumer 和 Provider 都异步通知监控中心。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 调用是同步的吗？</p>
<ul>
<li>默认情况下，调用是同步的方式。</li>
</ul>
</li>
<li><p>谈谈对 Dubbo 的异常处理机制？</p>
<ul>
<li>dubbo的异常处理类是com.alibaba.dubbo.rpc.filter.ExceptionFilter 类,源码这里就不贴了.归纳下对异常的处理分为下面几类:<ul>
<li>1)如果provider实现了GenericService接口,直接抛出</li>
<li>2)如果是checked异常，直接抛出</li>
<li>3)在方法签名上有声明，直接抛出</li>
<li>4)异常类和接口类在同一jar包里，直接抛出</li>
<li>5)是JDK自带的异常，直接抛出</li>
<li>6)是Dubbo本身的异常，直接抛出</li>
<li>7)否则，包装成RuntimeException抛给客户端（防止客户端反序列化失败.前面几种情况都能保证反序列化正常.）</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq315737546/article/details/53915067">dubbo异常处理</a></li>
</ul>
</li>
<li><p>Dubbo 如何做参数校验？</p>
<ul>
<li>参数校验功能，通过参数校验过滤器 ValidationFilter 来实现。</li>
<li>ValidationFilter 在 Dubbo Provider 和 Consumer 都可生效。<ul>
<li>如果我们将校验注解写在 Service 接口的方法上，那么 Consumer 在本地就会校验。如果校验不通过，直接抛出校验失败的异常，不会发起 Dubbo 调用。</li>
<li>如果我们将校验注解写在 Service 实现的方法上，那么 Consumer 在本地不会校验，而是由 Provider 校验。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 可以对调用结果进行缓存吗?</p>
<ul>
<li>Dubbo 通过 CacheFilter 过滤器，提供结果缓存的功能，且既可以适用于 Consumer 也可以适用于 Provider 。</li>
<li>通过结果缓存，用于加速热门数据的访问速度，Dubbo 提供声明式缓存，以减少用户加缓存的工作量。</li>
<li>Dubbo 目前提供三种实现：<ul>
<li>lru ：基于最近最少使用原则删除多余缓存，保持最热的数据被缓存。</li>
<li>threadlocal ：当前线程缓存，比如一个页面渲染，用到很多 portal，每个 portal 都要去查用户信息，通过线程缓存，可以减少这种多余访问。</li>
<li>jcache ：与 JSR107 集成，可以桥接各种缓存实现。</li>
</ul>
</li>
</ul>
</li>
<li><p>注册中心挂了还可以通信吗？</p>
<ul>
<li>可以。对于正在运行的 Consumer 调用 Provider 是不需要经过注册中心，所以不受影响。并且，Consumer 进程中，内存已经缓存了 Provider 列表。</li>
<li>此时 Provider 如果下线呢？<ul>
<li>如果 Provider 是正常关闭，它会主动且直接对和其处于连接中的 Consumer 们，发送一条“我要关闭”了的消息。那么，Consumer 们就不会调用该 Provider ，而调用其它的 Provider 。</li>
<li>因为 Consumer 也会持久化 Provider 列表到本地文件。所以，此处如果 Consumer 重启，依然能够通过本地缓存的文件，获得到 Provider 列表。</li>
<li>一般情况下，注册中心是一个集群，如果一个节点挂了，Dubbo Consumer 和 Provider 将自动切换到集群的另外一个节点上。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 在 Zookeeper 存储了哪些信息？</p>
<pre><code> - ![](https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850292909765.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10)
</code></pre>
<ul>
<li>服务提供者启动时: 向 /dubbo/com.foo.BarService/providers 目录下写入自己的 URL 地址</li>
<li>服务消费者启动时: 订阅 /dubbo/com.foo.BarService/providers 目录下的提供者 URL 地址。并向 /dubbo/com.foo.BarService/consumers 目录下写入自己的 URL 地址（服务消费者启动后，不仅仅订阅了 “providers” 分类，也订阅了 “routes” “configurations” 分类。）</li>
<li>监控中心启动时: 订阅 /dubbo/com.foo.BarService 目录下的所有提供者和消费者 URL 地址。</li>
<li>Zookeeper 的节点层级，自上而下是：<ul>
<li> Root 层：根目录，可通过 &lt;dubbo:registry group=”dubbo” /&gt; 的 “group” 设置 Zookeeper 的根节点，缺省使用 “dubbo” 。</li>
<li> Service 层：服务接口全名。</li>
<li> Type 层：分类。目前除了我们在图中看到的 “providers”( 服务提供者列表 ) “consumers”( 服务消费者列表 ) 外，还有 “routes”( 路由规则列表 ) 和 “configurations”( 配置规则列表 )。</li>
<li> URL 层：URL ，根据不同 Type 目录，下面可以是服务提供者 URL 、服务消费者 URL 、路由规则 URL 、配置规则 URL 。</li>
<li> 实际上 URL 上带有 “category” 参数，已经能判断每个 URL 的分类，但是 Zookeeper 是基于节点目录订阅的，所以增加了 Type 层。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo Provider 如何实现优雅停机？</p>
<ul>
<li>Dubbo 是通过 JDK 的 ShutdownHook 来完成优雅停机的，所以如果用户使用 kill -9 PID 等强制关闭指令，是不会执行优雅停机的，只有通过 kill PID 时，才会执行。</li>
<li>因为大多数情况下，Dubbo 的声明周期是交给 Spring 进行管理，所以在最新的 Dubbo 版本中，增加了对 Spring 关闭事件的监听，从而关闭 Dubbo 服务</li>
<li>服务提供方的优雅停机过程<ul>
<li>首先，从注册中心中取消注册自己，从而使消费者不要再拉取到它。</li>
<li>然后，sleep 10 秒( 可配 )，等到服务消费，接收到注册中心通知到该服务提供者已经下线，加大了在不重试情况下优雅停机的成功率。</li>
<li>之后，广播 READONLY 事件给所有 Consumer 们，告诉它们不要在调用我了！！！如果此处注册中心挂掉的情况，依然能达到告诉 Consumer ，我要下线了的功能。</li>
<li>再之后，sleep 10 毫秒，保证 Consumer 们，尽可能接收到该消息。</li>
<li>再再之后，先标记为不接收新请求，新请求过来时直接报错，让客户端重试其它机器。</li>
<li>再再再之后，关闭心跳线程。</li>
<li>最后，检测线程池中的线程是否正在运行，如果有，等待所有线程执行完成，除非超时，则强制关闭。</li>
<li>最最后，关闭服务器。</li>
</ul>
</li>
<li>服务消费方的优雅停机过程<ul>
<li>停止时，不再发起新的调用请求，所有新的调用在客户端即报错。</li>
<li>然后，检测有没有请求的响应还没有返回，等待响应返回，除非超时，则强制关闭。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo Provider 异步关闭时，如何从注册中心下线？</p>
<ul>
<li>服务提供者，注册到 Zookeeper 上时，创建的是 EPHEMERAL 临时节点。所以在服务提供者异常关闭时，等待 Zookeeper 会话超时，那么该临时节点就会自动删除。</li>
</ul>
</li>
<li><p>Dubbo Consumer 只能调用从注册中心获取的 Provider 么？</p>
<ul>
<li>不是，Consumer 可以强制直连 Provider 。</li>
<li>在开发及测试环境下，经常需要绕过注册中心，只测试指定服务提供者，这时候可能需要点对点直连，点对点直连方式，将以服务接口为单位，忽略注册中心的提供者列表，A 接口配置点对点，不影响 B 接口从注册中心获取列表。</li>
<li>另外，直连 Dubbo Provider 时，如果要 Debug 调试 Dubbo Provider ，可以通过配置，禁用该 Provider 注册到注册中心。否则，会被其它 Consumer 调用到</li>
</ul>
</li>
<li><p>Dubbo 支持哪些通信协议？对应【protocol 远程调用层】。</p>
<ul>
<li>dubbo://</li>
<li>rest://</li>
<li>rmi://</li>
<li>webservice://</li>
<li>hessian://</li>
<li>thrift://</li>
<li>memcached://</li>
<li>redis://</li>
<li>http://</li>
</ul>
</li>
<li><p>什么是本地暴露和远程暴露，他们的区别？</p>
<ul>
<li>远程暴露:每次 Consumer 调用 Provider 都是跨进程，需要进行网络通信。</li>
<li>本地暴露:使用了 injvm:// 协议，是一个伪协议，它不开启端口，不发起远程调用，只在 JVM 内直接关联，但执行 Dubbo 的 Filter 链。</li>
</ul>
</li>
<li><p>Dubbo 使用什么通信框架？对应【transport 网络传输层】。</p>
<ul>
<li>Netty3</li>
<li>Netty4</li>
<li>Mina</li>
<li>Grizzly</li>
<li>在 Dubbo 的最新版本，默认使用 Netty4 的版本</li>
</ul>
</li>
<li><p>Dubbo 支持哪些序列化方式？对应【serialize 数据序列化层】。</p>
<ul>
<li>Dubbo 目前支付如下 7 种序列化方式：<ul>
<li>【重要】Hessian2 ：基于 Hessian 实现的序列化拓展。dubbo:// 协议的默认序列化方案。<ul>
<li>Hessian 除了是 Web 服务，也提供了其序列化实现，因此 Dubbo 基于它实现了序列化拓展。</li>
<li>另外，Dubbo 维护了自己的 hessian-lite ，对 Hessian 2 的 序列化 部分的精简、改进、BugFix 。</li>
</ul>
</li>
<li>Dubbo ：Dubbo 自己实现的序列化拓展。</li>
<li>Kryo ：基于 Kryo 实现的序列化拓展。</li>
<li>FST ：基于 FST 实现的序列化拓展。</li>
<li>JSON ：基于 Fastjson 实现的序列化拓展。</li>
<li>NativeJava ：基于 Java 原生的序列化拓展。</li>
<li>CompactedJava ：在 NativeJava 的基础上，实现了对 ClassDescriptor 的处理。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 有哪些负载均衡策略？对应【cluster 路由层】的 LoadBalance 组件。</p>
<ul>
<li>Random LoadBalance <ul>
<li>随机，按权重设置随机概率。</li>
<li>在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。</li>
</ul>
</li>
<li>RoundRobin LoadBalance<ul>
<li>轮询，按公约后的权重设置轮询比率。</li>
<li>存在慢的提供者累积请求的问题，比如</li>
</ul>
</li>
<li>LeastActive LoadBalance<ul>
<li>最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。</li>
<li>使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。</li>
</ul>
</li>
<li>ConsistentHash LoadBalance<ul>
<li>一致性 Hash，相同参数的请求总是发到同一提供者。</li>
<li>当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 有哪些集群容错策略？对应【cluster 路由层】的 Cluster 组件。</p>
<ul>
<li><p>Consumer 仅仅引用服务 ***-api.jar 包，那么可以获得到需要服务的 XXXService 接口。那么，通过动态创建对应调用 Dubbo 服务的实现类。简化代码如下：</p>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ProxyFactory.java</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * create proxy.</span></span><br><span class="line"><span class="comment"> * 创建 Proxy ，在引用服务调用。</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> invoker Invoker 对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> proxy</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Adaptive(&#123;Constants.PROXY_KEY&#125;)</span></span><br><span class="line">&lt;T&gt; <span class="function">T <span class="title">getProxy</span><span class="params">(Invoker&lt;T&gt; invoker)</span> <span class="keyword">throws</span> RpcException</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>方法参数 invoker ，实现了调用 Dubbo 服务的逻辑。</li>
<li>返回的 <T> 结果，就是 XXXService 的实现类，而这个实现类，就是通过动态代理的工具类进行生成。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo SPI 的设计思想是什么？</p>
<ul>
<li>？？？？？？？？？？？？？？？？？</li>
</ul>
</li>
<li><p>Dubbo 服务如何监控和管理？</p>
<ul>
<li>Dubbo 管理平台 + 监控平台<ul>
<li>dubbo-monitor 监控平台，基于 Dubbo 的【monitor 监控层】，实现相应的监控数据的收集到监控平台。</li>
<li>dubbo-admin 管理平台，基于注册中心，可以获取到服务相关的信息。</li>
</ul>
</li>
<li>链路追踪<ul>
<li>目前能够实现链路追踪的组件还是比较多的，如下：<ul>
<li>Apache SkyWalking 【推荐】</li>
<li>Zipkin</li>
<li>Cat</li>
<li>PinPoint</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 服务如何做降级？比如说服务 A 调用服务 B，结果服务 B 挂掉了。服务 A 再重试几次调用服务 B，还是不行，那么直接降级，走一个备用的逻辑，给用户返回响应。</p>
<ul>
<li>Dubbo 原生自带的服务降级功能：不能实现现代微服务的熔断器的功能</li>
<li>引入支持服务降级的组件<ul>
<li>目前开源社区常用的有两种组件支持服务降级的功能，分别是：<ul>
<li>Alibaba Sentinel</li>
<li>Netflix Hystrix</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 如何做限流？</p>
<ul>
<li>Dubbo 原生自带的限流功能：通过 TpsLimitFilter 实现，仅适用于服务提供者</li>
<li>引入支持限流的组件：推荐集成 Sentinel 组件。</li>
</ul>
</li>
<li><p>Dubbo 的失败重试是什么？</p>
<ul>
<li>所谓失败重试，就是 consumer 调用 provider 要是失败了，比如抛异常了，此时应该是可以重试的，或者调用超时了也可以重试。</li>
<li>实际场景下，我们一般会禁用掉重试。因为，因为超时后重试会有问题，超时你不知道是成功还是失败。例如，可能会导致两次扣款的问题。</li>
<li>所以，我们一般使用 failfast 集群容错策略，而不是 failover 策略。配置如下：  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dubbo:service</span> <span class="attr">cluster</span>=<span class="string">&quot;failfast&quot;</span> <span class="attr">timeout</span>=<span class="string">&quot;2000&quot;</span> /&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>一定一定一定要配置适合自己业务的超时时间。</li>
<li>当然，可以将操作分成读和写两种，前者支持重试，后者不支持重试。因为，读操作天然具有幂等性。</li>
</ul>
</li>
<li><p>Dubbo 支持哪些注册中心？</p>
<ul>
<li>【默认】Zookeeper </li>
<li>Redis </li>
<li>Multicast</li>
<li>Simple 注册中心</li>
<li>Nacos </li>
</ul>
</li>
<li><p>Dubbo 如何升级接口？</p>
<ul>
<li>当一个接口实现，出现不兼容升级时，可以用版本号过渡，版本号不同的服务相互间不引用。</li>
<li>可以按照以下的步骤进行版本迁移：<ul>
<li>在低压力时间段，先升级一半提供者为新版本。</li>
<li>再将所有消费者升级为新版本。</li>
<li>然后将剩下的一半提供者升级为新版本。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 在安全机制方面是如何解决的？</p>
<pre><code> - ![](https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850320360538.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10)
</code></pre>
<ul>
<li>通过令牌验证在注册中心控制权限，以决定要不要下发令牌给消费者，可以防止消费者绕过注册中心访问提供者。</li>
<li>另外通过注册中心可灵活改变授权方式，而不需修改或升级提供者。</li>
</ul>
</li>
<li><p>Dubbo 需要 Web 容器吗？Dubbo 服务启动是否需要启动类似 Tomcat、Jetty 等服务器。</p>
<ul>
<li>这个答案可以是，也可以是不是。为什么呢？根据协议的不同，Provider 会启动不同的服务器。<ul>
<li>在使用 dubbo:// 协议时，答案是否，因为 Provider 启动 Netty、Mina 等 NIO Server 。</li>
<li>在使用 rest:// 协议时，答案是是，Provider 启动 Tomcat、Jetty 等 HTTP 服务器，或者也可以使用 Netty 封装的 HTTP 服务器。</li>
<li>在使用 hessian:// 协议时，答案是是，Provider 启动 Jetty、Tomcat 等 HTTP 服务器。</li>
</ul>
</li>
</ul>
</li>
<li><p>为什么要将系统进行拆分？SOA?微服务?</p>
<ul>
<li>维护成本</li>
<li>分布式挑战</li>
</ul>
</li>
<li><p>Dubbo 如何集成配置中心？</p>
<ul>
<li>对于使用了 Dubbo 的系统，配置分成两类：<ul>
<li>① Dubbo 自身配置。如：Dubbo 请求超时，Dubbo 重试次数等等。</li>
<li>② 非 Dubbo 自身配置<ul>
<li>基建配置，例如：数据库、Redis 等配置。</li>
<li>业务配置，例如：订单超时时间，下单频率等等配置。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 如何实现分布式事务？</p>
<ul>
<li>？？？？？？？？？？？？？？？？？？</li>
</ul>
</li>
<li><p>Spring Cloud 与 Dubbo 怎么选择？</p>
<ul>
<li>？？？？？？？？？？？？</li>
</ul>
</li>
<li><p>如何自己设计一个类似 Dubbo 的 RPC 框架？</p>
<ul>
<li>服务提供者在注册中心服务发布</li>
<li>消费者去注册中心拿对应的服务信息</li>
<li>基于动态代理发起请求</li>
<li>负载均衡算法</li>
<li>通信方式，序列化方式</li>
<li>服务提供者生成一个动态代理，监听某个网络端口，然后代理本地的服务代码。接收到请求的时候，就调用对应的服务代码</li>
</ul>
</li>
</ol>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Dubbo/" rel="tag">Dubbo</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RPC/" rel="tag">RPC</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8/" rel="tag">面试宝典</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Hive进阶"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/11/07/Hive%E8%BF%9B%E9%98%B6/"
    >Hive进阶</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/11/07/Hive%E8%BF%9B%E9%98%B6/" class="article-date">
  <time datetime="2021-11-07T00:11:52.000Z" itemprop="datePublished">2021-11-07</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hive/">Hive</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Hive进阶"><a href="#Hive进阶" class="headerlink" title="Hive进阶"></a>Hive进阶</h1><h1 id="第1章-Explain-查看执行计划"><a href="#第1章-Explain-查看执行计划" class="headerlink" title="第1章 Explain 查看执行计划"></a>第1章 Explain 查看执行计划</h1><h2 id="1-1-创建测试用表"><a href="#1-1-创建测试用表" class="headerlink" title="1.1 创建测试用表"></a>1.1 创建测试用表</h2><ol>
<li><p>建大表、小表和 JOIN 后表的语句</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建大表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable (</span><br><span class="line">    id        <span class="type">bigint</span>,</span><br><span class="line">    t         <span class="type">bigint</span>,</span><br><span class="line">    uid       string,</span><br><span class="line">    keyword   string,</span><br><span class="line">    url_rank  <span class="type">int</span>,</span><br><span class="line">    click_num <span class="type">int</span>,</span><br><span class="line">    click_url string</span><br><span class="line">) <span class="type">row</span> format delimited</span><br><span class="line">    fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建小表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> smalltable</span><br><span class="line">(</span><br><span class="line">    id        <span class="type">bigint</span>,</span><br><span class="line">    t         <span class="type">bigint</span>,</span><br><span class="line">    uid       string,</span><br><span class="line">    keyword   string,</span><br><span class="line">    url_rank  <span class="type">int</span>,</span><br><span class="line">    click_num <span class="type">int</span>,</span><br><span class="line">    click_url string</span><br><span class="line">) <span class="type">row</span> format delimited</span><br><span class="line">    fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建 JOIN 后表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> jointable</span><br><span class="line">(</span><br><span class="line">    id        <span class="type">bigint</span>,</span><br><span class="line">    t         <span class="type">bigint</span>,</span><br><span class="line">    uid       string,</span><br><span class="line">    keyword   string,</span><br><span class="line">    url_rank  <span class="type">int</span>,</span><br><span class="line">    click_num <span class="type">int</span>,</span><br><span class="line">    click_url string</span><br><span class="line">) <span class="type">row</span> format delimited</span><br><span class="line">    fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>分别向大表和小表中导入数据</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/data/bigtable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> bigtable;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/data/smalltable&#x27;</span> <span class="keyword">into</span>  <span class="keyword">table</span> smalltable;</span><br></pre></td></tr></table></figure>
<h2 id="1-2-基本语法"><a href="#1-2-基本语法" class="headerlink" title="1.2 基本语法"></a>1.2 基本语法</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN [EXTENDED <span class="operator">|</span> DEPENDENCY <span class="operator">|</span> <span class="keyword">AUTHORIZATION</span>] query<span class="operator">-</span><span class="keyword">sql</span></span><br></pre></td></tr></table></figure>
<h2 id="1-3-案例实操"><a href="#1-3-案例实操" class="headerlink" title="1.3 案例实操"></a>1.3 案例实操</h2></li>
<li><p>查看下面这条语句的执行计划</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> explain <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> bigtable;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                      Explain                       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> STAGE DEPENDENCIES:                                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage<span class="number">-0</span> <span class="keyword">is</span> a root stage                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> STAGE PLANS:                                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage: Stage<span class="number">-0</span>                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>     <span class="keyword">Fetch</span> Operator                                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       limit: <span class="number">-1</span>                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Processor Tree:                              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>         TableScan                                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           alias: bigtable                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           <span class="keyword">Select</span> Operator                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             expressions: id (type: <span class="type">bigint</span>), t (type: <span class="type">bigint</span>), uid (type: string), keyword (type: string), url_rank (type: <span class="type">int</span>), click_num (type: <span class="type">int</span>), click_url (type: string) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             ListSink                               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">17</span> <span class="keyword">rows</span> selected (<span class="number">0.112</span> seconds)</span><br><span class="line"></span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span>  explain <span class="keyword">select</span> click_url, <span class="built_in">count</span>(<span class="operator">*</span>) ct <span class="keyword">from</span> bigtable <span class="keyword">group</span> <span class="keyword">by</span> click_url;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                      Explain                       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> STAGE DEPENDENCIES:                                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage<span class="number">-1</span> <span class="keyword">is</span> a root stage                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage<span class="number">-0</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-1</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> STAGE PLANS:                                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage: Stage<span class="number">-1</span>                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>     Map Reduce                                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Map Operator Tree:                           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           TableScan                                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             alias: bigtable                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             <span class="keyword">Select</span> Operator                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               expressions: click_url (type: string) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               outputColumnNames: click_url         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               <span class="keyword">Group</span> <span class="keyword">By</span> Operator                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 aggregations: <span class="built_in">count</span>()              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 keys: click_url (type: string)     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 mode: hash                         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 outputColumnNames: _col0, _col1    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 Reduce Output Operator             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   key expressions: _col0 (type: string) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   sort <span class="keyword">order</span>: <span class="operator">+</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   Map<span class="operator">-</span>reduce <span class="keyword">partition</span> columns: _col0 (type: string) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   <span class="keyword">value</span> expressions: _col1 (type: <span class="type">bigint</span>) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Execution mode: vectorized                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Reduce Operator Tree:                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>         <span class="keyword">Group</span> <span class="keyword">By</span> Operator                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           aggregations: <span class="built_in">count</span>(VALUE._col0)         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           keys: KEY._col0 (type: string)           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           mode: mergepartial                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           outputColumnNames: _col0, _col1          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           File Output Operator                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             compressed: <span class="literal">false</span>                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             <span class="keyword">table</span>:                                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 input format: org.apache.hadoop.mapred.SequenceFileInputFormat <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage: Stage<span class="number">-0</span>                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>     <span class="keyword">Fetch</span> Operator                                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       limit: <span class="number">-1</span>                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Processor Tree:                              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>         ListSink                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">49</span> <span class="keyword">rows</span> selected (<span class="number">0.072</span> seconds)</span><br></pre></td></tr></table></figure>
<ul>
<li>STAGE DEPENDENCIES: 各个stage之间的依赖性</li>
<li>STAGE PLANS: 各个stage的执行计划</li>
<li>Map Operator Tree: MAP端的执行计划树</li>
<li>Reduce Operator Tree: Reduce端的执行计划树</li>
<li>TableScan: 表扫描操作，常见的属性：<ul>
<li>alias： 表名称</li>
<li>Statistics： 表统计信息，包含表中数据条数，数据大小等</li>
<li>Select Operator： 选取操作，常见的属性 ：</li>
<li>expressions：需要的字段名称及字段类型</li>
<li>outputColumnNames：输出的列名称</li>
</ul>
</li>
<li>Group By Operator：分组聚合操作，常见的属性：<ul>
<li>aggregations：显示聚合函数信息</li>
<li>mode：聚合模式，值有 hash：随机聚合，就是hash partition；partial：局部聚合；final：最终聚合</li>
<li>keys：分组的字段，如果没有分组，则没有此字段</li>
<li>outputColumnNames：聚合之后输出列名</li>
<li>Statistics： 表统计信息，包含分组聚合之后的数据条数，数据大小等</li>
</ul>
</li>
<li>Reduce Output Operator：输出到reduce操作，常见属性：<ul>
<li>sort order：值为空 不排序；值为 + 正序排序，值为 - 倒序排序；值为 +- 排序的列为两列，第一列为正序，第二列为倒序</li>
</ul>
</li>
<li>Filter Operator：过滤操作，常见的属性：<ul>
<li>predicate：过滤条件，如sql语句中的where id&gt;=1，则此处显示(id &gt;= 1)</li>
</ul>
</li>
<li>Map Join Operator：join 操作，常见的属性：<ul>
<li>condition map：join方式 ，如Inner Join 0 to 1 Left Outer Join0 to 2</li>
<li>keys: join 的条件字段</li>
<li>outputColumnNames： join 完成之后输出的字段</li>
<li>Statistics： join 完成之后生成的数据条数，大小等</li>
</ul>
</li>
<li>File Output Operator：文件输出操作，常见的属性<ul>
<li>compressed：是否压缩</li>
<li>table：表的信息，包含输入输出文件格式化方式，序列化方式等</li>
</ul>
</li>
<li>Fetch Operator 客户端获取数据操作，常见的属性：<ul>
<li>limit，值为 -1 表示不限制条数，其他值为限制的条数</li>
</ul>
</li>
</ul>
</li>
<li><p>查看详细执行计划</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain extended <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> bigtable;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain extended <span class="keyword">select</span> click_url, <span class="built_in">count</span>(<span class="operator">*</span>) ct <span class="keyword">from</span> bigtable <span class="keyword">group</span> <span class="keyword">by</span> click_url;</span><br></pre></td></tr></table></figure>
<h1 id="第2章-Hive-建表优化"><a href="#第2章-Hive-建表优化" class="headerlink" title="第2章 Hive 建表优化"></a>第2章 Hive 建表优化</h1><h2 id="2-1-分区表"><a href="#2-1-分区表" class="headerlink" title="2.1 分区表"></a>2.1 分区表</h2><h2 id="2-2-分桶表"><a href="#2-2-分桶表" class="headerlink" title="2.2 分桶表"></a>2.2 分桶表</h2><h2 id="2-3-合适的文件格式"><a href="#2-3-合适的文件格式" class="headerlink" title="2.3 合适的文件格式"></a>2.3 合适的文件格式</h2><h2 id="2-4-合适的压缩格式"><a href="#2-4-合适的压缩格式" class="headerlink" title="2.4 合适的压缩格式"></a>2.4 合适的压缩格式</h2></li>
</ol>
<h1 id="第3章-HQL-语法优化"><a href="#第3章-HQL-语法优化" class="headerlink" title="第3章 HQL 语法优化"></a>第3章 HQL 语法优化</h1><h2 id="3-1-列裁剪与分区裁剪"><a href="#3-1-列裁剪与分区裁剪" class="headerlink" title="3.1 列裁剪与分区裁剪"></a>3.1 列裁剪与分区裁剪</h2><ul>
<li>列裁剪就是在查询时只读取需要的列，分区裁剪就是只读取需要的分区。当列很多或者<br>数据量很大时，如果 select * 或者不指定分区，全列扫描和全表扫描效率都很低。</li>
<li>Hive 在读数据的时候，可以只读取查询中所需要用到的列，而忽略其他的列。这样做<br>可以节省读取开销：中间表存储开销和数据整合开销。</li>
</ul>
<h2 id="3-2-Group-By"><a href="#3-2-Group-By" class="headerlink" title="3.2 Group By"></a>3.2 Group By</h2><ol>
<li>开启 Map 端聚合参数设置<ol>
<li>是否在 Map 端进行聚合，默认为 True<br> <code>set hive.map.aggr = true; </code></li>
<li>在 Map 端进行聚合操作的条目数目<br> <code>set hive.groupby.mapaggr.checkinterval = 100000;</code></li>
<li>有数据倾斜的时候进行负载均衡（默认是 false）<br> <code>set hive.groupby.skewindata = true;</code><br> 当选项设定为 true，生成的查询计划会有两个 MR Job。<ul>
<li>第一个 MR Job 中，Map 的输出结果会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce中，从而达到负载均衡的目的；</li>
<li>第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作（虽然能解决数据倾斜，但是不能让运行速度的更快）。<h2 id="3-3-Vectorization"><a href="#3-3-Vectorization" class="headerlink" title="3.3 Vectorization"></a>3.3 Vectorization</h2>vectorization : 矢量计算的技术，在计算类似scan, filter, aggregation的时候，vectorization技术以设置批处理的增量大小为 1024 行单次来达到比单条记录单次获得更高的效率。<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.vectorized.execution.enabled <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.vectorized.execution.reduce.enabled <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
</li>
</ol>
<h2 id="3-4-多重模式"><a href="#3-4-多重模式" class="headerlink" title="3.4 多重模式"></a>3.4 多重模式</h2><ul>
<li>如果碰到一堆 SQL，并且这一堆 SQL 的模式还一样。都是从同一个表进行扫描，做不<br>同的逻辑。</li>
<li>可优化的地方：如果有 n 条 SQL，每个 SQL 执行都会扫描一次这张表。  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> .... <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> age <span class="operator">&gt;</span> <span class="number">17</span>;</span><br><span class="line"><span class="keyword">insert</span> .... <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> age <span class="operator">&gt;</span> <span class="number">18</span>;</span><br><span class="line"><span class="keyword">insert</span> .... <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> age <span class="operator">&gt;</span> <span class="number">19</span>;</span><br></pre></td></tr></table></figure></li>
<li>隐藏了一个问题：这种类型的 SQL 有多少个，那么最终。这张表就被全表扫描了多少次  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>A). <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> city<span class="operator">=</span> A;</span><br><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>B). <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> city<span class="operator">=</span> B;</span><br><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>c). <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> city<span class="operator">=</span> c;</span><br><span class="line"><span class="comment">-- 修改为：</span></span><br><span class="line"><span class="keyword">from</span> student</span><br><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>A) <span class="keyword">select</span> id,name,sex, age <span class="keyword">where</span> city<span class="operator">=</span> A</span><br><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>B) <span class="keyword">select</span> id,name,sex, age <span class="keyword">where</span> city<span class="operator">=</span> B</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>如果一个 HQL 底层要执行 10 个 Job，那么能优化成 8 个一般来说，肯定能有所提高，多重插入就是一个非常实用的技能。一次读取，多次插入，有些场景是从一张表读取数据后，要多次利用。</li>
</ul>
</li>
</ul>
<h2 id="3-5-in-exists-语句"><a href="#3-5-in-exists-语句" class="headerlink" title="3.5 in/exists 语句"></a>3.5 in/exists 语句</h2><ul>
<li>使用 Hive 的一个高效替代方案：left semi join</li>
<li>比如：– in / exists 实现  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">where</span> a.id <span class="keyword">in</span> (<span class="keyword">select</span> b.id <span class="keyword">from</span> b);</span><br><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">where</span> <span class="keyword">exists</span> (<span class="keyword">select</span> id <span class="keyword">from</span> b <span class="keyword">where</span> a.id <span class="operator">=</span> b.id);</span><br></pre></td></tr></table></figure></li>
<li>可以使用 join 来改写：  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">join</span> b <span class="keyword">on</span> a.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure></li>
<li>应该转换成left semi join 实现  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">left</span> semi <span class="keyword">join</span> b <span class="keyword">on</span> a.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="3-6-CBO-优化"><a href="#3-6-CBO-优化" class="headerlink" title="3.6 CBO 优化"></a>3.6 CBO 优化</h2><ul>
<li>CBO 优化可以自动优化 HQL 中多个 Join 的顺序，并选择合适的 Join 算法。代价最小的执行计划就是最好的执行计划</li>
<li>要使用基于成本的优化（也称为 CBO），请在查询开始设置以下参数：<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.cbo.enable<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.compute.query.using.stats<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.stats.fetch.column.stats<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.stats.fetch.partition.stats<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="3-7-谓词下推"><a href="#3-7-谓词下推" class="headerlink" title="3.7 谓词下推"></a>3.7 谓词下推</h2><ul>
<li>将 SQL 语句中的 where 谓词逻辑都尽可能提前执行，减少下游处理的数据量。对应逻辑优化器是 PredicatePushDown，配置项为 hive.optimize.ppd，默认为 true。</li>
<li>案例实操：</li>
</ul>
<ol>
<li><p>打开谓词下推优化属性</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 谓词下推，默认是 true</span></span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">set</span> hive.optimize.ppd;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"><span class="operator">|</span>           <span class="keyword">set</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"><span class="operator">|</span> hive.optimize.ppd<span class="operator">=</span><span class="literal">true</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.01</span> seconds) </span><br></pre></td></tr></table></figure></li>
<li><p>查看先关联两张表，再用 where 条件过滤的执行计划</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain <span class="keyword">select</span> o.id <span class="keyword">from</span> bigtable b <span class="keyword">join</span> bigtable o <span class="keyword">on</span> o.id <span class="operator">=</span> b.id <span class="keyword">where</span> o.id <span class="operator">&lt;=</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure></li>
<li><p>查看子查询后，再关联表的执行计划</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain <span class="keyword">select</span> b.id <span class="keyword">from</span> bigtable b <span class="keyword">join</span> (<span class="keyword">select</span> id <span class="keyword">from</span> bigtable <span class="keyword">where</span> id <span class="operator">&lt;=</span> <span class="number">10</span>) o <span class="keyword">on</span> b.id <span class="operator">=</span> o.id;</span><br></pre></td></tr></table></figure>
<h2 id="3-8-MapJoin"><a href="#3-8-MapJoin" class="headerlink" title="3.8 MapJoin"></a>3.8 MapJoin</h2><p>MapJoin 是将 Join 双方比较小的表直接分发到各个 Map 进程的内存中，在 Map 进程中进行 Join 操 作，这样就不用进行 Reduce 步骤，从而提高了速度。如果不指定 MapJoin或者不符合 MapJoin 的条件，那么 Hive 解析器会将 Join 操作转换成 Common Join，即：在Reduce 阶段完成 Join。容易发生数据倾斜。可以用 MapJoin 把小表全部加载到内存在 Map端进行 Join，避免 Reducer 处理。</p>
</li>
<li><p>开启 MapJoin 参数设置</p>
<ol>
<li>设置自动选择 MapJoin <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 默认为 true</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join<span class="operator">=</span><span class="literal">true</span>; </span><br></pre></td></tr></table></figure></li>
<li>大表小表的阈值设置（默认 25M 以下认为是小表）： <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize<span class="operator">=</span><span class="number">25000000</span>;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>MapJoin 工作机制</p>
<ul>
<li>MapJoin 是将 Join 双方比较小的表直接分发到各个 Map 进程的内存中，在 Map 进程中进行 Join 操作，这样就不用进行 Reduce 步骤，从而提高了速度。</li>
</ul>
</li>
<li><p>案例实操：</p>
<ol>
<li>开启 MapJoin 功能 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 默认为 true</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join <span class="operator">=</span> <span class="literal">true</span>; </span><br></pre></td></tr></table></figure></li>
<li>执行小表 JOIN 大表语句<ul>
<li>注意：此时小表(左连接)作为主表，所有数据都要写出去，因此此时会走 reduce，mapjoin失效<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Explain <span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> smalltable s</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> bigtable b</span><br><span class="line"><span class="keyword">on</span> s.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>执行大表 JOIN 小表语句 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Explain <span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> bigtable b</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> smalltable s</span><br><span class="line"><span class="keyword">on</span> s.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure>
<h2 id="3-9-大表、大表-SMB-Join"><a href="#3-9-大表、大表-SMB-Join" class="headerlink" title="3.9 大表、大表 SMB Join"></a>3.9 大表、大表 SMB Join</h2></li>
</ol>
</li>
</ol>
<ul>
<li>SMB Join ：Sort Merge Bucket Join</li>
</ul>
<h2 id="3-10-笛卡尔积"><a href="#3-10-笛卡尔积" class="headerlink" title="3.10 笛卡尔积"></a>3.10 笛卡尔积</h2><p>Join 的时候不加 on 条件，或者无效的 on 条件，因为找不到 Join key，Hive 只能使用1 个 Reducer 来完成笛卡尔积。当 Hive 设定为严格模式（hive.mapred.mode=strictnonstrict） 时，不允许在 HQL 语句中出现笛卡尔积。</p>
<h1 id="第4章-数据倾斜"><a href="#第4章-数据倾斜" class="headerlink" title="第4章 数据倾斜"></a>第4章 数据倾斜</h1><h2 id="4-1-单表数据倾斜优化"><a href="#4-1-单表数据倾斜优化" class="headerlink" title="4.1 单表数据倾斜优化"></a>4.1 单表数据倾斜优化</h2><h3 id="4-1-1-使用参数"><a href="#4-1-1-使用参数" class="headerlink" title="4.1.1 使用参数"></a>4.1.1 使用参数</h3><p>当任务中存在 GroupBy 操作同时聚合函数为 count 或者 sum 可以设置参数来处理数据<br>倾斜问题。</p>
<ul>
<li>是否在 Map 端进行聚合，默认为 True  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>在 Map 端进行聚合操作的条目数目  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval <span class="operator">=</span> <span class="number">100000</span>;</span><br></pre></td></tr></table></figure></li>
<li>有数据倾斜的时候进行负载均衡（默认是 false）  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>当选项设定为 true，生成的查询计划会有两个 MR Job。</li>
</ul>
</li>
</ul>
<h3 id="4-1-2-增加-Reduce-数量"><a href="#4-1-2-增加-Reduce-数量" class="headerlink" title="4.1.2 增加 Reduce 数量"></a>4.1.2 增加 Reduce 数量</h3><ol>
<li>调整 reduce 个数方法一<ol>
<li>每个 Reduce 处理的数据量默认是 256MB <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer <span class="operator">=</span> <span class="number">256000000</span></span><br></pre></td></tr></table></figure></li>
<li>每个任务最大的 reduce 数，默认为 1009 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.max <span class="operator">=</span> <span class="number">1009</span></span><br></pre></td></tr></table></figure></li>
<li>计算 reducer 数的公式 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">N<span class="operator">=</span><span class="built_in">min</span>(参数 <span class="number">2</span>，总输入数据量<span class="operator">/</span>参数 <span class="number">1</span>)(参数 <span class="number">2</span> 指的是上面的 <span class="number">1009</span>，参数 <span class="number">1</span> 值得是 <span class="number">256</span>M)</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>调整 reduce 个数方法二<ol>
<li>在 hadoop 的 mapred-default.xml 文件中修改设置每个 job 的 Reduce 个数 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces <span class="operator">=</span> <span class="number">15</span>;</span><br></pre></td></tr></table></figure>
<h2 id="4-2-Join-数据倾斜优化"><a href="#4-2-Join-数据倾斜优化" class="headerlink" title="4.2 Join 数据倾斜优化"></a>4.2 Join 数据倾斜优化</h2><h3 id="4-2-1-使用参数"><a href="#4-2-1-使用参数" class="headerlink" title="4.2.1 使用参数"></a>4.2.1 使用参数</h3></li>
</ol>
</li>
</ol>
<ul>
<li>在编写 Join 查询语句时，如果确定是由于 join 出现的数据倾斜，那么请做如下设置：  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># <span class="keyword">join</span> 的键对应的记录条数超过这个值则会进行分拆，值根据具体数据量设置</span><br><span class="line"><span class="keyword">set</span> hive.skewjoin.key<span class="operator">=</span><span class="number">100000</span>;</span><br><span class="line"># 如果是 <span class="keyword">join</span> 过程出现倾斜应该设置为 <span class="literal">true</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.skewjoin<span class="operator">=</span><span class="literal">false</span>;</span><br></pre></td></tr></table></figure></li>
<li>如果开启了，在 Join 过程中 Hive 会将计数超过阈值 hive.skewjoin.key（默认 100000）的倾斜 key 对应的行临时写进文件中，然后再启动另一个 job 做 map join 生成结果。通过hive.skewjoin.mapjoin.map.tasks 参数还可以控制第二个 job 的 mapper 数量，默认 10000。  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.skewjoin.mapjoin.map.tasks<span class="operator">=</span><span class="number">10000</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="4-2-2-MapJoin"><a href="#4-2-2-MapJoin" class="headerlink" title="4.2.2 MapJoin"></a>4.2.2 MapJoin</h2><h1 id="第5章-Hive-Job-优化"><a href="#第5章-Hive-Job-优化" class="headerlink" title="第5章 Hive Job 优化"></a>第5章 Hive Job 优化</h1><h2 id="5-1-Hive-Map-优化"><a href="#5-1-Hive-Map-优化" class="headerlink" title="5.1 Hive Map 优化"></a>5.1 Hive Map 优化</h2><h3 id="5-1-1-复杂文件增加-Map-数"><a href="#5-1-1-复杂文件增加-Map-数" class="headerlink" title="5.1.1 复杂文件增加 Map 数"></a>5.1.1 复杂文件增加 Map 数</h3><ul>
<li><p>当 input 的文件都很大，任务逻辑复杂，map 执行非常慢的时候，可以考虑增加 Map 数，来使得每个 map 处理的数据量减少，从而提高任务的执行效率。增加 map 的方法为：根据<code>computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M </code>公式，调整 maxSize 最大值。让 maxSize 最大值低于 blocksize 就可以增加 map 的个数。</p>
</li>
<li><p>案例实操：</p>
<ol>
<li>执行查询 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> emp;</span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage<span class="number">-1</span>: number <span class="keyword">of</span> mappers: <span class="number">1</span>; number <span class="keyword">of</span> reducers: <span class="number">1</span></span><br></pre></td></tr></table></figure></li>
<li>设置最大切片值为 100 个字节 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.input.fileinputformat.split.maxsize<span class="operator">=</span><span class="number">100</span>;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> emp;</span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage<span class="number">-1</span>: number <span class="keyword">of</span> mappers: <span class="number">6</span>; number <span class="keyword">of</span> reducers: <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="5-1-2-小文件进行合并"><a href="#5-1-2-小文件进行合并" class="headerlink" title="5.1.2 小文件进行合并"></a>5.1.2 小文件进行合并</h2></li>
</ol>
</li>
</ul>
<ol>
<li>在 map 执行前合并小文件，减少 map 数：CombineHiveInputFormat 具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat 没有对小文件合并功能。 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure></li>
<li>在 Map-Reduce 的任务结束时合并小文件的设置： <ul>
<li>在 map-only 任务结束时合并小文件，默认 true  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.merge.mapfiles <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>在 map-reduce 任务结束时合并小文件，默认 false  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.merge.mapredfiles <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>合并文件的大小，默认 256M  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.merge.size.per.task <span class="operator">=</span> <span class="number">268435456</span>;</span><br></pre></td></tr></table></figure></li>
<li>当输出文件的平均大小小于该值时，启动一个独立的 map-reduce 任务进行文件 merge  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.merge.smallfiles.avgsize <span class="operator">=</span> <span class="number">16777216</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="5-1-3-Map-端聚合"><a href="#5-1-3-Map-端聚合" class="headerlink" title="5.1.3 Map 端聚合"></a>5.1.3 Map 端聚合</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 相当于 map 端执行 combiner</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<h3 id="5-1-4-推测执行"><a href="#5-1-4-推测执行" class="headerlink" title="5.1.4 推测执行"></a>5.1.4 推测执行</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#默认是 <span class="literal">true</span></span><br><span class="line"><span class="keyword">set</span> mapred.map.tasks.speculative.execution <span class="operator">=</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h2 id="5-2-Hive-Reduce-优化"><a href="#5-2-Hive-Reduce-优化" class="headerlink" title="5.2 Hive Reduce 优化"></a>5.2 Hive Reduce 优化</h2><h3 id="5-2-1-合理设置-Reduce-数"><a href="#5-2-1-合理设置-Reduce-数" class="headerlink" title="5.2.1 合理设置 Reduce 数"></a>5.2.1 合理设置 Reduce 数</h3><ol>
<li>调整 reduce 个数方法一<ol>
<li>每个 Reduce 处理的数据量默认是 256MB <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer <span class="operator">=</span> <span class="number">256000000</span></span><br></pre></td></tr></table></figure></li>
<li>每个任务最大的 reduce 数，默认为 1009 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.max <span class="operator">=</span> <span class="number">1009</span></span><br></pre></td></tr></table></figure></li>
<li>计算 reducer 数的公式 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">N=min(参数 2，总输入数据量/参数 1)(参数 2 指的是上面的 1009，参数 1 值得是 256M)</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>调整 reduce 个数方法二<ul>
<li>在 hadoop 的 mapred-default.xml 文件中修改,设置每个 job 的 Reduce 个数  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces <span class="operator">=</span> <span class="number">15</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>reduce 个数并不是越多越好<ol>
<li>过多的启动和初始化 reduce 也会消耗时间和资源；</li>
<li>另外，有多少个 reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</li>
<li>在设置 reduce 个数的时候也需要考虑这两个原则：<ol>
<li>处理大数据量利用合适的 reduce 数；</li>
<li>使单个 reduce 任务处理数据量大小要合适；</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="5-3-2-推测执行"><a href="#5-3-2-推测执行" class="headerlink" title="5.3.2 推测执行"></a>5.3.2 推测执行</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># hadoop 里面的</span><br><span class="line">mapred.reduce.tasks.speculative.execution</span><br><span class="line"># hive 里面相同的参数，效果和hadoop 里面的一样两个随便哪个都行</span><br><span class="line">hive.mapred.reduce.tasks.speculative.execution</span><br></pre></td></tr></table></figure>

<h2 id="5-3-Hive-任务整体优化"><a href="#5-3-Hive-任务整体优化" class="headerlink" title="5.3 Hive 任务整体优化"></a>5.3 Hive 任务整体优化</h2><h3 id="5-3-1-Fetch-抓取"><a href="#5-3-1-Fetch-抓取" class="headerlink" title="5.3.1 Fetch 抓取"></a>5.3.1 Fetch 抓取</h3><ul>
<li>Fetch 抓取是指，Hive 中对某些情况的查询可以不必使用 MapReduce 计算。例如：<code>SELECT * FROM emp</code></li>
<li>在这种情况下，Hive 可以简单地读取 emp 对应的存储目录下的文件，然后输出<br>查询结果到控制台。</li>
<li>在 hive-default.xml.template 文件中 hive.fetch.task.conversion 默认是 more，老版本 hive默认是 minimal，该属性修改为 more 以后，在全局查找、字段查找、limit 查找等都不走mapreduce。  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.fetch.task.conversion<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>more<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">        Expects one of [none, minimal, more].</span><br><span class="line">        Some select queries can be converted to single FETCH task minimizing latency.</span><br><span class="line">        Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incurs RS), lateral views and joins.</span><br><span class="line">        0. none : disable hive.fetch.task.conversion</span><br><span class="line">        1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only</span><br><span class="line">        2. more : SELECT, FILTER, LIMIT only (support TABLESAMPLE and </span><br><span class="line">        virtual columns)</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hive/" rel="tag">Hive</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Hadoop压缩、优化、高可用"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/11/07/Hadoop%E5%8E%8B%E7%BC%A9%E3%80%81%E4%BC%98%E5%8C%96%E3%80%81%E9%AB%98%E5%8F%AF%E7%94%A8/"
    >Hadoop压缩、优化、高可用</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/11/07/Hadoop%E5%8E%8B%E7%BC%A9%E3%80%81%E4%BC%98%E5%8C%96%E3%80%81%E9%AB%98%E5%8F%AF%E7%94%A8/" class="article-date">
  <time datetime="2021-11-07T00:10:55.000Z" itemprop="datePublished">2021-11-07</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Hadoop压缩、优化、高可用"><a href="#Hadoop压缩、优化、高可用" class="headerlink" title="Hadoop压缩、优化、高可用"></a>Hadoop压缩、优化、高可用</h1><h1 id="一、Hadoop数据压缩"><a href="#一、Hadoop数据压缩" class="headerlink" title="一、Hadoop数据压缩"></a>一、Hadoop数据压缩</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><h3 id="1-1-1-压缩概述"><a href="#1-1-1-压缩概述" class="headerlink" title="1.1.1 压缩概述"></a>1.1.1 压缩概述</h3><ul>
<li>压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在运行MR程序时，I/O操作、网络数据传输、 Shuffle和Merge要花大量的时间，尤其是数据规模很大和工作负载密集的情况下，因此，使用数据压缩显得非常重要。</li>
<li>鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。可以在任意MapReduce阶段启用压缩。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价。<h2 id="1-1-2-压缩策略和原则"><a href="#1-1-2-压缩策略和原则" class="headerlink" title="1.1.2 压缩策略和原则"></a>1.1.2 压缩策略和原则</h2></li>
<li>压缩是提高Hadoop运行效率的一种优化策略</li>
<li>通过对Mapper、Reducer运行过程的数据进行压缩，以减少磁盘IO，提高MR程序运行速度</li>
<li>注意：采用压缩技术减少了磁盘IO，但同时增加了CPU运算负担。所以，压缩特性运用得当能提高性能，但运用不当也可能降低性能</li>
<li>压缩基本原则<ul>
<li>运算密集型的job，少用压缩</li>
<li>IO密集型的job，多用压缩</li>
</ul>
</li>
</ul>
<hr>
<h2 id="1-2-MR支持的压缩编码"><a href="#1-2-MR支持的压缩编码" class="headerlink" title="1.2 MR支持的压缩编码"></a>1.2 MR支持的压缩编码</h2><ul>
<li>支持的压缩编码<table>
<thead>
<tr>
<th>压缩格式</th>
<th>hadoop自带？</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
<th>换成压缩格式后，原来的程序是否需要修改</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>Gzip</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>bzip2</td>
<td>是，直接使用</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>LZO</td>
<td>否，需要安装</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
<td>需要建索引，还需要指定输入格式</td>
</tr>
<tr>
<td>Snappy</td>
<td>是，直接使用</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
</tbody></table>
</li>
<li>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示。<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码/解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
</li>
<li>压缩性能的比较<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB/s</td>
<td>74.6MB/s</td>
</tr>
</tbody></table>
</li>
</ul>
<hr>
<h2 id="1-3-压缩方式选择"><a href="#1-3-压缩方式选择" class="headerlink" title="1.3 压缩方式选择"></a>1.3 压缩方式选择</h2><h3 id="1-3-1-Gzip"><a href="#1-3-1-Gzip" class="headerlink" title="1.3.1 Gzip"></a>1.3.1 Gzip</h3><ul>
<li>优点：压缩率比较高，而且压缩/解压速度也比较快；Hadoop本身支持，在应用中处理Gzip格式的文件就和直接处理文本一样；大部分Linux系统都自带Gzip命令，使用方便。</li>
<li>缺点：不支持Split。</li>
<li>应用场景：当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用Gzip压缩格式。例如说一天或者一个小时的日志压缩成一个Gzip文件。</li>
</ul>
<h3 id="1-3-2-Bzip2"><a href="#1-3-2-Bzip2" class="headerlink" title="1.3.2 Bzip2"></a>1.3.2 Bzip2</h3><ul>
<li>优点：支持Split；具有很高的压缩率，比Gzip压缩率都高；Hadoop本身自带，使用方便</li>
<li>缺点：压缩/解压速度慢</li>
<li>应用场景：适合对速度要求不高，但需要较高的压缩率的时候；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持Split，而且兼容之前的应用程序的情况</li>
</ul>
<h3 id="1-3-3-Lzo压缩"><a href="#1-3-3-Lzo压缩" class="headerlink" title="1.3.3 Lzo压缩"></a>1.3.3 Lzo压缩</h3><ul>
<li>优点：压缩/解压速度也比较快，合理的压缩率；支持Split，是Hadoop中最流行的压缩格式；可以在Linux系统下安装lzop命令，使用方便</li>
<li>缺点：压缩率比Gzip要低一些；Hadoop本身不支持，需要安装；在应用中对Lzo格式的文件需要做一些特殊处理（为了支持Split需要建索引，还需要指定InputFormat为Lzo格式）</li>
<li>应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，Lzo优点越越明显</li>
</ul>
<h3 id="1-3-4-Snappy压缩"><a href="#1-3-4-Snappy压缩" class="headerlink" title="1.3.4 Snappy压缩"></a>1.3.4 Snappy压缩</h3><ul>
<li>优点：高速压缩速度和合理的压缩率</li>
<li>缺点：不支持Split；压缩率比Gzip要低；Hadoop本身不支持，需要安装</li>
<li>应用场景：当MapReduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个MapReduce作业的输出和另外一个MapReduce作业的输入</li>
</ul>
<hr>
<h2 id="1-4-压缩位置选择"><a href="#1-4-压缩位置选择" class="headerlink" title="1.4 压缩位置选择"></a>1.4 压缩位置选择</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345428872480.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<hr>
<h2 id="1-5-压缩参数配置"><a href="#1-5-压缩参数配置" class="headerlink" title="1.5 压缩参数配置"></a>1.5 压缩参数配置</h2><p>要在Hadoop中启用压缩，可以配置如下参数：<br>|参数    |默认值    |阶段    |建议|<br>|——|——|——|—-|<br>|io.compression.codecs（在core-site.xml中配置）    |无，这个需要在命令行输入hadoop checknative查看    |输入压缩    |Hadoop使用文件扩展名判断是否支持某种编解码器|<br>|mapreduce.map.output.compress（在mapred-site.xml中配置）    |false    |mapper输出    |这个参数设为true启用压缩|<br>|mapreduce.map.output.compress.codec（在mapred-site.xml中配置）    |org.apache.hadoop.io.compress.DefaultCodec    |mapper输出    |企业多使用LZO或Snappy编解码器在此阶段压缩数据|<br>|mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置）    |false    |reducer输出    |这个参数设为true启用压缩|<br>|mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置）    |org.apache.hadoop.io.compress.DefaultCodec    |reducer输出    |使用标准工具或者编解码器，如gzip和bzip2|<br>|mapreduce.output.fileoutputformat.compress.type（在mapred-site.xml中配置）    |RECORD    |reducer输出    |SequenceFile输出使用的压缩类型：NONE和BLOCK|</p>
<hr>
<h2 id="1-6-压缩实操案例"><a href="#1-6-压缩实操案例" class="headerlink" title="1.6 压缩实操案例"></a>1.6 压缩实操案例</h2><h3 id="1-6-1-数据流的压缩和解压缩"><a href="#1-6-1-数据流的压缩和解压缩" class="headerlink" title="1.6.1 数据流的压缩和解压缩"></a>1.6.1 数据流的压缩和解压缩</h3><ul>
<li>使用createOutputStream(OutputStreamout)方法创建一个CompressionOutputStream，将其以压缩格式写入底层的流</li>
<li>调用createInputStream(InputStreamin)函数，获得一个CompressionInputStream，从底层的流读取未压缩的数据</li>
</ul>
<h3 id="1-6-2-Map输出端压缩"><a href="#1-6-2-Map输出端压缩" class="headerlink" title="1.6.2 Map输出端压缩"></a>1.6.2 Map输出端压缩</h3><ul>
<li>即使MapReduce的输入输出文件都是未压缩的文件，仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可。  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开启map端输出压缩</span></span><br><span class="line">conf.setBoolean(<span class="string">&quot;mapreduce.map.output.compress&quot;</span>, <span class="keyword">true</span>);</span><br><span class="line"><span class="comment">// 设置map端输出压缩方式</span></span><br><span class="line">conf.setClass(<span class="string">&quot;mapreduce.map.output.compress.codec&quot;</span>, BZip2Codec.class,CompressionCodec.class);</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="1-6-3-Reduce输出端压缩"><a href="#1-6-3-Reduce输出端压缩" class="headerlink" title="1.6.3 Reduce输出端压缩"></a>1.6.3 Reduce输出端压缩</h3><pre><code><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置reduce端输出压缩开启</span></span><br><span class="line">FileOutputFormat.setCompressOutput(job, <span class="keyword">true</span>);</span><br><span class="line"><span class="comment">// 设置压缩的方式</span></span><br><span class="line">FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class); </span><br></pre></td></tr></table></figure>
</code></pre>
<h1 id="二、Hadoop性能优化"><a href="#二、Hadoop性能优化" class="headerlink" title="二、Hadoop性能优化"></a>二、Hadoop性能优化</h1><h2 id="2-1-MapReduce跑得慢的原因"><a href="#2-1-MapReduce跑得慢的原因" class="headerlink" title="2.1 MapReduce跑得慢的原因"></a>2.1 MapReduce跑得慢的原因</h2><ol>
<li>计算机性能<ul>
<li>CPU，内存，磁盘，网络···</li>
</ul>
</li>
<li>I/O操作优化<ol>
<li>数据倾斜</li>
<li>Map和Reduce数设置不合理</li>
<li>Map运行时间太长，导致Reduce等待过久</li>
<li>小文件过多</li>
<li>大量不可切片的超大压缩文件</li>
<li>Spill次数过多</li>
<li>Merge次数过多</li>
</ol>
</li>
</ol>
<h2 id="2-2-MapReduce优化"><a href="#2-2-MapReduce优化" class="headerlink" title="2.2 MapReduce优化"></a>2.2 MapReduce优化</h2><ul>
<li>MapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题、常用的调优参数</li>
</ul>
<h3 id="2-2-1-数据输入"><a href="#2-2-1-数据输入" class="headerlink" title="2.2.1 数据输入"></a>2.2.1 数据输入</h3><ol>
<li>合并小文件：执行MR任务前将小文件合并，大量小文件会产生大量Map任务，增大Map任务装载次数，而任务的装载比较耗时，从而导致MR运行较慢</li>
<li>采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景</li>
</ol>
<h3 id="2-2-2-Map阶段"><a href="#2-2-2-Map阶段" class="headerlink" title="2.2.2 Map阶段"></a>2.2.2 Map阶段</h3><ol>
<li>减少溢写（Spill）次数：通过调整mapreduce.task.io.sort.mb及mapreduce.map.sort.spill.percent参数值，增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO</li>
<li>减少合并（Merge）次数：通过调整mapreduce.task.io.sort.factor参数，增大Merge的文件数目，减少Merge的次数，从而缩短MR处理时间</li>
<li>在Map之后，不影响业务逻辑前提下，先进行Combine处理，减少 I/O</li>
</ol>
<h3 id="2-2-3-Reduce阶段"><a href="#2-2-3-Reduce阶段" class="headerlink" title="2.2.3 Reduce阶段"></a>2.2.3 Reduce阶段</h3><ol>
<li>合理设置Map和Reduce数：两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致Map、Reduce任务间竞争资源，造成处理超时等错误</li>
<li>设置Map、Reduce共存：调整mapreduce.job.reduce.slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间</li>
<li>规避使用Reduce：因为Reduce在用于连接数据集的时候将会产生大量的网络消耗</li>
<li>合理设置Reduce端的Buffer：默认情况下，数据达到一个阈值的时候，Buffer中的数据就会写入磁盘，然后Reduce会从磁盘中获得所有的数据。也就是说，Buffer和Reduce是没有直接关联的，中间多次写磁盘-&gt;读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得Buffer中的一部分数据可以直接输送到Reduce，从而减少IO开销：<font color ='red' >mapreduce.reduce.input.buffer.percent，默认为0.0。当值大于0的时候，会保留指定比例的内存读Buffer中的数据直接拿给Reduce使用。这样一来，设置Buffer需要内存，读取数据需要内存，Reduce计算也要内存，所以要根据作业的运行情况进行调整</font></li>
</ol>
<h3 id="2-2-4-I-O传输"><a href="#2-2-4-I-O传输" class="headerlink" title="2.2.4 I/O传输"></a>2.2.4 I/O传输</h3><ol>
<li>采用数据压缩的方式，减少网络I/O传输的数据量，从而减少I/O传输时间，安装Snappy和LZO压缩编码器。</li>
<li>使用SequenceFile二进制文件</li>
</ol>
<h3 id="2-2-5-数据倾斜问题"><a href="#2-2-5-数据倾斜问题" class="headerlink" title="2.2.5 数据倾斜问题"></a>2.2.5 数据倾斜问题</h3><ol>
<li>数据倾斜现象<ul>
<li>数据频率倾斜——某一个区域的数据量要远远大于其他区域</li>
<li>数据大小倾斜——部分记录的大小远远大于平均值</li>
</ul>
</li>
<li>减少数据倾斜的方法<ul>
<li>方法1：抽样和范围分区<ol>
<li>可以通过对原始数据进行抽样得到的结果集来预设分区边界值</li>
</ol>
</li>
<li>方法2：自定义分区<ol>
<li>基于输出键的背景知识进行自定义分区。例如，如果Map输出键的单词来源于一本书。且其中某几个专业词汇较多。那么就可以自定义分区将这这些专业词汇发送给固定的一部分Reduce实例。而将其他的都发送给剩余的Reduce实例</li>
</ol>
</li>
<li>方法3：Combiner<ol>
<li>使用Combiner可以大量地减小数据倾斜。在可能的情况下，Combine的目的就是聚合并精简数据</li>
</ol>
</li>
<li>方法4：采用Map Join，尽量避免Reduce Join</li>
</ul>
</li>
</ol>
<h2 id="2-3常用的调优参数"><a href="#2-3常用的调优参数" class="headerlink" title="2.3常用的调优参数"></a>2.3常用的调优参数</h2><ol>
<li><p>资源相关参数：<br> 在MR应用程序中配置就可以生效（mapred-default.xml）</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.map.memory.mb</td>
<td>一个MapTask可使用的资源上限（单位:MB），默认为1024。如果MapTask实际使用的资源量超过该值，则会被强制杀死。</td>
</tr>
<tr>
<td>mapreduce.reduce.memory.mb</td>
<td>一个ReduceTask可使用的资源上限（单位:MB），默认为1024。如果ReduceTask实际使用的资源量超过该值，则会被强制杀死。</td>
</tr>
<tr>
<td>mapreduce.map.cpu.vcores</td>
<td>每个MapTask可使用的最多cpu core数目，默认值: 1</td>
</tr>
<tr>
<td>mapreduce.reduce.cpu.vcores</td>
<td>每个ReduceTask可使用的最多cpu core数目，默认值: 1</td>
</tr>
<tr>
<td>mapreduce.reduce.shuffle.parallelcopies</td>
<td>每个Reduce去Map中取数据的并行数。默认值是5</td>
</tr>
<tr>
<td>mapreduce.reduce.shuffle.merge.percent</td>
<td>Buffer中的数据达到多少比例开始写入磁盘。默认值0.66</td>
</tr>
<tr>
<td>mapreduce.reduce.shuffle.input.buffer.percent</td>
<td>Buffer大小占Reduce可用内存的比例。默认值0.7</td>
</tr>
<tr>
<td>mapreduce.reduce.input.buffer.percent</td>
<td>指定多少比例的内存用来存放Buffer中的数据，默认值是0.0</td>
</tr>
</tbody></table>
</li>
<li><p>在YARN启动之前就配置在服务器的配置文件中才能生效（yarn-default.xml）</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>yarn.scheduler.minimum-allocation-mb</td>
<td>给应用程序Container分配的最小内存，默认值：1024</td>
</tr>
<tr>
<td>yarn.scheduler.maximum-allocation-mb</td>
<td>给应用程序Container分配的最大内存，默认值：8192</td>
</tr>
<tr>
<td>yarn.scheduler.minimum-allocation-vcores</td>
<td>每个Container申请的最小CPU核数，默认值：1</td>
</tr>
<tr>
<td>yarn.scheduler.maximum-allocation-vcores</td>
<td>每个Container申请的最大CPU核数，默认值：32</td>
</tr>
<tr>
<td>yarn.nodemanager.resource.memory-mb</td>
<td>给Containers分配的最大物理内存，默认值：8192</td>
</tr>
</tbody></table>
</li>
<li><p>Shuffle性能优化的关键参数，应在YARN启动之前就配置好（mapred-default.xml）</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.task.io.sort.mb</td>
<td>Shuffle的环形缓冲区大小，默认100m</td>
</tr>
<tr>
<td>mapreduce.map.sort.spill.percent</td>
<td>环形缓冲区溢出的阈值，默认80%</td>
</tr>
</tbody></table>
</li>
<li><p>容错相关参数（MapReduce性能优化）</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.map.maxattempts</td>
<td>每个Map Task最大重试次数，一旦重试次数超过该值，则认为Map Task运行失败，默认值：4。</td>
</tr>
<tr>
<td>mapreduce.reduce.maxattempts</td>
<td>每个Reduce Task最大重试次数，一旦重试次数超过该值，则认为Map Task运行失败，默认值：4。</td>
</tr>
<tr>
<td>mapreduce.task.timeout</td>
<td>Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是600000（10分钟）。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是：“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”。</td>
</tr>
</tbody></table>
</li>
</ol>
<h2 id="2-4-Hadoop小文件优化方法"><a href="#2-4-Hadoop小文件优化方法" class="headerlink" title="2.4 Hadoop小文件优化方法"></a>2.4 Hadoop小文件优化方法</h2><h3 id="2-4-1-Hadoop小文件弊端"><a href="#2-4-1-Hadoop小文件弊端" class="headerlink" title="2.4.1 Hadoop小文件弊端"></a>2.4.1 Hadoop小文件弊端</h3><ul>
<li>HDFS上每个文件都要在NameNode上创建对应的元数据，这个元数据的大小约为150byte，这样当小文件比较多的时候，就会产生很多的元数据文件，一方面会大量占用NameNode的内存空间，另一方面就是元数据文件过多，使得寻址索引速度变慢。</li>
<li>小文件过多，在进行MR计算时，会生成过多切片，需要启动过多的MapTask。每个MapTask处理的数据量小，导致MapTask的处理时间比启动时间还小，白白消耗资源。<h3 id="2-4-2-Hadoop小文件解决方案"><a href="#2-4-2-Hadoop小文件解决方案" class="headerlink" title="2.4.2 Hadoop小文件解决方案"></a>2.4.2 Hadoop小文件解决方案</h3></li>
</ul>
<ol>
<li>小文件优化的方向：<ul>
<li>（1）在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS。</li>
<li>（2）在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并。</li>
<li>（3）在MapReduce处理时，可采用CombineTextInputFormat提高效率。</li>
<li>（4）开启uber模式，实现jvm重用</li>
</ul>
</li>
<li>Hadoop Archive<ul>
<li>是一个高效的将小文件放入HDFS块中的文件存档工具，能够将多个小文件打包成一个HAR文件，从而达到减少NameNode的内存使用</li>
</ul>
</li>
<li>SequenceFile<ul>
<li>SequenceFile是由一系列的二进制k/v组成，如果为key为文件名，value为文件内容，可将大批小文件合并成一个大文件</li>
</ul>
</li>
<li>CombineTextInputFormat<ul>
<li>CombineTextInputFormat用于将多个小文件在切片过程中生成一个单独的切片或者少量的切片。 </li>
</ul>
</li>
<li>开启uber模式，实现jvm重用。默认情况下，每个Task任务都需要启动一个jvm来运行，如果Task任务计算的数据量很小，我们可以让同一个Job的多个Task运行在一个Jvm中，不必为每个Task都开启一个Jvm. <ul>
<li>开启uber模式，在mapred-site.xml中添加如下配置  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--  开启uber模式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- uber模式中最大的mapTask数量，可向下修改  --&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.maxmaps<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>9<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- uber模式中最大的reduce数量，可向下修改 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.maxreduces<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- uber模式中最大的输入数据量，默认使用dfs.blocksize 的值，可向下修改 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.maxbytes<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h1 id="三、Hadoop新特性"><a href="#三、Hadoop新特性" class="headerlink" title="三、Hadoop新特性"></a>三、Hadoop新特性</h1><h2 id="3-1-Hadoop2-x新特性"><a href="#3-1-Hadoop2-x新特性" class="headerlink" title="3.1 Hadoop2.x新特性"></a>3.1 Hadoop2.x新特性</h2><h3 id="3-1-1-集群间数据拷贝"><a href="#3-1-1-集群间数据拷贝" class="headerlink" title="3.1.1 集群间数据拷贝"></a>3.1.1 集群间数据拷贝</h3><ul>
<li>采用distcp命令实现两个Hadoop集群之间的递归数据复制<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ hadoop distcp hdfs://hadoop002:9820/WeCom_3.1.18.90318.dmg hdfs://hadoop002:9820/testDistct</span><br><span class="line">···</span><br><span class="line">···</span><br><span class="line">2021-10-19 17:56:03,705 INFO mapreduce.Job: Job job_1634633871057_0003 completed successfully</span><br><span class="line">2021-10-19 17:56:03,754 INFO mapreduce.Job: Counters: 36</span><br><span class="line">	File System Counters</span><br><span class="line">		FILE: Number of bytes <span class="built_in">read</span>=0</span><br><span class="line">		FILE: Number of bytes written=227128</span><br><span class="line">		FILE: Number of <span class="built_in">read</span> operations=0</span><br><span class="line">		FILE: Number of large <span class="built_in">read</span> operations=0</span><br><span class="line">		FILE: Number of write operations=0</span><br><span class="line">		HDFS: Number of bytes <span class="built_in">read</span>=325449295</span><br><span class="line">		HDFS: Number of bytes written=325448910</span><br><span class="line">		HDFS: Number of <span class="built_in">read</span> operations=19</span><br><span class="line">		HDFS: Number of large <span class="built_in">read</span> operations=0</span><br><span class="line">		HDFS: Number of write operations=5</span><br><span class="line">	Job Counters</span><br><span class="line">		Launched map tasks=1</span><br><span class="line">		Other <span class="built_in">local</span> map tasks=1</span><br><span class="line">		Total time spent by all maps <span class="keyword">in</span> occupied slots (ms)=5517</span><br><span class="line">		Total time spent by all reduces <span class="keyword">in</span> occupied slots (ms)=0</span><br><span class="line">		Total time spent by all map tasks (ms)=5517</span><br><span class="line">		Total vcore-milliseconds taken by all map tasks=5517</span><br><span class="line">		Total megabyte-milliseconds taken by all map tasks=5649408</span><br><span class="line">	Map-Reduce Framework</span><br><span class="line">		Map input records=1</span><br><span class="line">		Map output records=0</span><br><span class="line">		Input split bytes=136</span><br><span class="line">		Spilled Records=0</span><br><span class="line">		Failed Shuffles=0</span><br><span class="line">		Merged Map outputs=0</span><br><span class="line">		GC time elapsed (ms)=52</span><br><span class="line">		CPU time spent (ms)=1730</span><br><span class="line">		Physical memory (bytes) snapshot=270876672</span><br><span class="line">		Virtual memory (bytes) snapshot=2578894848</span><br><span class="line">		Total committed heap usage (bytes)=217055232</span><br><span class="line">		Peak Map Physical memory (bytes)=270876672</span><br><span class="line">		Peak Map Virtual memory (bytes)=2578894848</span><br><span class="line">	File Input Format Counters</span><br><span class="line">		Bytes Read=249</span><br><span class="line">	File Output Format Counters</span><br><span class="line">		Bytes Written=0</span><br><span class="line">	DistCp Counters</span><br><span class="line">		Bandwidth <span class="keyword">in</span> Btyes=81362227</span><br><span class="line">		Bytes Copied=325448910</span><br><span class="line">		Bytes Expected=325448910</span><br><span class="line">		Files Copied=1</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="3-1-2-小文件存档"><a href="#3-1-2-小文件存档" class="headerlink" title="3.1.2 小文件存档"></a>3.1.2 小文件存档</h3><ol>
<li>HDFS存储小文件弊端<ul>
<li>每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关。例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB</li>
</ul>
</li>
<li>解决存储小文件办法之一<ul>
<li>HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，HDFS存档文件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346376074095.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
<li>案例实操<ul>
<li>归档文件:把/user/atguigu/input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/user/atguigu/output路径下。  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop archive -archiveName input.har -p  /user/atguigu/input   /user/atguigu/output</span><br></pre></td></tr></table></figure></li>
<li>查看归档  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -ls /user/atguigu/output/input.har</span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -ls har:///user/atguigu/output/input.har</span><br></pre></td></tr></table></figure></li>
<li>解归档文件  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -cp har:/// user/atguigu/output/input.har/*    /user/atguigu</span><br></pre></td></tr></table></figure>
<h3 id="3-1-2-回收站"><a href="#3-1-2-回收站" class="headerlink" title="3.1.2 回收站"></a>3.1.2 回收站</h3></li>
</ul>
</li>
</ol>
<ul>
<li>开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用</li>
<li>开启回收站功能参数说明<ol>
<li>默认值fs.trash.interval=0，0表示禁用回收站;其他值表示设置文件的存活时间</li>
<li>默认值fs.trash.checkpoint.interval=0，检查回收站的间隔时间。如果该值为0，则该值设置和fs.trash.interval的参数值相等</li>
<li>要求fs.trash.checkpoint.interval&lt;=fs.trash.interval</li>
</ol>
</li>
<li>回收站工作机制<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346388522940.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>回收站使用<ol>
<li>启用回收站：修改core-site.xml，配置垃圾回收时间为1分钟。 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.checkpoint.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>查看回收站 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#回收站目录在hdfs集群中的路径：</span></span><br><span class="line">/user/atguigu/.Trash/</span><br></pre></td></tr></table></figure></li>
<li>通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Trash trash = <span class="function">New <span class="title">Trash</span><span class="params">(conf)</span></span>;</span><br><span class="line">trash.moveToTrash(path);</span><br></pre></td></tr></table></figure></li>
<li>通过网页上直接删除的文件也不会走回收站。</li>
<li>只有在命令行利用hadoop fs -rm命令删除的文件才会走回收站。 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -rm -r /user/atguigu/input</span><br><span class="line">2020-07-14 16:13:42,643 INFO fs.TrashPolicyDefault: Moved: <span class="string">&#x27;hdfs://hadoop102:9820/user/atguigu/input&#x27;</span> to trash at: hdfs://hadoop102:9820/user/atguigu/.Trash/Current/user/atguigu/input</span><br></pre></td></tr></table></figure></li>
<li>恢复回收站数据 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -mv /user/atguigu/.Trash/Current/user/atguigu/input    /user/atguigu/input</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<h2 id="3-2-Hadoop3-x新特性"><a href="#3-2-Hadoop3-x新特性" class="headerlink" title="3.2 Hadoop3.x新特性"></a>3.2 Hadoop3.x新特性</h2><h3 id="3-2-1-多NN的HA架构"><a href="#3-2-1-多NN的HA架构" class="headerlink" title="3.2.1 多NN的HA架构"></a>3.2.1 多NN的HA架构</h3><ul>
<li>HDFS NameNode高可用性的初始实现为单个活动NameNode和单个备用NameNode，将edits复制到三个JournalNode。该体系结构能够容忍系统中一个NN或一个JN的故障。但是，某些部署需要更高程度的容错能力。</li>
<li>Hadoop3.x允许用户运行多个备用NameNode。例如，通过配置三个NameNode和五个JournalNode，群集能够容忍两个节点而不是一个节点的故障。<h3 id="3-2-2-纠删码"><a href="#3-2-2-纠删码" class="headerlink" title="3.2.2 纠删码"></a>3.2.2 纠删码</h3></li>
<li>HDFS中的默认3副本方案在存储空间和其他资源（例如，网络带宽）中具有200％的开销。但是，对于I / O活动相对较低暖和冷数据集，在正常操作期间很少访问其他块副本，但仍会消耗与第一个副本相同的资源量。</li>
<li>纠删码（Erasure Coding）能够在不到50% 的数据冗余情况下提供和3副本相同的容错能力，因此，使用纠删码作为副本机制的改进是自然而然的。</li>
<li>查看集群支持的纠删码策略：hdfs ec -listPolicies</li>
</ul>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346151537039.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h1 id="四、HadoopHA高可用"><a href="#四、HadoopHA高可用" class="headerlink" title="四、HadoopHA高可用"></a>四、HadoopHA高可用</h1><h2 id="4-1-现有集群存在哪些问题？"><a href="#4-1-现有集群存在哪些问题？" class="headerlink" title="4.1 现有集群存在哪些问题？"></a>4.1 现有集群存在哪些问题？</h2><ol>
<li>HDFS集群 单个NN场景下NN如果故障了，整个HDFS集群就不可用（中心化集群） <ul>
<li>解决方案：配置多个NN !!!</li>
</ul>
</li>
<li>多个NN的场景下由哪一台对外进行服务？<ul>
<li>当HDFS实现多NN的高可用后，但是只有一台 NN 对外提供服务(Active)，其他的NN都是替补（Standby），当正在提供服务的NN宕机故障，其他的NN自动切换成Active状态</li>
</ul>
</li>
<li>HA实现后，元数据的管理策略是否发生改变？<ul>
<li>不改变，但是在HA的HDFS中合并 fsimage和edits编辑日志的合并工作交给Standby状态的NN去完成！</li>
</ul>
</li>
<li>2NN 在高可用的集群中还要不要？<ul>
<li>不要了！2NN的工作有Standby状态的NN完成！</li>
</ul>
</li>
<li>为了保证整个集群中的所有NN 能够共享元数据信息，会新增一个 JournalNode 服务，在集群中我们会启动多个JournalNode服务 形成一个集群，每个JournalNode服务对应一个NN。进行数共享！</li>
<li>JournalNode如何实现元数据的共享？<ul>
<li>在集群状态下，当一个请求对元数据进行更改的时候，此时Active状态的NN会处理请求，会往磁盘上的编辑日志edits文件追加记录，并且会通过当前机器的JournalNode服务同步edits日志文件。接下来请求也会被转发到Standby状态的NN上，Standby状态的NN接收到请求后，只去读取自己的JournalNode服务中保存的最新的编辑日志信息，加载内存中形成最新的元数据映像，保证一旦Active状态的NN宕机，Standby自己马上顶上后能够展示最新的元数据。  </li>
<li>到达checkPoint之后，Standby都会尝试进行合并Edit和Fsimage，以接收到的第一个为准</li>
</ul>
</li>
<li>当一台NN故障后，其他NN如何争抢上位？<ul>
<li>采用高可用集群中的自动故障转移机制来完成切换。</li>
</ul>
</li>
<li>自动故障转移的机制如何实现？</li>
</ol>
<h2 id="4-2-HDFS-HA工作机制"><a href="#4-2-HDFS-HA工作机制" class="headerlink" title="4.2 HDFS-HA工作机制"></a>4.2 HDFS-HA工作机制</h2><ul>
<li>通过多个NameNode消除单点故障<h3 id="4-2-1-HDFS-HA工作要点"><a href="#4-2-1-HDFS-HA工作要点" class="headerlink" title="4.2.1 HDFS-HA工作要点"></a>4.2.1 HDFS-HA工作要点</h3></li>
</ul>
<ol>
<li>元数据管理方式需要改变<ul>
<li>内存中各自保存一份元数据；</li>
<li>Edits日志只有Active状态的NameNode节点可以做写操作；</li>
<li>所有的NameNode都可以读取Edits；</li>
<li>共享的Edits放在一个共享存储中管理（qjournal和NFS两个主流实现）；</li>
</ul>
</li>
<li>需要一个状态管理功能模块<ul>
<li>实现了一个zkfailover，常驻在每一个namenode所在的节点，每一个zkfailover负责监控自己所在NameNode节点，利用zk进行状态标识，当需要进行状态切换时，由zkfailover来负责切换，切换时需要防止brain split现象的发生。</li>
</ul>
</li>
<li>必须保证两个NameNode之间能够ssh无密码登录</li>
<li>隔离（Fence），即同一时刻仅仅有一个NameNode对外提供服务</li>
</ol>
<h3 id="4-2-2-HDFS-HA自动故障转移工作机制"><a href="#4-2-2-HDFS-HA自动故障转移工作机制" class="headerlink" title="4.2.2 HDFS-HA自动故障转移工作机制"></a>4.2.2 HDFS-HA自动故障转移工作机制</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346403266012.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>自动故障转移为HDFS部署增加了两个新组件：ZooKeeper和ZKFailoverController（ZKFC）进程。</li>
<li>ZooKeeper是维护少量协调数据，通知客户端这些数据的改变和监视客户端故障的高可用服务。</li>
<li>HA的自动故障转移依赖于ZooKeeper的以下功能：<ol>
<li>故障检测<ul>
<li>集群中的每个NameNode在ZooKeeper中维护了一个会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode需要触发故障转移。</li>
</ul>
</li>
<li>现役NameNode选择<ul>
<li>ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode。</li>
</ul>
</li>
</ol>
</li>
<li>ZKFC是自动故障转移中的另一个新组件，是ZooKeeper的客户端，也监视和管理NameNode的状态。每个运行NameNode的主机也运行了一个ZKFC进程，ZKFC负责：<ol>
<li>健康监测<ul>
<li>ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。</li>
</ul>
</li>
<li>ZooKeeper会话管理<ul>
<li>当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。</li>
</ul>
</li>
<li>基于ZooKeeper的选择<ul>
<li>如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为Active。</li>
</ul>
</li>
</ol>
</li>
</ul>
<h2 id="4-3-HDFS-HA集群配置"><a href="#4-3-HDFS-HA集群配置" class="headerlink" title="4.3 HDFS-HA集群配置"></a>4.3 HDFS-HA集群配置</h2><h3 id="4-3-1-环境准备"><a href="#4-3-1-环境准备" class="headerlink" title="4.3.1 环境准备"></a>4.3.1 环境准备</h3><ul>
<li>准备三台服务器</li>
<li>JDK,Hadoop安装包</li>
<li>干净的集群</li>
<li>环境变量<h3 id="4-3-2-规划集群"><a href="#4-3-2-规划集群" class="headerlink" title="4.3.2 规划集群"></a>4.3.2 规划集群</h3><table>
<thead>
<tr>
<th>hadoop102</th>
<th>hadoop103</th>
<th>hadoop104</th>
</tr>
</thead>
<tbody><tr>
<td>NameNode</td>
<td>NameNode</td>
<td>NameNode</td>
</tr>
<tr>
<td>ZKFC</td>
<td>ZKFC</td>
<td>ZKFC</td>
</tr>
<tr>
<td>JournalNode</td>
<td>JournalNode</td>
<td>JournalNode</td>
</tr>
<tr>
<td>DataNode</td>
<td>DataNode</td>
<td>DataNode</td>
</tr>
<tr>
<td>ZK</td>
<td>ZK</td>
<td>ZK</td>
</tr>
<tr>
<td>-</td>
<td>ResourceManager</td>
<td>-</td>
</tr>
<tr>
<td>NodeManager</td>
<td>NodeManager</td>
<td>NodeManager</td>
</tr>
</tbody></table>
</li>
</ul>
<h3 id="4-3-3-配置HA集群"><a href="#4-3-3-配置HA集群" class="headerlink" title="4.3.3 配置HA集群"></a>4.3.3 配置HA集群</h3><ol>
<li>修改配置文件 core-site.xml  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 把多个NameNode的地址组装成一个集群mycluster --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- 指定hadoop数据的存储目录 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/ha/hadoop-3.1.3/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- 配置HDFS网页登录使用的静态用户为atguigu --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- 配置该atguigu(superUser)允许通过代理访问的主机节点 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 配置该atguigu(superUser)允许通过代理用户所属组 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 配置该atguigu(superUser)允许通过代理的用户--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.users<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>修改配置文件 hdfs-site.xml    <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- NameNode数据存储目录 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- DataNode数据存储目录 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- JournalNode数据存储目录 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;hadoop.tmp.dir&#125;/jn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 完全分布式HDFS集群名称 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 集群中NameNode节点都有哪些 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2,nn3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- NameNode的RPC通信地址 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:9820<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:9820<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:9820<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- NameNode的Web通信地址 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 指定NameNode元数据在JournalNode上的存放位置 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 访问代理类：client用于确定哪个NameNode为Active --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 使用隔离机制时需要ssh秘钥登录--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/atguigu/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	</span><br><span class="line">  <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>修改每一台机器的HADOOP_HOME 的环境变量打开 /etc/profile.d/set_evn.sh 修改如下： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME=/opt/module/ha/hadoop-3.1.3</span><br></pre></td></tr></table></figure></li>
<li>将/opt/ha/hadoop-3.1.3 分发到103 和 104 并且修改103和104的 HADOOP_HOME=/opt/module/ha/hadoop-3.1.3<ul>
<li>注意：修改完环境变量后一定要重新加载 profile 文件</li>
</ul>
</li>
<li>在102、103、104 各个JournalNode节点上，输入以下命令启动journalnode服务 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs --daemon start journalnode</span><br></pre></td></tr></table></figure></li>
<li>在 hadoop102的 nn1 上，对其进行格式化，并启动 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br><span class="line">hdfs --daemon start namenode</span><br></pre></td></tr></table></figure></li>
<li>分别在 hadoop103的nn2 和 hadoop104的nn3上，同步nn1的元数据信息 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure></li>
<li>分别在 hadoop103上启动nn2 和 hadoop104上启动nn3 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs --daemon start namenode</span><br></pre></td></tr></table></figure></li>
<li>通过web地址访问nn1 nn2 nn3    <ul>
<li>nn1:<a target="_blank" rel="noopener" href="http://hadoop102:9870/">http://hadoop102:9870</a></li>
<li>nn2:<a target="_blank" rel="noopener" href="http://hadoop103:9870/">http://hadoop103:9870</a></li>
<li>nn3:<a target="_blank" rel="noopener" href="http://hadoop104:9870/">http://hadoop104:9870</a></li>
</ul>
</li>
<li>在每台机器上启动DN <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">    hdfs --daemon start datanode</span><br><span class="line">    ```   </span><br><span class="line">9. 将其中的一个nn切换成Active状态</span><br><span class="line">    ```bash</span><br><span class="line">    hdfs haadmin -transitionToActive nn1</span><br></pre></td></tr></table></figure></li>
<li>查看是否Active<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs haadmin -getServiceState nn1</span><br></pre></td></tr></table></figure>
<h3 id="4-3-4-实现HA的故障自动转移"><a href="#4-3-4-实现HA的故障自动转移" class="headerlink" title="4.3.4 实现HA的故障自动转移"></a>4.3.4 实现HA的故障自动转移</h3></li>
<li>在core-site.xml文件中增加 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定zkfc要连接的zkServer地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:2181,hadoop103:2181,hadoop104:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>在hdfs-site.xml中增加 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 启用nn故障自动转移 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>修改后分发配置文件<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   xsync /opt/module/ha/hadoop-3.1.3/etc/hadoop </span><br><span class="line">   ```   </span><br><span class="line">4. 关闭HDFS集群</span><br><span class="line">   ```bash</span><br><span class="line">   stop-dfs.sh</span><br></pre></td></tr></table></figure></li>
<li>启动Zookeeper集群<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zk.sh start</span><br></pre></td></tr></table></figure></li>
<li>初始化HA在Zookeeper中状态<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs zkfc -formatZK</span><br></pre></td></tr></table></figure></li>
<li>启动HDFS服务<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure></li>
<li>可以去zkCli.sh客户端查看Namenode选举锁节点内容 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get /hadoop-ha/mycluster/ActiveStandbyElectorLock</span><br></pre></td></tr></table></figure></li>
<li>测试故障自动转移<ul>
<li>将当前状态为Active的namenode 杀死</li>
<li>刷新另外两台namenode的web端，关注状态</li>
<li>最后可以到zk中验证锁内容的名称</li>
</ul>
</li>
</ol>
<h2 id="4-4-YARN-HA配置"><a href="#4-4-YARN-HA配置" class="headerlink" title="4.4 YARN-HA配置"></a>4.4 YARN-HA配置</h2><p>YARN HA 集群搭建步骤</p>
<ol>
<li>修改yarn-site.xml <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 启用resourcemanager ha --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 声明三台resourcemanager的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster-yarn1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定resourcemanager的逻辑列表--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2,rm3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- ========== rm1的配置 ========== --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定rm1的主机名 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定rm1的web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定rm1的 RPC 通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定AM向rm1申请资源的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定供NM连接的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- ========== rm2的配置 ========== --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定rm2的主机名 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- ========== rm3的配置 ========== --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定rm3的主机名 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定zookeeper集群的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:2181,hadoop103:2181,hadoop104:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 启用自动恢复,启用自动故障转移 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定resourcemanager的状态信息存储在zookeeper集群 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>将yarn-site.xml文件进行分发<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xsync /opt/module/ha/hadoop-3.1.3/etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure></li>
<li>在任意的机器上启动yarn <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure></li>
<li>通过访问web地址验证<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346426629749.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"> </li>
<li>测试Yarn故障自动转移<ul>
<li>kill 当前active节点，会有另一个standby节点自动升级成active</li>
</ul>
</li>
</ol>
<h2 id="4-5-HDFS-Federation架构设计"><a href="#4-5-HDFS-Federation架构设计" class="headerlink" title="4.5 HDFS Federation架构设计"></a>4.5 HDFS Federation架构设计</h2><h3 id="4-5-1-NameNode架构的局限性"><a href="#4-5-1-NameNode架构的局限性" class="headerlink" title="4.5.1 NameNode架构的局限性"></a>4.5.1 NameNode架构的局限性</h3><ol>
<li>Namespace（命名空间）的限制<ul>
<li>由于NameNode在内存中存储所有的元数据（metadata），因此单个NameNode所能存储的对象（文件+块）数目受到NameNode所在JVM的heap size的限制。50G的heap能够存储20亿（200million）个对象，这20亿个对象支持4000个DataNode，12PB的存储（假设文件平均大小为40MB）。随着数据的飞速增长，存储的需求也随之增长。单个DataNode从4T增长到36T，集群的尺寸增长到8000个DataNode。存储的需求从12PB增长到大于100PB。</li>
</ul>
</li>
<li>隔离问题<ul>
<li>由于HDFS仅有一个NameNode，无法隔离各个程序，因此HDFS上的一个实验程序就很有可能影响整个HDFS上运行的程序。</li>
</ul>
</li>
<li>性能的瓶颈<ul>
<li>由于是单个NameNode的HDFS架构，因此整个HDFS文件系统的吞吐量受限于单个NameNode的吞吐量。</li>
</ul>
</li>
</ol>
<h3 id="4-5-2-HDFS-Federation架构设计"><a href="#4-5-2-HDFS-Federation架构设计" class="headerlink" title="4.5.2 HDFS Federation架构设计"></a>4.5.2 HDFS Federation架构设计</h3><p>多个NameNode集群管理不同业务线的元数据<br>| NameNode | NameNode | NameNode          |<br>| ——– | ——– | —————– |<br>| 元数据   | 元数据   | 元数据            |<br>| Log      | machine  | 电商数据/话单数据 |</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346432680052.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="4-5-3-HDFS-Federation应用思考"><a href="#4-5-3-HDFS-Federation应用思考" class="headerlink" title="4.5.3 HDFS Federation应用思考"></a>4.5.3 HDFS Federation应用思考</h3><p>不同应用可以使用不同NameNode进行数据管理图片业务、爬虫业务、日志审计业务。Hadoop生态系统中，不同的框架使用不同的NameNode进行管理NameSpace。（隔离性）<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346433814694.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="4-5-4-联邦机制原理："><a href="#4-5-4-联邦机制原理：" class="headerlink" title="4.5.4 联邦机制原理："></a>4.5.4 联邦机制原理：</h3><ol>
<li>将NameNode划分成不同的命名空间并进行编号。不同的命名空间之间相互隔离互不干扰。</li>
<li>在DataNode中创建目录，此目录对应命名空间的编号。</li>
<li>由此，编号相同的数据由对应的命名空间进行管理</li>
<li>适用场景分析 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">128G(内存空间大小) * 1024(M) * 1024(KB) * 1024(bety) / 150 = xxx（元数据的数量）</span><br><span class="line">xxx * 256M（每一个文件大小） = yyy</span><br><span class="line">yyy / 1024(G) / 1024(TB) / 1024(PB) = 200 左右PB的数据</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="问答"><a href="#问答" class="headerlink" title="问答"></a>问答</h1><h3 id="一、Hadoop中的压缩作为一种常用的优化手段，经常被用在什么场景下？"><a href="#一、Hadoop中的压缩作为一种常用的优化手段，经常被用在什么场景下？" class="headerlink" title="一、Hadoop中的压缩作为一种常用的优化手段，经常被用在什么场景下？"></a>一、Hadoop中的压缩作为一种常用的优化手段，经常被用在什么场景下？</h3><ol>
<li>降低磁盘占用</li>
<li>减少网络IO</li>
</ol>
<h3 id="二、如果想要使用压缩，Hadoop如何对某一种压缩编码格式进行取舍？"><a href="#二、如果想要使用压缩，Hadoop如何对某一种压缩编码格式进行取舍？" class="headerlink" title="二、如果想要使用压缩，Hadoop如何对某一种压缩编码格式进行取舍？"></a>二、如果想要使用压缩，Hadoop如何对某一种压缩编码格式进行取舍？</h3><ol>
<li>压缩/解压的速度，资源占用</li>
<li>压缩率</li>
<li>压缩后是否支持切片</li>
</ol>
<h3 id="三、你们公司常用的压缩方式有哪些？"><a href="#三、你们公司常用的压缩方式有哪些？" class="headerlink" title="三、你们公司常用的压缩方式有哪些？"></a>三、你们公司常用的压缩方式有哪些？</h3><ol>
<li>单文件压缩后再130M以内使用gzip，如每天的日志文件，可以支持并行处理</li>
<li>单文件压缩后大于400M考虑支持切片的lzo 或者bzip</li>
</ol>
<h3 id="四、从哪些方面定位MR执行的效率（如何分析MR执行慢的原因）"><a href="#四、从哪些方面定位MR执行的效率（如何分析MR执行慢的原因）" class="headerlink" title="四、从哪些方面定位MR执行的效率（如何分析MR执行慢的原因）"></a>四、从哪些方面定位MR执行的效率（如何分析MR执行慢的原因）</h3><ol>
<li>计算机性能<ul>
<li>cpu，内存，磁盘，网络</li>
</ul>
</li>
<li>I/O操作优化<ol>
<li>数据倾斜</li>
<li>Map和Reduce数量设置不合理</li>
<li>Map运行时间过长，导致Reduce等待过久</li>
<li>小文件过多</li>
<li>大量不可切片的超大压缩文件</li>
<li>Spill次数过多</li>
<li>Merge次数过多</li>
</ol>
</li>
</ol>
<h3 id="五、如果想对MR程序进行优化，应该从哪些方面入手以及可能用到的优化手段？"><a href="#五、如果想对MR程序进行优化，应该从哪些方面入手以及可能用到的优化手段？" class="headerlink" title="五、如果想对MR程序进行优化，应该从哪些方面入手以及可能用到的优化手段？"></a>五、如果想对MR程序进行优化，应该从哪些方面入手以及可能用到的优化手段？</h3><ol>
<li>数据输入<ol>
<li>合并小文件</li>
<li>使用CombineTextInputFormat作为输入解决小文件场景</li>
</ol>
</li>
<li>Map阶段<ol>
<li>减少Spill溢写次数，调整环形缓冲区大小和触发溢写的内存上限，减少磁盘IO</li>
<li>减少Merge合并次数：调整一次merge合并的Spill溢写文件数量，减少Merge次数</li>
<li>Map之后合理使用Combine，减少网络IO</li>
</ol>
</li>
<li>Reduce阶段<ol>
<li>合理设置Map和Reduce数量</li>
<li>设置Map和Reduce并行处理，减少Reduce等待时间</li>
<li>规避使用Reduce，减少连接数据集产生的网络IO</li>
<li>合理设置Reduce端的Buffer</li>
</ol>
</li>
<li>IO传输<ol>
<li>选择合适的压缩算法，减少网络I\O传输的数据量</li>
<li>使用sequenceFile二进制文件</li>
</ol>
</li>
<li>数据倾斜<ol>
<li>对原始数据进行抽样得到结果集来预设分区边界值</li>
<li>根据业务分析自定义分区</li>
<li>使用combiner减少数据倾斜</li>
<li>尽量使用MapJoin，避免ReduceJoin</li>
</ol>
</li>
<li>参数调优<ol>
<li>MR源相关配置：MapTask和ReduceTask使用的内存，cpu</li>
<li>Yarn资源配置：Container使用的内存，cpu</li>
<li>Shuffer配置：环形缓冲区的大小，触发溢写的内存比例</li>
<li>容错配置：任务重试次数，超时时间</li>
</ol>
</li>
</ol>
<h3 id="六、在Hadoop针对小文件的处理方案有哪些？"><a href="#六、在Hadoop针对小文件的处理方案有哪些？" class="headerlink" title="六、在Hadoop针对小文件的处理方案有哪些？"></a>六、在Hadoop针对小文件的处理方案有哪些？</h3><ol>
<li>数据采集的时候，将小文件或小批数据合并成大文件在上传HDFS，从源头上避免小文件产生</li>
<li>业务处理之前，使用MapReduce程序对HDFS上的小文件进行合并</li>
<li>使用CombineTextInputFormat处理小文件的输入</li>
<li>开启uber模式，实现jvm重用</li>
</ol>
<h3 id="七、如何解决MR中Reduce的数据倾斜问题？"><a href="#七、如何解决MR中Reduce的数据倾斜问题？" class="headerlink" title="七、如何解决MR中Reduce的数据倾斜问题？"></a>七、如何解决MR中Reduce的数据倾斜问题？</h3><ol>
<li>对原始数据进行抽样得到结果集来预设分区边界值</li>
<li>根据业务分析自定义分区</li>
<li>使用combiner减少数据倾斜</li>
<li>尽量使用MapJoin，避免ReduceJoin</li>
</ol>
<h3 id="八、大概简述一下-Hadoop每一代版本的新特性？"><a href="#八、大概简述一下-Hadoop每一代版本的新特性？" class="headerlink" title="八、大概简述一下 Hadoop每一代版本的新特性？"></a>八、大概简述一下 Hadoop每一代版本的新特性？</h3><ol>
<li>Hadoop 2.x<ul>
<li>distcp命令实现两个Hadoop集群之间的递归数据复制</li>
<li>小文件存档</li>
<li>回收站</li>
</ul>
</li>
<li>Hadoop 3.x<ul>
<li>多NN的HA架构：提高集群的可用性</li>
<li>纠删码：降低磁盘占用</li>
</ul>
</li>
</ol>
<h3 id="九、什么是Hadoop的HA"><a href="#九、什么是Hadoop的HA" class="headerlink" title="九、什么是Hadoop的HA?"></a>九、什么是Hadoop的HA?</h3><ol>
<li>集群可实现7*24小时不中断服务</li>
<li>不存在单点故障</li>
<li>可以实现故障自动转移</li>
</ol>
<h3 id="十、描述一下HDFS-HA的工作机制？"><a href="#十、描述一下HDFS-HA的工作机制？" class="headerlink" title="十、描述一下HDFS-HA的工作机制？"></a>十、描述一下HDFS-HA的工作机制？</h3><ol>
<li>多NN消除单点故障</li>
<li>由Active状态的NN负责写操作，JournalNode负责同步Edits，Standby状态的NN读取自己的Edit，加载到内存形成完整元数据</li>
<li>Standby状态的NN负责合并Edit和FsImage</li>
<li>依赖Zookeeper实现故障自动转移</li>
</ol>
<h3 id="十一、如何实现HA的集群搭建-用话术描述即可！！！"><a href="#十一、如何实现HA的集群搭建-用话术描述即可！！！" class="headerlink" title="十一、如何实现HA的集群搭建?(用话术描述即可！！！)"></a>十一、如何实现HA的集群搭建?(用话术描述即可！！！)</h3><ol>
<li>配置集群名称</li>
<li>配置集群节点</li>
<li>配置JournalNode</li>
<li>配置zookeeper连接地址</li>
</ol>
<h3 id="十二、HDFS如何实现自动故障转移？"><a href="#十二、HDFS如何实现自动故障转移？" class="headerlink" title="十二、HDFS如何实现自动故障转移？"></a>十二、HDFS如何实现自动故障转移？</h3><ol>
<li>HDFS故障自动转移依赖zookeeper和zkfc进程</li>
<li>zookeeper实现功能<ol>
<li>故障检测：集群中的每个NameNode在ZooKeeper中维护了一个会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode触发故障转移</li>
<li>现役NameNode选择：ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode</li>
</ol>
</li>
<li>zkfc进程是ZooKeeper的客户端，也监视和管理NameNode的状态。每个运行NameNode的主机也运行了一个ZKFC进程，ZKFC负责：<ol>
<li>健康监测<ul>
<li>ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。</li>
</ul>
</li>
<li>ZooKeeper会话管理<ul>
<li>当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。</li>
</ul>
</li>
<li>基于ZooKeeper的选择<ul>
<li>如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为Active。</li>
</ul>
</li>
</ol>
</li>
</ol>
<h3 id="十三、什么是脑裂问题？HDFS-HA中如何解决的脑裂问题？"><a href="#十三、什么是脑裂问题？HDFS-HA中如何解决的脑裂问题？" class="headerlink" title="十三、什么是脑裂问题？HDFS-HA中如何解决的脑裂问题？"></a>十三、什么是脑裂问题？HDFS-HA中如何解决的脑裂问题？</h3><ol>
<li>active节点的zkfc进程检测到namenode异常，会通知另一台NameNode的zkfc</li>
<li>接收通知的zkf会通过ssh在异常namenode上执行kill命令，确保异常NameNode死透</li>
<li>避免了出现两个active节点，解决了脑裂问题</li>
</ol>
<h3 id="十四、YARN-HA-实现高可用的思路"><a href="#十四、YARN-HA-实现高可用的思路" class="headerlink" title="十四、YARN-HA 实现高可用的思路"></a>十四、YARN-HA 实现高可用的思路</h3><ol>
<li>ResourceManager启动时候会向ZK的/rmstore目录写lock文件，写成功就为active，否则standby.</li>
<li>ResourceManager节点zkfc会一直监控这个lock文件是否存在，假如不存在，就为active，否则为standby.</li>
<li>zookeeper存储RMStateStore。选举active RM。</li>
<li>RMStateStore: 存储在zk的/rmstore目录下。</li>
<li>activeRM会向这个目录写APP信息</li>
<li>当activeRM挂了，另外一个standby RM通过ZKFC选举成功为active，会从/rmstore读取相应的作业信息。重新构建作业的内存信息，启动内部的服务，开始接收NM的心跳，构建集群的资源信息，并且接收客户端的作业提交请求。</li>
</ol>
<h3 id="十五、简单说一下-联邦架构-HDFS-Federation-架构设计思想。-了解"><a href="#十五、简单说一下-联邦架构-HDFS-Federation-架构设计思想。-了解" class="headerlink" title="十五、简单说一下 联邦架构(HDFS Federation) 架构设计思想。(了解)"></a>十五、简单说一下 联邦架构(HDFS Federation) 架构设计思想。(了解)</h3><ol>
<li>解决Namespace（命名空间）的限制</li>
<li>解决隔离问题</li>
<li>解决性能的瓶颈</li>
<li>将NameNode划分成不同的命名空间并进行编号。不同的命名空间之间相互隔离互不干扰。</li>
<li>在DataNode中创建目录，此目录对应命名空间的编号。</li>
<li>由此，编号相同的数据由对应的命名空间进行管理</li>
</ol>
<p>空间进行管理</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Zookeeper"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/11/07/Zookeeper/"
    >Zookeeper</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/11/07/Zookeeper/" class="article-date">
  <time datetime="2021-11-07T00:08:46.000Z" itemprop="datePublished">2021-11-07</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Zookeeper/">Zookeeper</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h1><h1 id="一、Zookeeper入门"><a href="#一、Zookeeper入门" class="headerlink" title="一、Zookeeper入门"></a>一、Zookeeper入门</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><ul>
<li>Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。</li>
<li>Zookeeper从设计模式角度来理解，是一个基于<font color ='red' >观察者模式</font>设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生了变化，Zookeeper就负责通知已经在Zookeeper上注册的那些观察者做出相应的反应.</li>
<li>Zookeeper = 文件系统 + 通知机制</li>
</ul>
<hr>
<h2 id="1-2-特点"><a href="#1-2-特点" class="headerlink" title="1.2 特点"></a>1.2 特点</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345175804501.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>Zookeeper：一个领导者（Leader），多个跟随者（Follower）组成的集群。</li>
<li>集群中只要有半数以上节点存活，Zookeeper集群就能正常服务</li>
<li>全局数据一致：每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的</li>
<li>更新请求顺序进行，来自同一个Client的更新请求按其发送顺序依次执行</li>
<li>数据更新原子性，一次数据更新要么全部成功（超集群半数节点），要么所有节点全部失败</li>
<li>实时性，在一定时间范围内，Client能读到最新数据</li>
</ul>
<hr>
<h2 id="1-3-数据结构"><a href="#1-3-数据结构" class="headerlink" title="1.3 数据结构"></a>1.3 数据结构</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345177645379.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。每一个ZNode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识。</li>
</ul>
<hr>
<h2 id="1-4-应用场景"><a href="#1-4-应用场景" class="headerlink" title="1.4 应用场景"></a>1.4 应用场景</h2><ul>
<li><p>提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等。</p>
</li>
<li><p>统一命名服务：在分布式环境下，经常需要对应用/服务进行统一命名，便于识别。<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345532077391.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>统一配置管理<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345533954441.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>分布式环境下，配置文件同步非常常见，配置管理可交由ZooKeeper实现</li>
<li>一般要求一个集群中，所有节点的配置信息是一致的，比如 Kafka 集群</li>
<li>对配置文件修改后，希望能够快速同步到各个节点上</li>
<li>将配置信息写入ZooKeeper上的一个Znode</li>
<li>各个客户端服务器监听这个Znode</li>
<li>Znode中的数据被修改，ZooKeeper将通知各个客户端服务器</li>
</ol>
</li>
<li><p>统一集群管理<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345535437380.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>分布式环境中，实时掌握每个节点的状态是必要的。 <ol>
<li>可根据节点实时状态做出一些调整。 </li>
</ol>
</li>
<li>ZooKeeper可以实现实时监控节点状态变化<ol>
<li>可将节点信息写入ZooKeeper上的一个ZNode</li>
<li>监听这个ZNode可获取它的实时状态变化。</li>
</ol>
</li>
</ol>
</li>
<li><p>服务器动态上下线<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345535923783.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>软负载均衡：在Zookeeper中记录每台服务器的访问数，让访问数最少的服务器去处理最新的客户端请求<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345536248922.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
</ul>
<hr>
<h1 id="二、Zookeeper-安装"><a href="#二、Zookeeper-安装" class="headerlink" title="二、Zookeeper 安装"></a>二、Zookeeper 安装</h1><h2 id="2-1-安装ZK"><a href="#2-1-安装ZK" class="headerlink" title="2.1 安装ZK:"></a>2.1 安装ZK:</h2><ol>
<li>把软件包上传的Linux的 /opt/software 下</li>
<li>加压ZK到 /opt/module 下</li>
<li>将加压后的目录名称修改一下（选做）</li>
<li>将zk的安装目录下 conf/zoo_sample.cfg 文件改名为 zoo.cfg</li>
<li>在ZK的安装目录下创建一个新的目录，作为zk的数据持久化目录</li>
<li>修改zoo.cfg配置文件<code>dataDir=/opt/module/zookeeper-3.5.7/zkData</code></li>
<li>配置ZK的环境变量 （选做）</li>
</ol>
<h2 id="2-2-单点模式的简单操作"><a href="#2-2-单点模式的简单操作" class="headerlink" title="2.2 单点模式的简单操作"></a>2.2 单点模式的简单操作</h2><ol>
<li>启停zk服务端 和 zk客户端<ul>
<li>启停 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">    [atguigu@hadoop001 zookeeper-3.5.7]$ bin/zkServer.sh start</span><br><span class="line">    ZooKeeper JMX enabled by default</span><br><span class="line">    Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">    Starting zookeeper ... STARTED</span><br><span class="line">    [atguigu@hadoop001 zookeeper-3.5.7]$ jps</span><br><span class="line">    1456 NameNode</span><br><span class="line">    8225 QuorumPeerMain</span><br><span class="line">    1619 DataNode</span><br><span class="line">    2076 JobHistoryServer</span><br><span class="line">    1901 NodeManager</span><br><span class="line">    8269 Jps</span><br><span class="line">    [atguigu@hadoop001 zookeeper-3.5.7]$ bin/zkServer.sh stop</span><br><span class="line">    ZooKeeper JMX enabled by default</span><br><span class="line">    Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">    Stopping zookeeper ... STOPPED</span><br><span class="line">    [atguigu@hadoop001 zookeeper-3.5.7]$ jps</span><br><span class="line">    1456 NameNode</span><br><span class="line">    1619 DataNode</span><br><span class="line">    2076 JobHistoryServer</span><br><span class="line">    1901 NodeManager</span><br><span class="line">    8302 Jps</span><br><span class="line">    [atguigu@hadoop001 zookeeper-3.5.7]$</span><br><span class="line">    ```   </span><br><span class="line">- 客户端连接</span><br><span class="line">    ```bash</span><br><span class="line">    [atguigu@hadoop001 zookeeper-3.5.7]$ bin/zkCli.sh -server hadoop001:2181</span><br><span class="line">    Connecting to hadoop001:2181</span><br><span class="line">    ········</span><br><span class="line"></span><br><span class="line">    2021-10-18 16:36:10,536 [myid:hadoop001:2181] - INFO  [main-SendThread(hadoop001:2181):ClientCnxn<span class="variable">$SendThread</span>@1394] - Session establishment complete on server hadoop001/192.168.2.6:2181, sessionid = 0x10000a6d44b0000, negotiated timeout = 30000</span><br><span class="line">    </span><br><span class="line">    WATCHER::</span><br><span class="line">    </span><br><span class="line">    WatchedEvent state:SyncConnected <span class="built_in">type</span>:None path:null</span><br><span class="line">    [zk: hadoop001:2181(CONNECTED) 0]</span><br></pre></td></tr></table></figure>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ jps</span><br><span class="line">1456 NameNode</span><br><span class="line">1619 DataNode</span><br><span class="line">8483 ZooKeeperMain</span><br><span class="line">8532 Jps</span><br><span class="line">8378 QuorumPeerMain</span><br><span class="line">2076 JobHistoryServer</span><br><span class="line">1901 NodeManager</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>查看一下zk的服务端和客户端对应的进程 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ jps</span><br><span class="line">1456 NameNode</span><br><span class="line">1619 DataNode</span><br><span class="line">8483 ZooKeeperMain      <span class="comment">#客户端</span></span><br><span class="line">8532 Jps</span><br><span class="line">8378 QuorumPeerMain     <span class="comment">#服务端</span></span><br><span class="line">2076 JobHistoryServer</span><br><span class="line">1901 NodeManager</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
<li>退出客户端 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 0] quit</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:Closed <span class="built_in">type</span>:None path:null</span><br><span class="line">2021-10-18 16:41:05,769 [myid:] - INFO  [main:ZooKeeper@1422] - Session: 0x10000a6d44b0001 closed</span><br><span class="line">2021-10-18 16:41:05,769 [myid:] - INFO  [main-EventThread:ClientCnxn<span class="variable">$EventThread</span>@524] - EventThread shut down <span class="keyword">for</span> session: 0x10000a6d44b0001</span><br><span class="line">[atguigu@hadoop001 zookeeper-3.5.7]$ jps</span><br><span class="line">1456 NameNode</span><br><span class="line">1619 DataNode</span><br><span class="line">8552 Jps</span><br><span class="line">8378 QuorumPeerMain</span><br><span class="line">2076 JobHistoryServer</span><br><span class="line">1901 NodeManager</span><br><span class="line">[atguigu@hadoop001 zookeeper-3.5.7]$</span><br></pre></td></tr></table></figure></li>
<li>当客户端启动连接后不能单独关闭，当服务端关闭后，客户端也就消失了。</li>
</ol>
<h2 id="2-2-配置参数解读"><a href="#2-2-配置参数解读" class="headerlink" title="2.2 配置参数解读"></a>2.2 配置参数解读</h2><p>Zookeeper中的配置文件zoo.cfg中参数含义解读如下：</p>
<ol>
<li>tickTime =2000：通信心跳数，Zookeeper服务器与客户端心跳时间，单位毫秒<ul>
<li>Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。</li>
<li>它用于心跳检测机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime)</li>
</ul>
</li>
<li>initLimit =10：LF初始通信时限<ul>
<li>集群中的Follower跟随者服务器与Leader领导者服务器之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。</li>
</ul>
</li>
<li>syncLimit =5：LF同步通信时限<ul>
<li>集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。</li>
</ul>
</li>
<li>dataDir：数据文件目录+数据持久化路径<ul>
<li>用于保存Zookeeper中的数据。</li>
</ul>
</li>
<li>clientPort=2181：客户端连接端口<ul>
<li>监听客户端连接的端口。</li>
</ul>
</li>
</ol>
<h1 id="三、Zookeeper-实战"><a href="#三、Zookeeper-实战" class="headerlink" title="三、Zookeeper 实战"></a>三、Zookeeper 实战</h1><h2 id="3-1-搭建ZK的集群"><a href="#3-1-搭建ZK的集群" class="headerlink" title="3.1 搭建ZK的集群"></a>3.1 搭建ZK的集群</h2><ol>
<li>集群规划：在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper </li>
<li>每个节点执行单节点部署的步骤</li>
<li>修改zoo.cfg 配置文件 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 数据存储路径</span><br><span class="line">dataDir=/opt/module/zookeeper-3.5.7/zkData</span><br><span class="line"></span><br><span class="line">#集群节点配置</span><br><span class="line">server.2=hadoop102:2888:3888</span><br><span class="line">server.3=hadoop103:2888:3888</span><br><span class="line">server.4=hadoop104:2888:3888</span><br></pre></td></tr></table></figure></li>
<li>zkData目录下创建myid的文件对应上述配置编号</li>
<li>配置参数解读<ul>
<li>server.A=B:C:D</li>
<li>A是一个数字，表示这个是第几号服务器，对应zkData/myid中的编号；Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server</li>
<li>B是Zookeeper节点的地址（域名/IP）</li>
<li>C是这个服务器Follower与集群中的Leader服务器交换信息的端口</li>
<li>D是集群中的Leader服务器挂掉之后，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口</li>
</ul>
</li>
<li>编写启动/停止Zookeeper集群脚本 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#检验参数</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$#</span> -lt 1 ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&#x27;参数不能为空！！！&#x27;</span></span><br><span class="line">    <span class="built_in">exit</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#循环遍历每一台机器，分别启动或者停止ZK服务</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> host <span class="keyword">in</span> hadoop001 hadoop002 hadoop003 hadoop004 hadoop005; <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line">    <span class="string">&quot;start&quot;</span>)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;*****************<span class="variable">$1</span> <span class="variable">$host</span> zookeeper****************&quot;</span></span><br><span class="line">        ssh <span class="variable">$host</span> /opt/module/zookeeper-3.5.7/bin/zkServer.sh <span class="variable">$1</span></span><br><span class="line">        ;;</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;stop&quot;</span>)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;*****************<span class="variable">$1</span> <span class="variable">$host</span> zookeeper****************&quot;</span></span><br><span class="line">        ssh <span class="variable">$host</span> /opt/module/zookeeper-3.5.7/bin/zkServer.sh <span class="variable">$1</span></span><br><span class="line">        ;;</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;status&quot;</span>)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;*****************<span class="variable">$1</span> <span class="variable">$host</span> zookeeper****************&quot;</span></span><br><span class="line">        ssh <span class="variable">$host</span> /opt/module/zookeeper-3.5.7/bin/zkServer.sh <span class="variable">$1</span></span><br><span class="line">        ;;</span><br><span class="line"></span><br><span class="line">    *)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&#x27;参数有误！！！&#x27;</span></span><br><span class="line">        <span class="built_in">exit</span></span><br><span class="line">        ;;</span><br><span class="line">    <span class="keyword">esac</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></li>
<li>执行脚本 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 bin]$ zookeeper_cluster.sh start</span><br><span class="line">*****************start hadoop001 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">*****************start hadoop002 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">*****************start hadoop003 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">[atguigu@hadoop001 bin]$ zookeeper_cluster.sh status</span><br><span class="line">*****************status hadoop001 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: follower</span><br><span class="line">*****************status hadoop002 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: follower</span><br><span class="line">*****************status hadoop003 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: leader</span><br><span class="line">[atguigu@hadoop001 bin]$ zookeeper_cluster.sh stop</span><br><span class="line">*****************stop hadoop001 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Stopping zookeeper ... STOPPED</span><br><span class="line">*****************stop hadoop002 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Stopping zookeeper ... STOPPED</span><br><span class="line">*****************stop hadoop003 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Stopping zookeeper ... STOPPED</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="3-2-客户端命令行操作"><a href="#3-2-客户端命令行操作" class="headerlink" title="3.2 客户端命令行操作"></a>3.2 客户端命令行操作</h2><h2 id="3-2-1-基本命令"><a href="#3-2-1-基本命令" class="headerlink" title="3.2.1 基本命令"></a>3.2.1 基本命令</h2><table>
<thead>
<tr>
<th>命令基本语法</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td>help</td>
<td>显示所有操作命令</td>
</tr>
<tr>
<td>ls path</td>
<td>使用 ls 命令来查看当前znode的子节点 <br> -w  监听子节点变化 <br>-s   附加次级信息</td>
</tr>
<tr>
<td>create</td>
<td>普通创建 <br>-s  含有序列 <br>-e  临时（重启或者超时消失）</td>
</tr>
<tr>
<td>get path</td>
<td>获得节点的值<br>-w  监听节点内容变化<br>-s   附加次级信息</td>
</tr>
<tr>
<td>set</td>
<td>设置节点的具体值</td>
</tr>
<tr>
<td>stat</td>
<td>查看节点状态</td>
</tr>
<tr>
<td>delete</td>
<td>删除节点</td>
</tr>
<tr>
<td>deleteall</td>
<td>递归删除节点</td>
</tr>
</tbody></table>
<h2 id="3-2-2-具体操作"><a href="#3-2-2-具体操作" class="headerlink" title="3.2.2 具体操作"></a>3.2.2 具体操作</h2><ol>
<li><p>启动客户端</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 zookeeper-3.5.7]$ bin/zkCli.sh -server hadoop001:2181</span><br><span class="line">Connecting to hadoop001:2181</span><br><span class="line">·······</span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:None path:null</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 0]</span><br></pre></td></tr></table></figure></li>
<li><p>显示所有操作命令</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 0] <span class="built_in">help</span></span><br><span class="line">ZooKeeper -server host:port cmd args</span><br><span class="line">	addauth scheme auth</span><br><span class="line">	close</span><br><span class="line">	config [-c] [-w] [-s]</span><br><span class="line">	connect host:port</span><br><span class="line">	create [-s] [-e] [-c] [-t ttl] path [data] [acl]</span><br><span class="line">	delete [-v version] path</span><br><span class="line">	deleteall path</span><br><span class="line">	delquota [-n|-b] path</span><br><span class="line">	get [-s] [-w] path</span><br><span class="line">	getAcl [-s] path</span><br><span class="line">	<span class="built_in">history</span></span><br><span class="line">	listquota path</span><br><span class="line">	ls [-s] [-w] [-R] path</span><br><span class="line">	ls2 path [watch]</span><br><span class="line">	printwatches on|off</span><br><span class="line">	quit</span><br><span class="line">	reconfig [-s] [-v version] [[-file path] | [-members serverID=host:port1:port2;port3[,...]*]] | [-add serverId=host:port1:port2;port3[,...]]* [-remove serverId[,...]*]</span><br><span class="line">	redo cmdno</span><br><span class="line">	removewatches path [-c|-d|-a] [-l]</span><br><span class="line">	rmr path</span><br><span class="line">	<span class="built_in">set</span> [-s] [-v version] path data</span><br><span class="line">	setAcl [-s] [-v version] [-R] path acl</span><br><span class="line">	setquota -n|-b val path</span><br><span class="line">	<span class="built_in">stat</span> [-w] path</span><br><span class="line">	sync path</span><br></pre></td></tr></table></figure></li>
<li><p>查看当前znode中所包含的内容</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 2] ls /</span><br><span class="line">[zookeeper]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 3]</span><br></pre></td></tr></table></figure></li>
<li><p>查看当前节点详细数据</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 3] ls -s /</span><br><span class="line">[zookeeper]cZxid = 0x0</span><br><span class="line">ctime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">mZxid = 0x0</span><br><span class="line">mtime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">pZxid = 0x100000011</span><br><span class="line">cversion = 1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 0</span><br><span class="line">numChildren = 1</span><br><span class="line"></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 4]</span><br></pre></td></tr></table></figure></li>
<li><p>分别创建2个普通节点</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 4] create /sanguo <span class="string">&quot;diaochan&quot;</span></span><br><span class="line">Created /sanguo</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 6] create /sanguo/shuguo <span class="string">&quot;liubei&quot;</span></span><br><span class="line">Created /sanguo/shuguo</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 8] ls  /</span><br><span class="line">[sanguo, zookeeper]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 9] ls  /sanguo</span><br><span class="line">[shuguo]</span><br></pre></td></tr></table></figure></li>
<li><p>获得节点的值</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 10] get /sanguo</span><br><span class="line">diaochan</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 11] get -s /sanguo</span><br><span class="line">diaochan</span><br><span class="line">cZxid = 0x300000004</span><br><span class="line">ctime = Mon Oct 18 17:31:28 CST 2021</span><br><span class="line">mZxid = 0x300000004</span><br><span class="line">mtime = Mon Oct 18 17:31:28 CST 2021</span><br><span class="line">pZxid = 0x300000006</span><br><span class="line">cversion = 1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 8</span><br><span class="line">numChildren = 1</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 12] get -s /sanguo/shuguo</span><br><span class="line">liubei</span><br><span class="line">cZxid = 0x300000006</span><br><span class="line">ctime = Mon Oct 18 17:32:12 CST 2021</span><br><span class="line">mZxid = 0x300000006</span><br><span class="line">mtime = Mon Oct 18 17:32:12 CST 2021</span><br><span class="line">pZxid = 0x300000006</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 6</span><br><span class="line">numChildren = 0</span><br></pre></td></tr></table></figure></li>
<li><p>创建临时节点</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建临时节点</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 13] create -e /sanguo/wuguo <span class="string">&quot;zhouyu&quot;</span></span><br><span class="line">Created /sanguo/wuguo</span><br><span class="line"><span class="comment"># 查看节点存在</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 15] ls /sanguo</span><br><span class="line">[shuguo, wuguo]</span><br><span class="line"><span class="comment"># 退出客户端</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 16] quit</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:Closed <span class="built_in">type</span>:None path:null</span><br><span class="line">2021-10-18 17:35:37,751 [myid:] - INFO  [main:ZooKeeper@1422] - Session: 0x10000c30dbf0001 closed</span><br><span class="line">2021-10-18 17:35:37,752 [myid:] - INFO  [main-EventThread:ClientCnxn<span class="variable">$EventThread</span>@524] - EventThread shut down <span class="keyword">for</span> session: 0x10000c30dbf0001</span><br><span class="line"><span class="comment"># 重新连接客户端</span></span><br><span class="line">[atguigu@hadoop001 zookeeper-3.5.7]$ bin/zkCli.sh -server hadoop001:2181</span><br><span class="line">Connecting to hadoop001:2181</span><br><span class="line">···</span><br><span class="line">···</span><br><span class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:None path:null</span><br><span class="line"><span class="comment"># 再次查看节点 发现临时节点被删除</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 0] ls /sanguo</span><br><span class="line">[shuguo]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 1]</span><br></pre></td></tr></table></figure></li>
<li><p>创建带序号的节点</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 6] ls /sanguo/weiguo</span><br><span class="line">[]</span><br><span class="line"><span class="comment"># 创建普通节点xiaobing</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 7] create /sanguo/weiguo/xiaobing</span><br><span class="line">Created /sanguo/weiguo/xiaobing</span><br><span class="line"><span class="comment"># 再次创建普通节点xiaobing--Node already exists</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 8] create /sanguo/weiguo/xiaobing</span><br><span class="line">Node already exists: /sanguo/weiguo/xiaobing</span><br><span class="line"><span class="comment"># 创建普通节点xiaobing1</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 9] create /sanguo/weiguo/xiaobing1</span><br><span class="line">Created /sanguo/weiguo/xiaobing1</span><br><span class="line"><span class="comment"># 创建带序号的节点xiaobing</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 10] create -s /sanguo/weiguo/xiaobing</span><br><span class="line">Created /sanguo/weiguo/xiaobing0000000002</span><br><span class="line"><span class="comment"># 创建带序号的节点xiaobing</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 11] create -s /sanguo/weiguo/xiaobing</span><br><span class="line">Created /sanguo/weiguo/xiaobing0000000003</span><br><span class="line"><span class="comment"># 创建带序号的节点xiaobing</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 12] create -s /sanguo/weiguo/xiaobing</span><br><span class="line">Created /sanguo/weiguo/xiaobing0000000004</span><br><span class="line"><span class="comment"># 查看节点，如果节点下原来没有子节点，序号从0开始依次递增。如果原节点下已有2个节点，则再排序时从2开始，以此类推</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 13] ls /sanguo/weiguo</span><br><span class="line">[xiaobing, xiaobing0000000002, xiaobing0000000003, xiaobing0000000004, xiaobing1]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 14]</span><br></pre></td></tr></table></figure></li>
<li><p>修改节点数据值</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 0] get /sanguo/weiguo</span><br><span class="line">caocao</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 1] <span class="built_in">set</span> /sanguo/weiguo <span class="string">&#x27;caopi&#x27;</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 2] get /sanguo/weiguo</span><br><span class="line">caopi</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 3]</span><br></pre></td></tr></table></figure></li>
<li><p>节点的值变化监听</p>
<ol>
<li>在hadoop002主机上注册监听/sanguo节点数据变化 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[zk: hadoop002:2181(CONNECTED) 0] get -w /sanguo</span><br><span class="line">null</span><br><span class="line">[zk: hadoop002:2181(CONNECTED) 1]</span><br></pre></td></tr></table></figure></li>
<li>在hadoop001主机上修改/sanguo节点的数据 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 5] <span class="built_in">set</span> /sanguo <span class="string">&quot;xishi&quot;</span></span><br></pre></td></tr></table></figure></li>
<li>观察hadoop002主机收到数据变化的监听 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:NodeDataChanged path:/sanguo</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>节点的子节点变化监听路径变化.</p>
<ol>
<li>在hadoop002主机上注册监听/sanguo节点的子节点变化 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: hadoop002:2181(CONNECTED) 0] ls -w /sanguo</span><br><span class="line">[shuguo, weiguo, wuguo]</span><br></pre></td></tr></table></figure></li>
<li>在hadoop001主机/sanguo节点上创建子节点 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 4] create /sanguo/jin</span><br><span class="line">Created /sanguo/jin</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 5]</span><br></pre></td></tr></table></figure></li>
<li>观察hadoop002主机收到子节点变化的监听 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zk: hadoop002:2181(CONNECTED) 1]</span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:NodeChildrenChanged path:/sanguo</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>删除节点</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 7] ls /sanguo</span><br><span class="line">[jin, shuguo, weiguo, wuguo]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 8] delete /sanguo/jin</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 9] ls /sanguo</span><br><span class="line">[shuguo, weiguo, wuguo]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 10]</span><br></pre></td></tr></table></figure></li>
<li><p>递归删除节点</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 12] ls /sanguo/weiguo</span><br><span class="line">[xiaobing, xiaobing0000000002, xiaobing0000000003, xiaobing0000000004, xiaobing1]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 13] deleteall /sanguo/weiguo</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 14] ls /sanguo/weiguo</span><br><span class="line">Node does not exist: /sanguo/weiguo</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 15]</span><br></pre></td></tr></table></figure></li>
<li><p>查看节点状态</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 15] <span class="built_in">stat</span> /sanguo</span><br><span class="line">cZxid = 0x300000016</span><br><span class="line">ctime = Mon Oct 18 17:41:54 CST 2021</span><br><span class="line">mZxid = 0x300000028</span><br><span class="line">mtime = Mon Oct 18 17:58:53 CST 2021</span><br><span class="line">pZxid = 0x300000030</span><br><span class="line">cversion = 6</span><br><span class="line">dataVersion = 1</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 5</span><br><span class="line">numChildren = 2</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 16]</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="四、Zookeeper-原理"><a href="#四、Zookeeper-原理" class="headerlink" title="四、Zookeeper 原理"></a>四、Zookeeper 原理</h1><h2 id="4-1-节点类型"><a href="#4-1-节点类型" class="headerlink" title="4.1 节点类型"></a>4.1 节点类型</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345542078737.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>持久（Persistent）：客户端和服务器端断开连接后，创建的节点不删除</li>
<li>短暂（Ephemeral）：客户端和服务器端断开连接后，创建的节点自己删除</li>
<li>在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序</li>
<li>创建znode时设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护</li>
</ul>
<h2 id="4-2-Stat结构体"><a href="#4-2-Stat结构体" class="headerlink" title="4.2 Stat结构体"></a>4.2 Stat结构体</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 15] <span class="built_in">stat</span> /sanguo</span><br><span class="line"><span class="comment"># czxid-创建节点的事务zxid</span></span><br><span class="line"><span class="comment">#   每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。</span></span><br><span class="line"><span class="comment">#   事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。</span></span><br><span class="line">cZxid = 0x300000016</span><br><span class="line"><span class="comment"># ctime - znode被创建的毫秒数(从1970年开始)</span></span><br><span class="line">ctime = Mon Oct 18 17:41:54 CST 2021</span><br><span class="line"><span class="comment"># mzxid - znode最后更新的事务zxid</span></span><br><span class="line">mZxid = 0x300000028</span><br><span class="line"><span class="comment"># mtime - znode最后修改的毫秒数(从1970年开始)</span></span><br><span class="line">mtime = Mon Oct 18 17:58:53 CST 2021</span><br><span class="line"><span class="comment"># pZxid - znode最后更新的子节点zxid</span></span><br><span class="line">pZxid = 0x300000030</span><br><span class="line"><span class="comment"># cversion - znode子节点变化号，znode子节点修改次数</span></span><br><span class="line">cversion = 6</span><br><span class="line"><span class="comment"># dataversion - znode数据变化号</span></span><br><span class="line">dataVersion = 1</span><br><span class="line"><span class="comment"># aclVersion - znode访问控制列表的变化号</span></span><br><span class="line">aclVersion = 0</span><br><span class="line"><span class="comment"># ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。</span></span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line"><span class="comment"># dataLength - znode的数据长度</span></span><br><span class="line">dataLength = 5</span><br><span class="line"><span class="comment"># numChildren - znode子节点数量</span></span><br><span class="line">numChildren = 2</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 16]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="4-3-监听器原理"><a href="#4-3-监听器原理" class="headerlink" title="4.3 监听器原理"></a>4.3 监听器原理</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345549940089.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="4-3-1-监听原理详解"><a href="#4-3-1-监听原理详解" class="headerlink" title="4.3.1 监听原理详解"></a>4.3.1 监听原理详解</h3><ol>
<li>首先要有一个main()线程</li>
<li>在main线程中创建Zookeeper客户端，这时就会创建两个线程，一个负责网络连接通信（connet），一个负责监听（listener）。 </li>
<li>通过connect线程将注册的监听事件发送给Zookeeper。 </li>
<li>在Zookeeper的注册监听器列表中将注册的监听事件添加到列表中。 </li>
<li>Zookeeper监听到有数据或路径变化，就会将这个消息发送给listener线程。 </li>
<li>listener线程内部调用了process()方法。 <h3 id="4-3-2-常见的监听"><a href="#4-3-2-常见的监听" class="headerlink" title="4.3.2 常见的监听"></a>4.3.2 常见的监听</h3></li>
<li>监听节点数据的变化<ul>
<li><code>get path [watch]</code></li>
</ul>
</li>
<li>监听子节点增减的变化<ul>
<li><code>ls path [watch]</code></li>
</ul>
</li>
</ol>
<h2 id="4-4-选举机制"><a href="#4-4-选举机制" class="headerlink" title="4.4 选举机制"></a>4.4 选举机制</h2><ol>
<li>半数机制：集群中半数以上机器存活，集群可用。所以<font color ='red' >Zookeeper适合安装奇数台服务器</font>。</li>
<li>Zookeeper虽然在配置文件中并没有指定Master和Slave。但是，Zookeeper工作时，是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的。</li>
<li>以一个简单的例子来说明整个选举的过程。<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345388234410.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><ul>
<li>假设有五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么。</li>
<li>Zookeeper的选举机制<ol>
<li>服务器1启动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上（3票），选举无法完成，服务器1状态保持为LOCKING；</li>
<li>服务器2启动，再发起一次选举。服务器1和2分别投自己一票并交换选票信息：此时服务器1发现服务器2的ID比自己目前投票推举的（服务器1）大，更改选票为推举服务器2。此时服务器1票数0票，服务器2票数2票，没有半数以上结果，选举无法完成，服务器1，2状态保持LOCKING</li>
<li>服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING；</li>
<li>服务器4启动，发起一次选举。此时服务器1，2，3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为FOLLOWING；</li>
<li>服务器5启动，同4一样当小弟。</li>
</ol>
</li>
<li>举例1<ul>
<li>场景：以5台机器为例，集群的机器顺时启动，当前集群中没有任何数据。<ol>
<li>server1 启动，首先server1给自己投一票，然后看当前票数是否超过半数，结果没有超过，这时候leader就没选出来，当前选举状态是Locking状态。</li>
<li>server2 启动，首先server2先给自己投一票，因为当前集群已经有两台机器已启动，所以server1</li>
<li>server2会交换选票，交换后发现各自有一票，接下来比较 myid 发现server2的myid值 &gt; server1的myid值此时server2胜出，最后server2有两票。最后再看当前票数是否半，发现未过半，集群的选举状态集训保持locking状态。</li>
<li>server3启动， 首先自己投自己一票，server1和server2也会投自己一票，然后交换选票发现都一样，接着比较myid 最后server3胜出，此时server3就有3票，同时server3的票数超过半数。所以server3成为leader。</li>
<li>server4启动，发现当前集群已经有leader 它自己自动成为follower</li>
<li>server5启动，发现当前集群已经有leader 它自己自动成为follower</li>
</ol>
</li>
</ul>
</li>
<li>举例说明2<ul>
<li>场景：以5台机器为例，当前集群正在使用（有数据/没数据），leader突然宕机的情况。</li>
<li>当集群中的leader挂掉，集群会重新选出一个leader，此时首先会比较每一台机器的mzxid,mzxid最大的被选为leader。极端情况，mzxid都相等的情况，那么就会直接比较myid。</li>
</ul>
</li>
<li>举例说明3<ul>
<li>场景：集群中有数据，重启的时候Leader该如何选？</li>
<li>和场景一选举机制是一样！</li>
</ul>
</li>
</ul>
</li>
<li>一般情况下ZK集群更推荐使用奇数台机器原因？<ul>
<li>在ZK集群中 奇数台 和 偶数台（接近的台数） 机器的容错能力是一样的，所以在考虑资源节省的情况我们推荐使用奇数台方案</li>
</ul>
</li>
</ol>
<h2 id="4-5-写数据流程"><a href="#4-5-写数据流程" class="headerlink" title="4.5 写数据流程"></a>4.5 写数据流程</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345398429599.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>客户端会向ZK集群中的一台机器server1发送写数据的请求。</li>
<li>server1接收到请求后，马上会通知leader 有写数据的请求来了</li>
<li>leader拿到请求后，进行广播，让集群每一台机器都准备要写数据</li>
<li>集群中的所有机机器接收到leader广播后都回应一下leader</li>
<li>leader接收机器数过半的机器回应后，再次进行广播 开始写数据，<br>其他机器接收到广播后也开始写数据</li>
<li>数据成功写入后，回应leader，最后由leader来做整个事务提交</li>
<li>当数据成功写入后，由最初和客户端发生连接的 server1 回应客户端数据写入成功。</li>
</ol>
<h1 id="五、Zookeeper-面试真题"><a href="#五、Zookeeper-面试真题" class="headerlink" title="五、Zookeeper 面试真题"></a>五、Zookeeper 面试真题</h1><h2 id="5-1-请简述ZooKeeper的选举机制"><a href="#5-1-请简述ZooKeeper的选举机制" class="headerlink" title="5.1 请简述ZooKeeper的选举机制"></a>5.1 请简述ZooKeeper的选举机制</h2><h2 id="5-2-ZooKeeper的监听原理是什么？"><a href="#5-2-ZooKeeper的监听原理是什么？" class="headerlink" title="5.2 ZooKeeper的监听原理是什么？"></a>5.2 ZooKeeper的监听原理是什么？</h2><h2 id="5-3-ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？"><a href="#5-3-ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？" class="headerlink" title="5.3 ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？"></a>5.3 ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？</h2><ol>
<li>部署方式单机模式、集群模式</li>
<li>角色：Leader和Follower</li>
<li>集群最少需要机器数：3</li>
</ol>
<h2 id="5-4-ZooKeeper的常用命令Keeper的常用命令"><a href="#5-4-ZooKeeper的常用命令Keeper的常用命令" class="headerlink" title="5.4 ZooKeeper的常用命令Keeper的常用命令"></a>5.4 ZooKeeper的常用命令Keeper的常用命令</h2> 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Zookeeper/" rel="tag">Zookeeper</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Yarn"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/11/07/Yarn/"
    >Yarn</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/11/07/Yarn/" class="article-date">
  <time datetime="2021-11-07T00:08:44.000Z" itemprop="datePublished">2021-11-07</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Yarn/">Yarn</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h1><h1 id="一、Yarn资源调度器"><a href="#一、Yarn资源调度器" class="headerlink" title="一、Yarn资源调度器"></a>一、Yarn资源调度器</h1><ul>
<li>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个<font color ='red' >分布式操作系统平台</font>，而MapReduce等运算程序则相当于<font color ='red' >运行于操作系统之上的应用程序</font>。<h2 id="1-1-Yarn基本架构"><a href="#1-1-Yarn基本架构" class="headerlink" title="1.1 Yarn基本架构"></a>1.1 Yarn基本架构</h2></li>
<li>YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16344718758198.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
<h2 id="1-2-Yarn工作机制"><a href="#1-2-Yarn工作机制" class="headerlink" title="1.2 Yarn工作机制"></a>1.2 Yarn工作机制</h2><ol>
<li>MR程序在客户端通过job.submit()方法提交任务到本地或者远程hadoop集群，创建YranRunner</li>
<li>YarnRunner向ResourceManager申请一个Application</li>
<li>ResourceManager将程序运行的资源路径(资源提交路径及application_id)返回给Yarnrunner</li>
<li>程序将运行所需资源提交到hdfs上（jar包，配置文件，split信息）</li>
<li>程序资源提交之后申请运行MRAppMaster</li>
<li>ResourceManager将用户请求初始化为一个Task，该task会被放到任务队列中，等待调度器分配资源</li>
<li>NodeManager领取task任务</li>
<li>该NodeManager创建container，并启动MRAppMaster</li>
<li>container从hdfs拷贝资源到本地</li>
<li>MRAppMaster向RM申请运行MapTask资源</li>
<li>ResourceManager将MapTask任务分配给NodeManager，领取到任务的NodeManager创建容器</li>
<li>MRAppMaster向接收到任务的NodeManager发送启动程序脚本，NodeManeger启动MapTask,执行map逻辑，输出分区排序后的结果</li>
<li>MRAppMaster等待所有MapTak运行结束，或者指定数量的MapTask运行结束后，向ResourceManager申请容器，运行ReduceTask.</li>
<li>ReduceTask从MapTask输出中取对应分区的数据，执行reduce逻辑</li>
<li>程序运行结束，MRAppMaster会向ResourceManager申请注销自己<h2 id="1-3-作业提交过程"><a href="#1-3-作业提交过程" class="headerlink" title="1.3 作业提交过程"></a>1.3 作业提交过程</h2><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16344721580647.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16344723700779.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>作业提交阶段<ol>
<li>client调用job.waitForCompletion()方法，提交任务到集群</li>
<li>client想ResourceManager申请一个作业id</li>
<li>ResourceManager返回改job资源的提交路径和作业id</li>
<li>client提交jar，切片信息，配置文件到指定的资源提交路径</li>
<li>client提交资源后，向ResourceManager申请运行MRAppMaster</li>
</ol>
</li>
<li>作业初始化<ol>
<li>ResourceManager收到请求后，将job添加到任务队列</li>
<li>某一个空闲的NodeManager领取到任务后，创建Container，运行MRAppMaster</li>
<li>下载client提交的资源文件到本地</li>
</ol>
</li>
<li>任务分配<ol>
<li>MrAppMaster向ResourceManager申请MapTask运行资源</li>
<li>ResourceManager将MapTask分配给其他NodeManager</li>
<li>领取到任务的NodeManager创建MapTask容器</li>
</ol>
</li>
<li>任务运行<ol>
<li>MRAppMaster向领取到任务的NodeManager发送启动程序脚本</li>
<li>NodeManeger启动MapTask,执行map逻辑，输出分区排序后的结果</li>
<li>MRAppMaster等待所有MapTask运行结束，或者指定数量的MapTask运行结束，向ResourceManager申请容器，运行ReduceTask</li>
<li>ReduceTask从MapTask的输出文件中获取响应分区数据，执行Reduce逻辑</li>
<li>程序运行结束，MRAppMaster会向ResourceManager申请注销自己</li>
</ol>
</li>
<li>更新运行状态和进度<ol>
<li>Yarn中的任务进度和状态返回给应用管理器，客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求更新进度，展示给用户</li>
</ol>
</li>
<li>作业完成<ol>
<li>除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。</li>
</ol>
</li>
</ol>
<h2 id="1-4-资源调度器"><a href="#1-4-资源调度器" class="headerlink" title="1.4 资源调度器"></a>1.4 资源调度器</h2><ul>
<li><p>目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。Hadoop3.1.3默认的资源调度器是Capacity Scheduler。</p>
</li>
<li><p>具体设置详见：yarn-default.xml文件</p>
  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The class to use as the resource scheduler.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="1-4-1-FIFO先进先出调度器"><a href="#1-4-1-FIFO先进先出调度器" class="headerlink" title="1.4.1 FIFO先进先出调度器"></a>1.4.1 FIFO先进先出调度器</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16362766014573.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>单队列，根据提交作业的先后顺序，先来先服务。</p>
</li>
<li><p>优点：简单易懂；</p>
</li>
<li><p>缺点：不支持多队列，生产环境很少使用；</p>
</li>
</ul>
<h3 id="1-4-2-容量调度器（Capacity-Scheduler）"><a href="#1-4-2-容量调度器（Capacity-Scheduler）" class="headerlink" title="1.4.2 容量调度器（Capacity Scheduler）"></a>1.4.2 容量调度器（Capacity Scheduler）</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16362766636662.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>Capacity Scheduler Capacity Scheduler 是Yahoo开发的多用户调度器，它以队列为单位划分资源，每个队列可设定一定比例的资源最低保证和使用上限，同时，每个用户也可设定一定的资源使用上限以防止资源滥用。而当一个队列的资源有剩余时，可暂时将剩余资源共享给其他队列。</li>
<li>总之，Capacity Scheduler 主要有以下几个特点：<ol>
<li>多队列：每个队列可配置一定的资源量，每个队列采用FIFO调度策略</li>
<li>容量保证：管理员可为每个队列设置资源最低保证和资源使用上限，所有提交到该队列的应用程序共享这些资源。</li>
<li>灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。这种资源灵活分配的方式可明显提高资源利用率。</li>
<li>多重租赁：支持多用户共享集群和多应用程序同时运行。为防止单个应用程序、用户或者队列独占集群中的资源，管理员可为之增加多重约束（比如单个应用程序同时运行的任务数等）。</li>
<li>安全保证：每个队列有严格的ACL列表规定它的访问用户，每个用户可指定哪些用户允许查看自己应用程序的运行状态或者控制应用程序（比如杀死应用程序）。此外，管理员可指定队列管理员和集群系统管理员。</li>
<li>动态更新配置文件：管理员可根据需要动态修改各种配置参数，以实现在线集群管理。</li>
</ol>
</li>
</ul>
<h3 id="1-4-3-公平调度器（Fair-Scheduler）"><a href="#1-4-3-公平调度器（Fair-Scheduler）" class="headerlink" title="1.4.3 公平调度器（Fair Scheduler）"></a>1.4.3 公平调度器（Fair Scheduler）</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16362766917702.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>Fair Scheduler Fair Schedulere是Facebook开发的多用户调度器。</li>
<li>公平调度器的目的是让所有的作业随着时间的推移，都能平均地获取等同的共享资源。当有作业提交上来，系统会将空闲的资源分配给新的作业，每个任务大致上会获取平等数量的资源。和传统的调度策略不同的是它会让小的任务在合理的时间完成，同时不会让需要长时间运行的耗费大量资源的任务挨饿！同Capacity Scheduler类似，它以队列为单位划分资源，每个队列可设定一定比例的资源最低保证和使用上限，同时，每个用户也可设定一定的资源使用上限以防止资源滥用；当一个队列的资源有剩余时，可暂时将剩余资源共享给其他队列。当然，Fair Scheduler也存在很多与Capacity Scheduler不同之处，这主要体现在以下几个方面：<ol>
<li>资源公平共享。在每个队列中，Fair Scheduler 可选择按照FIFO、Fair或DRF策略为应用程序分配资源。其中， </li>
<li>FIFO策略: 公平调度器每个队列资源分配策略如果选择FIFO的话，就是禁用掉每个队列中的Task共享队列资源，此时公平调度器相当于上面讲过的容量调度器。</li>
<li>Fair策略: 基于最大最小公平算法实现的资源多路复用方式，默认情况下，每个队列内部采用该方式分配资源。这意味着，如果一个队列中有两个应用程序同时运行，则每个应用程序可得到1/2的资源；如果三个应用程序同时运行，则每个应用程序可得到1/3的资源。<ul>
<li>扩展：最大最小公平算法举例：<ol>
<li>不加权(关注点是job的个数)： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> 有一条队列总资源12个, 有4个job，对资源的需求分别是: </span><br><span class="line">job1-&gt;1,  job2-&gt;2 , job3-&gt;6,  job4-&gt;5</span><br><span class="line">     第一次算:  12 / 4 = 3 </span><br><span class="line">		job1: 分3 --&gt; 多2个 </span><br><span class="line">		job2: 分3 --&gt; 多1个</span><br><span class="line">		job3: 分3 --&gt; 差3个</span><br><span class="line">		job4: 分3 --&gt; 差2个</span><br><span class="line">	第二次算: 3 / 2  = 1.5 </span><br><span class="line">         job1: 分1</span><br><span class="line">		job2: 分2</span><br><span class="line">		job3: 分3 --&gt; 差3个 --&gt; 分1.5 --&gt; 最终: 4.5 </span><br><span class="line">		job4: 分3 --&gt; 差2个 --&gt; 分1.5 --&gt; 最终: 4.5 </span><br><span class="line">	第n次算: 一直算到没有空闲资源</span><br></pre></td></tr></table></figure></li>
<li>加权(关注点是job的权重)： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">有一条队列总资源16，有4个job </span><br><span class="line">对资源的需求分别是: job1-&gt;4   job2-&gt;2  job3-&gt;10  job4-&gt;4 </span><br><span class="line">每个job的权重为:   job1-&gt;5   job2-&gt;8  job3-&gt;1   job4-&gt;2	</span><br><span class="line">	第一次算: 16 / (5+8+1+2) =  1</span><br><span class="line">	    job1:  分5 --&gt; 多1</span><br><span class="line">	    job2:  分8 --&gt; 多6</span><br><span class="line">	    job3:  分1 --&gt; 少9</span><br><span class="line">	    job4:  分2 --&gt; 少2            </span><br><span class="line">	第二次算: 7 / (1+2) = 7/3</span><br><span class="line">	    job1: 分4</span><br><span class="line">	    job2: 分2</span><br><span class="line">	    job3: 分1 --&gt; 分7/3（2.33） --&gt; 少 6.67</span><br><span class="line">	    job4: 分2 --&gt; 分14/3(4.66) --&gt;多2.66</span><br><span class="line">    第三次算: </span><br><span class="line">	    job1: 分4</span><br><span class="line">	    job2: 分2</span><br><span class="line">	    job3: 分1 --&gt; 分7/3 --&gt; 分2.66</span><br><span class="line">	    job4: 分4</span><br><span class="line">    第n次算: 一直算到没有空闲资源</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<ol start="4">
<li>DRF策略: DRF(Dominant Resource Fairness)，我们之前说的资源，都是单一标准，例如只考虑内存(也是yarn默认的情况)。但是很多时候我们资源有很多种，例如内存，CPU，网络带宽等，这样我们很难衡量两个应用应该分配的资源比例。那么在YARN中，我们用DRF来决定如何调度：假设集群一共有100 CPU和10T 内存，而应用A需要(2 CPU, 300GB)，应用B需要(6 CPU, 100GB)。则两个应用分别需要A(2%CPU, 3%内存)和B(6%CPU, 1%内存)的资源，这就意味着A是内存主导的, B是CPU主导的，针对这种情况，我们可以选择DRF策略对不同应用进行不同资源（CPU和内存）的一个不同比例的限制。 <ul>
<li>支持资源抢占。当某个队列中有剩余资源时，调度器会将这些资源共享给其他队列，而当该队列中有新的应用程序提交时，调度器要为它回收资源。为了尽可能降低不必要的计算浪费，调度器采用了先等待再强制回收的策略，即如果等待一段时间后尚有未归还的资源，则会进行资源抢占：从那些超额使用资源的队列中杀死一部分任务，进而释放资源。yarn.scheduler.fair.preemption=true 通过该配置开启资源抢占。</li>
<li>提高小应用程序响应时间。由于采用了最大最小公平算法，小作业可以快速获取资源并运行完成<h2 id="1-5-Yarn常用命令"><a href="#1-5-Yarn常用命令" class="headerlink" title="1.5 Yarn常用命令"></a>1.5 Yarn常用命令</h2><h3 id="1-5-1-yarn-application查看任务"><a href="#1-5-1-yarn-application查看任务" class="headerlink" title="1.5.1 yarn application查看任务"></a>1.5.1 yarn application查看任务</h3></li>
</ul>
</li>
</ol>
</li>
</ol>
</li>
</ul>
<ol>
<li>yarn application -list 查看所有任务 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn application -list</span><br><span class="line">Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):1</span><br><span class="line">                Application-Id	    Application-Name	    Application-Type	      User	     Queue	             State	       Final-State	       Progress	                       Tracking-URL</span><br><span class="line">application_1635042010561_0004	sgg-hadoop-mapreduce-0.0.1-SNAPSHOT.jar	           MAPREDUCE	   atguigu	      hive	           RUNNING	         UNDEFINED	             5%	             http://hadoop002:38122</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
<li>根据Application状态过滤：yarn application -list -appStates<ul>
<li>所有状态：ALL、NEW、NEW_SAVING、SUBMITTED、ACCEPTED、RUNNING、FINISHED、FAILED、KILLED<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn application -list -appStates FINISHED</span><br><span class="line">Total number of applications (application-types: [], states: [FINISHED] and tags: []):15</span><br><span class="line">                Application-Id	    Application-Name	    Application-Type	      User	     Queue	             State	       Final-State	       Progress	                       Tracking-URL</span><br><span class="line">application_1635042010561_0004	sgg-hadoop-mapreduce-0.0.1-SNAPSHOT.jar	           MAPREDUCE	   atguigu	      hive	          FINISHED	         SUCCEEDED	           100%	http://hadoop002:19888/jobhistory/job/job_1635042010561_0004</span><br><span class="line">application_1634633871057_0005	hadoop-archives-3.1.3.jar	           MAPREDUCE	   atguigu	   default	          FINISHED	         SUCCEEDED	           100%	http://hadoop001:19888/jobhistory/job/job_1634633871057_0005</span><br><span class="line">application_1634633871057_0004	hadoop-archives-3.1.3.jar	           MAPREDUCE	   atguigu	   default	          FINISHED	         SUCCEEDED	           100%	http://hadoop001:19888/jobhistory/job/job_1634633871057_0004</span><br><span class="line">application_1634633871057_0003	              distcp	           MAPREDUCE	   atguigu	   default	          FINISHED	         SUCCEEDED	           100%	http://hadoop001:19888/jobhistory/job/job_1634633871057_0003</span><br><span class="line">application_1634633871057_0002	sgg-hadoop-mapreduce-0.0.1-SNAPSHOT.jar	           MAPREDUCE	   atguigu	   default	          FINISHED	         SUCCEEDED	           100%	http://hadoop001:19888/jobhistory/job/job_1634633871057_0002</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>Kill掉Application：yarn application -kill Application-Id <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn application -<span class="built_in">kill</span> application_1635042010561_0006</span><br><span class="line">Killing application application_1635042010561_0006</span><br><span class="line">2021-10-24 13:25:54,458 INFO impl.YarnClientImpl: Killed application application_1635042010561_0006</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
 客户端 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[INFO] [2021-10-24 13:25:55][org.apache.hadoop.mapreduce.Job]Job job_1635042010561_0006 failed with state KILLED due to: Application application_1635042010561_0006 was killed by user atguigu at 192.168.2.17</span><br><span class="line">[INFO] [2021-10-24 13:25:55][org.apache.hadoop.mapreduce.Job]Counters: 0</span><br></pre></td></tr></table></figure>
<h3 id="1-5-2-yarn-logs查看日志"><a href="#1-5-2-yarn-logs查看日志" class="headerlink" title="1.5.2 yarn logs查看日志"></a>1.5.2 yarn logs查看日志</h3></li>
<li>查询Application日志：yarn logs -applicationId <ApplicationId> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn logs -applicationId application_1635042010561_0007 &gt; application_1635042010561_0007.log</span><br><span class="line">2021-10-24 13:31:26,239 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">2021-10-24 13:31:26,308 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">[atguigu@hadoop001 ~]$ head application_1635042010561_0007.log</span><br><span class="line">Container: container_e06_1635042010561_0007_01_000003 on hadoop001_34161</span><br><span class="line">LogAggregationType: AGGREGATED</span><br><span class="line">========================================================================</span><br><span class="line">LogType:directory.info</span><br><span class="line">LogLastModifiedTime:星期日 十月 24 13:30:33 +0800 2021</span><br><span class="line">LogLength:2011</span><br><span class="line">LogContents:</span><br><span class="line">ls -l:</span><br><span class="line">total 20</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu  129 Oct 24 13:30 container_tokens</span><br><span class="line">[atguigu@hadoop001 ~]$ tail application_1635042010561_0007.log</span><br><span class="line">2021-10-24 13:30:31,976 INFO [Thread-75] org.apache.hadoop.ipc.Server: Stopping server on 33721</span><br><span class="line">2021-10-24 13:30:31,977 INFO [IPC Server Responder] org.apache.hadoop.ipc.Server: Stopping IPC Server Responder</span><br><span class="line">2021-10-24 13:30:31,978 INFO [IPC Server listener on 0] org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 0</span><br><span class="line">2021-10-24 13:30:31,981 INFO [Thread-75] org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.w.WebAppContext@3cd26422&#123;/,null,UNAVAILABLE&#125;&#123;/mapreduce&#125;</span><br><span class="line">2021-10-24 13:30:31,984 INFO [Thread-75] org.eclipse.jetty.server.AbstractConnector: Stopped ServerConnector@57e388c3&#123;HTTP/1.1,[http/1.1]&#125;&#123;0.0.0.0:0&#125;</span><br><span class="line">2021-10-24 13:30:31,984 INFO [Thread-75] org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@234a8f27&#123;/static,jar:file:/opt/module/ha-hadoop-3.1.3/share/hadoop/yarn/hadoop-yarn-common-3.1.3.jar!/webapps/static,UNAVAILABLE&#125;</span><br><span class="line"></span><br><span class="line">End of LogType:syslog</span><br><span class="line">***********************************************************************</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
<li>查询Container日志：yarn logs -applicationId <ApplicationId> -containerId <ContainerId>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn logs -applicationId application_1635042010561_0007 -containerId container_e06_1635042010561_0007_01_000003 &gt; container_e06_1635042010561_0007_01_000003.log</span><br><span class="line">2021-10-24 13:33:52,284 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">2021-10-24 13:33:52,353 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">[atguigu@hadoop001 ~]$ head container_e06_1635042010561_0007_01_000003.log</span><br><span class="line">Container: container_e06_1635042010561_0007_01_000003 on hadoop001_34161</span><br><span class="line">LogAggregationType: AGGREGATED</span><br><span class="line">========================================================================</span><br><span class="line">LogType:directory.info</span><br><span class="line">LogLastModifiedTime:星期日 十月 24 13:30:33 +0800 2021</span><br><span class="line">LogLength:2011</span><br><span class="line">LogContents:</span><br><span class="line">ls -l:</span><br><span class="line">total 20</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu  129 Oct 24 13:30 container_tokens</span><br><span class="line">[atguigu@hadoop001 ~]$ tail container_e06_1635042010561_0007_01_000003.log</span><br><span class="line">2021-10-24 13:30:25,651 INFO [main] org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16734 bytes</span><br><span class="line">2021-10-24 13:30:25,661 INFO [main] org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: Merged 1 segments, 16746 bytes to disk to satisfy reduce memory <span class="built_in">limit</span></span><br><span class="line">2021-10-24 13:30:25,661 INFO [main] org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: Merging 1 files, 16750 bytes from disk</span><br><span class="line">2021-10-24 13:30:25,661 INFO [main] org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce</span><br><span class="line">2021-10-24 13:30:25,661 INFO [main] org.apache.hadoop.mapred.Merger: Merging 1 sorted segments</span><br><span class="line">2021-10-24 13:30:25,663 INFO [main] org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16734 bytes</span><br><span class="line"></span><br><span class="line">End of LogType:syslog.shuffle</span><br><span class="line">*******************************************************************************</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h3 id="1-5-3-yarn-application-attempt查看尝试运行的任务"><a href="#1-5-3-yarn-application-attempt查看尝试运行的任务" class="headerlink" title="1.5.3 yarn application attempt查看尝试运行的任务"></a>1.5.3 yarn application attempt查看尝试运行的任务</h3></li>
<li>列出所有Application尝试的列表：yarn applicationattempt -list <ApplicationId> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn applicationattempt -list application_1635042010561_0007</span><br><span class="line">Total number of application attempts :1</span><br><span class="line">         ApplicationAttempt-Id	               State	                    AM-Container-Id	                       Tracking-URL</span><br><span class="line">appattempt_1635042010561_0007_000001	            FINISHED	container_e06_1635042010561_0007_01_000001	http://hadoop001:8088/proxy/application_1635042010561_0007/</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
<li>打印ApplicationAttemp状态：yarn applicationattempt -status <ApplicationAttemptId> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn applicationattempt -status appattempt_1635042010561_0007_000001</span><br><span class="line">Application Attempt Report :</span><br><span class="line">	ApplicationAttempt-Id : appattempt_1635042010561_0007_000001</span><br><span class="line">	State : FINISHED</span><br><span class="line">	AMContainer : container_e06_1635042010561_0007_01_000001</span><br><span class="line">	Tracking-URL : http://hadoop001:8088/proxy/application_1635042010561_0007/</span><br><span class="line">	RPC Port : 33721</span><br><span class="line">	AM Host : hadoop002</span><br><span class="line">	Diagnostics :</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="1-5-4-yarn-container查看容器"><a href="#1-5-4-yarn-container查看容器" class="headerlink" title="1.5.4 yarn container查看容器"></a>1.5.4 yarn container查看容器</h3><font color ='blue' >注：只有在任务跑的途中才能看到container的状态</font></li>
<li>打印Container状态：    yarn container -status <ContainerId> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn container -list appattempt_1635042010561_0010_000001</span><br><span class="line">Total number of containers :3</span><br><span class="line">                  Container-Id	          Start Time	         Finish Time	               State	                Host	   Node Http Address	                            LOG-URL</span><br><span class="line">container_e06_1635042010561_0010_01_000001	星期日 十月 24 13:45:46 +0800 2021	                 N/A	             RUNNING	     hadoop002:35693	http://hadoop002:8042	http://hadoop002:8042/node/containerlogs/container_e06_1635042010561_0010_01_000001/atguigu</span><br><span class="line">container_e06_1635042010561_0010_01_000003	星期日 十月 24 13:45:50 +0800 2021	                 N/A	             RUNNING	     hadoop005:37776	http://hadoop005:8042	http://hadoop005:8042/node/containerlogs/container_e06_1635042010561_0010_01_000003/atguigu</span><br><span class="line">container_e06_1635042010561_0010_01_000005	星期日 十月 24 13:46:33 +0800 2021	                 N/A	             RUNNING	     hadoop002:35693	http://hadoop002:8042	http://hadoop002:8042/node/containerlogs/container_e06_1635042010561_0010_01_000005/atguigu</span><br><span class="line">[atguigu@hadoop001 ~]$ yarn container -status container_e06_1635042010561_0010_01_000001</span><br><span class="line">Container Report :</span><br><span class="line">	Container-Id : container_e06_1635042010561_0010_01_000001</span><br><span class="line">	Start-Time : 1635054346921</span><br><span class="line">	Finish-Time : 0</span><br><span class="line">	State : RUNNING</span><br><span class="line">	Execution-Type : GUARANTEED</span><br><span class="line">	LOG-URL : http://hadoop002:8042/node/containerlogs/container_e06_1635042010561_0010_01_000001/atguigu</span><br><span class="line">	Host : hadoop002:35693</span><br><span class="line">	NodeHttpAddress : http://hadoop002:8042</span><br><span class="line">	Diagnostics : null</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
<li>列出所有Container：yarn container -list <ApplicationAttemptId> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn applicationattempt -list application_1635042010561_0009</span><br><span class="line">Total number of application attempts :1</span><br><span class="line">         ApplicationAttempt-Id	               State	                    AM-Container-Id	                       Tracking-URL</span><br><span class="line">appattempt_1635042010561_0009_000001	             RUNNING	container_e06_1635042010561_0009_01_000001	http://hadoop001:8088/proxy/application_1635042010561_0009/</span><br><span class="line">[atguigu@hadoop001 ~]$ yarn container -list appattempt_1635042010561_0009_000001</span><br><span class="line">Total number of containers :4</span><br><span class="line">                  Container-Id	          Start Time	         Finish Time	               State	                Host	   Node Http Address	                            LOG-URL</span><br><span class="line">container_e06_1635042010561_0009_01_000003	星期日 十月 24 13:43:31 +0800 2021	                 N/A	             RUNNING	     hadoop005:37776	http://hadoop005:8042	http://hadoop005:8042/node/containerlogs/container_e06_1635042010561_0009_01_000003/atguigu</span><br><span class="line">container_e06_1635042010561_0009_01_000002	星期日 十月 24 13:43:31 +0800 2021	                 N/A	             RUNNING	     hadoop005:37776	http://hadoop005:8042	http://hadoop005:8042/node/containerlogs/container_e06_1635042010561_0009_01_000002/atguigu</span><br><span class="line">container_e06_1635042010561_0009_01_000001	星期日 十月 24 13:43:27 +0800 2021	                 N/A	             RUNNING	     hadoop005:37776	http://hadoop005:8042	http://hadoop005:8042/node/containerlogs/container_e06_1635042010561_0009_01_000001/atguigu</span><br><span class="line">container_e06_1635042010561_0009_01_000004	星期日 十月 24 13:43:31 +0800 2021	                 N/A	             RUNNING	     hadoop005:37776	http://hadoop005:8042	http://hadoop005:8042/node/containerlogs/container_e06_1635042010561_0009_01_000004/atguigu</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h3 id="1-5-5-yarn-node查看节点状态"><a href="#1-5-5-yarn-node查看节点状态" class="headerlink" title="1.5.5 yarn node查看节点状态"></a>1.5.5 yarn node查看节点状态</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn node -list -all</span><br><span class="line">Total Nodes:5</span><br><span class="line">         Node-Id	     Node-State	Node-Http-Address	Number-of-Running-Containers</span><br><span class="line"> hadoop004:38785	        RUNNING	   hadoop004:8042	                           0</span><br><span class="line"> hadoop005:37776	        RUNNING	   hadoop005:8042	                           0</span><br><span class="line"> hadoop003:39188	        RUNNING	   hadoop003:8042	                           0</span><br><span class="line"> hadoop002:35693	        RUNNING	   hadoop002:8042	                           0</span><br><span class="line"> hadoop001:34161	        RUNNING	   hadoop001:8042	                           0</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h3 id="1-5-6-yarn-rmadmin更新配置"><a href="#1-5-6-yarn-rmadmin更新配置" class="headerlink" title="1.5.6 yarn rmadmin更新配置"></a>1.5.6 yarn rmadmin更新配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn rmadmin -refreshQueues</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h3 id="1-5-7-yarn-queue查看队列"><a href="#1-5-7-yarn-queue查看队列" class="headerlink" title="1.5.7 yarn queue查看队列"></a>1.5.7 yarn queue查看队列</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$  yarn queue -status default</span><br><span class="line">Queue Information :</span><br><span class="line">Queue Name : default</span><br><span class="line">	State : RUNNING</span><br><span class="line">	Capacity : 40.0%</span><br><span class="line">	Current Capacity : .0%</span><br><span class="line">	Maximum Capacity : 60.0%</span><br><span class="line">	Default Node Label expression : &lt;DEFAULT_PARTITION&gt;</span><br><span class="line">	Accessible Node Labels : *</span><br><span class="line">	Preemption : disabled</span><br><span class="line">	Intra-queue Preemption : disabled</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h2 id="1-6-Yarn生产环境核心参数"><a href="#1-6-Yarn生产环境核心参数" class="headerlink" title="1.6 Yarn生产环境核心参数"></a>1.6 Yarn生产环境核心参数</h2><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16350552254539.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ol>
<table>
<thead>
<tr>
<th>类型</th>
<th>配置</th>
<th>描述</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>ResourceManager相关</td>
<td>yarn.resourcemanager.scheduler.class</td>
<td>配置调度器，默认容量</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.resourcemanager.scheduler.client.thread-count</td>
<td>ResourceManager处理调度器请求的线程数量，默认50</td>
<td></td>
</tr>
<tr>
<td>NodeManager相关</td>
<td>yarn.nodemanager.resource.detect-hardware-capabilities</td>
<td>是否让yarn自己检测硬件进行配置，默认false</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.resource.count-logical-processors-as-core</td>
<td>是否将虚拟核数当作CPU核数，默认false</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.resource.pcores-vcores-multiplier</td>
<td>虚拟核数和物理核数乘数，例如：4核8线程，该参数就应设为2，默认1.0</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.resource.memory-mb</td>
<td>NodeManager使用内存，默认8G</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.resource.system-reserved-memory-mb</td>
<td>NodeManager为系统保留多少内存</td>
<td>以上二个参数配置一个即可</td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.resource.cpu-vcores</td>
<td>NodeManager使用CPU核数，默认8个</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.pmem-check-enabled</td>
<td>是否开启物理内存检查限制container，默认打开</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.vmem-check-enabled</td>
<td>是否开启虚拟内存检查限制container，默认打开</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.vmem-pmem-ratio</td>
<td>虚拟内存物理内存比例，默认2.1</td>
<td></td>
</tr>
<tr>
<td>Container相关</td>
<td>yarn.scheduler.minimum-allocation-mb</td>
<td>容器最最小内存，默认1G</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.scheduler.maximum-allocation-mb</td>
<td>容器最最大内存，默认8G</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.scheduler.minimum-allocation-vcores</td>
<td>容器最小CPU核数，默认1个</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.scheduler.maximum-allocation-vcores</td>
<td>容器最大CPU核数，默认4个</td>
<td></td>
</tr>
</tbody></table>
<ol>
<li> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">	            </span><br><span class="line">yarn.resourcemanager.scheduler.client.thread-count     ResourceManager处理调度器请求的线程数量，默认50</span><br></pre></td></tr></table></figure>
<h1 id="二、容量调度器多队列提交案例"><a href="#二、容量调度器多队列提交案例" class="headerlink" title="二、容量调度器多队列提交案例"></a>二、容量调度器多队列提交案例</h1><h3 id="4-5-1-需求"><a href="#4-5-1-需求" class="headerlink" title="4.5.1 需求"></a>4.5.1 需求</h3></li>
</ol>
<ul>
<li>Yarn默认的容量调度器是一条单队列的调度器，在实际使用中会出现单个任务阻塞整个队列的情况。同时，随着业务的增长，公司需要分业务限制集群使用率。这就需要我们按照业务种类配置多条任务队列。</li>
</ul>
<h3 id="4-5-2-配置多队列的容量调度器"><a href="#4-5-2-配置多队列的容量调度器" class="headerlink" title="4.5.2 配置多队列的容量调度器"></a>4.5.2 配置多队列的容量调度器</h3><ul>
<li>默认Yarn的配置下，容量调度器只有一条Default队列。在capacity-scheduler.xml中可以配置多条队列，并降低default队列资源占比：  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定多队列，增加hive队列 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.queues<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>default,hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      The queues at the this level (root is the root queue).</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 降低default队列资源额定容量为40%，默认100% --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.default.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>40<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 降低default队列资源最大容量为60%，默认100% --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.default.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>同时为新加队列添加必要属性：  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定hive队列的资源额定容量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.user-limit-factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定hive队列的资源最大容量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.state<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>RUNNING<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_submit_applications<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_administer_queue<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_application_max_priority<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.maximum-application-lifetime<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.default-application-lifetime<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>在配置完成后，重启Yarn或者执行yarn rmadmin -refreshQueues刷新队列，就可以看到两条队列：</li>
</ul>
<h1 id="问答"><a href="#问答" class="headerlink" title="问答"></a>问答</h1><h3 id="一、MR中的一个Job是如何提交的？"><a href="#一、MR中的一个Job是如何提交的？" class="headerlink" title="一、MR中的一个Job是如何提交的？"></a>一、MR中的一个Job是如何提交的？</h3><ol>
<li>作业提交阶段<ol>
<li>client调用job.waitForCompletion()方法，提交任务到集群</li>
<li>client想ResourceManager申请一个作业id</li>
<li>ResourceManager返回改job资源的提交路径和作业id</li>
<li>client提交jar，切片信息，配置文件到指定的资源提交路径</li>
<li>client提交资源后，向ResourceManager申请运行MRAppMaster</li>
</ol>
</li>
<li>作业初始化<ol>
<li>ResourceManager收到请求后，将job添加到任务队列</li>
<li>某一个空闲的NodeManager领取到任务后，创建Container，运行MRAppMaster</li>
<li>下载client提交的资源文件到本地</li>
</ol>
</li>
<li>任务分配<ol>
<li>MrAppMaster向ResourceManager申请MapTask运行资源</li>
<li>ResourceManager将MapTask分配给其他NodeManager</li>
<li>领取到任务的NodeManager创建MapTask容器</li>
</ol>
</li>
<li>任务运行<ol>
<li>MRAppMaster向领取到任务的NodeManager发送启动程序脚本</li>
<li>NodeManeger启动MapTask,执行map逻辑，输出分区排序后的结果</li>
<li>MRAppMaster等待所有MapTask运行结束，或者指定数量的MapTask运行结束，向ResourceManager申请容器，运行ReduceTask</li>
<li>ReduceTask从MapTask的输出文件中获取响应分区数据，执行Reduce逻辑</li>
<li>程序运行结束，MRAppMaster会向ResourceManager申请注销自己</li>
</ol>
</li>
<li>更新运行状态和进度<ol>
<li>Yarn中的任务进度和状态返回给应用管理器，客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求更新进度，展示给用户</li>
</ol>
</li>
<li>作业完成<ol>
<li>除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。</li>
</ol>
</li>
</ol>
<h3 id="二、描述一下YARN的工作机制。"><a href="#二、描述一下YARN的工作机制。" class="headerlink" title="二、描述一下YARN的工作机制。"></a>二、描述一下YARN的工作机制。</h3><ol>
<li>MR程序在客户端通过job.submit()方法提交任务到本地或者远程hadoop集群，创建YranRunner</li>
<li>YarnRunner向ResourceManager申请一个Application</li>
<li>ResourceManager将程序运行的资源路径(资源提交路径及application_id)返回给Yarnrunner</li>
<li>程序将运行所需资源提交到hdfs上（jar包，配置文件，split信息）</li>
<li>程序资源提交之后申请运行MRAppMaster</li>
<li>ResourceManager将用户请求初始化为一个Task，该task会被放到任务队列中，等待调度器分配资源</li>
<li>NodeManager领取task任务</li>
<li>该NodeManager创建container，并启动MRAppMaster</li>
<li>container从hdfs拷贝资源到本地</li>
<li>MRAppMaster向RM申请运行MapTask资源</li>
<li>ResourceManager将MapTask任务分配给NodeManager，领取到任务的NodeManager创建容器</li>
<li>MRAppMaster向接收到任务的NodeManager发送启动程序脚本，NodeManeger启动MapTask,执行map逻辑，输出分区排序后的结果</li>
<li>MRAppMaster等待所有MapTak运行结束，或者指定数量的MapTask运行结束后，向ResourceManager申请容器，运行ReduceTask.</li>
<li>ReduceTask从MapTask输出中取对应分区的数据，执行reduce逻辑</li>
<li>程序运行结束，MRAppMaster会向ResourceManager申请注销自己</li>
</ol>
<h3 id="三、YARN中有几种资源调度实现策略？分别阐述一下各自的特点"><a href="#三、YARN中有几种资源调度实现策略？分别阐述一下各自的特点" class="headerlink" title="三、YARN中有几种资源调度实现策略？分别阐述一下各自的特点"></a>三、YARN中有几种资源调度实现策略？分别阐述一下各自的特点</h3><ol>
<li>FIFO调度器：<ul>
<li>单队列，根据作业提交的顺序，先到先分配资源</li>
<li>优点：简单易懂</li>
<li>缺点：不支持多队列，无法确定优先级，不够灵活</li>
</ul>
</li>
<li>CapacityScheduler容量调度区：<ul>
<li>多队列：每个队列可以配置一定比例的资源，每个队列内部采用FIFO调度策略</li>
<li>容量保证：可以设置每个队列的资源最低保证和资源使用上限，所有提交到该队列的应用程序共享这些资源</li>
<li>灵活性：如果一个队列中资源有空闲，可以暂时共享给需要资源的队列；一旦空闲队列有新的应用程序提交，借调资源的队列会归还资源，通过这种方式提高了资源的利用率</li>
<li>多重租赁：支持多用户共享集群和多应用程序同时运行，可以对用户，程序添加多重约束，避免产生用户或者程序独占集群资源</li>
<li>安全保证：每个队列有严格的ACL列表限制访问用户，可指用户对应用程序的查看权限，控制权限</li>
<li>动态更新配置文件：管理员可以根据需要动态修改参数配置，实现在线集群管理</li>
</ul>
</li>
</ol>
<h3 id="四、简述一下如何配置一个自定的-队列？"><a href="#四、简述一下如何配置一个自定的-队列？" class="headerlink" title="四、简述一下如何配置一个自定的 队列？"></a>四、简述一下如何配置一个自定的 队列？</h3><ol>
<li>配置多队列</li>
<li>调整队列资源分配</li>
<li>配置新队列属性</li>
<li>刷新队列</li>
</ol>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Yarn/" rel="tag">Yarn</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Shell"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/11/07/Shell/"
    >Shell</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/11/07/Shell/" class="article-date">
  <time datetime="2021-11-07T00:08:42.000Z" itemprop="datePublished">2021-11-07</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Linux/">Linux</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Shell编程"><a href="#Shell编程" class="headerlink" title="Shell编程"></a>Shell编程</h1><h3 id="初识Shell"><a href="#初识Shell" class="headerlink" title="初识Shell"></a>初识Shell</h3><h4 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h4><p>shell是一个命令行解释器，它接收应用程序/用户命令，然后调用操作系统内核。shell还是一个功能强大的编程语言，易编写、易调试、灵活性强</p>
<p>Linux提供的shell解释器：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ cat /etc/shells </span><br><span class="line">/bin/sh</span><br><span class="line">/bin/bash</span><br><span class="line">/sbin/nologin</span><br><span class="line">/usr/bin/sh</span><br><span class="line">/usr/bin/bash</span><br><span class="line">/usr/sbin/nologin</span><br><span class="line">/bin/tcsh</span><br><span class="line">/bin/csh</span><br></pre></td></tr></table></figure>

<p>Centos默认的解析器是bash</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ echo $SHELL</span><br><span class="line">/bin/bash</span><br></pre></td></tr></table></figure>

<p>bash和sh的关系</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ ll /bin/ | grep bash</span><br><span class="line">-rwxr-xr-x. 1 root root     964544 4月  11 2018 bash</span><br><span class="line">lrwxrwxrwx. 1 root root         10 7月  28 00:28 bashbug -&gt; bashbug-64</span><br><span class="line">-rwxr-xr-x. 1 root root       6964 4月  11 2018 bashbug-64</span><br><span class="line">lrwxrwxrwx. 1 root root          4 7月  28 00:28 sh -&gt; bash</span><br></pre></td></tr></table></figure>

<h4 id="2、入门"><a href="#2、入门" class="headerlink" title="2、入门"></a>2、入门</h4><p>注意：shell脚本后缀为.sh文件，脚本以<code>#!/bin/bash</code>开头，目的是为了指定解析器</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &#x27;hello word!&#x27;</span><br></pre></td></tr></table></figure>

<p>运行脚本的方式：</p>
<ol>
<li><p>第一种：采用bash或sh+脚本的相对路径或绝对路径（不用赋予脚本+x权限）</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> sh ./helloword.sh </span><br><span class="line">hello word!</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> sh /home/atguigu/study/helloword.sh </span><br><span class="line">hello word!</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> bash ./helloword.sh </span><br><span class="line">hello word!</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> bash /home/atguigu/study/helloword.sh </span><br><span class="line">hello word!</span><br></pre></td></tr></table></figure></li>
<li><p>采用输入脚本的绝对路径或相对路径执行脚本（必须具有可执行权限+x）</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> ll</span><br><span class="line">总用量 <span class="number">4</span></span><br><span class="line"><span class="literal">-rw</span><span class="literal">-rw</span><span class="literal">-r</span>--. <span class="number">1</span> atguigu atguigu <span class="number">32</span> <span class="number">8</span>月   <span class="number">6</span> <span class="number">18</span>:<span class="number">29</span> helloword.sh</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> chmod u+x helloword.sh </span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> ll</span><br><span class="line">总用量 <span class="number">4</span></span><br><span class="line"><span class="literal">-rwxrw</span><span class="literal">-r</span>--. <span class="number">1</span> atguigu atguigu <span class="number">32</span> <span class="number">8</span>月   <span class="number">6</span> <span class="number">18</span>:<span class="number">29</span> helloword.sh</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> ./helloword.sh </span><br><span class="line">hello word!</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> /home/atguigu/study/helloword.sh hello word!</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><h4 id="1、系统变量"><a href="#1、系统变量" class="headerlink" title="1、系统变量"></a>1、系统变量</h4><p><code>$PATH</code>、<code>$HOME</code>、<code>$PWD</code>、<code>$SHELL</code>、<code>$USER</code></p>
<p>显示shell中所有变量：set</p>
<p>说明：在终端输入sh进入到shell交互式界面，当然也可以直接在黑屏终端下操作测试，使用exit回退到终端</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ echo $PATH</span><br><span class="line">/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/atguigu/.local/bin:/home/atguigu/bin</span><br><span class="line">[atguigu@centos7_base study]$ sh</span><br><span class="line">sh-4.2$ echo $PATH</span><br><span class="line">/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/atguigu/.local/bin:/home/atguigu/bin</span><br><span class="line">sh-4.2$ exit</span><br><span class="line">exit</span><br><span class="line">[atguigu@centos7_base study]$sh</span><br><span class="line">sh-4.2$ set</span><br></pre></td></tr></table></figure>

<h4 id="2、自定义变量"><a href="#2、自定义变量" class="headerlink" title="2、自定义变量"></a>2、自定义变量</h4><p>定义变量：<code>变量=值</code></p>
<p>撤销变量：<code>unset 变量</code></p>
<p>声明静态变量：<code>readonly 变量</code>，静态变量不能unset</p>
<p>定义规则：</p>
<ol>
<li>变量名称可以由字母、数字和下划线组成，但是不能以数字开头，环境变量名建议大写</li>
<li>等号两侧不能有空格</li>
<li>在bash中，变量默认类型都是字符串类型，无法直接进行数值运算</li>
<li>变量的值如果有空格，需要使用双引号或单引号括起来</li>
</ol>
<p>单引号与双引号的区别：</p>
<ol>
<li>以单引号包围变量的值时，单引号里面是什么就输出什么，即使内容中有变量和命令（命令需要反引起来）也会把它们原样输出。这种方式比较适合定义显示纯字符串的情况，即不希望解析变量、命令等的场景</li>
<li>以双引号包围变量的值时，输出时会先解析里面的变量和命令，而不是把双引号中的变量名和命令原样输出。这种方式比较适合字符串中附带有变量和命令并且想将其解析后再输出的变量定义</li>
</ol>
<p>基本使用：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ sh</span><br><span class="line">sh-4.2$ a = 1</span><br><span class="line">sh: a: 未找到命令</span><br><span class="line">sh-4.2$ a= 1</span><br><span class="line">sh: 1: 未找到命令</span><br><span class="line">sh-4.2$ a =1</span><br><span class="line">sh: a: 未找到命令</span><br><span class="line">sh-4.2$ a=1</span><br><span class="line">sh-4.2$ echo $a</span><br><span class="line">1</span><br><span class="line">sh-4.2$ a=11</span><br><span class="line">sh-4.2$ echo $a</span><br><span class="line">11</span><br><span class="line">sh-4.2$ unset a</span><br><span class="line">sh-4.2$ echo $a</span><br><span class="line"></span><br><span class="line">sh-4.2$ readonly b=2</span><br><span class="line">sh-4.2$ echo $b</span><br><span class="line">2</span><br><span class="line">sh-4.2$ b=22</span><br><span class="line">sh: b: 只读变量</span><br><span class="line">sh-4.2$ unset b</span><br><span class="line">sh: unset: b: 无法反设定: 只读 variable</span><br><span class="line">sh-4.2$ c=1+2</span><br><span class="line">sh-4.2$ echo $c</span><br><span class="line">1+2</span><br><span class="line">sh-4.2$ d=sunck is a good man</span><br><span class="line">sh: is: 未找到命令</span><br><span class="line">sh-4.2$ d=&quot;sunck is a good man&quot;</span><br><span class="line">sh-4.2$ echo $d</span><br><span class="line">sunck is a good man</span><br><span class="line">sh-4.2$ e=&#x27;sunck is a good man&#x27;</span><br><span class="line">sh-4.2$ echo $e</span><br><span class="line">sunck is a good man</span><br><span class="line">sh-4.2$ f=200</span><br><span class="line">sh-4.2$ echo &#x27;变量f的值为：$f&#x27;</span><br><span class="line">变量f的值为：$g</span><br><span class="line">sh-4.2$ echo &quot;变量f的值为：$f&quot;</span><br><span class="line">变量f的值为：200</span><br></pre></td></tr></table></figure>

<p>将变量提升为全局变量：export</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sh-4.2$ G=100</span><br><span class="line">sh-4.2$ echo $G</span><br><span class="line">100</span><br><span class="line">sh-4.2$ vim helloword.sh </span><br><span class="line">sh-4.2$ cat helloword.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &#x27;hello word!&#x27;</span><br><span class="line">echo &quot;变量G的值为：$G&quot;</span><br><span class="line">sh-4.2$ ./helloword.sh </span><br><span class="line">hello word!</span><br><span class="line">变量G的值为：</span><br><span class="line">sh-4.2$ export G</span><br><span class="line">sh-4.2$ ./helloword.sh </span><br><span class="line">hello word!</span><br><span class="line">变量G的值为：100</span><br></pre></td></tr></table></figure>

<h4 id="3、特殊变量"><a href="#3、特殊变量" class="headerlink" title="3、特殊变量"></a>3、特殊变量</h4><p><strong>$n</strong></p>
<p>功能描述：n为数字，$0代表该脚本名称，$1-$9代表第一到第九个参数，十以上的参数需要用大括号包含，如${10}</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">sh-4.2$ vim parameter1.sh</span><br><span class="line">sh-4.2$ cat parameter1.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &quot;命令：$0&quot;</span><br><span class="line">echo &quot;第1个参数：$1&quot;</span><br><span class="line">echo &quot;第2个参数：$2&quot;</span><br><span class="line">echo &quot;第3个参数：$3&quot;</span><br><span class="line">echo &quot;第4个参数：$4&quot;</span><br><span class="line">echo &quot;第5个参数：$5&quot;</span><br><span class="line">echo &quot;第6个参数：$6&quot;</span><br><span class="line">echo &quot;第7个参数：$7&quot;</span><br><span class="line">echo &quot;第8个参数：$8&quot;</span><br><span class="line">echo &quot;第9个参数：$9&quot;</span><br><span class="line">echo &quot;第10个参数：$&#123;10&#125;&quot;</span><br><span class="line">sh-4.2$ sh ./parameter1.sh 1 2 3 4 5 6 7 8 9 a</span><br><span class="line">命令：./parameter1.sh</span><br><span class="line">第1个参数：1</span><br><span class="line">第2个参数：2</span><br><span class="line">第3个参数：3</span><br><span class="line">第4个参数：4</span><br><span class="line">第5个参数：5</span><br><span class="line">第6个参数：6</span><br><span class="line">第7个参数：7</span><br><span class="line">第8个参数：8</span><br><span class="line">第9个参数：9</span><br><span class="line">第10个参数：a</span><br></pre></td></tr></table></figure>

<p><strong>$#</strong></p>
<p>功能描述：获取所有输入参数个数，常用于循环</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sh-4.2$ vim parameter2.sh</span><br><span class="line">sh-4.2$ cat parameter2.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &quot;参数个数：$#&quot;</span><br><span class="line">sh-4.2$ sh ./parameter2.sh </span><br><span class="line">参数个数：0</span><br><span class="line">sh-4.2$ sh ./parameter2.sh 1</span><br><span class="line">参数个数：1</span><br><span class="line">sh-4.2$ sh ./parameter2.sh 1 2 3</span><br><span class="line">参数个数：3</span><br></pre></td></tr></table></figure>

<p>**$<em>与$@</em>*</p>
<p>$* 功能描述：这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体</p>
<p>$@ 功能描述：这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待</p>
<p>说明：如果想让$*和$@ 体现区别必须用双引号括起来，并使用循环变量才能看到效果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sh-4.2$ vim parameter3.sh</span><br><span class="line">sh-4.2$ cat parameter3.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo $*</span><br><span class="line">echo $@</span><br><span class="line">sh-4.2$ sh ./parameter3.sh 1 2 3</span><br><span class="line">1 2 3</span><br><span class="line">1 2 3</span><br></pre></td></tr></table></figure>

<p><strong>$?</strong></p>
<p>功能描述：最后一次执行的命令的返回状态。如果这个变量的值为0，证明上一个命令正确执行；如果这个变量的值为非0（具体是哪个数，由命令自己来决定），则证明上一个命令执行不正确了</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sh-4.2$ sh ./parameter3.sh 1 2 3</span><br><span class="line">1 2 3</span><br><span class="line">1 2 3</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure>

<h4 id="4、运算符"><a href="#4、运算符" class="headerlink" title="4、运算符"></a>4、运算符</h4><p>基本语法：<code>$((运算式))</code>或<code>$[运算式]</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sh-4.2$ h=$(((2+3)*4))</span><br><span class="line">sh-4.2$ i=$[(2+3)*4]</span><br><span class="line">sh-4.2$ echo $h $i</span><br><span class="line">20 20</span><br></pre></td></tr></table></figure>

<h4 id="5、控制台输入"><a href="#5、控制台输入" class="headerlink" title="5、控制台输入"></a>5、控制台输入</h4><p>基本语法：read (选项) (参数)</p>
<table>
<thead>
<tr>
<th>选项值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-p</td>
<td>指定读取值时的提示符</td>
</tr>
<tr>
<td>-t</td>
<td>指定读取值时等待的时间（秒）</td>
</tr>
</tbody></table>
<p>参数：一般为变量，指定读取值的变量名</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sh-4.2$ vim read.sh</span><br><span class="line">sh-4.2$ cat read.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -p &quot;请输入一个数字：&quot; num1</span><br><span class="line">echo &quot;你第一次输入的数字为：$num1&quot;</span><br><span class="line"></span><br><span class="line">read -t 5 -p &quot;请在5秒内输入一个数字：&quot; num2</span><br><span class="line">echo &quot;你第二次输入的数字为：$num2&quot;</span><br><span class="line">sh-4.2$ sh ./read.sh </span><br><span class="line">请输入一个数字：1</span><br><span class="line">你第一次输入的数字为：1</span><br><span class="line">请在5秒内输入一个数字：2</span><br><span class="line">你第二次输入的数字为：2</span><br><span class="line">sh-4.2$ sh ./read.sh </span><br><span class="line">请输入一个数字：1</span><br><span class="line">你第一次输入的数字为：1</span><br><span class="line">请在5秒内输入一个数字：你第二次输入的数字为：</span><br><span class="line">sh-4.2$</span><br></pre></td></tr></table></figure>

<h4 id="6、条件判断"><a href="#6、条件判断" class="headerlink" title="6、条件判断"></a>6、条件判断</h4><p><strong>基本语法</strong>：</p>
<ul>
<li><code>test condition</code></li>
<li><code>[ condition ]</code>（注意condition前后要有空格）</li>
</ul>
<p>注意：条件非空即为true，例如[ atguigu ]与[ 0 ]返回true，[] 返回false</p>
<p><strong>常用判断条件</strong>：</p>
<ul>
<li><p>字符串比较是否相等</p>
<p><code>==</code></p>
</li>
<li><p>整数比较</p>
<p><code>-lt</code>：小于（less than）</p>
<p><code>-le</code>：小于等于（less equal）</p>
<p><code>-eq</code>：等于（equal）</p>
<p><code>-ne</code>：不等于（not equal）</p>
<p><code>-gt</code>：大于（greater than）</p>
<p><code>-ge</code>：大于等于（greater equal）</p>
</li>
<li><p>按照文件权限判断</p>
<p><code>-r</code>：有读的权限（read）</p>
<p><code>-w</code>：有写的权限（write）</p>
<p><code>-x</code>：-x 有执行的权限（execute）</p>
</li>
<li><p>按照文件类型判断</p>
<p><code>-f</code>：文件存在并且是一个常规的文件（file）</p>
<p><code>-d</code>：文件存在并是一个目录（directory）</p>
<p><code>-e</code>：文件存在（existence）</p>
</li>
<li><p>与或非</p>
<p><code>-a</code>、<code>-o</code>、<code>!</code>：在中括号内使用</p>
<p><code>&amp;&amp;</code>、<code>||</code>、<code>!</code>：在中括号外使用，计算多个中括号中的条件判断式</p>
</li>
</ul>
<p>实操：</p>
<ol>
<li><p>判断两个字符相等</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sh-4.2$ [ &quot;sunck&quot; == &quot;sunck&quot; ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure></li>
<li><p>2是否大于等于1</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sh-4.2$ [ 2 -gt 1 ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure></li>
<li><p>helloword.sh是否具有读权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sh-4.2$ [ -r ./helloword.sh ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure></li>
<li><p>/home/atguigu/study/a.txt文件是否存在</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sh-4.2$ [ -e ./a.txt ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">1</span><br><span class="line">sh-4.2$ [ -e ./helloword.sh ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure></li>
<li><p>helloword.sh是普通文件并且可读</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sh-4.2$ [ -f ./helloword.sh -a -r ./helloword.sh ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br><span class="line">sh-4.2$ [ -d ./helloword.sh -a -r ./helloword.sh ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">1</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sh-4.2$ [ -d ./helloword.sh ] &amp;&amp; [ -r ./helloword.sh ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">1</span><br><span class="line">sh-4.2$ [ -f ./helloword.sh ] &amp;&amp; [ -r ./helloword.sh ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="选择判断语句"><a href="#选择判断语句" class="headerlink" title="选择判断语句"></a>选择判断语句</h3><h4 id="1、if-语句"><a href="#1、if-语句" class="headerlink" title="1、if 语句"></a>1、if 语句</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if [ 条件判断式 ]</span><br><span class="line">then</span><br><span class="line">	语句</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>逻辑：当程序执行到if语句时，首先计算”条件判断式“，如果”条件判断式“成立，则执行”语句“，否则结束整个if语句继续向下执行</p>
<p>需求：定义一个变量，如果变量的值大于10则输出“sunck is a good man”</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -p &quot;输入一个数字：&quot; num</span><br><span class="line">if [ $num -gt 10 ]</span><br><span class="line">then</span><br><span class="line">	echo &quot;sunck is a good man&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<h4 id="2、if-else语句"><a href="#2、if-else语句" class="headerlink" title="2、if-else语句"></a>2、if-else语句</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">if [ 条件判断式 ]</span><br><span class="line">then</span><br><span class="line">	语句1</span><br><span class="line">else</span><br><span class="line">	语句2</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>逻辑：当程序执行到if语句时，首先计算”条件判断式“，如果”条件判断式“成立，则执行”语句1“，否则执行“语句2”</p>
<p>需求：定义一个变量，如果变量的值大于10则输出“sunck is a good man”，否则输出“sunck is a nice man”</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -p &quot;输入一个数字：&quot; num</span><br><span class="line">if [ $num -gt 10 ]</span><br><span class="line">then</span><br><span class="line">	echo &quot;sunck is a good man&quot;</span><br><span class="line">else</span><br><span class="line">	echo &quot;sunck is a nice man&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<h4 id="3、if-elif-else语句"><a href="#3、if-elif-else语句" class="headerlink" title="3、if-elif-else语句"></a>3、if-elif-else语句</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">if [ 条件判断式1 ];then</span><br><span class="line">	语句1</span><br><span class="line">elif [ 条件判断式2 ];then</span><br><span class="line">	语句2</span><br><span class="line">elif [ 条件判断式3 ];then</span><br><span class="line">	语句3</span><br><span class="line">……</span><br><span class="line">else</span><br><span class="line">	语句e</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>逻辑：当程序执行到if-elif-else语句时，首先计算“条件判断式1”，如果“条件判断式1”成立则执行“语句1”，否则计算“条件判断式2”。如果“条件判断式2”成立则执行“语句2”，否则计算“条件判断式3”，直到某个“条件判断式”成立为止。如果所有的“条件判断式”都不成立，且有else语句，则执行“语句e”，否则结束整个if-elif-else语句继续向下运行</p>
<p>需求：定义一个age变量，如果age的值小于等于0则输出“age有误”，如果age的值大于0小于等于3则输出“婴儿”，如果age的值大于3小于等于6则输出“幼儿”，如果age的值大于6小于等于12则输出“童年”，如果age的值大于12小于等于18则输出“少年”，如果age的值大于18小于等于30则输出“青年”，如果age的值大于30小于等于40则输出“壮年”，如果age的值大于40小于等于50则输出“中年”，如果age的值大于50小于等于150则输出“老年”，其余则输出“妖怪”</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -p &quot;输入一个数字：&quot; age</span><br><span class="line">if [ $age -le 0 ];then</span><br><span class="line">	echo &quot;age有误！&quot;</span><br><span class="line">elif [ $age -gt 0 ] &amp;&amp; [ $age -le 3 ];then</span><br><span class="line">	echo &quot;婴儿&quot;</span><br><span class="line">elif [ $age -gt 3 ] &amp;&amp; [ $age -le 6 ];then</span><br><span class="line">	echo &quot;幼儿&quot;</span><br><span class="line">elif [ $age -gt 6 ] &amp;&amp; [ $age -le 12 ];then</span><br><span class="line">	echo &quot;童年&quot;</span><br><span class="line">elif [ $age -gt 12 ] &amp;&amp; [ $age -le 18 ];then</span><br><span class="line">	echo &quot;少年&quot;</span><br><span class="line">elif [ $age -gt 18 ] &amp;&amp; [ $age -le 30 ];then</span><br><span class="line">	echo &quot;青年&quot;</span><br><span class="line">elif [ $age -gt 30 ] &amp;&amp; [ $age -le 40 ];then</span><br><span class="line">	echo &quot;壮年&quot;</span><br><span class="line">elif [ $age -gt 40 ] &amp;&amp; [ $age -le 50 ];then</span><br><span class="line">	echo &quot;中年&quot;</span><br><span class="line">elif [ $age -gt 50 ] &amp;&amp; [ $age -le 150 ];then</span><br><span class="line">	echo &quot;老年&quot;</span><br><span class="line">else</span><br><span class="line">	echo &quot;妖怪&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>精髓：每一个else都是对它上面所有表达式的否定</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -p &quot;输入一个数字：&quot; age</span><br><span class="line">if [ $age -le 0 ];then</span><br><span class="line">	echo &quot;age有误！&quot;</span><br><span class="line">elif [ $age -le 3 ];then</span><br><span class="line">	echo &quot;婴儿&quot;</span><br><span class="line">elif [ $age -le 6 ];then</span><br><span class="line">	echo &quot;幼儿&quot;</span><br><span class="line">elif [ $age -le 12 ];then</span><br><span class="line">	echo &quot;童年&quot;</span><br><span class="line">elif [ $age -le 18 ];then</span><br><span class="line">	echo &quot;少年&quot;</span><br><span class="line">elif [ $age -le 30 ];then</span><br><span class="line">	echo &quot;青年&quot;</span><br><span class="line">elif [ $age -le 40 ];then</span><br><span class="line">	echo &quot;壮年&quot;</span><br><span class="line">elif [ $age -le 50 ];then</span><br><span class="line">	echo &quot;中年&quot;</span><br><span class="line">elif [ $age -le 150 ];then</span><br><span class="line">	echo &quot;老年&quot;</span><br><span class="line">else</span><br><span class="line">	echo &quot;妖怪&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<h4 id="4、case语句"><a href="#4、case语句" class="headerlink" title="4、case语句"></a>4、case语句</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">case $变量名 in</span><br><span class="line">&quot;值1&quot;)</span><br><span class="line">	语句1</span><br><span class="line">	;;</span><br><span class="line">&quot;值2&quot;)</span><br><span class="line">	语句2</span><br><span class="line">	;;</span><br><span class="line">……</span><br><span class="line">*)</span><br><span class="line">	语句*</span><br><span class="line">	;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>逻辑：当程序执行到case语句时，匹配“变量”的值，匹配上哪个“值”就执行对应的“语句”，执行完“语句”后结束整个case变量。如果没有匹配的“值”则执行“语句*”</p>
<ol>
<li>case行尾必须为单词“in”，每一个模式匹配必须以右括号“）”结束</li>
<li>双分号“**;;**”表示命令序列结束，相当于java中的break</li>
<li>最后的“*）”表示默认模式，相当于java中的default</li>
</ol>
<p>需求：执行脚本时输入一个数字，输入数字1则打印“星期1”，输入数字2则打印“星期2”，输入数字3则打印“星期3”，输入数字4则打印“星期4”，输入数字5则打印“星期5”，输入数字6则打印“星期6”，输入数字7则打印“星期7”，其他输出“输入有误”</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -p &quot;请输入一个数字：&quot; num</span><br><span class="line"></span><br><span class="line">case $num in</span><br><span class="line">&quot;1&quot;)</span><br><span class="line">	echo &quot;星期1&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;2&quot;)</span><br><span class="line">	echo &quot;星期2&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;3&quot;)</span><br><span class="line">	echo &quot;星期3&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;4&quot;)</span><br><span class="line">	echo &quot;星期4&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;5&quot;)</span><br><span class="line">	echo &quot;星期5&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;6&quot;)</span><br><span class="line">	echo &quot;星期6&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;7&quot;)</span><br><span class="line">	echo &quot;星期7&quot;</span><br><span class="line">	;;</span><br><span class="line">*)</span><br><span class="line">	echo &quot;输入有误&quot;</span><br><span class="line">	;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<h3 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h3><h4 id="1、while循环"><a href="#1、while循环" class="headerlink" title="1、while循环"></a>1、while循环</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">while [ 条件判断式 ]</span><br><span class="line">do</span><br><span class="line">	语句</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>逻辑：当程序执行到while语句时，首先计算“条件判断式”的值，如果“条件判断式”不成立则结束整个while语句继续向下执行，如果“条件判断式”成立则执行“语句”，执行完“语句”再去计算“条件判断式”的值。如果“条件判断式”不成立则结束整个while语句继续向下执行，如果“条件判断式”成立则执行“语句”，执行完“语句”再去计算“条件判断式”的值。如此循环往复，直到“条件判断式”不成立才终止while语句。</p>
<p>需求：计算1加到100的和</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">ret=0</span><br><span class="line">index=1</span><br><span class="line"></span><br><span class="line">while [ $index -le 100 ]</span><br><span class="line">do</span><br><span class="line">	ret=$[$ret+$index]</span><br><span class="line">	index=$[$index+1]</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">echo &quot;1加到100的和为：$ret&quot;</span><br></pre></td></tr></table></figure>

<h4 id="2、for语句"><a href="#2、for语句" class="headerlink" title="2、for语句"></a>2、for语句</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for ((变量初始化;循环控制条件;变量变化))</span><br><span class="line">do</span><br><span class="line">	语句</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>逻辑：当程序执行到for语句时，首先初始化一个“变量”的值，再去计算”循环控制条件”，如果“循环控制条件”不成立则结束整个for语句继续向下运行，如果“循环控制条件”成立则执行“语句”，执行完“语句”在执行“变量变化”修改“变量的值”，再去计算“循环控制条件”。如此循环往复，直到“循环控制条件”不成立才停止整个for语句。</p>
<p>需求：计算1加到100的和</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">ret=0</span><br><span class="line">for ((i=0;i&lt;=100;i++))</span><br><span class="line">do</span><br><span class="line">	ret=$[$ret+$i]</span><br><span class="line">done</span><br><span class="line">echo &quot;1加到100的和为：$ret&quot;</span><br></pre></td></tr></table></figure>

<p>注意：只有在for里可以这么写</p>
<h4 id="3、for-in语句"><a href="#3、for-in语句" class="headerlink" title="3、for-in语句"></a>3、for-in语句</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for 变量 in 计算值1 计算值2 计算值3 ……</span><br><span class="line">do</span><br><span class="line">	语句</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>逻辑：按顺序便利in后面的所有“计算值”</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">a=1</span><br><span class="line">b=2</span><br><span class="line"></span><br><span class="line">for i in &#x27;good&#x27; &#x27;cool&#x27; $a $b $[1+2]</span><br><span class="line">do</span><br><span class="line">	echo &quot;i的值为：$i&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<h4 id="4、-与"><a href="#4、-与" class="headerlink" title="4、$*与$@"></a>4、$*与$@</h4><p>$*和$@都表示传递给函数或脚本的所有参数，不被双引号“”包含时，都以$1 $2 …$n的形式输出所有参数。当它们被双引号“”包含时，“$*”会将所有的参数作为一个整体，以“$1 $2 …$n”的形式输出所有参数；“$@”会将各个参数分开，以“$1” “$2”…”$n”的形式输出所有参数</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &#x27;------------$*--------------&#x27;</span><br><span class="line">for i in $*</span><br><span class="line">do</span><br><span class="line">	echo &quot;i的值为：$i&quot;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">echo &#x27;------------$@--------------&#x27;</span><br><span class="line">for j in $@</span><br><span class="line">do</span><br><span class="line">	echo &quot;j的值为：$i&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sh-4.2$ sh ./test.sh 1 2 3</span><br><span class="line"><span class="meta">------------$</span><span class="bash">*--------------</span></span><br><span class="line">i的值为：1</span><br><span class="line">i的值为：2</span><br><span class="line">i的值为：3</span><br><span class="line"><span class="meta">------------$</span><span class="bash">@--------------</span></span><br><span class="line">j的值为：3</span><br><span class="line">j的值为：3</span><br><span class="line">j的值为：3</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &#x27;------------$*--------------&#x27;</span><br><span class="line">for i in &quot;$*&quot;</span><br><span class="line">do</span><br><span class="line">	echo &quot;i的值为：$i&quot;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">echo &#x27;------------$@--------------&#x27;</span><br><span class="line">for j in &quot;$@&quot;</span><br><span class="line">do</span><br><span class="line">	echo &quot;j的值为：$i&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sh-4.2$ sh ./test.sh 1 2 3</span><br><span class="line"><span class="meta">------------$</span><span class="bash">*--------------</span></span><br><span class="line">i的值为：1 2 3</span><br><span class="line"><span class="meta">------------$</span><span class="bash">@--------------</span></span><br><span class="line">j的值为：1 2 3</span><br><span class="line">j的值为：1 2 3</span><br><span class="line">j的值为：1 2 3</span><br></pre></td></tr></table></figure>

<h4 id="5、break与continue"><a href="#5、break与continue" class="headerlink" title="5、break与continue"></a>5、break与continue</h4><p>break：调出距离最近的那一层循环</p>
<p>continue：跳过距离最近的那一层循环，继续下一次循环继</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">echo &quot;----------------break-------------------&quot;</span><br><span class="line">for ((i=1;i&lt;=10;i++))</span><br><span class="line">do</span><br><span class="line">	if [ $i -eq 5 ]</span><br><span class="line">	then</span><br><span class="line">		break</span><br><span class="line">	fi</span><br><span class="line">	echo &quot;i的值为：$i&quot;</span><br><span class="line">done</span><br><span class="line">echo &quot;----------------continue-------------------&quot;</span><br><span class="line">for ((j=1;j&lt;=10;j++))</span><br><span class="line">do</span><br><span class="line">	if [ $j -eq 5 ]</span><br><span class="line">	then</span><br><span class="line">		continue</span><br><span class="line">	fi</span><br><span class="line">	echo &quot;j的值为：$j&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sh-4.2$ sh ./test.sh </span><br><span class="line">----------------break-------------------</span><br><span class="line">i的值为：1</span><br><span class="line">i的值为：2</span><br><span class="line">i的值为：3</span><br><span class="line">i的值为：4</span><br><span class="line">----------------continue-------------------</span><br><span class="line">j的值为：1</span><br><span class="line">j的值为：2</span><br><span class="line">j的值为：3</span><br><span class="line">j的值为：4</span><br><span class="line">j的值为：6</span><br><span class="line">j的值为：7</span><br><span class="line">j的值为：8</span><br><span class="line">j的值为：9</span><br><span class="line">j的值为：10</span><br></pre></td></tr></table></figure>

<h3 id="系统函数"><a href="#系统函数" class="headerlink" title="系统函数"></a>系统函数</h3><h4 id="1、basename"><a href="#1、basename" class="headerlink" title="1、basename"></a>1、basename</h4><p>基本语法：<code>basename [string/pathname] [suffix]</code></p>
<p>功能描述：basename命令会删掉所有的前缀包括最后一个（‘/’）字符，然后将字符串显示出来</p>
<p>选项suffix：如果suffix被指定了，basename会将pathname或string中的suffix去掉</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> basename /a/b/c/d/e.txt</span><br><span class="line">e.txt</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> basename /a/b/c/d/e.txt txt</span><br><span class="line">e.</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> basename /a/b/c/d/e.txt .txt</span><br><span class="line">e</span><br></pre></td></tr></table></figure>

<h4 id="2、dirname"><a href="#2、dirname" class="headerlink" title="2、dirname"></a>2、dirname</h4><p>基本语法：<code>dirname 文件绝对路径</code></p>
<p>功能描述：从给定的包含绝对路径的文件名中去除文件名（非目录的部分），然后返回剩下的路径（目录的部分）</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> dirname /a/b/c/d/e.txt</span><br><span class="line">/a/b/c/d</span><br></pre></td></tr></table></figure>

<h3 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h3><h4 id="1、定义"><a href="#1、定义" class="headerlink" title="1、定义"></a>1、定义</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[function] funname [()]</span><br><span class="line">&#123;</span><br><span class="line">    action;</span><br><span class="line"></span><br><span class="line">    [return int;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>结构</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>function</td>
<td>说明定义函数，可以省略</td>
</tr>
<tr>
<td>funname</td>
<td>自定义函数名，遵循标识符规则</td>
</tr>
<tr>
<td>()</td>
<td>可以省略，和function不能同时省略</td>
</tr>
<tr>
<td>action</td>
<td>封装的功能</td>
</tr>
<tr>
<td>return int</td>
<td>函数返回值，只能通过$?系统变量来获取，可以显示添加。如果不添加，将以最后一条命令运行的结果作为返回值。注意return后的数值在[0~255]之间，如果超过255，将返回该值与256的余数。</td>
</tr>
<tr>
<td>;</td>
<td>语句结束标志，可以不写</td>
</tr>
</tbody></table>
<h4 id="2、调用"><a href="#2、调用" class="headerlink" title="2、调用"></a>2、调用</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">funname [……]</span><br><span class="line">echo &quot;函数的返回值为：$?&quot;</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>结果</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>funname</td>
<td>要调用的函数名</td>
</tr>
<tr>
<td>……</td>
<td>传递参数，如果无需传参即省略</td>
</tr>
<tr>
<td>$?</td>
<td>获取函数的返回值</td>
</tr>
<tr>
<td>“”</td>
<td>需要使用双引号，可以使用特殊变量</td>
</tr>
</tbody></table>
<p>注意：要在定义函数后再调用函数</p>
<h4 id="3、无参无返回值的函数"><a href="#3、无参无返回值的函数" class="headerlink" title="3、无参无返回值的函数"></a>3、无参无返回值的函数</h4><p>需求：打印“hello word”</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 定义函数</span></span><br><span class="line">function func()</span><br><span class="line">&#123;</span><br><span class="line">	echo &quot;hello word&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo &quot;-----函数开始执行------&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 调用函数</span></span><br><span class="line">func</span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取函数的返回值的值</span></span><br><span class="line">echo &quot;返回值为：$?&quot;</span><br><span class="line">echo &quot;-----函数执行完毕------&quot;</span><br></pre></td></tr></table></figure>

<h4 id="4、使用参数"><a href="#4、使用参数" class="headerlink" title="4、使用参数"></a>4、使用参数</h4><p>需求：计算两个数据的和</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">function func()</span><br><span class="line">&#123;</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 使用传递过来的参数值</span></span><br><span class="line">	ret=$[$1+$2]</span><br><span class="line">	echo &quot;$1+$2的结果为：$ret&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo &quot;-----函数开始执行------&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 调用函数，传递参数</span></span><br><span class="line">func 3 5</span><br><span class="line">echo &quot;返回值为：$?&quot;</span><br><span class="line">echo &quot;-----函数执行完毕------&quot;</span><br></pre></td></tr></table></figure>

<h4 id="5、返回值"><a href="#5、返回值" class="headerlink" title="5、返回值"></a>5、返回值</h4><p>说明：return后的数值在[0~255]之间，如果超过255，将返回该值与256的余数</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">function func()</span><br><span class="line">&#123;</span><br><span class="line">	ret=$[$1+$2]</span><br><span class="line">	return $ret</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo &quot;-----函数开始执行------&quot;</span><br><span class="line">func 3 5</span><br><span class="line">echo &quot;返回值为：$?&quot;</span><br><span class="line">echo &quot;-----函数执行完毕------&quot;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>需求：在执行脚本时传递两个数字求和</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">function func()</span><br><span class="line">&#123;</span><br><span class="line">	ret=$[$1+$2]</span><br><span class="line">	return $ret</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo &quot;-----函数开始执行------&quot;</span><br><span class="line">func $1 $2</span><br><span class="line">echo &quot;返回值为：$?&quot;</span><br><span class="line">echo &quot;-----函数执行完毕------&quot;</span><br></pre></td></tr></table></figure></li>
<li><p>需求：在执行脚本后从控制台输入两个数字求和</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">function func()</span><br><span class="line">&#123;</span><br><span class="line">	ret=$[$1+$2]</span><br><span class="line">	return $ret</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo &quot;-----函数开始执行------&quot;</span><br><span class="line">read -p &#x27;请输入第一个数字：&#x27; num1</span><br><span class="line">read -p &#x27;请输入第二个数字：&#x27; num2</span><br><span class="line">func $num1 $num2</span><br><span class="line">echo &quot;返回值为：$?&quot;</span><br><span class="line">echo &quot;-----函数执行完毕------&quot;</span><br></pre></td></tr></table></figure></li>
<li><p>返回值超过255，但是就想使用真实结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">function func()</span><br><span class="line">&#123;</span><br><span class="line">	ret=$[$1+$2]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo &quot;-----函数开始执行------&quot;</span><br><span class="line">read -p &#x27;请输入第一个数字：&#x27; num1</span><br><span class="line">read -p &#x27;请输入第二个数字：&#x27; num2</span><br><span class="line">func $num1 $num2</span><br><span class="line">echo &quot;结果为：$ret&quot;</span><br><span class="line">echo &quot;-----函数执行完毕------&quot;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h3><h4 id="1、常规匹配"><a href="#1、常规匹配" class="headerlink" title="1、常规匹配"></a>1、常规匹配</h4><p>一串不包含特殊字符的正则表达式匹配它自己</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ cat test.txt </span><br><span class="line">sunck is a good man</span><br><span class="line">kaige is a good man</span><br><span class="line">sunyz is a niee man</span><br><span class="line"></span><br><span class="line">sunck is a nice man!</span><br><span class="line"></span><br><span class="line">kaishen is a cool man!</span><br><span class="line">sunds is a handsome man</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep sunck</span><br><span class="line">sunck is a good man</span><br><span class="line">sunck is a nice man!</span><br></pre></td></tr></table></figure>

<p>查看test.txt文件中包含”sunck”的所有的行，匹配规则就是”sunck”</p>
<p>问题：匹配规则单一</p>
<h4 id="2、匹配单个字符与数字"><a href="#2、匹配单个字符与数字" class="headerlink" title="2、匹配单个字符与数字"></a>2、匹配单个字符与数字</h4><table>
<thead>
<tr>
<th>匹配</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.</td>
<td>匹配任意字符</td>
</tr>
<tr>
<td>[]</td>
<td>里面是字符集合，匹配[]里任意一个字符</td>
</tr>
<tr>
<td>[0123456789]</td>
<td>匹配任意一个数字字符</td>
</tr>
<tr>
<td>[0-9]</td>
<td>匹配任意一个数字字符</td>
</tr>
<tr>
<td>[a-z]</td>
<td>匹配任意一个小写英文字母字符</td>
</tr>
<tr>
<td>[A-Z]</td>
<td>匹配任意一个大写英文字母字符</td>
</tr>
<tr>
<td>[A-Za-z]</td>
<td>匹配任意一个英文字母字符</td>
</tr>
<tr>
<td>[A-Za-z0-9]</td>
<td>匹配任意一个数字或英文字母字符</td>
</tr>
<tr>
<td>[^sunck]</td>
<td>[]里的^称为脱字符，表示非，匹配不在[]内的任意一个字符</td>
</tr>
<tr>
<td>\d</td>
<td>匹配任意一个数字字符，相当于[0-9]</td>
</tr>
<tr>
<td>\D</td>
<td>匹配任意一个非数字字符，相当于<code>[^0-9]</code></td>
</tr>
<tr>
<td>\w</td>
<td>匹配字母、下划线、数字中的任意一个字符，相当于[0-9A-Za-z_]</td>
</tr>
<tr>
<td>\W</td>
<td>匹配非字母、下划线、数字中的任意一个字符，相当于<code>[^0-9A-Za-z_]</code></td>
</tr>
<tr>
<td>\s</td>
<td>匹配空白符(空格、换页、换行、回车、制表)，相当于[ \f\n\r\t]</td>
</tr>
<tr>
<td>\S</td>
<td>匹配非空白符(空格、换页、换行、回车、制表)，相当于<code>[^ \f\n\r\t]</code></td>
</tr>
</tbody></table>
<p>注意：在使用\w、\W、\s、\S时需要使用引号包裹，最好使用单引号</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;sunck.www.wang.wwwqqww1fww\nffwweeeww3ssssww gege&quot; | grep ww.</span><br><span class="line"></span><br><span class="line">echo &quot;sunck.www.wang.wwwqqww1fww\nffwweeeww3ssssww gege&quot; | grep ww[^0-9]</span><br><span class="line"></span><br><span class="line">echo &quot;sunck.www.wang.wwwqqww1fww\nffwweeeww3ssssww gege&quot; | grep &#x27;ww\w&#x27;</span><br><span class="line"></span><br><span class="line">echo &quot;sunck.www.wang.wwwqqww1fww\nffwweeeww3ssssww gege&quot; | grep &#x27;ww\s&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="3、匹配锚字符"><a href="#3、匹配锚字符" class="headerlink" title="3、匹配锚字符"></a>3、匹配锚字符</h4><table>
<thead>
<tr>
<th>匹配</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>^</td>
<td>行首匹配，和[]里的^不是一个意思</td>
</tr>
<tr>
<td>$</td>
<td>行尾匹配</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ cat test.txt </span><br><span class="line">sunck is a good man</span><br><span class="line">kaige is a good man</span><br><span class="line">sunyz is a niee man</span><br><span class="line"></span><br><span class="line">sunck is a nice man!</span><br><span class="line"></span><br><span class="line">kaishen is a cool man!</span><br><span class="line">sunds is a handsome man</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep ^sun</span><br><span class="line">sunck is a good man</span><br><span class="line">sunyz is a niee man</span><br><span class="line">sunck is a nice man!</span><br><span class="line">sunds is a handsome man</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep ^good</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep man$</span><br><span class="line">sunck is a good man</span><br><span class="line">kaige is a good man</span><br><span class="line">sunyz is a niee man</span><br><span class="line">sunds is a handsome man</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep ^$</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[atguigu@centos7_base study]$</span><br></pre></td></tr></table></figure>

<h4 id="4、匹配边界字符"><a href="#4、匹配边界字符" class="headerlink" title="4、匹配边界字符"></a>4、匹配边界字符</h4><table>
<thead>
<tr>
<th>匹配</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>\b</td>
<td>匹配一个单词的边界，指单词和空格的位置</td>
</tr>
<tr>
<td>\B</td>
<td>匹配非单词边界</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ cat test.txt </span><br><span class="line">sunck is a good man!</span><br><span class="line">ack is a nice ma</span><br><span class="line">sfeck,nckab</span><br><span class="line">aaackge</span><br><span class="line">bbbck geg</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep &#x27;ck\b&#x27;</span><br><span class="line">sunck is a good man!</span><br><span class="line">ack is a nice ma</span><br><span class="line">sfeck,nckab</span><br><span class="line">bbbck geg</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep &#x27;ck\B&#x27;</span><br><span class="line">sfeck,nckab</span><br><span class="line">aaackge</span><br></pre></td></tr></table></figure>

<h4 id="5、匹配多个字符"><a href="#5、匹配多个字符" class="headerlink" title="5、匹配多个字符"></a>5、匹配多个字符</h4><table>
<thead>
<tr>
<th>匹配</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>(xyz)</td>
<td>匹配括号内的xyz，作为一个整体去匹配</td>
</tr>
<tr>
<td>x?</td>
<td>匹配0个或者1个x，非贪婪匹配</td>
</tr>
<tr>
<td>x*</td>
<td>匹配0个或任意多个x</td>
</tr>
<tr>
<td>x+</td>
<td>匹配至少一个x</td>
</tr>
<tr>
<td>x{n}</td>
<td>确定匹配n个x，n是非负数</td>
</tr>
<tr>
<td>x{n,}</td>
<td>至少匹配n个x</td>
</tr>
<tr>
<td>x{n,m}</td>
<td>匹配至少n个最多m个x</td>
</tr>
<tr>
<td>x|y</td>
<td>|表示或的意思，匹配x或y</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;11111a2222aa3333aaa44444aaaa55555&quot; | grep &#x27;a*&#x27;</span><br><span class="line"></span><br><span class="line">echo &quot;12340567085465046567&quot; | grep &#x27;[0-9]*0&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="6、转义字符"><a href="#6、转义字符" class="headerlink" title="6、转义字符"></a>6、转义字符</h4><p><code>\ </code>表示转义，并不会单独使用。由于所有特殊字符都有其特定匹配模式，当我们想匹配某一特殊字符本身时（例如，我想找出所有包含 ‘$’ 的行），就会碰到困难。此时我们就要将转义字符和特殊字符连用，来表示特殊字符本身</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ cat test.txt </span><br><span class="line">sunck is a go$od man</span><br><span class="line">kaige is a goaod man</span><br><span class="line">sunyz is a nice mano</span><br><span class="line">sunck is a nice man!</span><br><span class="line">kaishen is a co$ol man!</span><br><span class="line">sunds is a handsome man</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep o$</span><br><span class="line">sunyz is a nice mano</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep o\$</span><br><span class="line">sunyz is a nice mano</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep &quot;o\$&quot;</span><br><span class="line">sunyz is a nice mano</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep &#x27;o\$&#x27;</span><br><span class="line">sunck is a go$od man</span><br><span class="line">kaishen is a co$ol man!</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep &#x27;o$&#x27;</span><br><span class="line">sunyz is a nice mano</span><br><span class="line">[atguigu@centos7_base study]$</span><br></pre></td></tr></table></figure>

<h4 id="7、常用正则表达式"><a href="#7、常用正则表达式" class="headerlink" title="7、常用正则表达式"></a>7、常用正则表达式</h4><p>详见《正则匹配示例.docx》</p>
<h3 id="shell工具"><a href="#shell工具" class="headerlink" title="shell工具"></a>shell工具</h3><p>ETL工程师做的一些数据清洗的工作，我们可以使用cut、awk、sort来对一些基本数据进行处理</p>
<h4 id="1、cut"><a href="#1、cut" class="headerlink" title="1、cut"></a>1、cut</h4><p>cut的工作就是“剪”，具体的说就是在文件中负责剪切数据用的。cut 命令从文件的每一行剪切字节、字符和字段并将这些字节、字符和字段输出</p>
<p>格式：cut [选项参数]  filename</p>
<p>说明：默认分隔符是制表符</p>
<table>
<thead>
<tr>
<th>选项参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-f</td>
<td>列号，提取第几列</td>
</tr>
<tr>
<td>-d</td>
<td>分隔符，按照指定分隔符分割列</td>
</tr>
<tr>
<td>-c</td>
<td>指定具体的字符</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">de hua good man</span><br><span class="line">hui mei nice women</span><br><span class="line">xue you cool man</span><br><span class="line">ruo tong good women</span><br><span class="line">dao lang cool man</span><br></pre></td></tr></table></figure>

<ul>
<li><p>切割cut.txt文件第一列</p>
<p><code>cut -d &quot; &quot; -f 1 cut.txt</code></p>
</li>
<li><p>切割cut.txt文件第二、三列</p>
<p><code>cut -d &quot; &quot; -f 2,3 cut.txt</code></p>
</li>
<li><p>切割cut.txt文件第二到四列</p>
<p><code>cut -d &quot; &quot; -f 2,3,4 cut.txt</code></p>
<p><code>cut -d &quot; &quot; -f 2-4 cut.txt</code></p>
</li>
<li><p>切割cut.txt文件第二到最后列</p>
<p><code>cut -d &quot; &quot; -f 2- cut.txt</code></p>
</li>
<li><p>在cut.txt文件中切割出hua</p>
<p><code>cat cut.txt | grep hua | cut -d &quot; &quot; -f 2</code></p>
</li>
<li><p>选取系统PATH变量值，第二个”:”开始后的所有路径</p>
<p><code>echo $PATH | cut -d : -f 3-</code></p>
</li>
<li><p>切割ifconfig后打印的IP地址</p>
<p><code>ifconfig | grep &quot;netmask&quot; | cut -d i -f 2- | cut -d &quot; &quot; -f 2</code></p>
</li>
</ul>
<h4 id="2、awk"><a href="#2、awk" class="headerlink" title="2、awk"></a>2、awk</h4><p>作用：一个强大的文本分析工具，把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行分析处理</p>
<p>语法：awk [选项参数] ‘pattern1{action1} pattern2{action2}…’ filename</p>
<table>
<thead>
<tr>
<th>结构</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>pattern</td>
<td>表示AWK在数据中查找的内容，就是匹配模式</td>
</tr>
<tr>
<td>action</td>
<td>在找到匹配内容时所执行的一系列命令</td>
</tr>
<tr>
<td>filename</td>
<td>文件路径</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>选项参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-F</td>
<td>指定输入文件折分隔符</td>
</tr>
<tr>
<td>-v</td>
<td>赋值一个用户定义变量</td>
</tr>
</tbody></table>
<ul>
<li><p>搜索passwd文件以root关键字开头的所有行，并输出该行的第7列<br><code>awk -F : &#39;/^root/&#123;print $7&#125;&#39; passwd.txt</code></p>
</li>
<li><p>搜索passwd文件以root关键字开头的所有行，并输出该行的第1列和第7列，中间以“，”号分割</p>
<p><code>awk -F : &#39;/^root/&#123;print $1 &quot;,&quot; $7&#125;&#39; passwd.txt</code></p>
</li>
<li><p>只显示/etc/passwd的第一列和第七列，以逗号分割，且在所有行前面添加列名user，shell在最后一行添加”dahaige，/bin/zuishuai”</p>
<p><code>awk -F : &#39;BEGIN&#123;print &quot;user,shell&quot;&#125;&#123;print $1 &quot;,&quot; $7&#125;END&#123;print &quot;sunck,/bin/bash&quot;&#125;&#39; passwd.txt</code></p>
</li>
<li><p>将passwd文件中的用户id增加数值1并输出</p>
<p><code>awk -v i=1 -F : &#39;&#123;print $3+i &quot;,&quot; $4&#125;&#39; passwd.txt</code></p>
</li>
</ul>
<table>
<thead>
<tr>
<th>awk内置变量</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>FILENAME</td>
<td>文件名</td>
</tr>
<tr>
<td>NR</td>
<td>已读的记录数（行数）</td>
</tr>
<tr>
<td>NF</td>
<td>浏览记录的域的个数（切割后，列的个数）</td>
</tr>
</tbody></table>
<ul>
<li><p>统计passwd文件名，每行的行号，每行的列数</p>
<p><code>awk -F : &#39;&#123;print &quot;filename:&quot; FILENAME &quot;,linenumber:&quot; NR &quot;,columns:&quot; NF&#125;&#39; passwd.txt</code></p>
</li>
<li><p>切割IP</p>
<p><code>ifconfig | grep netmask | awk -F &quot;inet &quot; &#39;&#123;print $2&#125;&#39; | awk -F &quot; &quot; &#39;&#123;print $1&#125;&#39;</code></p>
</li>
<li><p>查询cut.txt中空行所在的行号</p>
<p><code>awk &#39;/^$/&#123;print NR&#125;&#39; cut.txt</code></p>
</li>
</ul>
<h4 id="3、sort"><a href="#3、sort" class="headerlink" title="3、sort"></a>3、sort</h4><p>作用：sort命令是在Linux里非常有用，它将文件进行排序，并将排序结果标准输出</p>
<p>格式：sort (选项) (参数)</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-n</td>
<td>依照数值的大小排序</td>
</tr>
<tr>
<td>-r</td>
<td>以相反的顺序来排序</td>
</tr>
<tr>
<td>-t</td>
<td>设置排序时所用的分隔字符</td>
</tr>
<tr>
<td>-k</td>
<td>指定需要排序的列</td>
</tr>
</tbody></table>
<p>参数：指定待排序的文件列表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ cat sort.txt </span><br><span class="line">bb:40:5.4</span><br><span class="line">bd:20:4.2</span><br><span class="line">xz:50:2.3</span><br><span class="line">cls:10:3.5</span><br><span class="line">ss:30:1.6</span><br></pre></td></tr></table></figure>

<p>需求：按照“：”分割后的第三列倒序排序</p>
<p><code>sort -t : -nrk 3 sort.txt</code></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MySQL"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/11/07/MySQL/"
    >Mysql安装</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/11/07/MySQL/" class="article-date">
  <time datetime="2021-11-07T00:08:41.000Z" itemprop="datePublished">2021-11-07</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/MySQL/">MySQL</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Mysql安装"><a href="#Mysql安装" class="headerlink" title="Mysql安装"></a>Mysql安装</h1><h2 id="一、卸载Mysql"><a href="#一、卸载Mysql" class="headerlink" title="一、卸载Mysql"></a>一、卸载Mysql</h2><h4 id="1-查询Linux上是否安装Mariadb"><a href="#1-查询Linux上是否安装Mariadb" class="headerlink" title="1. 查询Linux上是否安装Mariadb"></a>1. 查询Linux上是否安装Mariadb</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@mysql ~]# rpm -qa | grep maria</span><br><span class="line">mariadb-libs-5.5.68-1.el7.x86_64</span><br><span class="line">[root@mysql ~]# rpm -e --nodeps mariadb-libs-5.5.68-1.el7.x86_64</span><br><span class="line">[root@mysql ~]# rpm -qa | grep maria</span><br><span class="line">[root@mysql ~]#</span><br></pre></td></tr></table></figure>

<h5 id="2-如果安装过需要卸载Mysql-一步到位"><a href="#2-如果安装过需要卸载Mysql-一步到位" class="headerlink" title="2. 如果安装过需要卸载Mysql 一步到位"></a>2. 如果安装过需要卸载Mysql 一步到位</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo rpm -qa | grep mysql | xargs -n2 sudo rpm -e --nodeps</span><br></pre></td></tr></table></figure>

<h5 id="3-查看-etc-my-cnf-文件，其中-datadir-var-lib-mysql-指向的目录下内容全部删除"><a href="#3-查看-etc-my-cnf-文件，其中-datadir-var-lib-mysql-指向的目录下内容全部删除" class="headerlink" title="3.查看 /etc/my.cnf 文件，其中 datadir=/var/lib/mysql 指向的目录下内容全部删除"></a>3.查看 /etc/my.cnf 文件，其中 datadir=/var/lib/mysql 指向的目录下内容全部删除</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo rm -rf /var/lib/mysql/*</span><br></pre></td></tr></table></figure>

<p>注意：如果首次安装MySQL，此目录下没有内容！而且如果有内容有可能普通用户即便加上sudo 也无法删除，那就切换root用户删除！！！</p>
<h2 id="二、安装MySql"><a href="#二、安装MySql" class="headerlink" title="二、安装MySql"></a>二、安装MySql</h2><h5 id="1-把Mysql安装包上传到Linux指定目录"><a href="#1-把Mysql安装包上传到Linux指定目录" class="headerlink" title="1.把Mysql安装包上传到Linux指定目录"></a>1.把Mysql安装包上传到Linux指定目录</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@mysql software]<span class="comment"># pwd</span></span><br><span class="line">/opt/software</span><br><span class="line">[root@mysql software]<span class="comment"># ls</span></span><br><span class="line">jdk-8u212-linux-x64.tar.gz  mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar</span><br><span class="line">[root@mysql software]<span class="comment">#</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="2-将Mysql安装包进行解包操作"><a href="#2-将Mysql安装包进行解包操作" class="headerlink" title="2. 将Mysql安装包进行解包操作"></a>2. 将Mysql安装包进行解包操作</h5><p>​注意：由于Mysql解包后会有多个文件，为了方便管理最好新建一个文件夹放入</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@mysql software]# mkdir mysql</span><br><span class="line">[root@mysql software]# tar -xvf mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar -C mysql</span><br><span class="line">mysql-community-embedded-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-embedded-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-libs-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-test-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-common-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-embedded-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-client-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-server-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">[root@mysql software]# ll mysql/</span><br><span class="line">total 595272</span><br><span class="line">-rw-r--r--. 1 7155 31415  45109364 Sep 30  2019 mysql-community-client-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415    318768 Sep 30  2019 mysql-community-common-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415   7037096 Sep 30  2019 mysql-community-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415  49329100 Sep 30  2019 mysql-community-embedded-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415  23354908 Sep 30  2019 mysql-community-embedded-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415 136837816 Sep 30  2019 mysql-community-embedded-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415   4374364 Sep 30  2019 mysql-community-libs-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415   1353312 Sep 30  2019 mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415 208694824 Sep 30  2019 mysql-community-server-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415 133129992 Sep 30  2019 mysql-community-test-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">[root@mysql software]#</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>注意：mysql解包后的所有rpm并非都要安装 我们可以把需要安装的通过改名的方式编号，方便后续的顺序安装，严格安装编号的顺序安装</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@mysql mysql]<span class="comment"># ll</span></span><br><span class="line">total 595272</span><br><span class="line">-rw-r--r--. 1 7155 31415    318768 Sep 30  2019 01-mysql-community-common-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415   4374364 Sep 30  2019 02-mysql-community-libs-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415   1353312 Sep 30  2019 03-mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415  45109364 Sep 30  2019 04-mysql-community-client-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415 208694824 Sep 30  2019 05-mysql-community-server-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415   7037096 Sep 30  2019 mysql-community-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415  49329100 Sep 30  2019 mysql-community-embedded-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415  23354908 Sep 30  2019 mysql-community-embedded-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415 136837816 Sep 30  2019 mysql-community-embedded-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415 133129992 Sep 30  2019 mysql-community-test-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">[root@mysql mysql]<span class="comment">#</span></span><br></pre></td></tr></table></figure>

<h5 id="3-按照编号顺序安装"><a href="#3-按照编号顺序安装" class="headerlink" title="3. 按照编号顺序安装"></a>3. 按照编号顺序安装</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@mysql mysql]<span class="comment"># rpm -ivh 0*.rpm</span></span><br><span class="line">warning: 01-mysql-community-common-5.7.28-1.el7.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-common-5.7.28-1.e<span class="comment">################################# [ 20%]</span></span><br><span class="line">   2:mysql-community-libs-5.7.28-1.el7<span class="comment">################################# [ 40%]</span></span><br><span class="line">   3:mysql-community-client-5.7.28-1.e<span class="comment">################################# [ 60%]</span></span><br><span class="line">   4:mysql-community-server-5.7.28-1.e<span class="comment">################################# [ 80%]</span></span><br><span class="line">   5:mysql-community-libs-compat-5.7.2<span class="comment">################################# [100%]</span></span><br><span class="line">[root@mysql mysql]<span class="comment">#</span></span><br></pre></td></tr></table></figure>
<h4 id="4-异常情况"><a href="#4-异常情况" class="headerlink" title="4. 异常情况"></a>4. 异常情况</h4><p>注意：安装第5个包时，如果首次安装会报错，原因是缺少依赖，通过yum安装的方式把依赖安装即可，然后再重新安装第5个包！！！<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20210624154408190.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20210624154408190"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install -y libaio</span><br></pre></td></tr></table></figure>

<h4 id="5-到此Mysql安装就完成了，可以通过一下命令检测是否安装完成"><a href="#5-到此Mysql安装就完成了，可以通过一下命令检测是否安装完成" class="headerlink" title="5. 到此Mysql安装就完成了，可以通过一下命令检测是否安装完成"></a>5. 到此Mysql安装就完成了，可以通过一下命令检测是否安装完成</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@mysql mysql]<span class="comment"># rpm -qa | grep mysql</span></span><br><span class="line">mysql-community-libs-5.7.28-1.el7.x86_64</span><br><span class="line">mysql-community-common-5.7.28-1.el7.x86_64</span><br><span class="line">mysql-community-client-5.7.28-1.el7.x86_64</span><br><span class="line">mysql-community-libs-compat-5.7.28-1.el7.x86_64</span><br><span class="line">mysql-community-server-5.7.28-1.el7.x86_64</span><br><span class="line">[root@mysql mysql]<span class="comment">#</span></span><br></pre></td></tr></table></figure>


<h2 id="三、配置MySql"><a href="#三、配置MySql" class="headerlink" title="三、配置MySql"></a>三、配置MySql</h2><h5 id="1-初始化数据库"><a href="#1-初始化数据库" class="headerlink" title="1. 初始化数据库"></a>1. 初始化数据库</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@mysql mysql]# mysqld --initialize --user=mysql</span><br><span class="line">[root@mysql mysql]#</span><br></pre></td></tr></table></figure>

<h5 id="2-启动Mysql服务"><a href="#2-启动Mysql服务" class="headerlink" title="2.启动Mysql服务"></a>2.启动Mysql服务</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@mysql mysql]# mysqld --initialize --user=mysql</span><br><span class="line">[root@mysql mysql]#</span><br><span class="line">[root@mysql mysql]# systemctl status mysqld</span><br><span class="line">● mysqld.service - MySQL Server</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: man:mysqld(8)</span><br><span class="line">           http://dev.mysql.com/doc/refman/en/using-systemd.html</span><br><span class="line">[root@mysql mysql]# systemctl is-enabled mysqld</span><br><span class="line">enabled</span><br><span class="line">[root@mysql mysql]# systemctl start mysqld</span><br><span class="line">[root@mysql mysql]#</span><br></pre></td></tr></table></figure>


<h5 id="3-查看Mysql的临时密码"><a href="#3-查看Mysql的临时密码" class="headerlink" title="3.查看Mysql的临时密码"></a>3.查看Mysql的临时密码</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@mysql mysql]<span class="comment"># cat /var/log/mysqld.log | grep password</span></span><br><span class="line">2021-10-20T04:26:51.462143Z 1 [Note] A temporary password is generated <span class="keyword">for</span> root@localhost: -NGjuzi5&amp;qlt</span><br><span class="line">[root@mysql mysql]<span class="comment">#</span></span><br></pre></td></tr></table></figure>


<h5 id="4-利用临时密码登录Mysql"><a href="#4-利用临时密码登录Mysql" class="headerlink" title="4. 利用临时密码登录Mysql"></a>4. 利用临时密码登录Mysql</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@mysql mysql]# mysql -uroot -p</span><br><span class="line">Enter password:</span><br><span class="line"><span class="meta">#</span><span class="bash">输入密码-NGjuzi5&amp;qlt</span></span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 2</span><br><span class="line">Server version: 5.7.28</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">mysql&gt;</span></span><br></pre></td></tr></table></figure>


<h5 id="5-修改密码"><a href="#5-修改密码" class="headerlink" title="5.修改密码"></a>5.修改密码</h5><p>注意：首次登录进来以后，必须要先修改指定自定义的密码，不然后续操作都会报错！</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show databases;</span></span><br><span class="line">ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement.</span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> <span class="built_in">set</span> password = password(<span class="string">&quot;123456&quot;</span>);</span></span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.00 sec)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show databases;</span></span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">| mysql              |</span><br><span class="line">| performance_schema |</span><br><span class="line">| sys                |</span><br><span class="line">+--------------------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">mysql&gt;</span></span><br></pre></td></tr></table></figure>

<h6 id="6-修改允许root远程登录"><a href="#6-修改允许root远程登录" class="headerlink" title="6. 修改允许root远程登录"></a>6. 修改允许root远程登录</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; use mysql;</span><br><span class="line">Reading table information <span class="keyword">for</span> completion of table and column names</span><br><span class="line">You can turn off this feature to get a quicker startup with -A</span><br><span class="line"></span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; update mysql.user <span class="built_in">set</span> host=<span class="string">&#x27;%&#x27;</span> <span class="built_in">where</span> user=<span class="string">&#x27;root&#x27;</span>;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">mysql&gt; flush privileges;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MySQL/" rel="tag">MySQL</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" rel="tag">数据库</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MySQL面试题"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/11/07/MySQL%E9%9D%A2%E8%AF%95%E9%A2%98/"
    >MySQL面试题</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/11/07/MySQL%E9%9D%A2%E8%AF%95%E9%A2%98/" class="article-date">
  <time datetime="2021-11-07T00:08:41.000Z" itemprop="datePublished">2021-11-07</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/MySQL/">MySQL</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h1><ol>
<li><p>为什么互联网公司一般选择 MySQL 而不是 Oracle?</p>
<ul>
<li>免费、流行、够用。</li>
</ul>
</li>
<li><p>数据库的三范式是什么？什么是反模式？</p>
<ul>
<li>第一范式，强调属性的原子性约束，要求属性具有原子性，不可再分解。</li>
<li>第二范式，强调记录的唯一性约束，表必须有一个主键，并且没有包含在主键中的列必须完全依赖于主键，而不能只依赖于主键的一部分。</li>
<li>第三范式，强调属性冗余性的约束，即非主键列必须直接依赖于主键。</li>
<li>反1空间换取时间，采取数据冗余的方式避免表之间的关联查询</li>
<li>使存储数据尽可能达到用户一致，保证系统经过一段较短的时间的自我恢复和修正，数据最终达到一致</li>
</ul>
</li>
<li><p>MySQL 有哪些数据类型？</p>
<ul>
<li>数值、日期/时间和字符串(字符)类型</li>
</ul>
</li>
<li><p>MySQL 中 varchar 与 char 的区别？varchar(50) 中的 50 代表的涵义？</p>
<ul>
<li>1、varchar 与 char 的区别，char 是一种固定长度的类型，varchar 则是一种可变长度的类型。</li>
<li>varchar(50) 中 50 的涵义最多存放 50 个字符。varchar(50) 和 (200) 存储 hello 所占空间一样，但后者在排序时会消耗更多内存，因为 ORDER BY col 采用 fixed_length 计算 col 长度(memory引擎也一样)。</li>
</ul>
</li>
<li><p>int(11) 中的 11 代表什么涵义？</p>
<ul>
<li>int(11) 中的 11 ，不影响字段存储的范围，只影响展示效果</li>
</ul>
</li>
<li><p>金额(金钱)相关的数据，选择什么数据类型？</p>
<ul>
<li>方式一，使用 int 或者 bigint 类型。如果需要存储到分的维度，需要 *100 进行放大。</li>
<li>方式二，使用 decimal 类型，避免精度丢失。如果使用 Java 语言时，需要使用 BigDecimal 进行对应。</li>
</ul>
</li>
<li><p>一张表，里面有 ID 自增主键，当 insert 了 17 条记录之后，删除了第 15,16,17 条记录，再把 MySQL 重启，再 insert 一条记录，这条记录的 ID 是 18 还是 15？</p>
<ul>
<li>一般情况下，我们创建的表的类型是 InnoDB ，如果新增一条记录（不重启 MySQL 的情况下），这条记录的 ID 是18 ；但是如果重启 MySQL 的话，这条记录的 ID 是 15 。因为 InnoDB 表只把自增主键的最大 ID 记录到内存中，所以重启数据库或者对表 OPTIMIZE 操作，都会使最大 ID 丢失。</li>
<li>但是，如果我们使用表的类型是 MyISAM ，那么这条记录的 ID 就是 18 。因为 MyISAM 表会把自增主键的最大 ID 记录到数据文件里面，重启 MYSQL 后，自增主键的最大 ID 也不会丢失。</li>
</ul>
</li>
<li><p>表中有大字段 X(例如：text 类型)，且字段 X 不会经常更新，以读为为主，请问您是选择拆成子表，还是继续放一起?写出您这样选择的理由</p>
<ul>
<li>拆带来的问题：连接消耗 + 存储拆分空间。<ul>
<li>如果能容忍拆分带来的空间问题，拆的话最好和经常要查询的表的主键在物理结构上放置在一起(分区) 顺序 IO ，减少连接消耗，最后这是一个文本列再加上一个全文索引来尽量抵消连接消耗。</li>
</ul>
</li>
<li>不拆可能带来的问题：查询性能。<ul>
<li>如果能容忍不拆分带来的查询性能损失的话，上面的方案在某个极致条件下肯定会出现问题，那么不拆就是最好的选择。</li>
</ul>
</li>
</ul>
</li>
<li><p>MySQL 有哪些存储引擎？</p>
<ul>
<li>InnoDB<ul>
<li>InnoDB是一个健壮的<strong>事务型</strong>存储引擎</li>
<li>更新密集的表。InnoDB存储引擎特别适合处理多重并发的更新请求。</li>
<li>事务。InnoDB存储引擎是支持事务的标准MySQL存储引擎。</li>
<li>自动灾难恢复。与其它存储引擎不同，InnoDB表能够自动从灾难中恢复。</li>
<li>外键约束。MySQL支持外键的存储引擎只有InnoDB。</li>
<li>支持自动增加列AUTO_INCREMENT属性。</li>
<li>从5.7开始innodb存储引擎成为默认的存储引擎。</li>
</ul>
</li>
<li>MyISAM<ul>
<li>MyISAM表无法处理事务，这就意味着有事务处理需求的表，不能使用MyISAM存储引擎。MyISAM存储引擎特别适合在以下几种情况下使用：</li>
<li>选择密集型的表。MyISAM存储引擎在筛选大量数据时非常迅速，这是它最突出的优点。</li>
<li>插入密集型的表。MyISAM的并发插入特性允许同时选择和插入数据。例如：MyISAM存储引擎很适合管理邮件或Web服务器日志数据。</li>
</ul>
</li>
</ul>
</li>
<li><p>如何选择合适的存储引擎？</p>
<ul>
<li>是否需要支持事务。</li>
<li>对索引和缓存的支持。</li>
<li>是否需要使用热备。</li>
<li>崩溃恢复，能否接受崩溃。</li>
<li>存储的限制。</li>
<li>MySQL 默认的存储引擎是 InnoDB ，并且也是最主流的选择。主要原因如下:<ul>
<li>【最重要】支持事务。</li>
<li>支持行级锁和表级锁，能支持更多的并发量。</li>
<li>查询不加锁，完全不影响查询。</li>
<li>支持崩溃后恢复。</li>
</ul>
</li>
<li>在 MySQL5.1 以及之前的版本，默认的存储引擎是 MyISAM ，但是目前已经不再更新，且它有几个比较关键的缺点：</li>
<li>不支持事务。</li>
<li>使用表级锁，如果数据量大，一个插入操作锁定表后，其他请求都将阻塞。</li>
</ul>
</li>
<li><p>请说明 InnoDB 和 MyISAM 的区别</p>
<table>
<thead>
<tr>
<th>对比项</th>
<th>InnoDB</th>
<th>MyISAM</th>
</tr>
</thead>
<tbody><tr>
<td>事务</td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td>存储限制</td>
<td>64TB</td>
<td>无</td>
</tr>
<tr>
<td>锁粒度</td>
<td>行锁</td>
<td>表锁</td>
</tr>
<tr>
<td>崩溃后的恢复</td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td>外键</td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td>全文检索</td>
<td>5.7 版本后支持</td>
<td>支持</td>
</tr>
</tbody></table>
</li>
<li><p>为什么 <code>SQL SELECT COUNT(*) FROM table </code> 在 InnoDB 比 MyISAM 慢？</p>
<ul>
<li>对于 SELECT COUNT(*) FROM table 语句，在没有 WHERE 条件的情况下，InnoDB 比 MyISAM 可能会慢很多，尤其在大表的情况下。因为，InnoDB 是去实时统计结果，会全表扫描；而 MyISAM 内部维持了一个计数器，预存了结果，所以直接返回即可。</li>
</ul>
</li>
</ol>
<h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><ol start="13">
<li><p>什么是索引？</p>
<ul>
<li>索引，类似于书籍的目录，想找到一本书的某个特定的主题，需要先找到书的目录，定位对应的页码。</li>
<li>MySQL 中存储引擎使用类似的方式进行查询，先去索引中查找对应的值，然后根据匹配的索引找到对应的数据行。</li>
</ul>
</li>
<li><p>索引有什么好处？</p>
<ul>
<li>提高数据的检索速度，降低数据库IO成本：使用索引的意义就是通过缩小表中需要查询的记录的数目从而加快搜索的速度。</li>
<li>降低数据排序的成本，降低CPU消耗：索引之所以查的快，是因为先将数据排好序，若该字段正好需要排序，则正好降低了排序的成本。</li>
</ul>
</li>
<li><p>索引有什么坏处？</p>
<ul>
<li>占用存储空间：索引实际上也是一张表，记录了主键与索引字段，一般以索引文件的形式存储在磁盘上。</li>
<li>降低更新表的速度：表的数据发生了变化，对应的索引也需要一起变更，从而减低的更新速度。否则索引指向的物理数据可能不对，这也是索引失效的原因之一。 </li>
</ul>
</li>
<li><p>索引的使用场景？</p>
<ul>
<li>对非常小的表，大部分情况下全表扫描效率更高。</li>
<li>对中大型表，索引非常有效。</li>
<li>特大型的表，建立和使用索引的代价随着增长，可以使用分区技术来解决。 </li>
</ul>
</li>
<li><p>索引的类型？</p>
<ul>
<li>索引，都是实现在存储引擎层的。主要有六种类型：<ul>
<li>普通索引：最基本的索引，没有任何约束。</li>
<li>唯一索引：与普通索引类似，但具有唯一性约束。</li>
<li>主键索引：特殊的唯一索引，不允许有空值。</li>
<li>复合索引：将多个列组合在一起创建索引，可以覆盖多个列。</li>
<li>外键索引：只有InnoDB类型的表才可以使用外键索引，保证数据的一致性、完整性和实现级联操作。</li>
<li>全文索引：MySQL 自带的全文索引只能用于 InnoDB、MyISAM ，并且只能对英文进行全文检索，一般使用全文索引引擎。</li>
</ul>
</li>
</ul>
</li>
<li><p>MySQL 索引的“创建”原则？</p>
<ul>
<li>最适合索引的列是出现在 WHERE 子句中的列，或连接子句中的列，而不是出现在 SELECT 关键字后的列。</li>
<li>索引列的基数越大，索引效果越好。</li>
<li>因为复合索引的基数会更大，根据情况创建复合索引可以提高查询效率。</li>
<li>避免创建过多的索引，索引会额外占用磁盘空间，降低写操作效率。</li>
<li>主键尽可能选择较短的数据类型，可以有效减少索引的磁盘占用提高查询效率。</li>
<li>对字符串进行索引，应该定制一个前缀长度，可以节省大量的索引空间。</li>
</ul>
</li>
<li><p>MySQL 索引的“使用”注意事项？</p>
<ul>
<li>应尽量避免在 WHERE 子句中使用 != 或 &lt;&gt; 操作符，否则将引擎放弃使用索引而进行全表扫描。优化器将无法通过索引来确定将要命中的行数,因此需要搜索该表的所有行。</li>
<li>应尽量避免在 WHERE 子句中使用 OR 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如：SELECT id FROM t WHERE num = 10 OR num = 20 。</li>
<li>应尽量避免在 WHERE 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。</li>
<li>应尽量避免在 WHERE 子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描</li>
<li>不要在 WHERE 子句中的 = 左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。</li>
<li>复合索引遵循前缀原则。</li>
<li>如果 MySQL 评估使用索引比全表扫描更慢，会放弃使用索引。如果此时想要索引，可以在语句中添加强制索引。</li>
<li>列类型是字符串类型，查询时一定要给值加引号，否则索引失效。</li>
<li>LIKE 查询，% 不能在前，因为无法使用索引。如果需要模糊匹配，可以使用全文索引。</li>
</ul>
</li>
<li><p>以下三条 SQL 如何建索引，只建一条怎么建？</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WHERE</span> a <span class="operator">=</span> <span class="number">1</span> <span class="keyword">AND</span> b <span class="operator">=</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">WHERE</span> b <span class="operator">=</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">WHERE</span> b <span class="operator">=</span> <span class="number">1</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="type">time</span> <span class="keyword">DESC</span></span><br></pre></td></tr></table></figure>
<ul>
<li>以顺序 b , a, time 建立复合索引，CREATE INDEX table1_b_a_time ON index_test01(b, a, time)。</li>
<li>对于第一条 SQL ，因为最新 MySQL 版本会优化 WHERE 子句后面的列顺序，以匹配复合索引顺序。</li>
</ul>
</li>
<li><p>想知道一个查询用到了哪个索引，如何查看?</p>
<ul>
<li>EXPLAIN 显示了 MYSQL 如何使用索引来处理 SELECT 语句以及连接表,可以帮助选择更好的索引和写出更优化的查询语句。</li>
<li><code>explain select * from table;</code><ul>
<li>id: id代表执行select子句或操作表的顺序</li>
<li>select_type: 查询的类型,用于区别普通查询,联合查询,子查询等复杂查询<ul>
<li>simple: 简单的select查询,查询中不包含子查询或union查询</li>
<li>primary: 查询中若包含任何复杂的子部分,最外层查询则被标记为primary</li>
<li>subquery 在select 或where 列表中包含了子查询</li>
<li>derived 在from列表中包含的子查询被标记为derived,mysql会递归这些子查询,把结果放在临时表里</li>
<li>union 做第二个select出现在union之后,则被标记为union,若union包含在from子句的子查询中,外层select将被标记为derived</li>
<li>union result 从union表获取结果的select</li>
</ul>
</li>
<li>table: 显示一行的数据时关于哪张表的</li>
<li>type: 查询类型从最好到最差依次是:system&gt;const&gt;eq_ref&gt;ref&gt;range&gt;index&gt;All,一般情况下,得至少保证达到range级别,最好能达到ref<ul>
<li>system:表只有一行记录,这是const类型的特例,平时不会出现</li>
<li>const:表示通过索引一次就找到了,const即常量,它用于比较primary key或unique索引,因为只匹配一行数据,所以效率很快,如将主键置于where条件中,mysql就能将该查询转换为一个常量 </li>
<li>eq_ref:唯一性索引扫描,对于每个索引键,表中只有一条记录与之匹配,常见于主键或唯一索引扫描</li>
<li>ref:非唯一性索引扫描,返回匹配某个单独值的行,它可能会找到多个符合条件的行,所以他应该属于查找和扫描的混合体</li>
<li>range:只检索给定范围的行,使用一个索引来选择行,如where语句中出现了between,&lt;,&gt;,in等查询,这种范围扫描索引比全表扫描要好，因为它只需要开始于索引的某一点，而结束于另一点，不用扫描全部索引。</li>
<li>index:index类型只遍历索引树,这通常比All快,因为索引文件通常比数据文件小,index是从索引中读取,all从硬盘中读取</li>
<li>all:全表扫描,是最差的一种查询类型</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>MySQL 有哪些索引方法？</p>
<ul>
<li>B-Tree 索引。</li>
<li>Hash 索引。</li>
</ul>
</li>
<li><p>什么是 B-Tree 索引？</p>
<ul>
<li>B-Tree 是为磁盘等外存储设备设计的一种平衡查找树。</li>
</ul>
</li>
<li><p>为什么索引结构默认使用B-Tree，而不是hash，二叉树，红黑树？</p>
<ul>
<li>hash：虽然可以快速定位，但是没有顺序，IO复杂度高。</li>
<li>二叉树：树的高度不均匀，不能自平衡，查找效率跟数据有关（树的高度），并且IO代价高。</li>
<li>红黑树：树的高度随着数据量增加而增加，IO代价高。</li>
</ul>
</li>
<li><p>为什么官方建议使用自增长主键作为索引。</p>
<ul>
<li><p>结合B+Tree的特点，自增主键是连续的，在插入过程中尽量减少页分裂，即使要进行页分裂，也只会分裂很少一部分。并且能减少数据的移动，每次插入都是插入到最后。总之就是减少分裂和移动的频率。</p>
</li>
<li><p>插入连续的数据：</p>
<ul>
<li><ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15848449061730.gif?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
</ul>
</li>
<li><p>插入非连续的数据</p>
<ul>
<li><ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15848449240023.gif?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>简述索引定位数据的过程</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15852273948834.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>根节点常驻内存</li>
<li>根据值确定节点位置，二分查找，最终确定叶子节点</li>
</ul>
</li>
<li><p>B-Tree 有哪些索引类型？</p>
<ul>
<li>主键索引的叶子节点存的数据是整行数据( 即具体数据 )。在 InnoDB 里，主键索引也被称为聚集索引（clustered index）。</li>
<li>非主键索引的叶子节点存的数据是整行数据的主键，键值是索引。在 InnoDB 里，非主键索引也被称为辅助索引（secondary index）。</li>
</ul>
</li>
<li><p>聚簇索引的注意点有哪些？</p>
<ul>
<li>聚簇索引表最大限度地提高了 I/O 密集型应用的性能，但它也有以下几个限制：<ul>
<li>插入速度严重依赖于插入顺序，按照主键的顺序插入是最快的方式，否则将会出现页分裂，严重影响性能。因此，对于 InnoDB 表，我们一般都会定义一个自增的 ID 列为主键。</li>
<li>更新主键的代价很高，因为将会导致被更新的行移动。因此，对于InnoDB 表，我们一般定义主键为不可更新。</li>
<li>二级索引访问需要两次索引查找，第一次找到主键值，第二次根据主键值找到行数据。</li>
<li>主键 ID 建议使用整型。因为，每个主键索引的 B+Tree 节点的键值可以存储更多主键 ID ，每个非主键索引的 B+Tree 节点的数据可以存储更多主键 ID 。</li>
</ul>
</li>
</ul>
</li>
<li><p>什么是索引的最左匹配特性？</p>
<ul>
<li>当 B+Tree 的数据项是复合的数据结构，比如索引 (name, age, sex) 的时候，B+Tree 是按照从左到右的顺序来建立搜索树的。<ul>
<li>比如当 (张三, 20, F) 这样的数据来检索的时候，B+Tree 会优先比较 name 来确定下一步的所搜方向，如果 name 相同再依次比较 age 和 sex ，最后得到检索的数据。</li>
<li>但当 (20, F) 这样的没有 name 的数据来的时候，B+Tree 就不知道下一步该查哪个节点，因为建立搜索树的时候 name 就是第一个比较因子，必须要先根据 name 来搜索才能知道下一步去哪里查询。</li>
<li>比如当 (张三, F) 这样的数据来检索时，B+Tree 可以用 name 来指定搜索方向，但下一个字段 age 的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是 F 的数据了。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h3><ol>
<li>事务的特性指的是？<ul>
<li>事务就是对一系列的数据库操作（比如插入多条数据）进行统一的提交或回滚操作，如果插入成功，那么一起成功，如果中间有一条出现异常，那么回滚之前的所有操作。</li>
<li>ACID ，如下图所示：<ul>
<li><ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15848457672689.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
<li>原子性 Atomicity ：一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被恢复（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。</li>
<li>一致性 Consistency ：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器)、级联回滚等。</li>
<li>隔离性 Isolation ：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。</li>
<li>持久性 Durability ：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。</li>
</ul>
</li>
</ul>
</li>
<li>事务的并发问题？<ul>
<li>脏读：事务 A 读取了事务 B 更新的数据，然后 B 回滚操作，那么 A 读取到的数据是脏数据。</li>
<li>不可重复读：事务 A 多次读取同一数据，事务 B 在事务 A 多次读取的过程中，对数据作了更新并提交，导致事务 A 多次读取同一数据时，结果不一致。</li>
<li>幻读：系统管理员 A 将数据库中所有学生的成绩从具体分数改为 ABCDE 等级，但是系统管理员 B 就在这个时候插入了一条具体分数的记录，当系统管理员 A 改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。</li>
</ul>
</li>
<li>MySQL 事务隔离级别会产生的并发问题？<ul>
<li>不同的隔离级别有不同的现象，并有不同的锁定/并发机制，隔离级别越高，数据库的并发性就越差。</li>
<li></li>
</ul>
</li>
</ol>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MySQL/" rel="tag">MySQL</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8/" rel="tag">面试宝典</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapReduce"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/11/07/MapReduce/"
    >MapReduce</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/11/07/MapReduce/" class="article-date">
  <time datetime="2021-11-07T00:08:39.000Z" itemprop="datePublished">2021-11-07</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/MR/">MR</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h1><h1 id="一、MapReduce概述"><a href="#一、MapReduce概述" class="headerlink" title="一、MapReduce概述"></a>一、MapReduce概述</h1><h2 id="1-1-MapReduce定义"><a href="#1-1-MapReduce定义" class="headerlink" title="1.1 MapReduce定义"></a>1.1 MapReduce定义</h2><ul>
<li>MapReduce是一个<font color ='red' >分布式运算程序的编程框架</font>，是开发“基于Hadoop的数据分析应用”的核心框架。</li>
<li>MapReduce核心功能是将<font color ='red' >用户编写的业务逻辑代码和自带默认组件</font>整合成一个完整的<font color ='red' >分布式运算程序</font>，并<font color ='red' >运行在一个Hadoop集群上</font>。</li>
</ul>
<hr>
<h2 id="1-2-MapReduce优缺点"><a href="#1-2-MapReduce优缺点" class="headerlink" title="1.2 MapReduce优缺点"></a>1.2 MapReduce优缺点</h2><h3 id="1-2-1-优点"><a href="#1-2-1-优点" class="headerlink" title="1.2.1 优点"></a>1.2.1 优点</h3><ol>
<li>MapReduce 易于编程<br> 它简单的<font color ='red' >实现一些接口，就可以完成一个分布式程序</font>，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。</li>
<li>良好的扩展性<br> 当你的计算资源不能得到满足的时候，你可以<font color ='red' >通过简单的增加机器来扩展它的计算能力</font>。</li>
<li>高容错性<br> MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如<font color ='red' >其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败</font>，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的。</li>
<li>适合PB级以上海量数据的离线处理<br> 可以实现上千台服务器集群并发工作，提供数据处理能力。</li>
</ol>
<h3 id="1-2-2-缺点"><a href="#1-2-2-缺点" class="headerlink" title="1.2.2 缺点"></a>1.2.2 缺点</h3><ol>
<li>不擅长实时计算<br> MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果。</li>
<li>不擅长流式计算<br> 流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。</li>
<li>不擅长DAG（有向无环图）计算<br> 多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。</li>
</ol>
<hr>
<h2 id="1-3-MapReduce核心思想"><a href="#1-3-MapReduce核心思想" class="headerlink" title="1.3 MapReduce核心思想"></a>1.3 MapReduce核心思想</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340999252814.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>数据输入：分片， Map阶段根据逻辑分片的理念对要计算的文件进行分片读取（128M）</li>
<li>Map阶段： 将文件进行逻辑划分后，进行分割处理 </li>
<li>Reduce阶段：将Map阶段处理好的数据进行汇总</li>
<li>数据输出：将Reduce阶段的输出结果保存至结果文件</li>
</ul>
<hr>
<h2 id="1-4-MapReduce进程"><a href="#1-4-MapReduce进程" class="headerlink" title="1.4 MapReduce进程"></a>1.4 MapReduce进程</h2><p>一个完整的MapReduce程序在分布式运行时有三类实例进程：</p>
<ol>
<li>MrAppMaster：负责整个程序的过程调度及状态协调。</li>
<li>MapTask：负责Map阶段的整个数据处理流程。</li>
<li>ReduceTask：负责Reduce阶段的整个数据处理流程。</li>
</ol>
<hr>
<h2 id="1-5-MR程序的编程规范"><a href="#1-5-MR程序的编程规范" class="headerlink" title="1.5 MR程序的编程规范"></a>1.5 MR程序的编程规范</h2><ul>
<li>驱动类（负责程序的job提交）</li>
<li>自定义 Mapper类型，并且继承Hadoop提供的Mapper<ul>
<li>重写map方法，在该方法中实现业务逻辑的编写</li>
</ul>
</li>
<li>自定义 Reducer类型，并且继承Hadoop提供的Reducer<ul>
<li>重写reduce方法，在该方法中实现业务逻辑的编写</li>
</ul>
</li>
</ul>
<h1 id="二、Hadoop序列化"><a href="#二、Hadoop序列化" class="headerlink" title="二、Hadoop序列化"></a>二、Hadoop序列化</h1><h2 id="2-1-序列化概述"><a href="#2-1-序列化概述" class="headerlink" title="2.1 序列化概述"></a>2.1 序列化概述</h2><h3 id="2-1-1-什么是序列化"><a href="#2-1-1-什么是序列化" class="headerlink" title="2.1.1 什么是序列化"></a>2.1.1 什么是序列化</h3><ul>
<li>序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。 </li>
<li>反序列化就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象。</li>
</ul>
<h3 id="2-1-2-为什么要序列化"><a href="#2-1-2-为什么要序列化" class="headerlink" title="2.1.2 为什么要序列化"></a>2.1.2 为什么要序列化</h3><ul>
<li>一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。</li>
</ul>
<h3 id="2-1-3-为什么不用Java的序列化"><a href="#2-1-3-为什么不用Java的序列化" class="headerlink" title="2.1.3 为什么不用Java的序列化"></a>2.1.3 为什么不用Java的序列化</h3><ul>
<li>Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。所以，Hadoop自己开发了一套序列化机制（Writable）。</li>
<li>Hadoop序列化特点：<ul>
<li>紧凑 ：高效使用存储空间。</li>
<li>快速：读写数据的额外开销小。</li>
<li>可扩展：随着通信协议的升级而可升级</li>
<li>互操作：支持多语言的交互</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-2-自定义bean对象实现序列化接口（Writable）"><a href="#2-2-自定义bean对象实现序列化接口（Writable）" class="headerlink" title="2.2 自定义bean对象实现序列化接口（Writable）"></a>2.2 自定义bean对象实现序列化接口（Writable）</h2><ul>
<li>在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口。</li>
<li>具体实现bean对象序列化步骤如下7步。</li>
</ul>
<ol>
<li>必须实现Writable接口</li>
<li>反序列化时，需要反射调用空参构造函数，所以必须有空参构造 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="keyword">super</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>重写序列化方法 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">	out.writeLong(upFlow);</span><br><span class="line">	out.writeLong(downFlow);</span><br><span class="line">	out.writeLong(sumFlow);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>重写反序列化方法 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">	upFlow = in.readLong();</span><br><span class="line">	downFlow = in.readLong();</span><br><span class="line">	sumFlow = in.readLong();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>注意反序列化的顺序和序列化的顺序完全一致</li>
<li>要想把结果显示在文件中，需要重写toString()，可用”\t”分开，方便后续用。</li>
<li>如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框中的Shuffle过程要求对key必须能排序。详见后面排序案例。 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line">	<span class="comment">// 倒序排列，从大到小</span></span><br><span class="line">	<span class="keyword">return</span> <span class="keyword">this</span>.sumFlow &gt; o.getSumFlow() ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<hr>
<h1 id="三、MapReduce框架原理"><a href="#三、MapReduce框架原理" class="headerlink" title="三、MapReduce框架原理"></a>三、MapReduce框架原理</h1><h2 id="3-1-MapReduce的工作流程"><a href="#3-1-MapReduce的工作流程" class="headerlink" title="3.1 MapReduce的工作流程"></a>3.1 MapReduce的工作流程</h2><h3 id="3-1-1-MapReduce的数据流"><a href="#3-1-1-MapReduce的数据流" class="headerlink" title="3.1.1 MapReduce的数据流"></a>3.1.1 MapReduce的数据流</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341977072186.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<hr>
<h3 id="3-1-2-MapReduce的执行流程"><a href="#3-1-2-MapReduce的执行流程" class="headerlink" title="3.1.2 MapReduce的执行流程"></a>3.1.2 MapReduce的执行流程</h3><ul>
<li>简易版：InputFormat –&gt; Mapper –&gt; Reduce –&gt; OutputFormat</li>
<li>详细版：InputFormat –&gt; map sort –&gt; copy sort group reduce –&gt; OutputFormat</li>
<li>MR执行的整体大致分为两个阶段<ul>
<li>提交Job<ul>
<li>对当前MR进行初始化工作以及对MT和RT进行规划</li>
</ul>
</li>
<li>执行Job<ul>
<li>执行的就是每一个MT和RT</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="3-2-InputFormat数据输入"><a href="#3-2-InputFormat数据输入" class="headerlink" title="3.2 InputFormat数据输入"></a>3.2 InputFormat数据输入</h2><h3 id="3-3-1-数据切片与MapTask并行度决定机制"><a href="#3-3-1-数据切片与MapTask并行度决定机制" class="headerlink" title="3.3.1 数据切片与MapTask并行度决定机制"></a>3.3.1 数据切片与MapTask并行度决定机制</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341008360509.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>切片的概念：切片是计算数据是从逻辑上文件的进行划分，切块存储数据时从物理上将文件进行切分</li>
<li>一个切片对应一个MapTask来处理</li>
<li>切片大小默认情况等于切块大小128M（这样做的目的是为了计算时读取读取数据效率更高，避免了跨机器读取）</li>
<li>切片的时候不考虑数据整体集，默认情况下对单个文件的进行切片</li>
</ol>
<h3 id="3-3-2-InputFormat类的体系结构"><a href="#3-3-2-InputFormat类的体系结构" class="headerlink" title="3.3.2 InputFormat类的体系结构"></a>3.3.2 InputFormat类的体系结构</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341011423965.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>InputFormat 抽象类的子实现类是 FileInputFormat </li>
<li>FileInputFormat 类的子实现类是 TextInputFormat </li>
<li>InputFormat抽象类中有两个抽象方法 <ul>
<li>getSplits() –&gt; 在 FileInputFormat 做了具体实现<br> 具体的切片的逻辑！！！</li>
<li>createRecordReader() –&gt; TextInputFormat 做了具体实现<br>  创建RecordReader对象， RecordReader最终帮助我们读取待处理的文件的数据，<br>  读取规则就是按行读取由LineRecordReader实现！！！</li>
</ul>
</li>
<li>遇到小文件计算的场景：<br> CombineTextInputFormat –&gt; FileInputFormat的子实现类（用于处理小文件场景）<h4 id="3-3-2-1-分析MapReduce中InputFormat的默认实现"><a href="#3-3-2-1-分析MapReduce中InputFormat的默认实现" class="headerlink" title="3.3.2.1 分析MapReduce中InputFormat的默认实现"></a>3.3.2.1 分析MapReduce中InputFormat的默认实现</h4>InputFormat-&gt;FileInputFormat-&gt;TextInputFormat</li>
<li>定位 驱动类的中的<code>job.waitForCompletion(true);</code></li>
<li>跟进 <code>org.apache.hadoop.mapreduce.Job#waitForCompletion</code></li>
<li>跟进 <code>org.apache.hadoop.mapreduce.Job#submit</code></li>
<li>定位 <code>return submitter.submitJobInternal(Job.this, cluster);</code></li>
<li>跟进 <code>org.apache.hadoop.mapreduce.JobSubmitter#submitJobInternal</code></li>
<li>关注 <code>JobSubmitter.java:200 int maps = writeSplits(job, submitJobDir);</code></li>
<li>跟进 <code>org.apache.hadoop.mapreduce.JobSubmitter#writeSplits</code></li>
<li>跟进 <code>org.apache.hadoop.mapreduce.JobSubmitter#writeNewSplits</code></li>
<li>关注 根据Job中设置的InputFormatClass，然后通过反射的手段获取 InputFormat 的实例  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">InputFormat&lt;?, ?&gt; input = ReflectionUtils.newInstance(job.getInputFormatClass(), conf);</span><br></pre></td></tr></table></figure></li>
<li>跟进 <code>job.getInputFormatClass()</code>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Class&lt;? extends InputFormat&lt;?,?&gt;&gt; getInputFormatClass() </span><br><span class="line">    <span class="keyword">throws</span> ClassNotFoundException;</span><br></pre></td></tr></table></figure></li>
<li>定位到 JobContextImpl 实现类  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Class&lt;? extends InputFormat&lt;?,?&gt;&gt; getInputFormatClass() <span class="keyword">throws</span> ClassNotFoundException &#123;</span><br><span class="line">    <span class="comment">// 先根据 Job中配置信息中的 mapreduce.job.inputformat.class 获取配置，如果获取不到</span></span><br><span class="line">    <span class="comment">// 走默认的 TextInputFormat.class</span></span><br><span class="line">    <span class="keyword">return</span> (Class&lt;? extends InputFormat&lt;?,?&gt;&gt;) </span><br><span class="line">    conf.getClass(INPUT_FORMAT_CLASS_ATTR, TextInputFormat.class);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>核对无默认配置<code>mapreduce.job.inputformat.class</code><a href="media/16340187305977/core-default.xml">hadoop-common-3.1.3.jar!/core-default.xml</a></li>
</ul>
<h4 id="3-3-2-2-分析Hadoop默认的切片规则"><a href="#3-3-2-2-分析Hadoop默认的切片规则" class="headerlink" title="3.3.2.2 分析Hadoop默认的切片规则"></a>3.3.2.2 分析Hadoop默认的切片规则</h4><ul>
<li>定位 <code>org.apache.hadoop.mapreduce.InputFormat#getSplits</code>抽象方法</li>
<li>定位 <code>org.apache.hadoop.mapreduce.lib.input.FileInputFormat#getSplits</code>实现  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Generate the list of files and make them into FileSplits.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> job the job context</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;InputSplit&gt; <span class="title">getSplits</span><span class="params">(JobContext job)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="comment">// 计时器，负责记录当前切片的所花费的时间 最后记录日志中</span></span><br><span class="line">  StopWatch sw = <span class="keyword">new</span> StopWatch().start();</span><br><span class="line">  <span class="comment">// minSize默认情况为1  但是可以通过配置 mapreduce.input.fileinputformat.split.minsize 改变它的值</span></span><br><span class="line">  <span class="keyword">long</span> minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));</span><br><span class="line">  <span class="comment">// maxSize默认情况为Long.MAX_VALUE   但是可以通过配置 mapreduce.input.fileinputformat.split.maxsize 改变它的值</span></span><br><span class="line">  <span class="keyword">long</span> maxSize = getMaxSplitSize(job);</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// generate splits</span></span><br><span class="line">  List&lt;InputSplit&gt; splits = <span class="keyword">new</span> ArrayList&lt;InputSplit&gt;();</span><br><span class="line">  List&lt;FileStatus&gt; files = listStatus(job);</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// ignoreDirs 默认是false</span></span><br><span class="line">  <span class="keyword">boolean</span> ignoreDirs = !getInputDirRecursive(job)</span><br><span class="line">          &amp;&amp; job.getConfiguration().getBoolean(INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS, <span class="keyword">false</span>);</span><br><span class="line">  <span class="keyword">for</span> (FileStatus file : files) &#123;</span><br><span class="line">    <span class="comment">// 默认情况下，对Job中设置的输入路径中的文件以及子目录中的文件全都处理</span></span><br><span class="line">    <span class="comment">// 如果考虑只处理当前设置的路径的子文件，而不管子目录中的文件需要自行定义</span></span><br><span class="line">    <span class="comment">// INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS 为 true</span></span><br><span class="line">    <span class="keyword">if</span> (ignoreDirs &amp;&amp; file.isDirectory()) &#123;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 获取当前文件的大小</span></span><br><span class="line">    Path path = file.getPath();</span><br><span class="line">    <span class="comment">// 对当前文件进行非空判断</span></span><br><span class="line">    <span class="keyword">long</span> length = file.getLen();</span><br><span class="line">    <span class="keyword">if</span> (length != <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// 获取当前文件的对应数据块</span></span><br><span class="line">      BlockLocation[] blkLocations;</span><br><span class="line">      <span class="keyword">if</span> (file <span class="keyword">instanceof</span> LocatedFileStatus) &#123;</span><br><span class="line">        blkLocations = ((LocatedFileStatus) file).getBlockLocations();</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        FileSystem fs = path.getFileSystem(job.getConfiguration());</span><br><span class="line">        blkLocations = fs.getFileBlockLocations(file, <span class="number">0</span>, length);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 判断当前文件是否可以进行切片</span></span><br><span class="line">      <span class="comment">// 主要考虑的是压缩文件这种情况</span></span><br><span class="line">      <span class="keyword">if</span> (isSplitable(job, path)) &#123;</span><br><span class="line">        <span class="comment">// 获取当前文件的块大小</span></span><br><span class="line">        <span class="keyword">long</span> blockSize = file.getBlockSize();</span><br><span class="line">        <span class="comment">// 计算切片大小计算切片逻辑如下</span></span><br><span class="line">        <span class="comment">// 切片大小是否可变？ 可变</span></span><br><span class="line">        <span class="comment">// 如果想调大：改变minSize</span></span><br><span class="line">        <span class="comment">// 如果想调小：改变maxSize</span></span><br><span class="line">        <span class="keyword">long</span> splitSize = computeSplitSize(blockSize, minSize, maxSize);</span><br><span class="line">        <span class="comment">// 当前文件的剩余大小</span></span><br><span class="line">        <span class="keyword">long</span> bytesRemaining = length;</span><br><span class="line">        <span class="comment">// 判断剩余文件是否要继续切分  剩余大小 / 切片大小 是否大于 1.1</span></span><br><span class="line">        <span class="comment">// 目的：就是为了更合理的使用资源计算数据</span></span><br><span class="line">        <span class="keyword">while</span> (((<span class="keyword">double</span>) bytesRemaining) / splitSize &gt; SPLIT_SLOP) &#123;</span><br><span class="line">          <span class="keyword">int</span> blkIndex = getBlockIndex(blkLocations, length - bytesRemaining);</span><br><span class="line">          splits.add(makeSplit(path, length - bytesRemaining, splitSize,</span><br><span class="line">                  blkLocations[blkIndex].getHosts(),</span><br><span class="line">                  blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">          bytesRemaining -= splitSize;</span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> (bytesRemaining != <span class="number">0</span>) &#123;</span><br><span class="line">          <span class="keyword">int</span> blkIndex = getBlockIndex(blkLocations, length - bytesRemaining);</span><br><span class="line">          splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,</span><br><span class="line">                  blkLocations[blkIndex].getHosts(),</span><br><span class="line">                  blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123; <span class="comment">// not splitable</span></span><br><span class="line">        <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">          <span class="comment">// Log only if the file is big enough to be splitted</span></span><br><span class="line">          <span class="keyword">if</span> (length &gt; Math.min(file.getBlockSize(), minSize)) &#123;</span><br><span class="line">            LOG.debug(<span class="string">&quot;File is not splittable so no parallelization &quot;</span></span><br><span class="line">                    + <span class="string">&quot;is possible: &quot;</span> + file.getPath());</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        splits.add(makeSplit(path, <span class="number">0</span>, length, blkLocations[<span class="number">0</span>].getHosts(),</span><br><span class="line">                blkLocations[<span class="number">0</span>].getCachedHosts()));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">//Create empty hosts array for zero length files</span></span><br><span class="line">      splits.add(makeSplit(path, <span class="number">0</span>, length, <span class="keyword">new</span> String[<span class="number">0</span>]));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Save the number of input files for metrics/loadgen</span></span><br><span class="line">  job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());</span><br><span class="line">  sw.stop();</span><br><span class="line">  <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">    LOG.debug(<span class="string">&quot;Total # of splits generated by getSplits: &quot;</span> + splits.size()</span><br><span class="line">            + <span class="string">&quot;, TimeTaken: &quot;</span> + sw.now(TimeUnit.MILLISECONDS));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> splits;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>总结</li>
</ul>
<h4 id="3-3-2-3-CombineTextInputFormat切片机制"><a href="#3-3-2-3-CombineTextInputFormat切片机制" class="headerlink" title="3.3.2.3 CombineTextInputFormat切片机制"></a>3.3.2.3 CombineTextInputFormat切片机制</h4><ol>
<li>框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。</li>
<li>应用场景：CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理。</li>
<li>虚拟存储切片最大值设置CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m<br> 注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</li>
<li>CombineTextInputFormat切片机制<ol>
<li>用户设置切片大小</li>
<li>虚拟过程：根据和切片大小进行比较 <ul>
<li>如果当前文件 &gt; 设置的大小 且 小于2倍的设置的大小就一分为2</li>
<li>如果当前文件大于2倍的设置的大小就先切分出设置大小的块，然后重复步骤2虚拟切分</li>
</ul>
</li>
<li>切片过程：根据虚拟后的结果 把每个虚拟文件和 设置的大小比较<ul>
<li>如果大于等于设置的大小就单独形成一个切片</li>
<li>如果小于设置大小就和下一个虚拟文件进行合并，重复执行</li>
<li>如果合并后大于设置大小就单独形成一个切片</li>
</ul>
</li>
</ol>
</li>
</ol>
<hr>
<h2 id="3-3-Shuffle机制"><a href="#3-3-Shuffle机制" class="headerlink" title="3.3 Shuffle机制"></a>3.3 Shuffle机制</h2><p>Shuffle：Map方法之后，Reduce方法之前的数据处理过程<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341965884201.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="3-3-1-Shuffle机制"><a href="#3-3-1-Shuffle机制" class="headerlink" title="3.3.1 Shuffle机制"></a>3.3.1 Shuffle机制</h3><h4 id="3-3-1-1-Shuffle过程实现的作用"><a href="#3-3-1-1-Shuffle过程实现的作用" class="headerlink" title="3.3.1.1 Shuffle过程实现的作用"></a>3.3.1.1 Shuffle过程实现的作用</h4><ol>
<li>分区：由业务决定<ol>
<li>决定当前的Key交给那个Reduce进行处理</li>
<li>相同的key必须由同一个Reduce进行处理</li>
<li>默认根据Key的Hash值，对Reduce的个数取模</li>
</ol>
</li>
<li>分组：<ol>
<li>将相同Key的value进行合并</li>
<li>相同Key的value分到同一个组</li>
</ol>
</li>
<li>排序<ol>
<li>对key的index进行排序</li>
<li>排序算法为快排，顺序为字典顺序</li>
</ol>
</li>
<li>合并<ol>
<li>相同key溢写的文件合并成一个文件</li>
<li>保证1个MapTask结果输出一个文件</li>
</ol>
</li>
</ol>
<h4 id="3-3-1-2-map端Shuffle"><a href="#3-3-1-2-map端Shuffle" class="headerlink" title="3.3.1.2 map端Shuffle"></a>3.3.1.2 map端Shuffle</h4><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341966144406.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>Partition：将map的结果发送到相应的reduce端，总的partition的数目等于reducer的数量。<ol>
<li>map输出的key-value结果由Partitioner#getPartition方法决定交由那个reducer处理</li>
<li>默认由HashPartitioner实现，对key取hash值后进行取模运算 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">  <span class="comment">/** Use &#123;<span class="doctag">@link</span> Object#hashCode()&#125; to partition. */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value,<span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>将数据写入到内存缓冲区，缓冲区的作用是批量收集map结果，减少磁盘IO的影响。key-value对以及Partition的结果都会被写入缓冲区。在写入之前，key与value值都会被序列化成字节数组</li>
</ol>
</li>
<li>spill: 溢写，sort &amp; combiner<ol>
<li>把内存缓冲区中的数据写入到本地磁盘，在写入本地磁盘时先按照partition、再按照key进行排序（quick sort）</li>
<li>spill是由另外单独的线程来完成，不影响往缓冲区写map结果的线程；</li>
<li>内存缓冲区默认大小限制为100MB，它有个溢写比例（spill.percent），默认为0.8，当缓冲区的数据达到阈值时，溢写线程就会启动，先锁定这80MB的内存，执行溢写过程，maptask的输出结果还可以往剩下的20MB内存中写，互不影响。然后再重新利用这块缓冲区，因此Map的内存缓冲区又叫做环形缓冲区</li>
<li>sort：在将数据写入磁盘之前，先要对要写入磁盘的数据进行一次排序操作，先按&lt;key,value,partition&gt;中的partition分区号排序，然后再按key排序，最后溢出的小文件是分区的，且同一个分区内是保证key有序的；</li>
<li>combine：执行combine操作要求程序中通过job.setCombinerClass(myCombine.class)自定义combine操作<ul>
<li>程序中有两个阶段可能会执行combine操作：<ol>
<li>map输出数据根据分区排序完成后，在写入文件之前会执行一次combine操作（前提是作业中设置了这个操作）；</li>
<li>如果map输出比较大，溢出文件个数大于3（此值可以通过属性min.num.spills.for.combine配置）时，在merge的过程（多个spill文件合并为一个大文件）中还会执行combine操作；</li>
</ol>
</li>
<li>combine主要是把形如&lt;aa,1&gt;,&lt;aa,2&gt;这样的key值相同的数据进行计算，计算规则与reduce一致，比如：当前计算是求key对应的值求和，则combine操作后得到&lt;aa,3&gt;这样的结果。</li>
<li>注意事项：不是每种作业都可以做combine操作的，只有满足以下条件才可以：<ul>
<li>reduce的输入输出类型都一样，因为combine本质上就是reduce操作；</li>
<li>计算逻辑上，combine操作后不会影响计算结果，像求和就不会影响；</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li>merge: 当map很大时，每次溢写会产生一个spill_file，这样会有多个spill_file，而最终的一个map task输出只有一个文件，因此，最终的结果输出之前会对多个中间过程进行多次溢写文件（spill_file）的合并，此过程就是merge过程。也即是，待Map Task任务的所有数据都处理完后，会对任务产生的所有中间数据文件做一次合并操作，以确保一个Map Task最终只生成一个中间数据文件。<ol>
<li>如果生成的文件太多，可能会执行多次合并，每次最多能合并的文件数默认为10，可以通过属性min.num.spills.for.combine配置；</li>
<li>多个溢写文件合并时，会进行一次排序，排序算法是<font color ='red' >多路归并排序</font>；</li>
<li>是否还需要做combine操作，一是看是否设置了combine，二是看溢出的文件数是否大于等于3；</li>
<li>最终生成的文件格式与单个溢出文件一致，也是按分区顺序存储，并且输出文件会有一个对应的索引文件，记录每个分区数据的起始位置，长度以及压缩长度，这个索引文件名叫做file.out.index。</li>
</ol>
</li>
<li>内存缓冲区<ol>
<li>在MapTask任务的业务处理方法map()中，最后一步通过<code>OutputCollector.collect(key,value)</code>或<code>context.write(key,value)</code>输出Map Task的中间处理结果，在<code>collect(key,value)</code>方法中，会调用<code>Partitioner.getPartition(K2 key, V2 value, int numPartitions)</code>方法获得输出的key-value对应的分区号(分区号可以认为对应着一个要执行Reduce Task的节点)，然后将&lt;key,value,partition&gt;暂时保存在内存中的MapOutputBuffe内部的环形数据缓冲区，该缓冲区的默认大小是100MB，可以通过参数io.sort.mb来调整其大小</li>
<li>当缓冲区中的数据使用率达到一定阀值后，触发一次Spill操作，将环形缓冲区中的部分数据写到磁盘上，生成一个临时的本地数据的spill文件；然后在缓冲区的使用率再次达到阀值后，再次生成一个spill文件。直到数据处理完毕，在磁盘上会生成很多的临时文件</li>
<li>触发spill操作时，map输出还会接着往剩下的20%的内存空间写入，但是写满的80%的内存空间会被锁定，数据溢出写入磁盘。当这部分溢出的数据写完后，空出的内存空间可以接着被使用，形成像环一样的被循环使用的效果，所以又叫做环形内存缓冲区</li>
<li>MapOutputBuffe内部存数的数据采用了两个索引结构，涉及三个环形内存缓冲区。两级索引结构如下：<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341959070345.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><ol>
<li>写入到缓冲区的数据会进行压缩，由CompressionCodec提供实现</li>
<li>kvoffsets缓冲区：也叫偏移量索引数组，用于保存key-value信息在位置索引 kvindices 中的偏移量。当 kvoffsets 的使用率超过io.sort.spill.percent (默认为80%)后，便会触发一次 SpillThread 线程的“溢写”操作，也就是开始一次 Spill 阶段的操作。</li>
<li>kvindices缓冲区：也叫位置索引数组，用于保存 key-value 在数据缓冲区 kvbuffer 中的起始位置。</li>
<li>kvbuffer数据缓冲区：用于保存实际的 key-value 的值。默认情况下该缓冲区最多可以使用io.sort.mb的95%，当kvbuffer使用率超过io.sort.spill.percent(默认为80%)后，便会出发一次 SpillThread 线程的“溢写”操作，也就是开始一次 Spill 阶段的操作。</li>
<li>写入到本地磁盘时，对数据进行排序，实际上是对kvoffsets这个偏移量索引数组进行排序。</li>
</ol>
</li>
</ol>
</li>
<li>MapTask结束，通知appmaster,appmaster通过Reduce拉取数据</li>
</ol>
<h4 id="3-3-1-3-reduce端Shuffle"><a href="#3-3-1-3-reduce端Shuffle" class="headerlink" title="3.3.1.3 reduce端Shuffle"></a>3.3.1.3 reduce端Shuffle</h4><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341965290492.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>copy过程<ul>
<li>作用：拉取MapTask处理完成的数据</li>
<li>过程：Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求MapTask所在的TaskTracker获取MapTask的输出文件。因为这时MapTask已经结束，这些文件就由TaskTracker管理在本地磁盘中。</li>
<li>默认情况下，当整个MapReduce作业的所有已执行完成的MapTask任务数超过MapTask总数的5%后，JobTracker便会开始调度执行ReduceTask任务。然后ReduceTask任务默认启动mapred.reduce.parallel.copies(默认为5）个MapOutputCopier线程到已完成的MapTask任务节点上分别copy一份属于自己的数据。 这些copy的数据会首先保存的内存缓冲区中，当内冲缓冲区的使用率达到一定阀值后，触发溢写写到磁盘上。</li>
<li>内存缓冲区<ol>
<li>内存缓冲区大小通过mapred.job.shuffle.input.buffer.percent（default 0.7）参数来设置，控制shuffle在reduce内存中的数据最多使用内存量为：0.7 × maxHeap of ReduceTask。</li>
<li>ReduceTask使用最大heap的一定比例用来缓存数据（最大heap通常通过mapred.child.java.opts来设置，比如设置为-Xmx1024m）。默认情况下，reduce会使用其heapsize的70%来在内存中缓存数据。如果reduce的heap由于业务原因调整的比较大，相应的缓存大小也会变大，这也是为什么reduce用来做缓存的参数是一个百分比，而不是一个固定的值了。</li>
</ol>
</li>
</ul>
</li>
<li>merge过程<ul>
<li>merge 有三种形式：1)内存到内存 2)内存到磁盘 3)磁盘到磁盘。默认情况下第一种形式是不启用的。</li>
<li>当内存中的数据量到达一定阈值，就启动内存到磁盘的 merge（图中的第一个merge，之所以进行merge是因为reduce端在从多个map端copy数据的时候，并没有进行sort，只是把它们加载到内存，当达到阈值写入磁盘时，需要进行merge） 。这和map端的很类似，这实际上就是溢写的过程，在这个过程中如果你设置有Combiner，它也是会启用的，然后在磁盘中生成了众多的溢写文件，这种merge方式一直在运行，直到没有 map 端的数据时才结束，然后才会启动第三种磁盘到磁盘的 merge （图中的第二个merge）方式生成最终的那个文件。</li>
<li>在远程copy数据的同时，ReduceTask在后台启动了两个后台线程对内存和磁盘上的数据文件做合并操作，以防止内存使用过多或磁盘生的文件过多。</li>
</ul>
</li>
<li>reducer的输入文件<ul>
<li>merge的最后会生成一个文件，大多数情况下存在于磁盘中，但是需要将其放入内存中。当reducer 输入文件已定，整个 Shuffle 阶段才算结束。然后就是 Reducer 执行，把结果放到 HDFS 上。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="3-3-2-Partition分区"><a href="#3-3-2-Partition分区" class="headerlink" title="3.3.2 Partition分区"></a>3.3.2 Partition分区</h3><h4 id="3-3-2-1-分区使用场景"><a href="#3-3-2-1-分区使用场景" class="headerlink" title="3.3.2.1 分区使用场景"></a>3.3.2.1 分区使用场景</h4><ul>
<li>要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区）</li>
<li>分区是由需求决定的，而分区编号的产生是由ReducerTask的数量控制的。<h4 id="3-3-2-2-Hadoop默认的分区规则"><a href="#3-3-2-2-Hadoop默认的分区规则" class="headerlink" title="3.3.2.2 Hadoop默认的分区规则"></a>3.3.2.2 Hadoop默认的分区规则</h4></li>
<li>根据key的hashCode对ReduceTasks个数取模得到的。无法控制哪个key存储到哪个分区。</li>
<li>默认分区规则源码分析<ul>
<li>定位Mapper逻辑中的 context.write(outk, outv);</li>
<li>跟进 write(outk, outv);  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(KEYOUT key, VALUEOUT value)</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br></pre></td></tr></table></figure></li>
<li>具体实现 TaskInputOutputContextImpl#write  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(KEYOUT key, VALUEOUT value</span></span></span><br><span class="line"><span class="params"><span class="function">                )</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    output.write(key, value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进 output.write(key, value)  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(K key, V value)</span> <span class="keyword">throws</span> IOException, </span></span><br><span class="line"><span class="function">    InterruptedException</span>;</span><br></pre></td></tr></table></figure></li>
<li>具体实现实现类RecordWriterWithCounter#write  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Object key, Object value)</span> </span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    context.getCounter(COUNTERS_GROUP, counterName).increment(<span class="number">1</span>);</span><br><span class="line">    writer.write(key, value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进 writer.write(key, value);具体实现 NewOutputCollector#write<ul>
<li>collector-&gt;MapOutputCollector ： 此对象就是环形缓冲区对象<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(K key, V value)</span> <span class="keyword">throws</span> IOException,InterruptedException </span>&#123;</span><br><span class="line">    collector.collect(key, value,</span><br><span class="line">        partitioner.getPartition(key, value, partitions));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>跟进 partitioner.getPartition(key, value, partitions)<br>  注意：跟进后发现来到了一个 叫做 Partitioner的抽象类中，如果想知道<br>  Hadoop默认的分区规则，必须得知道 当前Partitioner的默认实现类！<ul>
<li>查找Partitioner的默认实现类<ul>
<li>关注：partitioner赋值发生在NewOutputCollector构造方法中   <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">NewOutputCollector(org.apache.hadoop.mapreduce.JobContext jobContext,</span><br><span class="line">                JobConf job,</span><br><span class="line">                TaskUmbilicalProtocol umbilical,</span><br><span class="line">                TaskReporter reporter</span><br><span class="line">                ) <span class="keyword">throws</span> IOException, ClassNotFoundException &#123;</span><br><span class="line">    <span class="comment">//获取环形缓冲区对象</span></span><br><span class="line">    collector = createSortingCollector(job, reporter);</span><br><span class="line">    <span class="comment">//获取ReduceTask数量作为分区数量</span></span><br><span class="line">    partitions = jobContext.getNumReduceTasks();</span><br><span class="line">    <span class="comment">//分区大于1个</span></span><br><span class="line">    <span class="keyword">if</span> (partitions &gt; <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="comment">//根据 jobContext.getPartitionerClass() 获取Partitioner实现类</span></span><br><span class="line">        partitioner = (org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;)</span><br><span class="line">            ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        partitioner = <span class="keyword">new</span> org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> partitions - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进 JobContext#getPartitionerClass 实现类 JobContextImpl#getPartitionerClass  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">            <span class="comment">// 根据PARTITIONER_CLASS_ATTR枚举值对一个mapreduce.job.partitioner.class配置项</span></span><br><span class="line"><span class="comment">// 获取 Partitioner 的实现类，发现默认没有配置，那就使用后面默认的HashPartitioner.class</span></span><br><span class="line">            <span class="keyword">public</span> Class&lt;? extends Partitioner&lt;?,?&gt;&gt; getPartitionerClass() </span><br><span class="line">                    <span class="keyword">throws</span> ClassNotFoundException &#123;</span><br><span class="line">                <span class="keyword">return</span> (Class&lt;? extends Partitioner&lt;?,?&gt;&gt;) </span><br><span class="line">                    conf.getClass(PARTITIONER_CLASS_ATTR, HashPartitioner.class);</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进 HashPartitioner<ul>
<li>根据以上分析 Partitioner 的实现类是 HashPartitioner.class，以下就是Hadoop的默认分区规则  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/** Use &#123;<span class="doctag">@link</span> Object#hashCode()&#125; to partition. */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value,</span></span></span><br><span class="line"><span class="params"><span class="function">                            <span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>结束：根据当前的key的hashcode值和 ReduceTask的数量取模操作得到当前key的所属分区编号</li>
<li>在MR中使用分区，通常要结合业务去做自定义分区规则！</li>
</ul>
</li>
</ul>
<h4 id="3-3-2-3-自定义Partitioner步骤"><a href="#3-3-2-3-自定义Partitioner步骤" class="headerlink" title="3.3.2.3 自定义Partitioner步骤"></a>3.3.2.3 自定义Partitioner步骤</h4><ol>
<li>自定义类继承Partitioner，重写getPartition()方法 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MobileModPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowDTO</span>&gt;</span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, FlowDTO flowDTO, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> mobileNum = Long.parseLong(flowDTO.getMobile());</span><br><span class="line">        <span class="keyword">int</span> partitionNum = (<span class="keyword">int</span>) (mobileNum % numPartitions);</span><br><span class="line">        log.info(<span class="string">&quot;执行分区操作 key:&#123;&#125; partition;&#123;&#125;&quot;</span>, mobileNum, partitionNum);</span><br><span class="line">        <span class="keyword">return</span> partitionNum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>在Job驱动中，设置自定义Partitioner <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setPartitionerClass(CustomPartitioner.class);</span><br></pre></td></tr></table></figure></li>
<li>自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure></li>
<li>分区器使用时注意事项<ul>
<li>当ReduceTask的数量设置 &gt; 实际用到的分区数 此时会生成空的分区文件</li>
<li>当ReduceTask的数量设置 &lt; 实际用到的分区数 此时会报错</li>
<li>当ReduceTask的数量设置 = 1 结果文件会输出到一个文件中，由以下源码可以论证：<ul>
<li>位置 NewOutputCollector#NewOutputCollector <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">NewOutputCollector(org.apache.hadoop.mapreduce.JobContext jobContext,</span><br><span class="line">                    JobConf job,</span><br><span class="line">                    TaskUmbilicalProtocol umbilical,</span><br><span class="line">                    TaskReporter reporter</span><br><span class="line">                    ) <span class="keyword">throws</span> IOException, ClassNotFoundException &#123;</span><br><span class="line">    collector = createSortingCollector(job, reporter);</span><br><span class="line">    <span class="comment">// 获取当前ReduceTask的数量</span></span><br><span class="line">    partitions = jobContext.getNumReduceTasks();</span><br><span class="line">    <span class="comment">// 判断ReduceTask的数量 是否大于1，找指定分区器对象</span></span><br><span class="line">    <span class="keyword">if</span> (partitions &gt; <span class="number">1</span>) &#123;</span><br><span class="line">    partitioner = (org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;)</span><br><span class="line">        ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 执行默认的分区规则，最终返回一个唯一的0号分区</span></span><br><span class="line">        partitioner = <span class="keyword">new</span> org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> partitions - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>分区编号生成的规则：根据指定的ReduceTask的数量 从0开始，依次累加。</li>
</ul>
</li>
</ol>
<h3 id="3-3-3-WritableComparable排序"><a href="#3-3-3-WritableComparable排序" class="headerlink" title="3.3.3 WritableComparable排序"></a>3.3.3 WritableComparable排序</h3><h4 id="3-3-3-1-排序概述"><a href="#3-3-3-1-排序概述" class="headerlink" title="3.3.3.1 排序概述"></a>3.3.3.1 排序概述</h4><ul>
<li>MapTask和ReduceTask均会对数据<font color ='red' >按照key进行排序</font>。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。</li>
<li><font color ='red' >默认排序是按照字典顺序升序排序，且实现该排序的方法是快速排序</font>。</li>
<li>对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序</li>
<li>对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序</li>
</ul>
<h4 id="3-3-3-2-排序分类"><a href="#3-3-3-2-排序分类" class="headerlink" title="3.3.3.2 排序分类"></a>3.3.3.2 排序分类</h4><ol>
<li>部分排序：分区内排序<ul>
<li>MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序</li>
</ul>
</li>
<li>全排序<ul>
<li>最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构</li>
</ul>
</li>
<li>辅助排序：GroupingComparator分组<ul>
<li>在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部字段比较不相同）的key进入到同一个reduce方法时，可以采用分组排序。</li>
</ul>
</li>
<li>二次排序<ul>
<li>在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序</li>
</ul>
</li>
</ol>
<h4 id="3-3-3-3-实现排序比较的方式"><a href="#3-3-3-3-实现排序比较的方式" class="headerlink" title="3.3.3.3 实现排序比较的方式"></a>3.3.3.3 实现排序比较的方式</h4><ol>
<li>直接让参与比较的对象实现WritableComparable 接口，并在该类中实现compareTo，在compareTo中定义自己的比较规则。这种情况 当运行的的时候Hadoop会自动生成比较器对象WritableComparator <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CompareFlowDTO</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">CompareFlowDTO</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String mobile;</span><br><span class="line">    <span class="keyword">private</span> Long upFlow;</span><br><span class="line">    <span class="keyword">private</span> Long downFlow;</span><br><span class="line">    <span class="keyword">private</span> Long sumFlow;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        out.writeUTF(mobile);</span><br><span class="line">        out.writeLong(upFlow);</span><br><span class="line">        out.writeLong(downFlow);</span><br><span class="line">        out.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        mobile = in.readUTF();</span><br><span class="line">        upFlow = in.readLong();</span><br><span class="line">        downFlow = in.readLong();</span><br><span class="line">        sumFlow = in.readLong();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(CompareFlowDTO o)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> o.getSumFlow().compareTo(getSumFlow());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">&quot;\t&quot;</span> + downFlow + <span class="string">&quot;\t&quot;</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>继承Hadoop提供的WritableComparator类，重写该类compare() 方法，在该方法中定义比较规则，注意在自定义的比较器对象中通过调用父类的super方法将自定义的比较器对象和要参与比较的对象进行关联。最后再Driver类中指定自定义的比较器对象。 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CompareFlowDTOComparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">CompareFlowDTOComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(CompareFlowDTO.class, <span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line">        CompareFlowDTO aCompareFlowDTO = (CompareFlowDTO) a;</span><br><span class="line">        CompareFlowDTO bCompareFlowDTO = (CompareFlowDTO) b;</span><br><span class="line">        <span class="keyword">return</span> aCompareFlowDTO.getSumFlow().compareTo(bCompareFlowDTO.getSumFlow());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>继承Hadoop提供的WritableComparator类，重写该类compare() 方法，在该方法中定义比较规则，注意在自定义的比较器对象中通过调用父类的super方法将自定义的比较器对象和要参与比较的对象进行关联。在比较对象的类定义中添加静态代码块，主动注册比较器 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//主动注册</span></span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">    WritableComparator.define(CompareFlowDTO.class, <span class="keyword">new</span> CompareFlowDTOComparator());</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="3-3-3-4-Hadoop中获取比较器对象的规则源码分析"><a href="#3-3-3-4-Hadoop中获取比较器对象的规则源码分析" class="headerlink" title="3.3.3.4 Hadoop中获取比较器对象的规则源码分析"></a>3.3.3.4 Hadoop中获取比较器对象的规则源码分析</h4><ul>
<li>入口 <code>org.apache.hadoop.mapred.MapTask.MapOutputBuffer#init</code></li>
<li>定位MapTask.java:1018  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">comparator = job.getOutputKeyComparator();</span><br></pre></td></tr></table></figure></li>
<li>跟进 org.apache.hadoop.mapred.JobConf#getOutputKeyComparator  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> RawComparator <span class="title">getOutputKeyComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 配置文件查找对应配置JobContext.KEY_COMPARATOR-&gt;mapreduce.job.output.key.comparator.class </span></span><br><span class="line">  <span class="comment">// 如果配置存在且实现了Comparator接口，返回配置的比较器</span></span><br><span class="line">  <span class="comment">// 配置存在但是没有实现Comparator接口，抛出异常</span></span><br><span class="line">  <span class="comment">// 配置不存在，取默认值null</span></span><br><span class="line">  Class&lt;? extends RawComparator&gt; theClass = getClass(</span><br><span class="line">    JobContext.KEY_COMPARATOR, <span class="keyword">null</span>, RawComparator.class);</span><br><span class="line">  <span class="keyword">if</span> (theClass != <span class="keyword">null</span>)</span><br><span class="line">      <span class="comment">// 如果通过JobContext.KEY_COMPARATOR 获取到了 直接通过反射的形式实例化对象</span></span><br><span class="line">      <span class="keyword">return</span> ReflectionUtils.newInstance(theClass, <span class="keyword">this</span>);</span><br><span class="line">  <span class="comment">// 如果 JobContext.KEY_COMPARATOR 没获取到，就走一下流程获取参与排序的对象的比较器对象</span></span><br><span class="line">  <span class="comment">// 首先会检验 当前Map端输出的key是否实现WritableComparable接口</span></span><br><span class="line">  <span class="keyword">return</span> WritableComparator.get(getMapOutputKeyClass().asSubclass(WritableComparable.class), <span class="keyword">this</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进WritableComparator#get  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> WritableComparator <span class="title">get</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      Class&lt;? extends WritableComparable&gt; c, Configuration conf)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 通过当前比较的对象的class类对象到comparators这个Map容器中获取比较器对象</span></span><br><span class="line">    <span class="comment">// 凡是在comparators能获取到的比较器对象，那当前参与比较的对象一定Hadoop自身的数据类型。</span></span><br><span class="line">    WritableComparator comparator = comparators.get(c);</span><br><span class="line">    <span class="keyword">if</span> (comparator == <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="comment">// 考虑到加载的类可能遇到内存的一些错误，导致GC,所以再强制加载一次 已过时</span></span><br><span class="line">      forceInit(c);</span><br><span class="line">      <span class="comment">// 强制加载后再获取</span></span><br><span class="line">      comparator = comparators.get(c);</span><br><span class="line">      <span class="comment">// 如果还没有获取到，那当前参与比较的对象就不是Hadoop自身的数据类型</span></span><br><span class="line">      <span class="keyword">if</span> (comparator == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">//如果到这还没获取到，那就是我们自定义的数据类型，此时Hadoop创建一个比较器</span></span><br><span class="line">        comparator = <span class="keyword">new</span> WritableComparator(c, conf, <span class="keyword">true</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Newly passed Configuration objects should be used.</span></span><br><span class="line">    ReflectionUtils.setConf(comparator, conf);</span><br><span class="line">    <span class="keyword">return</span> comparator;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-3-3-5-Hadoop中获取自身数据类型的比较器对象源码分析"><a href="#3-3-3-5-Hadoop中获取自身数据类型的比较器对象源码分析" class="headerlink" title="3.3.3.5 Hadoop中获取自身数据类型的比较器对象源码分析"></a>3.3.3.5 Hadoop中获取自身数据类型的比较器对象源码分析</h4></li>
<li>以org.apache.hadoop.io.Text为例</li>
<li>实现了org.apache.hadoop.io.WritableComparable接口</li>
<li>在Text本类中已经声明了比较器对象 并且做了关联  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Comparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Comparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    #关联比较器和比较对象</span><br><span class="line">    <span class="keyword">super</span>(Text.class);</span><br><span class="line">  &#125;</span><br><span class="line">  #具体比较实现</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(<span class="keyword">byte</span>[] b1, <span class="keyword">int</span> s1, <span class="keyword">int</span> l1,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">byte</span>[] b2, <span class="keyword">int</span> s2, <span class="keyword">int</span> l2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n1 = WritableUtils.decodeVIntSize(b1[s1]);</span><br><span class="line">    <span class="keyword">int</span> n2 = WritableUtils.decodeVIntSize(b2[s2]);</span><br><span class="line">    <span class="keyword">return</span> compareBytes(b1, s1+n1, l1-n1, b2, s2+n2, l2-n2);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>注册Comparator  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">  <span class="comment">// register this comparator</span></span><br><span class="line">  WritableComparator.define(Text.class, <span class="keyword">new</span> Comparator());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进 define()方法   <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">define</span><span class="params">(Class c, WritableComparator comparator)</span> </span>&#123;</span><br><span class="line">  comparators.put(c, comparator);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>破案：当Text类加载的时候，会将当前Text.class 做为key  它的比较器对象作为value会放入comparators Map容器中。</li>
</ul>
<hr>
<h3 id="3-3-4-Combiner流程"><a href="#3-3-4-Combiner流程" class="headerlink" title="3.3.4 Combiner流程"></a>3.3.4 Combiner流程</h3><ol>
<li>Combiner组件的父类就是Reducer。</li>
<li>Combiner和Reducer的区别在于运行的位置<ol>
<li>Combiner是在每一个MapTask所在的节点运行</li>
<li>Reducer是接收全局所有Mapper的输出结果</li>
</ol>
</li>
<li>Combiner的使用场景：总的来说，为了提升MR程序的运行效率，为了减轻ReduceTask的压力，另外减少IO的开销。</li>
<li>使用Combiner<ol>
<li>自定一个Combiner类 继承Hadoop提供的Reducer</li>
<li>在Job中指定自定义的Combiner类</li>
<li>Combiner的输出kv应该跟Reducer的输入kv类型要对应起来 </li>
</ol>
</li>
<li>Combiner能够应用的前提是不能影响最终的业务逻辑</li>
<li>Combiner不适用的场景：Reduce端处理的数据考虑到多个MapTask的数据的整体集时就不能提前合并了。</li>
<li>示例 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> IntWritable valueOut = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String word = key.toString();</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (values.iterator().hasNext()) &#123;</span><br><span class="line">            sum += values.iterator().next().get();</span><br><span class="line">        &#125;</span><br><span class="line">        valueOut.set(sum);</span><br><span class="line">        log.info(<span class="string">&quot;combine-word: &#123;&#125; 累计出现次数:&#123;&#125;&quot;</span>, word, sum);</span><br><span class="line">        context.write(key, valueOut);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h2 id="3-5-OutputFormat"><a href="#3-5-OutputFormat" class="headerlink" title="3.5 OutputFormat"></a>3.5 OutputFormat</h2><p>OutputFormat主要负责最终数据的写出</p>
<h3 id="3-5-1-OutputFormat实现类"><a href="#3-5-1-OutputFormat实现类" class="headerlink" title="3.5.1 OutputFormat实现类"></a>3.5.1 OutputFormat实现类</h3><ol>
<li>探索OutputFormat的默认实现<ul>
<li>OutputFormat的实现的功能中有一个检验输出路径的方法org.apache.hadoop.mapreduce.OutputFormat#checkOutputSpecs</li>
<li>考虑到检验输出路径应该在Job提交流程中完成(不设置会报错)  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">   org.apache.hadoop.mapred.InvalidJobConfException: Output directory not set.</span><br><span class="line">at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:<span class="number">156</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:<span class="number">277</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:<span class="number">143</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at org.apache.hadoop.mapreduce.Job$<span class="number">11.</span>run(Job.java:<span class="number">1570</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at org.apache.hadoop.mapreduce.Job$<span class="number">11.</span>run(Job.java:<span class="number">1567</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at java.security.AccessController.doPrivileged(Native Method) ~[?:<span class="number">1.8</span><span class="number">.0_291</span>]</span><br><span class="line">at javax.security.auth.Subject.doAs(Subject.java:<span class="number">422</span>) ~[?:<span class="number">1.8</span><span class="number">.0_291</span>]</span><br><span class="line">at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:<span class="number">1729</span>) ~[hadoop-common-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at org.apache.hadoop.mapreduce.Job.submit(Job.java:<span class="number">1567</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:<span class="number">1588</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at sgg.hadoop.mapreduce.combiner.WordCountCombinerDriver.main(WordCountCombinerDriver.java:<span class="number">53</span>) [classes/:?]</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>跟进Job提交流程org.apache.hadoop.mapreduce.Job#waitForCompletion</li>
<li>跟进org.apache.hadoop.mapreduce.Job#submit</li>
<li>跟进org.apache.hadoop.mapreduce.JobSubmitter#submitJobInternal</li>
<li>跟进org.apache.hadoop.mapreduce.JobSubmitter#checkSpecs  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">checkSpecs</span><span class="params">(Job job)</span> <span class="keyword">throws</span> ClassNotFoundException, </span></span><br><span class="line"><span class="function">    InterruptedException, IOException </span>&#123;</span><br><span class="line">      JobConf jConf = (JobConf)job.getConfiguration();</span><br><span class="line">      <span class="comment">// Check the output specification</span></span><br><span class="line">      <span class="keyword">if</span> (jConf.getNumReduceTasks() == <span class="number">0</span> ? </span><br><span class="line">          jConf.getUseNewMapper() : jConf.getUseNewReducer()) &#123;</span><br><span class="line">          <span class="comment">//获取OutputFormat</span></span><br><span class="line">        org.apache.hadoop.mapreduce.OutputFormat&lt;?, ?&gt; output =</span><br><span class="line">          ReflectionUtils.newInstance(job.getOutputFormatClass(),</span><br><span class="line">            job.getConfiguration());</span><br><span class="line">        output.checkOutputSpecs(job);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        jConf.getOutputFormat().checkOutputSpecs(jtFs, jConf);</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进org.apache.hadoop.mapreduce.task.JobContextImpl#getOutputFormatClass  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Class&lt;? extends OutputFormat&lt;?,?&gt;&gt; getOutputFormatClass() </span><br><span class="line">     <span class="keyword">throws</span> ClassNotFoundException &#123;</span><br><span class="line">    <span class="keyword">return</span> (Class&lt;? extends OutputFormat&lt;?,?&gt;&gt;) </span><br><span class="line">      conf.getClass(OUTPUT_FORMAT_CLASS_ATTR, TextOutputFormat.class);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>破案：OutputFormat默认实现就是TextOutputFormat</li>
</ul>
</li>
<li>OutputFormat 类的体系结构<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16342996424249.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><ul>
<li>FileOutputFormat 是 OutputFormat的子类（实现类）<ul>
<li>对 checkOutputSpecs() 做了具体的实现</li>
</ul>
</li>
<li>TextOutputFormat 是 FileOutputFormat的子类<ul>
<li>对 getRecordWriter 做了具体实现</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="3-5-2-OutputFormat的使用场景"><a href="#3-5-2-OutputFormat的使用场景" class="headerlink" title="3.5.2 OutputFormat的使用场景"></a>3.5.2 OutputFormat的使用场景</h3><ul>
<li>当我们对MR最终的结果有个性化制定的需求，就可以通过自定义OutputFormat来实现</li>
</ul>
<h3 id="3-5-3-自定义OutputFormat"><a href="#3-5-3-自定义OutputFormat" class="headerlink" title="3.5.3 自定义OutputFormat"></a>3.5.3 自定义OutputFormat</h3><ul>
<li>自定一个 OutputFormat 类，继承Hadoop提供的OutputFormat，在该类中实现getRecordWriter() ,返回一个RecordWriter</li>
<li>自定义一个 RecordWriter 并且继承Hadoop提供的RecordWriter类，在该类中重写 write()  和 close()  在这些方法中完成自定义输出。</li>
<li>示例  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountOutputFormat</span> <span class="keyword">extends</span> <span class="title">FileOutputFormat</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RecordWriter&lt;Text, IntWritable&gt; <span class="title">getRecordWriter</span><span class="params">(TaskAttemptContext job)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> WordCountRecordWriter(job);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountRecordWriter</span> <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> FileSystem fileSystem;</span><br><span class="line">    HashMap&lt;Integer, FSDataOutputStream&gt; fsDataOutputStreamHashMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">WordCountRecordWriter</span><span class="params">(TaskAttemptContext job)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            fileSystem = FileSystem.get(job.getConfiguration());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            log.error(<span class="string">&quot;WordCountRecordWriter创建失败&quot;</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Text key, IntWritable value)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String word = key.toString();</span><br><span class="line">        Integer first = word.length();</span><br><span class="line">        FSDataOutputStream fsDataOutputStream = fsDataOutputStreamHashMap.get(first);</span><br><span class="line">        <span class="keyword">if</span> (fsDataOutputStream == <span class="keyword">null</span>) &#123;</span><br><span class="line">            fsDataOutputStream = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">&quot;/Users/zhenan/atguigu/project/sgg-big-data/sgg-hadoop/sgg-hadoop-mapreduce/src/main/resources/outputformat/&quot;</span> + first + <span class="string">&quot;.txt&quot;</span>));</span><br><span class="line">            fsDataOutputStreamHashMap.put(first, fsDataOutputStream);</span><br><span class="line">        &#125;</span><br><span class="line">        fsDataOutputStream.write((word + <span class="string">&quot;\t&quot;</span> + value.get()+<span class="string">&quot;\n&quot;</span>).getBytes());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        IOUtils.closeStream(fileSystem);</span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;Integer, FSDataOutputStream&gt; entry : fsDataOutputStreamHashMap.entrySet()) &#123;</span><br><span class="line">            IOUtils.closeStream(entry.getValue());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h2 id="3-6-Join多种应用"><a href="#3-6-Join多种应用" class="headerlink" title="3.6 Join多种应用"></a>3.6 Join多种应用</h2><h3 id="3-6-1-Reduce-Join"><a href="#3-6-1-Reduce-Join" class="headerlink" title="3.6.1 Reduce Join"></a>3.6.1 Reduce Join</h3><ol>
<li>概念：在MR程序中计算数据的时候，出现输入文件是多个且文件之间存在关联性，需要在计算过程中通过两个文件之间相互关联才能获取最终的计算结果。</li>
<li>ReduceJoin的思想：<ol>
<li>分析文件之间的关系，然后定位关联字段</li>
<li>在Map阶段对多个文件进行数据整合，并且让关联字段作为输出数据的key </li>
<li>当一组相同key的values进入Reduce阶段的reduce方法中第一步：先把两个文件数据分离出来，分别放到各自的对象中维护。</li>
<li>把当前一组维护好的数据进行关联操作，得到想要的数据结果。</li>
</ol>
</li>
</ol>
<h3 id="3-6-2-Map-Join"><a href="#3-6-2-Map-Join" class="headerlink" title="3.6.2 Map Join"></a>3.6.2 Map Join</h3><ol>
<li>概念：考虑MR整体的执行效率，且业务场景是一个大文件和一个小文件进行关联操作，可以使用MapJoin来实现。另外MapJoin也是解决ReduceJoin数据倾斜问题很有效的办法。</li>
<li>MapJoin的思想：<ol>
<li>分析文件之间的关系，然后定位关联字段</li>
<li>将小文件的数据映射到内存中的一个容器维护起来。 </li>
<li>当MapTask处理大文件的数据时，每读取一行数据，就根据当前行中的关联字段到内存的容器里获取对象的信息。</li>
<li>封装结果将其输出</li>
</ol>
</li>
<li>具体办法：采用DistributedCache<ol>
<li>在Mapper的setup阶段，将文件读取到缓存集合中。</li>
<li>在Driver驱动类中加载缓存 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//缓存普通文件到Task运行节点。</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">&quot;file:///e:/cache/pd.txt&quot;</span>));</span><br><span class="line"><span class="comment">//如果是集群运行,需要设置HDFS路径</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9820/cache/pd.txt&quot;</span>));</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<hr>
<h2 id="3-7-计数器"><a href="#3-7-计数器" class="headerlink" title="3.7 计数器"></a>3.7 计数器</h2><ul>
<li>Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量</li>
<li>计数器API<ol>
<li>采用枚举的方式统计计数</li>
<li>采用计数器组、计数器名称的方式统计</li>
<li>计数结果在程序运行后的控制台上查看</li>
</ol>
</li>
</ul>
<hr>
<h2 id="3-8-数据清洗（ETL）"><a href="#3-8-数据清洗（ETL）" class="headerlink" title="3.8 数据清洗（ETL）"></a>3.8 数据清洗（ETL）</h2><ul>
<li>在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序。</li>
<li>Mapper程序不符合规则直接return</li>
</ul>
<hr>
<h2 id="3-9-MapReduce工作流程梳理"><a href="#3-9-MapReduce工作流程梳理" class="headerlink" title="3.9 MapReduce工作流程梳理"></a>3.9 MapReduce工作流程梳理</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16344606281489.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16344606667460.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>MapTask收集map()方法输出的kv对，放到内存缓冲区中</li>
<li>从内存缓冲区不断溢写本地磁盘文件，可能会溢写多个文件</li>
<li>多个溢出文件会被合并成大的溢写文件</li>
<li>在溢写过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序</li>
<li>ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据</li>
<li>ReduceTask会抓取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序）</li>
<li>合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法）</li>
<li>注意：<ol>
<li>Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。</li>
<li>缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb默认100M。</li>
</ol>
</li>
<li>MapTask源码分析<ul>
<li>定位 map方法输出结果的位置，TaskInputOutputContext#write<ul>
<li>实现类TaskInputOutputContextImpl#write</li>
</ul>
</li>
<li>跟进org.apache.hadoop.mapred.MapTask.NewOutputCollector#write<ul>
<li>获取分区编号org.apache.hadoop.mapreduce.Partitioner#getPartition</li>
<li>k-v放入环形缓冲区org.apache.hadoop.mapred.MapOutputCollector#collect</li>
<li>map端所有的kv全部写出后会执行org.apache.hadoop.mapred.MapTask.NewOutputCollector#close<ul>
<li>执行溢写org.apache.hadoop.mapred.MapTask.MapOutputBuffer#flush </li>
<li>排序溢写org.apache.hadoop.mapred.MapTask.MapOutputBuffer#sortAndSpill<ul>
<li>执行combiner org.apache.hadoop.mapred.Task.CombinerRunner#combine</li>
<li>初始化combiner org.apache.hadoop.mapred.Task.CombinerRunner#create </li>
</ul>
</li>
<li>合并文件org.apache.hadoop.mapred.MapTask.MapOutputBuffer#mergeParts</li>
<li>结束</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>ReduceTask源码分析<ul>
<li>ReduceTask执行入口org.apache.hadoop.mapred.ReduceTask#run<ul>
<li>初始化ReduceTask org.apache.hadoop.mapred.Task#initialize<ul>
<li>获取OutputFormat org.apache.hadoop.mapred.Task:605</li>
<li>获取Shuffer Consumer  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  Class&lt;? extends ShuffleConsumerPlugin&gt; clazz =</span><br><span class="line">job.getClass(MRConfig.SHUFFLE_CONSUMER_PLUGIN, Shuffle.class, ShuffleConsumerPlugin.class);</span><br></pre></td></tr></table></figure></li>
<li>初始化shuffle consumer org.apache.hadoop.mapred.ShuffleConsumerPlugin#init<ul>
<li>创建shuffle实现类org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl<ul>
<li>获取MapTask数量 <code>totalMaps = job.getNumMapTasks();</code></li>
</ul>
</li>
<li>创建合并管理器org.apache.hadoop.mapreduce.task.reduce.Shuffle#createMergeManager<ul>
<li>实现类 org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl#MergeManagerImpl<ul>
<li>ReduceTask内存最大值  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment">// Allow unit tests to fix Runtime memory</span></span><br><span class="line"><span class="keyword">this</span>.memoryLimit = (<span class="keyword">long</span>)(jobConf.getLong(</span><br><span class="line">    MRJobConfig.REDUCE_MEMORY_TOTAL_BYTES,</span><br><span class="line">    Runtime.getRuntime().maxMemory()) * maxInMemCopyUse);</span><br></pre></td></tr></table></figure></li>
<li>创建内存合并器 org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl#createInMemoryMerger<ul>
<li>实现类 org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.InMemoryMerger</li>
<li>合并方法org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.InMemoryMerger#merge</li>
<li>Combiner + 溢写org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl#combineAndSpill</li>
</ul>
</li>
<li>创建磁盘合并器 org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.OnDiskMerger</li>
</ul>
</li>
</ul>
</li>
<li>开始抓取数据org.apache.hadoop.mapreduce.task.reduce.Shuffle:107  <code>eventFetcher.start();</code></li>
<li>抓取数据结束org.apache.hadoop.mapreduce.task.reduce.Shuffle:141 <code>eventFetcher.shutDown();</code></li>
<li>copy阶段完成，启动下一个阶段sort org.apache.hadoop.mapreduce.task.reduce.Shuffle:151 <code>// copyPhase.complete();</code></li>
<li>标记进入sort阶段 org.apache.hadoop.mapreduce.task.reduce.Shuffle:152 <code>taskStatus.setPhase(TaskStatus.Phase.SORT);</code></li>
</ul>
</li>
<li>sort阶段完成 开启下一阶段reduce org.apache.hadoop.mapred.ReduceTask:382 <code>sortPhase.complete();</code></li>
</ul>
</li>
<li>reduce();  //reduce阶段调用的就是我们自定义的reduce方法，会被调用多次</li>
<li>cleanup(context); //reduce完成之前，会最后调用一次Reducer里面的cleanup方法</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="3-9-1分析Job提交流程的源码"><a href="#3-9-1分析Job提交流程的源码" class="headerlink" title="3.9.1分析Job提交流程的源码"></a>3.9.1分析Job提交流程的源码</h3><ul>
<li>定位提交入口 org.apache.hadoop.mapreduce.Job#waitForCompletion</li>
<li>跟进 <code>org.apache.hadoop.mapreduce.Job#waitForCompletion</code>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">waitForCompletion</span><span class="params">(<span class="keyword">boolean</span> verbose</span></span></span><br><span class="line"><span class="params"><span class="function">                              )</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">  <span class="comment">// 判断当前Job的状态是否为定义阶段</span></span><br><span class="line">  <span class="keyword">if</span> (state == JobState.DEFINE) &#123;</span><br><span class="line">    <span class="comment">//提交方法</span></span><br><span class="line">    submit();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (verbose) &#123;</span><br><span class="line">    monitorAndPrintJob();</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// get the completion poll interval from the client.</span></span><br><span class="line">    <span class="keyword">int</span> completionPollIntervalMillis = </span><br><span class="line">      Job.getCompletionPollInterval(cluster.getConf());</span><br><span class="line">    <span class="keyword">while</span> (!isComplete()) &#123;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(completionPollIntervalMillis);</span><br><span class="line">          &#125; <span class="keyword">catch</span> (InterruptedException ie) &#123;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> isSuccessful();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进<code>org.apache.hadoop.mapreduce.Job#submit</code>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">submit</span><span class="params">()</span> </span></span><br><span class="line"><span class="function">       <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">  <span class="comment">//判断当前为定义阶段</span></span><br><span class="line">  ensureState(JobState.DEFINE);</span><br><span class="line">  <span class="comment">//兼容老版本API</span></span><br><span class="line">  setUseNewAPI();</span><br><span class="line">  <span class="comment">//连接集群（如果是本地模式结果就是LocalRunner, 如果Yarn集群结果就是YARNRuuner）</span></span><br><span class="line">  connect();</span><br><span class="line">  <span class="comment">// 开始提交Job</span></span><br><span class="line">  <span class="keyword">final</span> JobSubmitter submitter = </span><br><span class="line">      getJobSubmitter(cluster.getFileSystem(), cluster.getClient());</span><br><span class="line">  status = ugi.doAs(<span class="keyword">new</span> PrivilegedExceptionAction&lt;JobStatus&gt;() &#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> JobStatus <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, </span></span><br><span class="line"><span class="function">    ClassNotFoundException </span>&#123;</span><br><span class="line">      <span class="comment">//执行提交动作</span></span><br><span class="line">      <span class="keyword">return</span> submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">  state = JobState.RUNNING;</span><br><span class="line">  LOG.info(<span class="string">&quot;The url to track the job: &quot;</span> + getTrackingURL());</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进<code>org.apache.hadoop.mapreduce.JobSubmitter#submitJobInternal</code>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Internal method for submitting jobs to the system.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * &lt;p&gt;The job submission process involves:</span></span><br><span class="line"><span class="comment"> * &lt;ol&gt;</span></span><br><span class="line"><span class="comment"> *   &lt;li&gt;</span></span><br><span class="line"><span class="comment"> *   检测输入输出路径的合法性</span></span><br><span class="line"><span class="comment"> *   Checking the input and output specifications of the job.</span></span><br><span class="line"><span class="comment"> *   &lt;/li&gt;</span></span><br><span class="line"><span class="comment"> *   &lt;li&gt;</span></span><br><span class="line"><span class="comment"> *   给当前Job计算切片信息</span></span><br><span class="line"><span class="comment"> *   Computing the &#123;<span class="doctag">@link</span> InputSplit&#125;s for the job.</span></span><br><span class="line"><span class="comment"> *   &lt;/li&gt;</span></span><br><span class="line"><span class="comment"> *   &lt;li&gt;</span></span><br><span class="line"><span class="comment"> *   添加分布式缓存文件</span></span><br><span class="line"><span class="comment"> *   Setup the requisite accounting information for the </span></span><br><span class="line"><span class="comment"> *   &#123;<span class="doctag">@link</span> DistributedCache&#125; of the job, if necessary.</span></span><br><span class="line"><span class="comment"> *   &lt;/li&gt;</span></span><br><span class="line"><span class="comment"> *   &lt;li&gt;</span></span><br><span class="line"><span class="comment"> *   将必要的内容都拷贝到 job执行的临时目录（jar包、切片信息、配置文件）</span></span><br><span class="line"><span class="comment"> *   Copying the job&#x27;s jar and configuration to the map-reduce system</span></span><br><span class="line"><span class="comment"> *   directory on the distributed file-system. </span></span><br><span class="line"><span class="comment"> *   &lt;/li&gt;</span></span><br><span class="line"><span class="comment"> *   &lt;li&gt;</span></span><br><span class="line"><span class="comment"> *   提交Job</span></span><br><span class="line"><span class="comment"> *   Submitting the job to the &lt;code&gt;JobTracker&lt;/code&gt; and optionally</span></span><br><span class="line"><span class="comment"> *   monitoring it&#x27;s status.</span></span><br><span class="line"><span class="comment"> *   &lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;/ol&gt;&lt;/p&gt;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> job the configuration to submit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> cluster the handle to the Cluster</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> ClassNotFoundException</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">JobStatus <span class="title">submitJobInternal</span><span class="params">(Job job, Cluster cluster)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> ClassNotFoundException, InterruptedException, IOException </span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h2 id="3-8-MapReduce开发总结"><a href="#3-8-MapReduce开发总结" class="headerlink" title="3.8 MapReduce开发总结"></a>3.8 MapReduce开发总结</h2><ol>
<li>输入数据接口：InputFormat<ul>
<li>默认使用的实现类是：TextInputFormat</li>
<li>TextInputFormat 的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为 value 返回。</li>
<li>CombineTextInputFormat 可以把多个小文件合并成一个切片处理，提高处理效率。</li>
</ul>
</li>
<li>map逻辑处理接口：Mapper <ul>
<li>用户根据业务需求实现其中三个方法：map() setup() cleanup () </li>
</ul>
</li>
<li>Partitioner 分区<ul>
<li>有默认实现 HashPartitioner，逻辑是根据 key 的哈希值和 numReduces 来返回一个分区号；key.hashCode()&amp;Integer.MAXVALUE % numReduces</li>
<li>如果业务上有特别的需求，可以自定义分区。</li>
</ul>
</li>
<li>Comparable 排序<ul>
<li>当我们用自定义的对象作为 key 来输出时，就必须要实现 WritableComparable 接口，重写其中的 compareTo()方法。</li>
<li>部分排序：对最终输出的每一个文件进行内部排序。</li>
<li>全排序：对所有数据进行排序，通常只有一个 Reduce。 （4）二次排序：排序的条件有两个。</li>
</ul>
</li>
<li>Combiner 合并<ul>
<li>Combiner 合并可以提高程序执行效率，减少 IO 传输。但是使用时必须不能影响原有的业务处理结果。</li>
</ul>
</li>
<li>reduce逻辑处理接口：Reducer<ul>
<li>用户根据业务需求实现其中三个方法：reduce() setup() cleanup () </li>
</ul>
</li>
<li>输出数据接口：OutputFormat<ul>
<li>默认实现类是 TextOutputFormat，功能逻辑是：将每一个 KV 对，向目标文本文件输出一行。</li>
<li>用户还可以自定义 OutputFormat。</li>
</ul>
</li>
</ol>
<h2 id="问答"><a href="#问答" class="headerlink" title="问答"></a>问答</h2><h4 id="一、描述一下手写MR的大概流程和规范"><a href="#一、描述一下手写MR的大概流程和规范" class="headerlink" title="一、描述一下手写MR的大概流程和规范"></a>一、描述一下手写MR的大概流程和规范</h4><ol>
<li>继承Mapper重写map方法</li>
<li>继承Reducer重写reduce方法 </li>
<li>编写Driver配置Job参数</li>
<li>提交Job</li>
</ol>
<h4 id="二、如何实现Hadoop中的序列化，以及Hadoop的序列化和Java的序列化有什么区别？"><a href="#二、如何实现Hadoop中的序列化，以及Hadoop的序列化和Java的序列化有什么区别？" class="headerlink" title="二、如何实现Hadoop中的序列化，以及Hadoop的序列化和Java的序列化有什么区别？"></a>二、如何实现Hadoop中的序列化，以及Hadoop的序列化和Java的序列化有什么区别？</h4><ol>
<li>实现Writeable接口</li>
<li>无参构造</li>
<li>重写序列化方法write</li>
<li>重写反序列化方法readFields</li>
<li>write 方法和readFields方法保持一致</li>
</ol>
<h4 id="三、概述一下MR程序的执行流程"><a href="#三、概述一下MR程序的执行流程" class="headerlink" title="三、概述一下MR程序的执行流程"></a>三、概述一下MR程序的执行流程</h4><ol>
<li>数据读取阶段：InputFormat进行切片读取</li>
<li>map阶段：执行map方法业务逻辑，输出处理后的kv数据</li>
<li>shuffle阶段：对map阶段输出的kv进行分区，排序，分组，通知reduce取数据</li>
<li>reduce阶段：执行reduce方法业务逻辑，输出数据</li>
<li>输出阶段：OutputFormat处理输出数据，写入文件</li>
</ol>
<h4 id="四、InputFormat负责数据写的时候要进行切片，为什么切片大小默认是128M"><a href="#四、InputFormat负责数据写的时候要进行切片，为什么切片大小默认是128M" class="headerlink" title="四、InputFormat负责数据写的时候要进行切片，为什么切片大小默认是128M"></a>四、InputFormat负责数据写的时候要进行切片，为什么切片大小默认是128M</h4><ol>
<li>HDFS默认的Block大小为128M</li>
<li>默认128M切片可以从单个数据块读取到全部数据</li>
<li>避免了跨机器读取导致大量IO</li>
</ol>
<h4 id="五、描述一下切片的逻辑（从源码角度描述）"><a href="#五、描述一下切片的逻辑（从源码角度描述）" class="headerlink" title="五、描述一下切片的逻辑（从源码角度描述）"></a>五、描述一下切片的逻辑（从源码角度描述）</h4><ol>
<li>定位入口InputFormat#getSplits</li>
<li>由FileInputFormat#getSplits具体实现</li>
<li>确定最小切片大小默认1，最大切片大小默认Long.MAX_VALUE</li>
<li>获取是否对输入路径递归执行的参数默认false，递归处理输入路径下的所有文件</li>
<li>判断是否能够切分，压缩文件不进行切分</li>
<li>获取文件大小和块大小</li>
<li>计算切片大小max(最小切块大小，min(块大小，最大切块大小))</li>
<li>判断剩余文件大小是否可以继续切分，大于1.1倍的切片大小则继续切分</li>
</ol>
<h4 id="六、CombineTextInputFormat机制是怎么实现的"><a href="#六、CombineTextInputFormat机制是怎么实现的" class="headerlink" title="六、CombineTextInputFormat机制是怎么实现的"></a>六、CombineTextInputFormat机制是怎么实现的</h4><ol>
<li>CombineTextInputFormat默认切片大小为4m</li>
<li>虚拟切片过程：文件和切片大小进行比较<ol>
<li>当前文件&gt;切片大小 且 小于2倍的切片大小，就切成2片</li>
<li>当前文件&gt;大于2倍的切片大小，直接切出切片大小的文件，重复执行虚拟切片过程</li>
</ol>
</li>
<li>实际切片过程：比较虚拟切片的结果文件大小和设置切片大小<ol>
<li>如果大于等于切片大小就单独行程一个切片</li>
<li>如果小于设置切片大小就和下一个虚拟文件进行合并，重复执行至大于切片大小</li>
<li>合并后大于设置切片大小单独就形成一个切片</li>
</ol>
</li>
</ol>
<h4 id="七、阐述一下-Shuffle机制-流程？"><a href="#七、阐述一下-Shuffle机制-流程？" class="headerlink" title="七、阐述一下 Shuffle机制 流程？"></a>七、阐述一下 Shuffle机制 流程？</h4><ol>
<li>Shuffle机制处于Map过程和Reduce过程的中间阶段</li>
<li>具体实现的功能包括，分区，分组，排序，合并</li>
<li>map端的Shuffle<ol>
<li>partition: 获取分区编号保存到元数据中，数据写入环形缓冲区</li>
<li>spill: 环形缓冲区默认100M，到达80%时触发spill溢写，剩余20%继续执行写入</li>
<li>sort: spill溢写过程根据分区编号，Key比较规则进行排序（升序，快排），溢写文件保证分区内有序</li>
<li>combine: 触发spill进行sort之后，写入文件之前会进行combine操作；溢写文件大于3个时merge的过程中也会执行combine</li>
<li>merge: 对多个溢写文件进行合并，算法为多路归并排序，最终生成一个文件作为map阶段的输出</li>
<li>通知reduce拉取数据</li>
</ol>
</li>
<li>reduce端的Shuffle<ol>
<li>copy过程：拉取MapTask处理完的数据，使用0.7 × maxHeap大小的堆内存空间作为内存缓冲区</li>
<li>merge过程：内存中数据到达阈值会触发内存到磁盘的合并；map端数据读取完成后触发磁盘到磁盘的合并，算法为归并排序</li>
<li>reduce输入：merge阶段合并为一个大文件作Reduce数据文件执行Reduce逻辑，Shuffle阶段结束</li>
</ol>
</li>
</ol>
<h4 id="八、在MR程序中由谁来决定分区的数量，哪个阶段环节会开始往分区中写数据？"><a href="#八、在MR程序中由谁来决定分区的数量，哪个阶段环节会开始往分区中写数据？" class="headerlink" title="八、在MR程序中由谁来决定分区的数量，哪个阶段环节会开始往分区中写数据？"></a>八、在MR程序中由谁来决定分区的数量，哪个阶段环节会开始往分区中写数据？</h4><ol>
<li>分区由业务逻辑决定</li>
<li>分区规则有ReduceTask数量控制</li>
<li>Shuffle阶段确定分区编号</li>
<li>溢写阶段进行分区</li>
<li>Reduce执行结束，写入分区文件</li>
</ol>
<h4 id="九、阐述MR中实现分区的思路（从源码角度分析）"><a href="#九、阐述MR中实现分区的思路（从源码角度分析）" class="headerlink" title="九、阐述MR中实现分区的思路（从源码角度分析）"></a>九、阐述MR中实现分区的思路（从源码角度分析）</h4><ol>
<li>Shuffle阶段确定分区编号</li>
<li>溢写阶段根据分区编号生成不同的溢写文件</li>
<li>Reduce从多个map输出的文件中取自己分区的数据，处理后生成该分区的结果文件</li>
<li>默认分区规则为根据Key.hashcode 取模，作为分区编号</li>
</ol>
<h4 id="十、描述一下Hadoop中实现排序比较的规则"><a href="#十、描述一下Hadoop中实现排序比较的规则" class="headerlink" title="十、描述一下Hadoop中实现排序比较的规则"></a>十、描述一下Hadoop中实现排序比较的规则</h4><ol>
<li>Hadoop中实现排序依赖Comparator#compare方法</li>
<li>Comparator获取逻辑如下<ol>
<li>首先从jobContext中获取配置比较器的类名</li>
<li>如果获取到直接通过反射创建比较器，流程结束</li>
<li>如果未配置比较器类名，先从比较器缓存Map中根据输出key类对象获取比较器，如果获取到，直接返回比较器，流程结束</li>
<li>如果比较器缓存中未获取到比较器，强制加载后重新获取</li>
<li>如果还获取不到，Hadoop会使用输出key的class对象向创建一个比较器（要求必须实现了WritableComparable）</li>
</ol>
</li>
</ol>
<h4 id="十一、Hadoop中实现排序的两种方案分别是什么？"><a href="#十一、Hadoop中实现排序的两种方案分别是什么？" class="headerlink" title="十一、Hadoop中实现排序的两种方案分别是什么？"></a>十一、Hadoop中实现排序的两种方案分别是什么？</h4><ol>
<li>比较对象实现WritableComparable接口，重写compareTo方法</li>
<li>自定义Comparator，继承WritableComparator，重写compare方法，在Driver中指定</li>
<li>自定义Comparator，继承WritableComparator，重写compare方法，静态代码注册比较器</li>
</ol>
<h4 id="十二、编写MR的时候什么情况下使用Combiner-具体实现流程是什么？"><a href="#十二、编写MR的时候什么情况下使用Combiner-具体实现流程是什么？" class="headerlink" title="十二、编写MR的时候什么情况下使用Combiner 具体实现流程是什么？"></a>十二、编写MR的时候什么情况下使用Combiner 具体实现流程是什么？</h4><ol>
<li>为了提高MR运行效率，减轻ReduceTask压力，减少copy环节IO开销</li>
<li>Combiner执行不影响最终的业务逻辑</li>
<li>Reduce端对MaoTask数据的整体性没有要求</li>
<li>Combiner实现流程<ol>
<li>自定义Combiner类，继承Reducer，重写reduce方法</li>
<li>Job中指定Combiner</li>
<li>输入k-v为map的输出，输出k-v为reduce的输入</li>
</ol>
</li>
</ol>
<h4 id="十三、OutputFormat自定义实现流程描述一下"><a href="#十三、OutputFormat自定义实现流程描述一下" class="headerlink" title="十三、OutputFormat自定义实现流程描述一下"></a>十三、OutputFormat自定义实现流程描述一下</h4><ol>
<li>自定义OutputFormat类，继承OutputFormat，实现getRecordWriter抽象方法，返回自定义RecordWriter</li>
<li>自定义RecordWriter类，继承RecordWriter，重写write实现数据写出的逻辑，重写close方法对资源进行关闭</li>
<li>Job中指定OutputFormat处理类</li>
</ol>
<h4 id="十四、MR实现-ReduceJoin-的思路，以及ReduceJoin方案有哪些不足？"><a href="#十四、MR实现-ReduceJoin-的思路，以及ReduceJoin方案有哪些不足？" class="headerlink" title="十四、MR实现 ReduceJoin 的思路，以及ReduceJoin方案有哪些不足？"></a>十四、MR实现 ReduceJoin 的思路，以及ReduceJoin方案有哪些不足？</h4><ul>
<li>思路<ol>
<li>分析文件关联，确定关联字段</li>
<li>定义统一对象，包含关联字段和数据来源</li>
<li>map端参与文件输出统一对象，key为关联字段</li>
<li>reduce端以关联字段为key，统一对象为value，从统一对象中根据数据来源拆分对象</li>
<li>根据拆分对象进行关联</li>
</ol>
</li>
<li>不足<ol>
<li>耗费性能，需要参与数据全部遍历才能进行join</li>
<li>无法解决数据倾斜问题</li>
<li>海量数据容易造成reduce崩溃，任务失败</li>
</ol>
</li>
</ul>
<h4 id="十五、MR实现-MapJoin-的思路，以及MapJoin的局限性是什么？"><a href="#十五、MR实现-MapJoin-的思路，以及MapJoin的局限性是什么？" class="headerlink" title="十五、MR实现 MapJoin 的思路，以及MapJoin的局限性是什么？"></a>十五、MR实现 MapJoin 的思路，以及MapJoin的局限性是什么？</h4><ul>
<li>思路：<ol>
<li>分析文件关联，确定关联字段</li>
<li>小文件使用DistributedCache加载到缓存中</li>
<li>map端每读取一行数据，都根据关联字段，在缓存中获取对应关联数据</li>
<li>输出包换关联信息的完整数据给reduce</li>
</ol>
</li>
<li>局限<ol>
<li>适用数据量差异比较大的两个数据集</li>
</ol>
</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MR/" rel="tag">MR</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Linux"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/11/07/Linux/"
    >Linux</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/11/07/Linux/" class="article-date">
  <time datetime="2021-11-07T00:08:37.000Z" itemprop="datePublished">2021-11-07</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Linux/">Linux</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h1><h3 id="1、Linux简介"><a href="#1、Linux简介" class="headerlink" title="1、Linux简介"></a>1、Linux简介</h3><p><strong>什么是操作系统？</strong></p>
<p>操作系统是管理计算机硬件与软件资源的计算机程序，同时也是计算机系统的内核与基石。操作系统需要处理如管理与配置内存、决定系统资源供需的优先次序、控制输入设备与输出设备、操作网络与管理文件系统等基本事务。操作系统也提供一个让用户与系统交互的操作界面</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211120630067.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211120630067"></p>
<p><strong>常见的操作系统</strong></p>
<ul>
<li>2</li>
<li>MAC OS</li>
<li>Android</li>
<li>iOS</li>
</ul>
<p><strong>操作系统的发展史</strong></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211120911148.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211120911148"></p>
<ul>
<li><p>Unix</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211120956539.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211120956539"></p>
<p>1965年之前的时候，电脑并不像现在一样普遍，它可不是一般人能碰的起的，除非是军事或者学院的研究机构，而且当时大型主机至多能提供30台终端（30个键盘、显示器)，连接一台电脑</p>
<p>为了解决数量不够的问题：</p>
<ul>
<li><p>1965年左后由贝尔实验室、麻省理工学院 以及 通用电气共同发起了Multics项目，想让大型主机支持300台终端</p>
</li>
<li><p>1969年前后这个项目进度缓慢，资金短缺，贝尔实验室退出了研究</p>
</li>
<li><p>1969年从这个项目中退出的Ken Thompson当时在实验室无聊时，为了让一台空闲的电脑上能够运行“星际旅行”游行，在8月份左右趁着其妻子探亲的时间，用了1个月的时间 编写出了 Unix操作系统的原型</p>
</li>
<li><p>1970年，美国贝尔实验室的 Ken Thompson，以 BCPL语言 为基础，设计出很简单且很接近硬件的 B语言（取BCPL的首字母），并且他用B语言写了第一个UNIX操作系统</p>
</li>
<li><p>因为B语言的跨平台性较差，为了能够在其他的电脑上也能够运行这个非常棒的Unix操作系统，Dennis Ritchie和Ken Thompson 从B语言的基础上准备研究一个更好的语言</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/jie-ping20200211121129.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="截屏2020-02-1112.11.29"></p>
</li>
<li><p>1972年，美国贝尔实验室的 Dennis Ritchie在B语言的基础上最终设计出了一种新的语言，他取了BCPL的第二个字母作为这种语言的名字，这就是C语言</p>
</li>
<li><p>1973年初，C语言的主体完成。Thompson和Ritchie迫不及待地开始用它完全重写了现在大名鼎鼎的Unix操作系统</p>
</li>
</ul>
</li>
<li><p>Minix</p>
<p>简介：因为AT&amp;T(通用电气)的政策改变，在Version 7 Unix推出之后，发布新的使用条款，将UNIX源代码私有化，在大学中不再能使用UNIX源代码。Andrew S. Tanenbaum(塔能鲍姆)教授为了能在课堂上教授学生操作系统运作的实务细节，决定在不使用任何AT&amp;T的源代码前提下，自行开发与UNIX兼容的操作系统，以避免版权上的争议。他以小型UNIX（mini-UNIX）之意，将它称为MINIX</p>
<p>没有火的原因：Minix的创始人说，MINIX 3没有统治世界是源于他在1992年犯下的一个错误，当时他认为BSD必然会一统天下，因为它是一个更稳定和更成熟的系统，其它操作系统难以与之竞争。因此他的MINIX的重心集中在教育上。四名BSD开发者已经成立了一家公司销售BSD系统，他们甚至还有一个有趣的电话号码1-800-ITS-UNIX。然而他们正因为这个电话号码而惹火上身。美国电话电报公司因电话号码而提起诉讼。官司打了三年才解决。在此期间，BSD陷于停滞，而Linux则借此一飞冲天。他的错误在于没有意识官司竟然持续了如此长的时间，以及BSD会因此受到削弱。如果美国电话电报公司没有起诉，Linux永远不会流行起来，BSD将统治世界</p>
</li>
<li><p>Linux</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211121238635.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211121238635"></p>
<p>因为Minix只是教学使用，因此功能并不强，因此Torvalds利用GNU的bash当做开发环境，gcc当做编译工具，编写了Linux内核-v0.02，但是一开始Linux并不能兼容Unix，即Unix上跑的应用程序不能在Linux上跑，即应用程序与内核之间的接口不一致，因为Unix是遵循POSIX规范的，因此Torvalds修改了Linux，并遵循POSIX（Portable Operating System Interface，他规范了应用程序与内核的接口规范）； 一开始Linux只适用于386，后来经过全世界的网友的帮助，最终能够兼容多种硬件</p>
<p>Linux发展的重要里程碑：</p>
<ul>
<li>1990, Linus Torvalds 首次接触 MINIX</li>
<li>1991, Linus Torvalds 开始在 MINIX 上编写各种驱动程序等操作系统内核组件</li>
<li>1991 底, Linus Torvalds 公开了 Linux 内核</li>
<li>1993, Linux 1.0 版发行，Linux 转向 GPL 版权协议</li>
<li>1994, Linux 的第一个商业发行版 Slackware 问世</li>
<li>1996, 美国国家标准技术局的计算机系统实验室确认 Linux 版本1.2.13（由 Open Linux 公司打包）符合 POSIX 标准</li>
<li>1999, Linux 的简体中文发行版相继问世</li>
</ul>
</li>
</ul>
<p><strong>Linux版本</strong></p>
<ul>
<li><p>Linux内核版本</p>
<p>内核(kernel)是系统的心脏，是运行程序和管理像磁盘和打印机等硬件设备的核心程序，它提供了一个在裸设备与应用程序间的抽象层</p>
<p>Linux内核版本又分为稳定版和开发版，两种版本是相互关联，相互循环</p>
<table>
<thead>
<tr>
<th align="left">版本</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">稳定版</td>
<td align="left">具有工业级强度，可以广泛地应用和部署。新的稳定版相对于较旧的只是修正一些bug或加入一些新的驱动程序</td>
</tr>
<tr>
<td align="left">开发版</td>
<td align="left">由于要试验各种解决方案，所以变化很快</td>
</tr>
</tbody></table>
<p>内核源码网址[<a href="http://www.kernel.org]，所有来自全世界的对Linux源码的修改最终都会汇总到这个网站，由Linus领导的开源社区对其进行甄别和修改最终决定是否进入到Linux主线内核源码中">http://www.kernel.org]，所有来自全世界的对Linux源码的修改最终都会汇总到这个网站，由Linus领导的开源社区对其进行甄别和修改最终决定是否进入到Linux主线内核源码中</a></p>
</li>
<li><p>Linux发行版本</p>
<p>通常包含了包括桌面环境、办公套件、媒体播放器、数据库等应用软件</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211121551282.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211121551282"></p>
<p>常见发行版本：</p>
<ul>
<li>Fedora</li>
<li>Redhat</li>
<li>Ubuntu</li>
<li>CentOS</li>
</ul>
</li>
</ul>
<p><strong>Linux应用领域</strong></p>
<table>
<thead>
<tr>
<th>领域</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>个人桌面领域</td>
<td>此领域是传统linux应用最薄弱的环节，传统linux由于界面简单、操作复杂、应用软件少的缺点，一直被windows所压制，但近些年来随着ubuntu、fedora等优秀桌面环境的兴起，同时各大硬件厂商对其支持的加大，linux在个人桌面领域的占有率在逐渐的提高</td>
</tr>
<tr>
<td>服务器领域</td>
<td>linux免费、稳定、高效等特点在这里得到了很好的体现，但早期因为维护、运行等原因同样受到了很大的限制，但近些年来linux服务器市场得到了飞速的提升，尤其在一些高端领域尤为广泛</td>
</tr>
<tr>
<td>嵌入式领域</td>
<td>linux运行稳定、对网络的良好支持性、低成本，且可以根据需要进行软件裁剪，内核最小可以达到几百KB等特点，使其近些年来在嵌入式领域的应用得到非常大的提高</td>
</tr>
</tbody></table>
<p><strong>Linux和Windows区别</strong></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/bu-huo.PNG?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="捕获"></p>
<p><strong>CentOS下载地址</strong></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/bu-huo1627694182903.PNG?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="捕获"></p>
<h3 id="2、安装VMware虚拟机"><a href="#2、安装VMware虚拟机" class="headerlink" title="2、安装VMware虚拟机"></a>2、安装VMware虚拟机</h3><h3 id="3、安装linux系统"><a href="#3、安装linux系统" class="headerlink" title="3、安装linux系统"></a>3、安装linux系统</h3><h3 id="4、文件和目录"><a href="#4、文件和目录" class="headerlink" title="4、文件和目录"></a>4、文件和目录</h3><p><strong>Windows文件系统</strong></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211134106179.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211134106179"></p>
<p>在 windows 平台下，打开“计算机”，我们看到的是一个个的驱动器盘符</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211134123743.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211134123743"></p>
<p>每个驱动器都有自己的根目录结构，这样形成了多个树并列的情形</p>
<p><strong>Linux文件系统</strong></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211145837589.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211145837589"></p>
<p>centos没有盘符这个概念，只有一个根目录/，所有文件都在它下面</p>
<p><strong>目录</strong></p>
<table>
<thead>
<tr>
<th>路径</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>/</td>
<td>根目录，一般根目录下只存放目录，在Linux下有且只有一个根目录。所有的东西都是从这里开始。当你在终端里输入“/home”，你其实是在告诉电脑，先从/（根目录）开始，再进入到home目录</td>
</tr>
<tr>
<td>/bin<br />/usr/bin</td>
<td>可执行二进制文件的目录，如常用的命令ls、tar、mv、cat等</td>
</tr>
<tr>
<td>/boot</td>
<td>放置linux系统启动时用到的一些文件，如Linux的内核文件：/boot/vmlinuz，系统引导管理器：/boot/grub</td>
</tr>
<tr>
<td>/dev</td>
<td>存放linux系统下的设备文件，访问该目录下某个文件，相当于访问某个设备，常用的是挂载光驱 mount /dev/cdrom /mnt</td>
</tr>
<tr>
<td>/etc</td>
<td>系统配置文件存放的目录，不建议在此目录下存放可执行文件，重要的配置文件有 /etc/inittab、/etc/fstab、/etc/init.d、/etc/X11、/etc/sysconfig、/etc/xinetd.d</td>
</tr>
<tr>
<td>/home</td>
<td>系统默认的用户家目录，新增用户账号时，用户的家目录都存放在此目录下，<del>表示当前用户的家目录，</del>edu 表示用户 edu 的家目录</td>
</tr>
<tr>
<td>/lib<br />/usr/lib<br />/usr/local/lib</td>
<td>系统使用的函数库的目录，程序在执行过程中，需要调用一些额外的参数时需要函数库的协助</td>
</tr>
<tr>
<td>/lost+fount</td>
<td>系统异常产生错误时，会将一些遗失的片段放置于此目录下</td>
</tr>
<tr>
<td>/mnt</td>
<td>/media：光盘默认挂载点，通常光盘挂载于 /mnt/cdrom 下，也不一定，可以选择任意位置进行挂载</td>
</tr>
<tr>
<td>/opt</td>
<td>给主机额外安装软件所摆放的目录</td>
</tr>
<tr>
<td>/proc</td>
<td>此目录的数据都在内存中，如系统核心，外部设备，网络状态，由于数据都存放于内存中，所以不占用磁盘空间，比较重要的目录有 /proc/cpuinfo、/proc/interrupts、/proc/dma、/proc/ioports、/proc/net/* 等</td>
</tr>
<tr>
<td>/root</td>
<td>系统管理员root的家目录</td>
</tr>
<tr>
<td>/sbin<br />/usr/sbin<br />/usr/local/sbin</td>
<td>放置系统管理员使用的可执行命令，如fdisk、shutdown、mount 等。与 /bin 不同的是，这几个目录是给系统管理员 root使用的命令，一般用户只能”查看”而不能设置和使用</td>
</tr>
<tr>
<td>/tmp</td>
<td>一般用户或正在执行的程序临时存放文件的目录，任何人都可以访问，重要数据不可放置在此目录下</td>
</tr>
<tr>
<td>/srv</td>
<td>服务启动之后需要访问的数据目录，如 www 服务需要访问的网页数据存放在 /srv/www 内</td>
</tr>
<tr>
<td>/usr</td>
<td>应用程序存放目录，/usr/bin 存放应用程序，/usr/share 存放共享数据，/usr/lib 存放不能直接运行的，却是许多程序运行所必需的一些函数库文件。/usr/local: 存放软件升级包。/usr/share/doc: 系统说明文件存放目录。/usr/share/man: 程序说明文件存放目录</td>
</tr>
<tr>
<td>/var</td>
<td>放置系统执行过程中经常变化的文件，如随时更改的日志文件 /var/log，/var/log/message：所有的登录文件存放目录，/var/spool/mail：邮件存放的目录，/var/run:程序或服务启动后，其PID存放在该目录下</td>
</tr>
</tbody></table>
<p><strong>路径</strong></p>
<ul>
<li><p>绝对路径</p>
<p>从/目录开始描述的路径为绝对路径</p>
</li>
<li><p>相对路径</p>
<p>从当前位置开始描述的路径为相对路径</p>
</li>
<li><p>.</p>
<p>代表当前目录</p>
</li>
<li><p>..</p>
<p>表示上一级目录</p>
<p>注意：根目录下的.和..都代表当前目录</p>
</li>
</ul>
<h3 id="5、命令概述"><a href="#5、命令概述" class="headerlink" title="5、命令概述"></a>5、命令概述</h3><p>Linux 提供了大量的命令，利用它可以有效地完成大量的工作，如磁盘操作、文件存取、目录操作、进程管理、文件权限设定等。Linux 发行版本最少的命令也有 200 多个，这里只介绍比较重要和使用频率最多的命令</p>
<ul>
<li><p>命令的使用方法</p>
<p>格式：command  [-options]  [parameter1]  …</p>
<p>command：命令名，相应功能的英文单词或单词的缩写</p>
<p>[-options]：选项，可用来对命令进行控制，也可以省略</p>
<p>[parameter1]  …：传给命令的参数，可以是零个一个或多个</p>
</li>
<li><p>查看帮助文档</p>
<ul>
<li><p>man</p>
<p>man是linux提供的一个手册，包含了绝大部分的命令、函数使用说明。该手册分成很多章节（section），使用man时可以指定不同的章节来浏览</p>
<img src="Linux.assets/image-20200211174846722.png" alt="image-20200211174846722" style="zoom:150%;" />

<p>功能键<img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211174857959.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211174857959"></p>
</li>
<li><p>help</p>
<p>获得shell内置命令的帮助信息</p>
<p>语法：help 命令</p>
<p><code>help cd</code></p>
</li>
<li><p>–help</p>
<p>一般是linux命令自带的帮助信息</p>
<p>例如：ls  –help</p>
</li>
</ul>
</li>
<li><p>自动补全</p>
<p>在敲出命令的前几个字母的同时，按下tab键，系统会自动帮我们补全命令</p>
</li>
<li><p>历史命令</p>
<p>当系统执行过一些命令后，可按上下键翻看以前的命令，history将执行过的命令列举出来</p>
</li>
</ul>
<h3 id="6、常用快捷键"><a href="#6、常用快捷键" class="headerlink" title="6、常用快捷键"></a>6、常用快捷键</h3><table>
<thead>
<tr>
<th>常用快捷键</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>ctrl + c</td>
<td>停止进程</td>
</tr>
<tr>
<td>ctrl+l</td>
<td>清屏:clear；彻底清屏是：reset</td>
</tr>
<tr>
<td>ctrl + q</td>
<td>退出</td>
</tr>
<tr>
<td>善于用tab键</td>
<td>提示(更重要的是可以防止敲错)</td>
</tr>
<tr>
<td>上下键</td>
<td>查找执行过的命令</td>
</tr>
<tr>
<td>ctrl +alt</td>
<td>linux和Windows之间切换</td>
</tr>
</tbody></table>
<h3 id="7、文件管理"><a href="#7、文件管理" class="headerlink" title="7、文件管理"></a>7、文件管理</h3><ul>
<li><p><code>ls</code></p>
<p>作用：显示指定目录下所有的文件和目录</p>
<p>选项：</p>
<ul>
<li><p>-a</p>
<p>显示指定目录下所有子目录与文件，包括隐藏文件。Linux文件或者目录名称最长可以有265个字符，“.”代表当前目录，“..”代表上一级目录，以“.”开头的文件为隐藏文件，需要用 -a 参数才能显示</p>
</li>
<li><p>-l</p>
<p>以列表方式显示文件的详细信息</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211181530984.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211181530984"></p>
<p>文件类型：</p>
<table>
<thead>
<tr>
<th>符号</th>
<th>类型</th>
</tr>
</thead>
<tbody><tr>
<td>-</td>
<td>普通文件</td>
</tr>
<tr>
<td>d</td>
<td>目录文件</td>
</tr>
<tr>
<td>l</td>
<td>链接文件</td>
</tr>
<tr>
<td>c</td>
<td>字符设备</td>
</tr>
<tr>
<td>b</td>
<td>块设备</td>
</tr>
</tbody></table>
</li>
<li><p>-h</p>
<p>配合 -l 以人性化的方式显示文件大小</p>
</li>
</ul>
<p>通配符：</p>
<table>
<thead>
<tr>
<th>符号</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td>*</td>
<td align="left">文件代表文件名中所有字符</td>
</tr>
<tr>
<td>ls te*</td>
<td align="left">查找以te开头的文件</td>
</tr>
<tr>
<td>ls *html</td>
<td align="left">查找结尾为html的文件</td>
</tr>
<tr>
<td>？</td>
<td align="left">代表文件名中任意一个字符</td>
</tr>
<tr>
<td>ls ?.c</td>
<td align="left">只找第一个字符任意，后缀为.c的文件</td>
</tr>
<tr>
<td>ls a.?</td>
<td align="left">只找只有3个字符，前2字符为a.，最后一个字符任意的文件</td>
</tr>
<tr>
<td>[]</td>
<td align="left">[”和“]”将字符组括起来，表示可以匹配字符组中的任意一个。“-”用于表示字符范围</td>
</tr>
<tr>
<td>[abc]</td>
<td align="left">匹配a、b、c中的任意一个</td>
</tr>
<tr>
<td>[a-f]</td>
<td align="left">匹配从a到f范围内的的任意一个字符</td>
</tr>
<tr>
<td>ls [a-f]*</td>
<td align="left">找到从a到f范围内的的任意一个字符开头的文件</td>
</tr>
<tr>
<td>ls a-f</td>
<td align="left">查找文件名为a-f的文件,当“-”处于方括号之外失去通配符的作用</td>
</tr>
<tr>
<td>\</td>
<td align="left">如果要使通配符作为普通字符使用，可以在其前面加上转义字符。“?”和“*”处于方括号内时不用使用转义字符就失去通配符的作用</td>
</tr>
<tr>
<td><code>ls \*a</code></td>
<td align="left">查找文件名为*a的文件</td>
</tr>
</tbody></table>
</li>
<li><p><code>pwd</code></p>
<p>作用：显示当前的工作目录</p>
</li>
<li><p><code>cd</code></p>
<p>作用：切换工作目录</p>
<p>注意：cd后面可跟绝对路径，也可以跟相对路径</p>
<p>特殊写法：</p>
<table>
<thead>
<tr>
<th>写法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>cd</td>
<td>切换到当前用户的主目录(/home/用户目录)，用户登陆的时候，默认的目录就是用户的主目录</td>
</tr>
<tr>
<td>cd ~</td>
<td>切换到当前用户的主目录(/home/用户目录)</td>
</tr>
<tr>
<td>cd .</td>
<td>切换到当前目录</td>
</tr>
<tr>
<td>cd ..</td>
<td>切换到上级目录</td>
</tr>
<tr>
<td>cd -</td>
<td>可进入上次所在的目录</td>
</tr>
<tr>
<td>cd -P</td>
<td>跳转到实际物理路径，而非快捷方式路径</td>
</tr>
</tbody></table>
</li>
<li><p><code>&gt;</code></p>
<p>作用：输出重定向，Linux允许将命令执行结果重定向到一个文件，本应显示在终端上的内容保存到指定文件中</p>
<p>示例：<code>ls &gt; test.txt</code></p>
<p>注意：如果文件不存在，则创建，存在则覆盖其内容</p>
</li>
<li><p><code>&gt;&gt;</code></p>
<p>作用：输出重定向，Linux允许将命令执行结果重定向到一个文件，本应显示在终端上的内容保存到指定文件中</p>
<p>示例：<code>ls &gt;&gt; test.txt</code></p>
<p>注意：如果文件不存在，则创建，存在则追加到文件的尾部</p>
</li>
<li><p><code>cat</code></p>
<p>作用：查看或者合并文件内容</p>
<p>合并文件示例：<code>cat  test1.txt  test2.txt &gt; test.txt</code></p>
</li>
<li><p><code>head</code></p>
<p>作用：查看文件</p>
<p>默认显示前10行：<code>head  test.txt</code></p>
<p>显示前n行：<code>head  -n  test.txt</code></p>
</li>
<li><p><code>tail</code></p>
<p>作用：查看文件  </p>
<p>默认显示后10行：<code>tail  test.txt</code></p>
<p>显示后n行：<code>tail -n  test.txt</code></p>
<p>监控文件变化：<code>tail -f  test.txt</code></p>
</li>
<li><p><code>more</code></p>
<p>作用：分屏显示，查看内容时，在信息过长无法在一屏上显示时，会出现快速滚屏，使得用户无法看清文件的内容，此时可以使用more命令，每次只显示一页，按下空格键可以显示下一页，按下q键退出显示，按下h键可以获取帮助</p>
</li>
<li><p><code>less</code></p>
<p>作用：分屏查看文件，它的功能与more指令类似，但是比more指令更加强大，支持各种显示终端。less指令在显示文件内容时，并不是一次将整个文件加载之后才显示，而是根据显示需要加载内容，对于显示大型文件具有较高的效率。</p>
<p>说明：敲enter键往下走一行，敲空格键，往下走一页，可以向上翻页，键盘上的pageup，pagedown</p>
</li>
<li><p><code>wc</code></p>
<p>作用：一次显示文件行数、字节数、文件名信息</p>
</li>
<li><p><code>echo</code></p>
<p>使用：echo [选项] [输出内容]</p>
<p>作用：输出内容</p>
<p>-e： 支持反斜线控制的字符转换</p>
<table>
<thead>
<tr>
<th>控制字符</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>\</td>
<td>输出\本身</td>
</tr>
<tr>
<td>\n</td>
<td>换行符</td>
</tr>
<tr>
<td>\t</td>
<td>制表符，也就是Tab键</td>
</tr>
</tbody></table>
<p><code>echo “hello\tworld”</code></p>
<p><code>echo -e “hello\tworld”</code></p>
</li>
<li><p><code>clear</code></p>
<p>作用：清除终端上的显示清除终端上的显示</p>
</li>
<li><p><code>mkdir</code></p>
<p>作用：创建一个新的目录</p>
<p>注意：新建目录的名称不能与当前目录中已有的目录或文件同名，并且目录创建者必须对当前目录具有写权限</p>
<p>递归创建目录：mkdir  -p  a/b/c/d  </p>
</li>
<li><p><code>touch</code></p>
<p>作用：创建一个新的普通文件</p>
</li>
<li><p><code>rmdir</code></p>
<p>作用：删除一个目录</p>
<p>注意：目录必须为空目录</p>
</li>
<li><p><code>rm</code></p>
<p>使用：rm  [选项]  deleteFile</p>
<p>作用：删除文件或目录，删除的文件不能恢复</p>
<p>参数：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-i</td>
<td>以进行交互式方式执行</td>
</tr>
<tr>
<td>-f</td>
<td>强制删除，忽略不存在的文件，无需提示</td>
</tr>
<tr>
<td>-r</td>
<td>递归地删除目录下的内容，删除文件夹时必须加此参数</td>
</tr>
<tr>
<td>-v</td>
<td>显示指令的详细执行过程</td>
</tr>
</tbody></table>
</li>
<li><p><code>cp</code></p>
<p>作用：将给出的文件或目录复制到另一个文件或目录中</p>
<p>格式：cp  文件名 目标目录</p>
<p>选项：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>该选项通常在复制目录时使用，它保留链接、文件属性，并递归地复制目录，简单而言，保持文件原有属性</td>
</tr>
<tr>
<td>-f</td>
<td>已经存在的目标文件而不提示</td>
</tr>
<tr>
<td>-i</td>
<td>交互式复制，在覆盖目标文件之前将给出提示要求用户确认</td>
</tr>
<tr>
<td>-r</td>
<td>若给出的源文件是目录文件，则cp将递归复制该目录下的所有子目录和文件，目标文件必须为一个目录名</td>
</tr>
<tr>
<td>-v</td>
<td>显示拷贝进度</td>
</tr>
</tbody></table>
</li>
<li><p><code>mv</code></p>
<p>作用:</p>
<table>
<thead>
<tr>
<th>说明</th>
<th>使用格式</th>
</tr>
</thead>
<tbody><tr>
<td>移动文件或目录</td>
<td>mv 文件  目标目录</td>
</tr>
<tr>
<td>重命名</td>
<td>mv  文件名  文件名</td>
</tr>
</tbody></table>
<p>参数：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-f</td>
<td>禁止交互式操作，如有覆盖也不会给出提示</td>
</tr>
<tr>
<td>-i</td>
<td>确认交互方式操作，如果mv操作将导致对已存在的目标文件的覆盖，系统会询问是否重写，要求用户回答以避免误覆盖文件</td>
</tr>
<tr>
<td>-v</td>
<td>显示移动进度</td>
</tr>
</tbody></table>
</li>
<li><p><code>ln</code></p>
<p>作用：建立链接文件，Linux链接文件类似于Windows下的快捷方式</p>
<table>
<thead>
<tr>
<th>链接文件分类</th>
<th>说明</th>
<th>创建格式</th>
<th>注意事项</th>
</tr>
</thead>
<tbody><tr>
<td>软连接</td>
<td>软链接不占用磁盘空间，源文件删除则软链接失效</td>
<td>ln  -s  源文件  链接文件</td>
<td>如果软链接文件和源文件不在同一个目录，源文件要使用绝对路径，不能使用相对路径</td>
</tr>
<tr>
<td>硬链接</td>
<td>硬链接只能链接普通文件，不能链接目录</td>
<td>ln  源文件  链接文件</td>
<td>两个文件占用相同大小的硬盘空间，即使删除了源文件，链接文件还是存在，所以-s选项是更常见的形式</td>
</tr>
</tbody></table>
<p>注意：删除软链接： rm -rf 软链接名，而不是rm -rf 软链接名/。如果使用 rm -rf 软链接名/ 删除，会把软链接对应的真实目录下内容删掉。</p>
</li>
</ul>
<h3 id="8、文件查找"><a href="#8、文件查找" class="headerlink" title="8、文件查找"></a>8、文件查找</h3><ul>
<li><p><code>find</code></p>
<p>作用：查找文件</p>
<p>格式：find [搜索范围] [选项]</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>find  ./  -name  test.sh</td>
<td>查找当前目录下所有名为test.sh的文件</td>
</tr>
<tr>
<td>find  ./  -name  ‘*.sh’</td>
<td>查找当前目录下所有后缀为.sh的文件</td>
</tr>
<tr>
<td>find  ./  -name  “[A-Z]*”</td>
<td>查找当前目录下所有以大写字母开头的文件</td>
</tr>
<tr>
<td>find  /tmp  -size  2M</td>
<td>查找在/tmp 目录下等于2M的文件</td>
</tr>
<tr>
<td>find  /tmp  -size  +2M</td>
<td>查找在/tmp 目录下大于2M的文件</td>
</tr>
<tr>
<td>find  /tmp  -size  -2M</td>
<td>查找在/tmp 目录下小于2M的文件</td>
</tr>
<tr>
<td>find  ./  -size  +4k  -size  -5M</td>
<td>查找当前目录下大于4k，小于5M的文件</td>
</tr>
<tr>
<td>find  ./  -perm  0777</td>
<td>查找当前目录下权限为 777 的文件或目录</td>
</tr>
<tr>
<td>find ./ -uers atguigu</td>
<td>查找当前目录下属于atguigu用户的所有文件</td>
</tr>
</tbody></table>
<p>-size单位：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">b —— 块（512字节）</span><br><span class="line">c —— 字节</span><br><span class="line">w —— 字（2字节）</span><br><span class="line">k —— 千字节</span><br><span class="line">M —— 兆字节</span><br><span class="line">G —— 吉字节</span><br></pre></td></tr></table></figure></li>
<li><p>locate</p>
<p>作用：快速定位文件路径，locate指令利用事先建立的系统中所有文件名称及路径的locate数据库实现快速定位给定的文件。Locate指令无需遍历整个文件系统，查询速度较快。为了保证查询结果的准确度，管理员必须定期更新locate时刻。</p>
<p>格式：locate 搜索文件</p>
<p>注意：由于locate指令基于数据库进行查询，所以第一次运行前，必须使用updatedb指令创建locate数据库。<br>注意：tmp目录不会简历索引</p>
</li>
<li><p><code>which</code></p>
<p>作用：查看命令位置</p>
</li>
<li><p><code>|</code></p>
<p>名称：管道</p>
<p>说明：一个命令的输出可以通过管道做为另一个命令的输入</p>
<p>简述：管道我们可以理解现实生活中的管子，管子的一头塞东西进去，另一头取出来，这里“ | ”的左右分为两端，左端塞东西(写)，右端取东西(读)</p>
</li>
<li><p><code>grep</code></p>
<p>作用：文本搜索，强大的文本搜索工具，grep允许对文本文件进行模式查找，如果找到匹配模式， grep打印包含模式的所有行</p>
<p>格式：grep  [-选项]  ‘搜索内容串’  文件名</p>
<p>注意：搜索内容串可以是正则表达式</p>
<p>选项：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-v</td>
<td>显示不包含匹配文本的所有行（相当于求反）</td>
</tr>
<tr>
<td>-n</td>
<td>显示匹配行及行号</td>
</tr>
<tr>
<td>-i</td>
<td>忽略大小写</td>
</tr>
</tbody></table>
<p>示例：<code>ps -aux | grep java</code> </p>
</li>
</ul>
<h3 id="9、解压和压缩"><a href="#9、解压和压缩" class="headerlink" title="9、解压和压缩"></a>9、解压和压缩</h3><ul>
<li><p><code>tar</code></p>
<p>作用：归档管理，计算机中的数据经常需要备份，tar是Unix/Linux中最常用的备份工具，此命令可以把一系列文件归档到一个大文件中，也可以把档案文件解开以恢复数据</p>
<p>格式：tar  [参数]  打包文件名  文件</p>
<p>参数：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-c</td>
<td>生成档案文件，创建打包文件</td>
</tr>
<tr>
<td>-v</td>
<td>列出归档解档的详细过程，显示进度</td>
</tr>
<tr>
<td>-f</td>
<td>指定档案文件名称，f后面一定是.tar文件，所以必须放选项最后</td>
</tr>
<tr>
<td>-t</td>
<td>列出档案中包含的文件</td>
</tr>
<tr>
<td>-x</td>
<td>解开档案文件</td>
</tr>
</tbody></table>
<p>注意：</p>
<p>​        参数前面可以使用“-”，也可以不使用</p>
<p>​        除了f需要放在参数的最后，其它参数的顺序任意</p>
</li>
<li><p><code>gzip</code></p>
<p>作用：tar与gzip命令结合使用实现文件打包、压缩。 tar只负责打包文件，但不压缩，用gzip压缩tar打包后的文件，其扩展名一般用xxxx.tar.gz</p>
<p>解压格式：gzip  [选项]  待解压文件</p>
<p>压缩格式：gzip  [选项]  被压缩文件  压缩后文件名</p>
<p>选项：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td>-r</td>
<td>压缩所有子目录</td>
<td>gzip  -r  1.tar  1.tar.gz</td>
</tr>
<tr>
<td>-d</td>
<td>解压</td>
<td>gzip  -d  1.tar.gz</td>
</tr>
</tbody></table>
<p>注意：tar这个命令并没有压缩的功能，它只是一个打包的命令，但是在tar命令中增加一个选项(-z)可以调用gzip实现了一个压缩的功能，实行一个先打包后压缩的过程</p>
<p>结合tar使用：</p>
<p>​        压缩：tar  -cvzf  1.tar.gz  *</p>
<p>​        解压：</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>tar  -xvzf  1.tar.gz</td>
<td>解压到当前目录</td>
</tr>
<tr>
<td>tar  -xvzf  1.tar.gz  -C  /temp</td>
<td>解压到指定目录</td>
</tr>
</tbody></table>
</li>
<li><p><code>bzip2</code></p>
<p>作用：tar与bzip2命令结合使用实现文件打包、压缩(用法和gzip一样)。tar只负责打包文件，但不压缩，用bzip2压缩tar打包后的文件，其扩展名一般用xxxx.tar.bz2。在tar命令中增加一个选项(-j)可以调用bzip2实现了一个压缩的功能，实行一个先打包后压缩的过程</p>
<p>结合tar使用：</p>
<p>压缩：tar  -jcvf  压缩包包名  文件…(tar  jcvf  bk.tar.bz2  *.c)</p>
<p>解压：tar  -jxvf  压缩包包名  (tar  jxvf  bk.tar.bz2)</p>
</li>
<li><p><code>zip、unzip</code></p>
<p>作用：通过zip压缩文件的目标文件不需要指定扩展名，默认扩展名为zip</p>
<p>压缩：zip  [-r]  目标文件(没有扩展名)  源文件</p>
<p>解压：unzip  -d  解压后目录文件  压缩文件</p>
</li>
</ul>
<h3 id="10、vi编辑器"><a href="#10、vi编辑器" class="headerlink" title="10、vi编辑器"></a>10、vi编辑器</h3><p><code>gedit</code>：是一个Linux环境下的文本编辑器，类似windows下的写字板程序，在不需要特别复杂的编程环境下，作为基本的文本编辑器比较合适</p>
<ul>
<li><p>作用</p>
<p>打开文件编辑并保存退出文件</p>
</li>
<li><p>打开文件</p>
<p>格式：vim  文件名</p>
<p>说明：如果文件不存在则先打开，当关闭保存时自动创建该文件</p>
<p>示例：</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>vim  sunck.txt</td>
<td>打开文件，光标在第一行</td>
</tr>
<tr>
<td>vim  sunck.txt  +5</td>
<td>打开文件，将光标移动到第四行<br />如果文件不存在则没有效果</td>
</tr>
<tr>
<td>vim sunck.txt +</td>
<td>打开文件，将光标移动到最后一行<br />如果文件不存在则没有效果</td>
</tr>
</tbody></table>
</li>
<li><p>模式</p>
<ul>
<li><p>命令模式</p>
<p>作用：可以实行特定命令，快速操作文本</p>
<p>进入与退出命令模式：打开文件即进入命令模式，按ESC即退出</p>
<p>移动光标命令：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>上、下、左、右方向键</td>
<td>移动光标</td>
</tr>
<tr>
<td>G</td>
<td>光标快速的定位到末行行首</td>
</tr>
<tr>
<td>$</td>
<td>光标快速定位到该行行尾</td>
</tr>
<tr>
<td>^</td>
<td>光标快速定位到该行行首</td>
</tr>
<tr>
<td>gg</td>
<td>光标快速定位到第一行行首</td>
</tr>
<tr>
<td>ngg</td>
<td>光标快速定位到第n行行首</td>
</tr>
<tr>
<td>M</td>
<td>光标移动到中间行</td>
</tr>
<tr>
<td>L</td>
<td>光标移动到屏幕最后一行行首</td>
</tr>
<tr>
<td>w</td>
<td>向后一次移动一个字</td>
</tr>
<tr>
<td>b</td>
<td>向前一次移动一个字</td>
</tr>
<tr>
<td>ctr+d、ctr+u</td>
<td>向下、上翻半屏</td>
</tr>
<tr>
<td>ctr+f、ctr+b</td>
<td>向下、上翻一屏</td>
</tr>
<tr>
<td>h、j、k、l</td>
<td>左、下、上、右移动光标</td>
</tr>
</tbody></table>
<p>删除命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>x</td>
<td>删除光标后一个字符</td>
</tr>
<tr>
<td>X</td>
<td>删除光标前一个字符</td>
</tr>
<tr>
<td>dd</td>
<td>删除光标所在行</td>
</tr>
<tr>
<td>ndd</td>
<td>删除指定的行数</td>
</tr>
<tr>
<td>d0</td>
<td>删除光标前本行所有内容,不包含光标所在字符</td>
</tr>
<tr>
<td>dw</td>
<td>删除光标开始位置的字,包含光标所在字符</td>
</tr>
</tbody></table>
<p>撤销命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>u</td>
<td>一步一步撤销</td>
</tr>
<tr>
<td>ctr+r</td>
<td>反撤销</td>
</tr>
</tbody></table>
<p>重复命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.</td>
<td>重复上一次操作的命令</td>
</tr>
</tbody></table>
<p>文本行移动命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>shift+&gt;&gt;</td>
<td>文本行右移</td>
</tr>
<tr>
<td>shift+&gt;&gt;</td>
<td>问本行左移</td>
</tr>
</tbody></table>
<p>复制粘贴命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>yy</td>
<td>复制当前行</td>
</tr>
<tr>
<td>nyy</td>
<td>复制n行</td>
</tr>
<tr>
<td>p</td>
<td>在光标所在位置向下新开辟一行,粘贴</td>
</tr>
</tbody></table>
<p>剪切粘贴命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>dd、ndd</td>
<td>删除命令相当于剪切</td>
</tr>
<tr>
<td>p</td>
<td>在光标所在位置向下新开辟一行,粘贴</td>
</tr>
</tbody></table>
<p>可视模式命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>v</td>
<td>按字符移动,选中文本</td>
</tr>
<tr>
<td>V</td>
<td>按行移动,选中文本可视模式可以配合 d, y, &gt;&gt;, &lt;&lt; 实现对文本块的删除,复制,左右移动</td>
</tr>
</tbody></table>
</li>
<li><p>输入模式</p>
<p>作用：向文件中输入内容</p>
<table>
<thead>
<tr>
<th>进入方式</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>按ESC后按a</td>
<td>从光标之后开始输入</td>
</tr>
<tr>
<td>按ESC后按A</td>
<td>在光标所在行的末尾开始输入</td>
</tr>
<tr>
<td>按ESC后按i</td>
<td>从光标之前开始输入</td>
</tr>
<tr>
<td>按ESC后按I</td>
<td>从光标所在行第一个非空字符开始输入</td>
</tr>
<tr>
<td>按ESC后按o</td>
<td>在光标所在行下一行，另起一行开始输入</td>
</tr>
<tr>
<td>按ESC后按O</td>
<td>在光标所在行上一行，另起一行开始输入</td>
</tr>
<tr>
<td>按ESC后按s</td>
<td>删除光标所在字符开始输入</td>
</tr>
<tr>
<td>按ESC后按S</td>
<td>删除光标所在行开始输入</td>
</tr>
</tbody></table>
</li>
<li><p>末行模式</p>
<p>作用：可以实行特定命令，可用于查找替换、保存退出等</p>
<p>进入方式：按ESC后按Shift+冒号</p>
<p>光标命令：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>n</td>
<td>光标跳转到第n行</td>
</tr>
</tbody></table>
<p>存储命令：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>w</td>
<td>保存文件</td>
</tr>
<tr>
<td>wq</td>
<td>保存并退出文件</td>
</tr>
<tr>
<td>x</td>
<td>保存并退出文件</td>
</tr>
<tr>
<td>!</td>
<td>表示强制</td>
</tr>
<tr>
<td>w!</td>
<td>强制保存</td>
</tr>
<tr>
<td>q!</td>
<td>强制退出</td>
</tr>
<tr>
<td>wq!</td>
<td>强制保存退出</td>
</tr>
</tbody></table>
<p>查找命令：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td>/</td>
<td>正向查找，按n查看下一个</td>
<td>/sunck</td>
</tr>
<tr>
<td>?</td>
<td>反向查找，按n查看上一个</td>
<td>?sunck</td>
</tr>
</tbody></table>
<p>替换命令：</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>s/sunck/kaige</td>
<td>将光标所在行的第一个sunck替换为kaige</td>
</tr>
<tr>
<td>s/sunck/kaige/g</td>
<td>将光标所在行的所有sunck替换为kaige</td>
</tr>
<tr>
<td>n,s/sunck/kaige</td>
<td>将指定行的第一个sunck替换为kaige</td>
</tr>
<tr>
<td>n,s/sunck/kaige/g</td>
<td>将指定行的所有sunck替换为kaige</td>
</tr>
<tr>
<td>%s/sunck/kaige</td>
<td>将每一行的第一个sunck替换为kaige</td>
</tr>
<tr>
<td>%s/sunck/kaige/g</td>
<td>将每一行的所有sunck替换为kaige</td>
</tr>
</tbody></table>
<p>设置命令：</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>set  nu</td>
<td>显示行号</td>
</tr>
<tr>
<td>set  nonu</td>
<td>取消显示行号</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>转换关系</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200212103923712.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200212103923712"></p>
</li>
<li><p>非法关闭处理</p>
<p>说明：当非法关闭正在编辑的文件的时候，再次打开文件会有提示信息</p>
<p>解决：</p>
<p>​        敲击enter：进入文件</p>
<p>​        保存上次写的内容：vim -r 1.txt</p>
<p>​        将产生的交换文件删除掉：rm .1.txt.swp</p>
</li>
<li><p>配置</p>
<table>
<thead>
<tr>
<th>打开文件</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>vim  ~/.vimrc</td>
<td>修改当前用户配置</td>
</tr>
<tr>
<td>sudo vim /etc/vim/vimrc</td>
<td>修改所有用户配置</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">syntax on</span><br><span class="line">set nu</span><br><span class="line">set autoindent</span><br><span class="line">set smartindent</span><br><span class="line">set tabstop=4</span><br><span class="line">set shiftwidth=4</span><br><span class="line">set showmatch</span><br><span class="line">set ruler</span><br><span class="line">set cindent</span><br><span class="line">set background=dark</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="11、用户、权限管理"><a href="#11、用户、权限管理" class="headerlink" title="11、用户、权限管理"></a>11、用户、权限管理</h3><ul>
<li><p>概述</p>
<ul>
<li>用户是Unix/Linux系统工作中重要的一环，用户管理包括用户与组账号的管理</li>
<li>在Unix/Linux系统中，不论是由本机或是远程登录系统，每个系统都必须拥有一个账号，并且对于不同的系统资源拥有不同的使用权限</li>
<li>Unix/Linux系统中的root账号通常用于系统的维护和管理，它对Unix/Linux操作系统的所有部分具有不受限制的访问权限</li>
<li>在Unix/Linux安装的过程中，系统会自动创建许多用户账号，而这些默认的用户就称为“标准用户”</li>
<li>在大多数版本的Unix/Linux中，都不推荐直接使用root账号登录系统</li>
</ul>
</li>
<li><p><code>whoami</code></p>
<p>作用：查看当前系统当前账号的用户名。可通过cat /etc/passwd查看系统用户信息</p>
</li>
<li><p><code>who</code></p>
<p>作用：查看当前所有登录系统的用户信息</p>
<p>选项：</p>
<table>
<thead>
<tr>
<th>选项值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-m或am  I</td>
<td>只显示运行who命令的用户名、登录终端和登录时间</td>
</tr>
<tr>
<td>-q或–count</td>
<td>只显示用户的登录账号和登录用户的数量</td>
</tr>
<tr>
<td>-u或–heading</td>
<td>显示列标题</td>
</tr>
</tbody></table>
</li>
<li><p><code>exit</code></p>
<table>
<thead>
<tr>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>如果是图形界面，退出当前终端</td>
</tr>
<tr>
<td>如果是使用ssh远程登录，退出登陆账户</td>
</tr>
<tr>
<td>如果是切换后的登陆用户，退出则返回上一个登陆账号</td>
</tr>
</tbody></table>
</li>
<li><p><code>groupadd</code></p>
<p>作用：新建组账号</p>
<p>格式：groupadd 组名</p>
</li>
<li><p><code>groupdel</code></p>
<p>作用：删除组账号</p>
<p>格式：groupdel 组名</p>
</li>
<li><p><code>useradd</code></p>
<p>作用：在Unix/Linux中添加用户账号可以使用adduser或useradd命令，因为adduser命令是指向useradd命令的一个链接，因此，这两个命令的使用格式完全一样</p>
<p>格式：useradd  [参数]  新建用户账号</p>
<table>
<thead>
<tr>
<th>参数值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-d</td>
<td>指定用户登录系统时的主目录，如果不使用该参数，系统自动在/home目录下建立与用户名同名目录为主目录</td>
</tr>
<tr>
<td>-m</td>
<td>自动建立目录</td>
</tr>
<tr>
<td>-g</td>
<td>指定组名称</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>使用说明</th>
</tr>
</thead>
<tbody><tr>
<td>Linux每个用户都要有一个主目录，主目录就是第一次登陆系统，用户的默认当前目录(/home/用户)</td>
</tr>
<tr>
<td>每一个用户必须有一个主目录，所以用useradd创建用户的时候，一定给用户指定一个主目录</td>
</tr>
<tr>
<td>用户的主目录一般要放到根目录的home目录下，用户的主目录和用户名是相同的</td>
</tr>
<tr>
<td>如果创建用户的时候，不指定组名，那么系统会自动创建一个和用户名一样的组名</td>
</tr>
</tbody></table>
</li>
<li><p><code>passwd</code></p>
<p>作用：在Unix/Linux中，超级用户可以使用passwd命令为普通用户设置或修改用户口令。用户也可以直接使用该命令来修改自己的口令，而无需在命令后面使用用户名</p>
</li>
<li><p><code>userdel</code></p>
<p>作用：删除用户</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>userdel  kaige</td>
<td>删除kaige用户，但不会自动删除用户的主目录</td>
</tr>
<tr>
<td>userdel  -r  kaige</td>
<td>删除用户，同时删除用户的主目录</td>
</tr>
</tbody></table>
</li>
<li><p><code>su</code></p>
<p>作用：切换用户</p>
<p>注意：su后面可以加“-”。su和su –命令不同之处在于，su -切换到对应的用户时会将当前的工作目录自动转换到切换后的用户主目录</p>
</li>
<li><p>查看有哪些用户组</p>
<p>cat  /etc/group</p>
<p>groupmod  +  三次tab键</p>
</li>
<li><p><code>groups</code></p>
<p>作用：查看用户在哪些组</p>
<p>示例：groups  sunck</p>
</li>
<li><p><code>usermod</code></p>
<p>作用：修改用户所在组</p>
<p>格式：usermod  选项  用户组  用户名</p>
<table>
<thead>
<tr>
<th>选项值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-g</td>
<td>用来制定这个用户默认的用户组</td>
</tr>
<tr>
<td>-G</td>
<td>一般配合’-a’来完成向其它组添加</td>
</tr>
</tbody></table>
</li>
<li><p>为创建的普通用户添加sudo权限</p>
<p>注意：新创建的用户，默认不能sudo，需要进行一下操作</p>
<p>操作：</p>
<ul>
<li>修改/etc/sudoers<img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200212105606513.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200212105606513"></li>
<li>需要强制保存退出</li>
</ul>
</li>
<li><p><code>chmod</code></p>
<p>作用：修改文件权限</p>
<p>权限：<img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200212105732765.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200212105732765"></p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>r</td>
<td>read 表示可读取，对于一个目录，如果没有r权限，那么就意味着不能通过ls查看这个目录的内容</td>
</tr>
<tr>
<td>w</td>
<td>write 表示可写入，对于一个目录，如果没有w权限，那么就意味着不能在目录下创建新的文件</td>
</tr>
<tr>
<td>x</td>
<td>excute 表示可执行，对于一个目录，如果没有x权限，那么就意味着不能通过cd进入这个目录</td>
</tr>
</tbody></table>
<p>修改:</p>
<ul>
<li><p>字母法</p>
<p>格式：chmod  u/g/o/a  +/-/=  rwx  文件</p>
<table>
<thead>
<tr>
<th>u/g/o/a</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>u</td>
<td>user 表示该文件的所有者</td>
</tr>
<tr>
<td>g</td>
<td>group 表示与该文件的所有者属于同一组( group )者，即用户组</td>
</tr>
<tr>
<td>o</td>
<td>other 表示其他以外的人</td>
</tr>
<tr>
<td>a</td>
<td>all 表示这三者皆是</td>
</tr>
</tbody></table>
<p>[ +-= ]说明：</p>
<table>
<thead>
<tr>
<th>+-=</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>+</td>
<td>增加权限</td>
</tr>
<tr>
<td>-</td>
<td>撤销权限</td>
</tr>
<tr>
<td>=</td>
<td>设定权限</td>
</tr>
</tbody></table>
</li>
<li><p>数字法</p>
<table>
<thead>
<tr>
<th>rwx-</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>r</td>
<td>读取权限，数字代号为 “4”</td>
</tr>
<tr>
<td>w</td>
<td>写入权限，数字代号为 “2”</td>
</tr>
<tr>
<td>x</td>
<td>执行权限，数字代号为 “1”</td>
</tr>
<tr>
<td>-</td>
<td>不具任何权限，数字代号为 “0”</td>
</tr>
</tbody></table>
<p>示例：chmod  751  file</p>
<p>说明：</p>
<p>​        文件所有者：读、写、执行权限</p>
<p>​        同组用户：读、执行的权限</p>
<p>​        其它用户：执行的权限</p>
</li>
</ul>
</li>
<li><p><code>chown</code></p>
<p>作用：修改文件所有者</p>
<p>格式：chown  新用户名  文件名</p>
</li>
<li><p><code>chgrp</code></p>
<p>作用：修改文件所属组</p>
<p>格式：chgrp  新组名  文件名</p>
</li>
</ul>
<h3 id="12、时间日期命令"><a href="#12、时间日期命令" class="headerlink" title="12、时间日期命令"></a>12、时间日期命令</h3><ul>
<li><p><code>cal</code></p>
<p>作用：查看当前日历</p>
<p>显示整年日历：cal  -y</p>
</li>
<li><p>date</p>
<p>作用：显示或设置时间</p>
<p><code>date  [MMDDhhmm[[CC]YY][.ss]] +format</code></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200212110626684.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200212110626684"></p>
<p>显示当前：</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date</span></span><br><span class="line"><span class="number">2021</span>年 <span class="number">07</span>月 <span class="number">31</span>日 星期六 <span class="number">19</span>:<span class="number">42</span>:<span class="number">24</span> CST</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date +%Y</span></span><br><span class="line"><span class="number">2021</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date +%Y-%m-%d</span></span><br><span class="line"><span class="number">2021</span><span class="literal">-07</span><span class="literal">-31</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date +&quot;%Y-%m-%d %H:%M:%S&quot;</span></span><br><span class="line"><span class="number">2021</span><span class="literal">-07</span><span class="literal">-31</span> <span class="number">19</span>:<span class="number">43</span>:<span class="number">03</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure>

<p>显示非当前：</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date -d &quot;1 days ago&quot;</span></span><br><span class="line"><span class="number">2021</span>年 <span class="number">07</span>月 <span class="number">30</span>日 星期五 <span class="number">19</span>:<span class="number">44</span>:<span class="number">16</span> CST</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date -d &quot;-1 days ago&quot;</span></span><br><span class="line"><span class="number">2021</span>年 <span class="number">08</span>月 <span class="number">01</span>日 星期日 <span class="number">19</span>:<span class="number">44</span>:<span class="number">25</span> CST</span><br></pre></td></tr></table></figure>

<p>设置：</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date</span></span><br><span class="line"><span class="number">2021</span>年 <span class="number">07</span>月 <span class="number">31</span>日 星期六 <span class="number">19</span>:<span class="number">45</span>:<span class="number">40</span> CST</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date -s &quot;2020-11-23 20:12:03&quot;</span></span><br><span class="line"><span class="number">2020</span>年 <span class="number">11</span>月 <span class="number">23</span>日 星期一 <span class="number">20</span>:<span class="number">12</span>:<span class="number">03</span> CST</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date</span></span><br><span class="line"><span class="number">2020</span>年 <span class="number">11</span>月 <span class="number">23</span>日 星期一 <span class="number">20</span>:<span class="number">12</span>:<span class="number">05</span> CST</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date -s &quot;2021-07-31 11:46:45&quot;</span></span><br><span class="line"><span class="number">2021</span>年 <span class="number">07</span>月 <span class="number">31</span>日 星期六 <span class="number">11</span>:<span class="number">46</span>:<span class="number">45</span> CST</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date</span></span><br><span class="line"><span class="number">2021</span>年 <span class="number">07</span>月 <span class="number">31</span>日 星期六 <span class="number">11</span>:<span class="number">46</span>:<span class="number">47</span> CST</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="13、磁盘分区命令"><a href="#13、磁盘分区命令" class="headerlink" title="13、磁盘分区命令"></a>13、磁盘分区命令</h3><ul>
<li><p><code>df</code>（disk free）</p>
<p>作用：检测文件系统的磁盘空间占用和空余情况，可以显示所有文件系统对节点和磁盘块的使用情况（查看磁盘空间使用情况）</p>
<p>格式：df  选项</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>显示所有文件系统的磁盘使用情况</td>
</tr>
<tr>
<td>-m</td>
<td>以1024字节为单位显示</td>
</tr>
<tr>
<td>-T</td>
<td>显示文件系统</td>
</tr>
<tr>
<td>-h</td>
<td>以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示；</td>
</tr>
</tbody></table>
</li>
<li><p><code>fdisk</code></p>
<p>作用：查看磁盘分区详情</p>
<p>注意：该命令必须在root用户下才能使用</p>
<p>使用：<code>fdisk -l</code></p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-l</td>
<td>显示所有硬盘的分区列表</td>
</tr>
</tbody></table>
<p>Boot：引导</p>
<p>Start：从X磁柱开始</p>
<p>End：到Y磁柱结束</p>
<p>Blocks：容量</p>
<p>Id：分区类型ID</p>
<p>System：分区类型</p>
</li>
<li><p><code>lsblk</code></p>
<p>作用：查看设备挂载情况</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-f</td>
<td>查看详细的设备挂载情况，显示文件系统信息</td>
</tr>
</tbody></table>
</li>
<li><p><code>mount/umount</code></p>
<p>对于Linux用户来讲，不论有几个分区，分别分给哪一个目录使用，它总归就是一个根目录、一个独立且唯一的文件结构。Linux中每个分区都是用来组成整个文件系统的一部分，它在用一种叫做“挂载”的处理方法，它整个文件系统中包含了一整套的文件和目录，并将一个分区和一个目录联系起来，要载入的那个分区将使它的存储空间在这个目录下获得。</p>
<p>功能：挂载设备</p>
<p>格式：<code>mount [-t vfstype] [-o options] device dir</code></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-t vfstype</td>
<td>指定文件系统的类型，通常不必指定。mount 会自动选择正确的类型。常用类型有：  光盘或光盘镜像：iso9660  DOS fat16文件系统：msdos  <a target="_blank" rel="noopener" href="http://blog.csdn.net/hancunai0017/article/details/6995284">Windows</a> 9x fat32文件系统：vfat  Windows NT ntfs文件系统：ntfs  Mount Windows文件<a target="_blank" rel="noopener" href="http://blog.csdn.net/hancunai0017/article/details/6995284">网络</a>共享：smbfs  <a target="_blank" rel="noopener" href="http://blog.csdn.net/hancunai0017/article/details/6995284">UNIX</a>(LINUX) 文件网络共享：nfs</td>
</tr>
<tr>
<td>-o options</td>
<td>主要用来描述设备或档案的挂接方式。常用的参数有：  loop：用来把一个文件当成硬盘分区挂接上系统  ro：采用只读方式挂接设备  rw：采用读写方式挂接设备  　 iocharset：指定访问文件系统所用字符集</td>
</tr>
<tr>
<td>device</td>
<td>要挂接(mount)的设备</td>
</tr>
<tr>
<td>dir</td>
<td>设备在系统上的挂接点(mount point)</td>
</tr>
</tbody></table>
<p>挂载U盘：</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># ls /mnt/</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># mkdir /mnt/upan</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># ls /dev | grep sd</span></span><br><span class="line">sda</span><br><span class="line">sda1</span><br><span class="line">sda2</span><br></pre></td></tr></table></figure>

<p>虚拟机–&gt;可移动设备–&gt;设备名称–&gt;连接</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># ls /dev | grep sd</span></span><br><span class="line">sda</span><br><span class="line">sda1</span><br><span class="line">sda2</span><br><span class="line">sdb</span><br><span class="line">sdb1</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># ls /mnt/upan/</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># mount /dev/sdb1 /mnt/upan/</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># ls /mnt/upan/</span></span><br><span class="line"><span class="number">1</span>?Linux  bigData2105  System Volume Information</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># umount /dev/sdb1</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># ls /mnt/upan/</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># </span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="14、系统管理"><a href="#14、系统管理" class="headerlink" title="14、系统管理"></a>14、系统管理</h3><ul>
<li><p><code>ps</code></p>
<p>作用：查看进程信息</p>
<table>
<thead>
<tr>
<th>选项值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>显示终端上的所有进程，包括其他用户的进程</td>
</tr>
<tr>
<td>-u</td>
<td>显示进程的详细状态</td>
</tr>
<tr>
<td>-x</td>
<td>显示没有控制终端的进程</td>
</tr>
<tr>
<td>-w</td>
<td>显示加宽，以便显示更多的信息</td>
</tr>
<tr>
<td>-r</td>
<td>只显示正在运行的进程</td>
</tr>
</tbody></table>
<p>常用格式：</p>
<p><code>ps -aux</code>：查看系统中所有进程</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">USER：该进程是由哪个用户产生的</span><br><span class="line">PID：进程的ID号</span><br><span class="line">%CPU：该进程占用CPU资源的百分比，占用越高，进程越耗费资源；</span><br><span class="line">%MEM：该进程占用物理内存的百分比，占用越高，进程越耗费资源；</span><br><span class="line">VSZ：该进程占用虚拟内存的大小，单位KB；</span><br><span class="line">RSS：该进程占用实际物理内存的大小，单位KB；</span><br><span class="line">TTY：该进程是在哪个终端中运行的。其中tty1-tty7代表本地控制台终端，tty1-tty6是本地的字符界面终端，tty7是图形终端。pts/0-255代表虚拟终端。</span><br><span class="line">STAT：进程状态。常见的状态有：R：运行、S：睡眠、T：停止状态、s：包含子进程、+：位于后台</span><br><span class="line">START：该进程的启动时间</span><br><span class="line">TIME：该进程占用CPU的运算时间，注意不是系统时间</span><br><span class="line">COMMAND：产生此进程的命令名</span><br></pre></td></tr></table></figure>

<p><code>ps -ef</code>：可以查看子父进程之间的关系</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">UID：用户ID </span><br><span class="line">PID：进程ID </span><br><span class="line">PPID：父进程ID </span><br><span class="line">C：CPU用于计算执行优先级的因子。数值越大，表明进程是CPU密集型运算，执行优先级会降低；数值越小，表明进程是I/O密集型运算，执行优先级会提高 </span><br><span class="line">STIME：进程启动的时间 </span><br><span class="line">TTY：完整的终端名称 </span><br><span class="line">TIME：CPU时间 </span><br><span class="line">CMD：启动进程所用的命令和参数</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p><code>kill</code></p>
<p>作用：终止进程</p>
<p>格式：kill  [-signal]  pid</p>
<p>注意：信号值从0到15，其中9为绝对终止，可以处理一般信号无法终止的进程</p>
</li>
<li><p><code>pstree </code></p>
<p>作用：查看进程树</p>
<p>格式：pstree [选项]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-p</td>
<td>显示进程的PID</td>
</tr>
<tr>
<td>-u</td>
<td>显示进程的所属用户</td>
</tr>
</tbody></table>
</li>
<li><p><code>top</code></p>
<p>作用：动态显示进程</p>
<p>格式：top [选项]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-d 秒数</td>
<td>指定top命令每隔几秒更新。默认是3秒在top命令的交互模式当中可以执行的命令：</td>
</tr>
<tr>
<td>-i</td>
<td>使top不显示任何闲置或者僵死进程。</td>
</tr>
<tr>
<td>-p</td>
<td>通过指定监控进程ID来仅仅监控某个进程的状态。</td>
</tr>
</tbody></table>
<p>说明：能够在运行后，在指定的时间间隔更新显示信息。可以在使用top命令时加上-d 来指定显示信息更新的时间间隔</p>
<table>
<thead>
<tr>
<th>按键</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>M</td>
<td>根据内存使用量来排序</td>
</tr>
<tr>
<td>P</td>
<td>根据CPU占有率来排序</td>
</tr>
<tr>
<td>T</td>
<td>根据进程运行时间的长短来排序</td>
</tr>
<tr>
<td>U</td>
<td>可以根据后面输入的用户名来筛选进程</td>
</tr>
<tr>
<td>k</td>
<td>可以根据后面输入的PID来杀死进程</td>
</tr>
<tr>
<td>q</td>
<td>退出</td>
</tr>
<tr>
<td>h</td>
<td>获得帮助</td>
</tr>
</tbody></table>
<ol>
<li><p>第一行信息为任务队列信息</p>
<table>
<thead>
<tr>
<th align="left">内容</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">12:26:46</td>
<td>系统当前时间</td>
</tr>
<tr>
<td align="left">up 1 day, 13:32</td>
<td>系统的运行时间，本机已经运行1天  13小时32分钟</td>
</tr>
<tr>
<td align="left">2 users</td>
<td>当前登录了两个用户</td>
</tr>
<tr>
<td align="left">load average:   0.00, 0.00, 0.00</td>
<td>系统在之前1分钟，5分钟，15分钟的平均负载。一般认为小于1时，负载较小。如果大于1，系统已经超出负荷。</td>
</tr>
</tbody></table>
</li>
<li><p>第二行信息为任务队列信息</p>
<table>
<thead>
<tr>
<th></th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>Tasks: 95 total</td>
<td>系统中的进程总数</td>
</tr>
<tr>
<td>1 running</td>
<td>正在运行的进程数</td>
</tr>
<tr>
<td>94 sleeping</td>
<td>睡眠的进程</td>
</tr>
<tr>
<td>0 stopped</td>
<td>正在停止的进程</td>
</tr>
<tr>
<td>0 zombie</td>
<td>僵尸进程。如果不是0，需要手工检查僵尸进程</td>
</tr>
</tbody></table>
</li>
<li><p>第三信息为任务队列信息</p>
<table>
<thead>
<tr>
<th>内容</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>Cpu(s): 0.1%us</td>
<td>用户模式占用的CPU百分比</td>
</tr>
<tr>
<td>0.1%sy</td>
<td>系统模式占用的CPU百分比</td>
</tr>
<tr>
<td>0.0%ni</td>
<td>改变过优先级的用户进程占用的CPU百分比</td>
</tr>
<tr>
<td>99.7%id</td>
<td>空闲CPU的CPU百分比</td>
</tr>
<tr>
<td>0.1%wa</td>
<td>等待输入/输出的进程的占用CPU百分比</td>
</tr>
<tr>
<td>0.0%hi</td>
<td>硬中断请求服务占用的CPU百分比</td>
</tr>
<tr>
<td>0.1%si</td>
<td>软中断请求服务占用的CPU百分比</td>
</tr>
<tr>
<td>0.0%st</td>
<td>st（Steal time）虚拟时间百分比。就是当有虚拟机时，虚拟CPU等待实际CPU的时间百分比。</td>
</tr>
</tbody></table>
</li>
<li><p>第四行信息为任务队列信息</p>
<table>
<thead>
<tr>
<th>内容</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>Mem:  625344k total</td>
<td>物理内存的总量，单位KB</td>
</tr>
<tr>
<td>571504k used</td>
<td>已经使用的物理内存数量</td>
</tr>
<tr>
<td>53840k free</td>
<td>空闲的物理内存数量，我们使用的是虚拟机，总共只分配了628MB内存，所以只有53MB的空闲内存了</td>
</tr>
<tr>
<td>65800k buffers</td>
<td>作为缓冲的内存数量</td>
</tr>
</tbody></table>
</li>
<li><p>第五行为交换分区（swap）信息</p>
<table>
<thead>
<tr>
<th>Swap:  524280k total</th>
<th>交换分区（虚拟内存）的总大小</th>
</tr>
</thead>
<tbody><tr>
<td>0k used</td>
<td>已经使用的交互分区的大小</td>
</tr>
<tr>
<td>524280k free</td>
<td>空闲交换分区的大小</td>
</tr>
<tr>
<td>409280k cached</td>
<td>作为缓存的交互分区的大小</td>
</tr>
</tbody></table>
</li>
</ol>
</li>
<li><p><code>netstat</code></p>
<p>作用：显示网络统计信息和端口占用情况</p>
<ul>
<li><p><code>netstat -anp | grep 进程号</code>   </p>
<p>功能描述：查看该进程网络信息</p>
</li>
<li><p><code>netstat –nlp | grep 端口号</code>   </p>
<p>能描述：查看网络端口号占用情况</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-n</td>
<td>拒绝显示别名，能显示数字的全部转化成数字</td>
</tr>
<tr>
<td>-l</td>
<td>仅列出有在listen（监听）的服务状态</td>
</tr>
<tr>
<td>-p</td>
<td>表示显示哪个进程在调用</td>
</tr>
</tbody></table>
<p>查看某端口号是否被占用：</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> <span class="type">etc</span>]<span class="comment"># netstat -nltp | grep 22</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">192.168</span>.<span class="number">122.1</span>:<span class="number">53</span>        <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">1384</span>/dnsmasq        </span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">22</span>              <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">1009</span>/sshd           </span><br><span class="line">tcp6       <span class="number">0</span>      <span class="number">0</span> :::<span class="number">22</span>                   :::*                    LISTEN      <span class="number">1009</span>/sshd </span><br></pre></td></tr></table></figure>

<p>通过进程号查看该进程的网络信息：</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> <span class="type">etc</span>]<span class="comment"># netstat -anp | grep sshd</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">22</span>              <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">1009</span>/sshd           </span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">192.168</span>.<span class="number">100.5</span>:<span class="number">22</span>        <span class="number">192.168</span>.<span class="number">100.1</span>:<span class="number">52900</span>     ESTABLISHED <span class="number">1775</span>/sshd: root@pts </span><br><span class="line">tcp6       <span class="number">0</span>      <span class="number">0</span> :::<span class="number">22</span>                   :::*                    LISTEN      <span class="number">1009</span>/sshd           </span><br><span class="line">unix  <span class="number">3</span>      [ ]         STREAM     CONNECTED     <span class="number">23573</span>    <span class="number">1009</span>/sshd            </span><br><span class="line">unix  <span class="number">2</span>      [ ]         DGRAM                    <span class="number">31107</span>    <span class="number">1775</span>/sshd: root@pts  </span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p><code>reboot、shutdown、init</code></p>
<p>作用：关机重启</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>reboot</td>
<td>重新启动操作系统</td>
</tr>
<tr>
<td>shutdown –r now</td>
<td>重新启动操作系统，shutdown会给别的用户提示</td>
</tr>
<tr>
<td>shutdown -h now</td>
<td>立刻关机，其中now相当于时间为0的状态</td>
</tr>
<tr>
<td>shutdown -h 20:25</td>
<td>系统在今天的20:25 会关机</td>
</tr>
<tr>
<td>shutdown -h +10</td>
<td>系统再过十分钟后自动关机</td>
</tr>
<tr>
<td>init 0</td>
<td>关机</td>
</tr>
<tr>
<td>init 6</td>
<td>重启</td>
</tr>
</tbody></table>
</li>
</ul>
<h3 id="15、网络配置"><a href="#15、网络配置" class="headerlink" title="15、网络配置"></a>15、网络配置</h3><h3 id="16、系统定时任务"><a href="#16、系统定时任务" class="headerlink" title="16、系统定时任务"></a>16、系统定时任务</h3><ul>
<li><p>crond 服务</p>
<p><code>systemctl status crond</code></p>
<p><code>systemctl stop crond</code></p>
<p><code>systemctl start crond</code></p>
<p><code>systemctl restart crond</code></p>
</li>
<li><p>crontab 定时任务设置</p>
<p>语法：crontab [选项]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-e</td>
<td>编辑crontab定时任务</td>
</tr>
<tr>
<td>-l</td>
<td>查询crontab任务</td>
</tr>
<tr>
<td>-r</td>
<td>删除当前用户所有的crontab任务</td>
</tr>
</tbody></table>
<p>进入crontab编辑界面。会打开vim编辑你的工作</p>
<p>格式：* * * * * 执行的任务</p>
<table>
<thead>
<tr>
<th>项目</th>
<th>含义</th>
<th>范围</th>
</tr>
</thead>
<tbody><tr>
<td>第一个“*”</td>
<td>一小时当中的第几分钟</td>
<td>0-59</td>
</tr>
<tr>
<td>第二个“*”</td>
<td>一天当中的第几小时</td>
<td>0-23</td>
</tr>
<tr>
<td>第三个“*”</td>
<td>一个月当中的第几天</td>
<td>1-31</td>
</tr>
<tr>
<td>第四个“*”</td>
<td>一年当中的第几月</td>
<td>1-12</td>
</tr>
<tr>
<td>第五个“*”</td>
<td>一周当中的星期几</td>
<td>0-7（0和7都代表星期日）</td>
</tr>
</tbody></table>
<p>特殊符号:</p>
<table>
<thead>
<tr>
<th>特殊符号</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>*</td>
<td>代表任何时间。比如第一个“*”就代表一小时中每分钟都执行一次的意思。</td>
</tr>
<tr>
<td>，</td>
<td>代表不连续的时间。比如“0 8,12,16 * * * 命令”，就代表在每天的8点0分，12点0分，16点0分都执行一次命令</td>
</tr>
<tr>
<td>-</td>
<td>代表连续的时间范围。比如“0 5 *   * 1-6命令”，代表在周一到周六的凌晨5点0分执行命令</td>
</tr>
<tr>
<td>*/n</td>
<td>代表每隔多久执行一次。比如“*/10 *   * * * 命令”，代表每隔10分钟就执行一遍命令</td>
</tr>
</tbody></table>
<p>特定时间执行命令:</p>
<ul>
<li><p><code>45 22 * * * 命令</code></p>
<p>在22点45分执行命令</p>
</li>
<li><p><code>0 17 * * 1 命令</code></p>
<p>每周1 的17点0分执行命令</p>
</li>
<li><p><code>0 5 1,15 * * 命令</code></p>
<p>每月1号和15号的凌晨5点0分执行命令</p>
</li>
<li><p><code>40 4 * * 1-5 命令</code></p>
<p>每周一到周五的凌晨4点40分执行命令</p>
</li>
<li><p><code>*/10 4 * * * 命令</code></p>
<p>每天的凌晨4点，每隔10分钟执行一次命令</p>
</li>
<li><p><code>0 0 1,15 * 1 命令</code></p>
<p>每月1号和15号，每周1的0点0分都会执行命令。注意：星期几和几号最好不要同时出现，因为他们定义的都是天。非常容易让管理员混乱。</p>
</li>
</ul>
</li>
</ul>
<h3 id="17、软件包管理"><a href="#17、软件包管理" class="headerlink" title="17、软件包管理"></a>17、软件包管理</h3><h3 id="18、远程连接与拷贝"><a href="#18、远程连接与拷贝" class="headerlink" title="18、远程连接与拷贝"></a>18、远程连接与拷贝</h3><p>通常在工作过程中，公司中使用的真实服务器或者是云服务器，都不允许除运维人员之外的员工直接接触，因此就需要通过远程登录的方式来操作。所以，远程登录工具就是必不可缺的，目前，比较主流的有Xshell, SSH Secure Shell, SecureCRT,FinalShell等，同学们可以根据自己的习惯自行选择</p>
<ul>
<li><p>ssh</p>
<ul>
<li><p>概述</p>
<p>SSH为Secure Shell的缩写，由 IETF 的网络工作小组（Network Working Group）所制定；SSH 为建立在应用层和传输层基础上的安全协议</p>
<p>SSH是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议。常用于远程登录，以及用户之间进行资料拷贝</p>
<p>利用SSH协议可以有效防止远程管理过程中的信息泄露问题。SSH最初是 UNIX 系统上的一个程序，后来又迅速扩展到其他操作平台。SSH 在正确使用时可弥补网络中的漏洞。SSH 客户端适用于多种平台。几乎所有 UNIX 平台—包括 HP-UX、Linux、AIX、Solaris、Digital UNIX、Irix，以及其他平台，都可运行SSH</p>
<p>使用SSH服务，需要安装相应的服务器和客户端。客户端和服务器的关系：如果，A机器想被B机器远程控制，那么，A机器需要安装SSH服务器，B机器需要安装SSH客户端</p>
</li>
<li><p>远程连接</p>
<p>格式：ssh  用户名@IP</p>
<p>示例：ssh  <a href="mailto:&#x72;&#x6f;&#111;&#116;&#64;&#49;&#57;&#x32;&#46;&#49;&#x36;&#x38;&#46;&#48;&#x2e;&#x31;">&#x72;&#x6f;&#111;&#116;&#64;&#49;&#57;&#x32;&#46;&#49;&#x36;&#x38;&#46;&#48;&#x2e;&#x31;</a></p>
<p>注意：第一次连接会出现问题，输入yes后回车，以后ssh不会出现</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200212112656835.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200212112656835"></p>
<p>可能存在的问题：使用ssh访问，如访问出现错误。可查看是否有该文件 ～/.ssh/known_ssh 尝试删除该文件解决</p>
</li>
<li><p>注意</p>
<p>Windows可以通过使用Xshell实现远程连接Linux</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/20200212113130.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="2020-02-12_113130"></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/20200212113220.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="2020-02-12_113220"></p>
</li>
</ul>
</li>
<li><p>scp</p>
<p>作用：远程拷贝文件</p>
<p>本地文件复制到远程：scp  -r  本机文件的绝对路径或者相对路径  目标用户名@目标主机IP地址:目标文件的绝对路径</p>
<p>远程文件复制到本地：scp  -r  目标用户名@目标主机IP地址:目标文件的绝对路径  保存到本机的绝对路径或者相对路径</p>
<p>注意：Windows可以通过winscp实现远程拷贝</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/20200212113738.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="2020-02-12_113738"></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/20200212113751.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="2020-02-12_113751"></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/20200212113807.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="2020-02-12_113807"></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/20200212113832.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="2020-02-12_113832"></p>
</li>
</ul>
<h3 id="19、克隆虚拟机"><a href="#19、克隆虚拟机" class="headerlink" title="19、克隆虚拟机"></a>19、克隆虚拟机</h3><p><strong>创建基础系统(界面)</strong></p>
<ol>
<li><p>创建流程</p>
</li>
<li><p>切换到root用户</p>
</li>
<li><p>配置静态IP</p>
<p><code>vim /etc/sysconfig/network-scripts/ifcfg-ens33</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">TYPE=Ethernet</span><br><span class="line">PROXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=yes</span><br><span class="line">IPV6_AUTOCONF=yes</span><br><span class="line">IPV6_DEFROUTE=yes</span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line">IPV6_ADDR_GEN_MODE=stable-privacy</span><br><span class="line">NAME=ens33</span><br><span class="line">UUID=d901d918-f6af-48cc-845c-dfc0e3e41f71</span><br><span class="line">DEVICE=ens33</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=192.168.100.5</span><br><span class="line">GATEWAY=192.168.100.2</span><br><span class="line">DNS1=192.168.100.2</span><br></pre></td></tr></table></figure></li>
<li><p>修改主机名</p>
<p><code>vim /etc/hostname</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">centos7_base</span><br></pre></td></tr></table></figure></li>
<li><p>关闭防火墙</p>
<p><code>systemctl stop firewalld</code></p>
<p><code>systemctl disable firewalld</code></p>
</li>
<li><p>atguigu用户添加管理员权限</p>
<p><code>vim /etc/sudoers</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">%</span><span class="bash">wheel  ALL=(ALL)       ALL</span></span><br><span class="line">atguigu ALL=(ALL)       ALL</span><br></pre></td></tr></table></figure></li>
<li><p>添加vim配置</p>
<p><code>vim  ~/.vimrc</code>针对当前用户生效</p>
<p><code>vim /etc/vimrc</code>针对所有用户生效</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">syntax on</span><br><span class="line">set nu</span><br><span class="line">set autoindent</span><br><span class="line">set smartindent</span><br><span class="line">set tabstop=4</span><br><span class="line">set shiftwidth=4</span><br><span class="line">set showmatch</span><br><span class="line">set ruler</span><br><span class="line">set cindent</span><br><span class="line">set background=dark</span><br></pre></td></tr></table></figure></li>
<li><p>重启</p>
</li>
<li><p>安装epel-release</p>
<p><code>yum install -y epel-release</code></p>
</li>
</ol>
<p><strong>创建基础系统(最小化)</strong></p>
<ol>
<li><p>创建流程（使用最小化）</p>
</li>
<li><p>root用户登录</p>
</li>
<li><p>配置静态IP</p>
<p><code>vi /etc/sysconfig/network-scripts/ifcfg-ens33</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">TYPE=Ethernet</span><br><span class="line">PROXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=yes</span><br><span class="line">IPV6_AUTOCONF=yes</span><br><span class="line">IPV6_DEFROUTE=yes</span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line">IPV6_ADDR_GEN_MODE=stable-privacy</span><br><span class="line">NAME=ens33</span><br><span class="line">UUID=d901d918-f6af-48cc-845c-dfc0e3e41f71</span><br><span class="line">DEVICE=ens33</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=192.168.100.6</span><br><span class="line">GATEWAY=192.168.100.2</span><br><span class="line">DNS1=192.168.100.2</span><br></pre></td></tr></table></figure></li>
<li><p>修改主机名</p>
<p><code>vi /etc/hostname</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">centos7_mix</span><br></pre></td></tr></table></figure></li>
<li><p>关闭防火墙</p>
<p><code>systemctl stop firewalld</code></p>
<p><code>systemctl disable firewalld</code></p>
</li>
<li><p>atguigu用户添加管理员权限</p>
<p><code>vi /etc/sudoers</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">%</span><span class="bash">wheel  ALL=(ALL)       ALL</span></span><br><span class="line">atguigu ALL=(ALL)       ALL</span><br></pre></td></tr></table></figure></li>
<li><p>重启</p>
<p>reboot</p>
</li>
<li><p>安装epel-release</p>
<p><code>yum install -y epel-release</code></p>
</li>
<li><p>net-tool：工具包集合，包含ifconfig等命令</p>
<p><code>yum install -y net-tools</code></p>
</li>
<li><p>vim：编辑器</p>
<p><code>yum install -y vim</code></p>
</li>
<li><p>添加vim配置</p>
<p><code>vim  ~/.vimrc</code>针对当前用户生效</p>
<p><code>vim /etc/vimrc</code>针对所有用户生效</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">syntax on</span><br><span class="line">set nu</span><br><span class="line">set autoindent</span><br><span class="line">set smartindent</span><br><span class="line">set tabstop=4</span><br><span class="line">set shiftwidth=4</span><br><span class="line">set showmatch</span><br><span class="line">set ruler</span><br><span class="line">set cindent</span><br><span class="line">set background=dark</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>克隆虚拟机</strong></p>
<ol>
<li><code>su - root</code></li>
<li><code>vim /etc/sysconfig/network-scripts/ifcfg-ens33</code></li>
<li><code>vim /etc/hostname</code></li>
<li><code>vim /etc/hosts</code></li>
<li><code>reboot</code></li>
<li><code>C:\Windows\System32\drivers\etc</code></li>
</ol>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
  </article>
  

  
  <nav class="page-nav">
    
    <a class="extend prev" rel="prev" href="/">上一页</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2019-2021
        <i class="ri-heart-fill heart_icon"></i> Anzhen
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src=''></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="anzhen.tech"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/HDFS">HDFS</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Yarn">Yarn</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/MR">MR</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Hive">Hive</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86">数据采集</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/HBase">HBase</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Kafka">Kafka</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Spark">Spark</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Flink">Flink</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/MySQL">MySQL</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Java">Java</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/interview">面试宝典</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2019/11/07/about-me">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.png">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.png">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    <div id="music">
    
    
    
    <iframe frameborder="no" border="1" marginwidth="0" marginheight="0" width="200" height="52"
        src="//music.163.com/outchain/player?type=2&id=318916815&auto=1&height=32"></iframe>
</div>

<style>
    #music {
        position: fixed;
        right: 15px;
        bottom: 0;
        z-index: 998;
    }
</style>
    
    

  </div>
</body>

</html>